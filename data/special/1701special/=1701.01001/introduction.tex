Since the \emph{bootstrap particle filter} was introduced in \cite{gordon:salmond:smith:1993}, \emph{sequential Monte Carlo} (SMC) \emph{methods}, alternatively termed \emph{particle filters}, have been successfully applied within a wide range of applications, including computer vision, automatic control, signal processing, optimisation, robotics, econometrics, and finance; see, e.g.,  \cite{doucet:defreitas:gordon:2001,ristic:arulampalam:gordon:2004} for introductions to the topic. SMC methods approximate a given sequence of distributions by a sequence of possibly weighted empirical measures associated with a sample of \emph{particles} evolving recursively and randomly in time. Each iteration of the SMC algorithm comprises two operations: a \emph{mutation step}, which moves the particles randomly in the state space, and a \emph{selection step}, which duplicates/eliminates, through resampling, particles with high/low importance weights, respectively.  

In parallel with algorithmic developments, the theoretical properties of SMC have been studied extensively during the last twenty years, and there is currently a number of available results describing the convergence, as the number of particles tends to infinity, of Monte Carlo estimates produced by the algorithm; see, e.g., the monographs \cite{delmoral:2004,delmoral:2013} and \cite[Chapter~9]{cappe:moulines:ryden:2005}. The first \emph{central limit theorem} (CLT) for SMC methods was established in \cite{delmoral:guionnet:1999}, and this result was later refined in the series of papers \cite{chopin:2004,kuensch:2005,douc:moulines:2008}. In the mentioned CLT, the asymptotic variance of the weak Gaussian limit is expressed through a recursive formula involving high-dimensional integrals over generally complicated integrands and is hence intractable in general. 

Due to its complexity, only a very few recent works have treated the important---although challenging---topic of variance estimation in SMC algorithms. A breakthrough was made by H.~P. Chan and T.~L.~Lai, who proposed, in \cite{chan:lai:2013} and within the framework of general \emph{state-space models} (or, \emph{hidden Markov models}), an estimator, from now on referred to as the \emph{Chan \& Lai estimator} (CLE), that allows the sequence of asymptotic variances to be estimated on the basis of a \emph{single} realisation of the algorithm, without the need of additional simulations. Remarkably, the CLE can be shown to be consistent (see \cite[Theorem~2]{chan:lai:2013}), i.e., to converge in probability to the true asymptotic variance as the number of particles tends to infinity. At a given time step, the CLE estimates the asymptotic variance by tracing genealogically the time-zero ancestors of the particles at the time step in question. The variance estimators proposed recently in \cite{lee:whiteley:2016} are based on the same principle, and may be viewed as refinements of the CLE within a more general framework of \emph{Feynman-Kac models} and particle algorithms with time varying particle population sizes. Moreover, \cite{lee:whiteley:2016} provides an elegant, deepened  (asymptotic as well as non-asymptotic) theoretical analysis of the technique, and these results are essential for the development of the present paper. 

Appealingly, the set of time-zero ancestors may be updated recursively in the SMC algorithm by adding  just a single line to the code. This allows variance estimates to be computed online with essentially the same computational complexity and memory demands as the original algorithm. Nevertheless, since the SMC algorithm performs repeatedly selection, it is well established that all the particles in the sample will, eventually, share the \emph{same} time-zero ancestor (see, e.g., \cite{jacob:murray:rubenthaler:2015} for a theoretical analysis of this particle path degeneracy phenomenon). Unfortunately, this implies that the CLE collapses eventually to zero as time increases. Increasing the particle sample size or resampling less frequently the particles will postpone somewhat, but not avoid, this collapse. This makes the CLE impractical in the long run. Thus, although the SMC methodology has become a standard tool in statistics and engineering, a numerically stable estimator of the SMC variance has, surprisingly, hitherto been lacking, and the aim of the present paper is to---at least partially---fill this gap.

The natural solution that we propose in the present paper is to estimate the SMC asymptotic variance by tracing only a \emph{part} of the particles' genealogy rather than the full one (i.e., back to the time-zero ancestors). By tracing genealogically only the last generations, depleted ancestor sets are avoided as long as the particle sample size is at least moderately large. Still, this measure leads to a bias, whose size determines completely the success of the approach. Nevertheless, in \cite{douc:moulines:olsson:2014}, which studies the stochastic stability of the sequence of asymptotic variances within the framework of general hidden Markov models, or, viewed differently, randomly perturbed Feynman-Kac models (see \cite[Remark~1]{douc:moulines:olsson:2014}), it is established that the variance, which at time $n$ may be expressed in the form of a sum of $n + 1$ terms, is, in the case where the perturbations form a stationary sequence, uniformly stochastically bounded in time, or, \emph{tight}. Moreover, the analysis provided in \cite{douc:moulines:olsson:2014}, which is driven by mixing assumptions, indicates that the size of the $m^{\mathrm{th}}$ term in the variance at time $n$ decreases geometrically fast with the difference $n - m$. Consequently, we may expect the last terms of the sum to represent the major part of the variance, and as long as the number $\lambda$ of traced generations, the \emph{lag}, and the particle sample size are not too small, the bias should be negligible. This argument is confirmed by our main result, Theorem~\ref{thm:tightness:bias}, which at any time point $n$ provides an order $\rho^\lag$ bound on the asymptotic (as the number of particles tends to infinity) bias, where $\rho \in (0, 1)$ is a mixing rate. Consequently, as long as the number of particles is large enough, the bias stays numerically stable in the long run and may, as it decreases geometrically fast with the lag $\lag$, be controlled efficiently. Methodologically, the estimator that we propose has similarities with the \emph{fixed-lag smoothing} approach studied in \cite{kitagawa:sato:2001,olsson:cappe:douc:moulines:2006,olsson:strojby:2010}, and we here face the same bias-variance tradeoff, in the sense that a too greedy/generous lag design leads to high bias/variance, respectively.  

The developments of the present paper are cast into the framework of randomly perturbed Feynman-Kac models, and the theoretical analysis is driven by the assumptions of \cite{douc:moulines:2012,douc:moulines:olsson:2014} (going back to \cite{douc:fort:moulines:priouret:2009}), which are easily checked for many models used in practice. In particular, by replacing the now classical strong mixing assumption on the transition kernel of the underlying Markov chain---a standard assumption in the literature that typically requires the state space of the Markov chain to be a compact set---by a \emph{local Doeblin condition}, we are able to verify the assumptions for a wide class of models with possibly non-compact state space. 

As a numerical illustration, we apply our estimator in the context of SMC-based predictor flow approximation in general state-space models, including the widely used \emph{stochastic volatility} model proposed in \cite{hull:white:1987}. We are able to report an efficient, numerically stable performance of our variance estimator, with tight control of the bias at low variance. 

The paper is structured as follows. Section~\ref{sec:preliminaries} introduces some notation, defines the framework of perturbed Feynman-Kac models, and provides some background to SMC. In Section~\ref{sec:estimator}, focus is set on asymptotic variance estimation and after a prefatory discussion on the CLE we introduce the proposed fixed-lag variance estimator. We also describe how our estimator can be straightforwardly extended to so-called \emph{updated} Feynman-Kac distribution flows. Theoretical and numerical results are found in Section~\ref{sec:theoretical:results} and Section~\ref{sec:numerical:study}, respectively, and Appendix~\ref{sec:proofs} contains all proofs. Our theoretical analysis is divided into two parts: first, the identification of the limiting bias and, second, the construction of a tight upper bound on the same. The first part relies on the theoretical machinery developed in \cite{lee:whiteley:2016}, whose key elements are recalled briefly in the beginning of Section~\ref{sec:proof:consistency:fixed:lag}. Finally, Section~\ref{sec:conclusion} concludes the paper. 















 