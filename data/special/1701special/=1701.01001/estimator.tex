\subsection{The variance estimator proposed in \cite{chan:lai:2013}}
\label{sec:the:Lai:estimator}
Since Algorithm~\ref{alg:SMC} resamples the particles at each time step, the particle cloud may be associated with a tree describing the genealogical lineages of the particles. The estimators proposed in \cite{chan:lai:2013} and \cite{lee:whiteley:2016} are based on the particles' \emph{Eve indices} $\{ \eve{n}{i} \}_{i = 1}^\N$ (the terminology is adopted from \cite{lee:whiteley:2016}), which are, for all $n \in \nset$, defined as the indices of the time-zero ancestors of the particles $\{ \epart{n}{i} \}_{i = 1}^\N$. More specifically, the Eve indices may, for all $i \in \intvect{1}{\N}$, be computed recursively in Algorithm~\ref{alg:SMC} (just after Line~6) by letting 
$$
    \eve{n}{i} \eqdef
    \begin{cases}
        i & \mbox{for } n = 0, \\
        \eve{n - 1}{\ind{n}{i}} & \mbox{for } n \in \nsetpos. 
    \end{cases}
$$
Using the Eve indices, H.~P. Chan and T.~L.~Lai proposed, in \cite{chan:lai:2013}, for all $n \in \nset$, $\varest[2]{\chunk{z}{0}{n - 1}}(h)$, with
\begin{equation} \label{eq:Lai:estimator}
    \varest[2]{\chunk{z}{0}{n - 1}} : \bmf{\Xfd} \ni h \mapsto \frac{1}{\N} \sum_{i = 1}^\N \left( \sum_{j : \eve{n}{j} = i} \left\{ h(\epart{n}{j}) - \predpart[\chunk{z}{0}{n - 1}] h \right\} \right)^2,
\end{equation}
as an estimator of $\variance[2]{\chunk{z}{0}{n - 1}}(h)$ for all $h \in \bmf{\Xfd}$. As mentioned in the introduction, we will refer to this estimator as the CLE. (More precisely, in \cite{chan:lai:2013}, focus was set on the \emph{updated} distribution flows discussed in Section~\ref{sec:updated:measures} below; the adaptation is however straightforward.) In \cite{lee:whiteley:2016}, a generalisation of the CLE, allowing the particle population size $\N$ to vary between SMC iterations, is presented. As the main result of \cite{chan:lai:2013}, the consistency, as $\N$ tends to infinity, of the CLE is established; see also \cite[Theorem~1 and Corollary~1]{lee:whiteley:2016} for a generalisation. 

The CLE is indeed remarkable, as it allows the variance to be estimated online in a single run of the particle filter with no further simulation. Nevertheless, as explained in the introduction, the previous estimator has a serious flaw which is related to the well-known \emph{particle path depletion phenomenon} of SMC algorithms. More specifically, resampling the particles systematically at each time  leads without exception to a random time point before which all the genealogical traces coincide; we refer again to \cite{jacob:murray:rubenthaler:2015}, which provides a time uniform $\ordo(\N \log \N)$ bound on the expected number of generations back in time to this most recent common ancestor. Thus, as $n$ increases, the sets $\{ j \in \intvect{1}{\N} : \eve{n}{j} = i \}$ will eventually be empty for all indices $i \in \intvect{1}{\N}$ except one, say, $i_0$, for which $\{ j \in \intvect{1}{\N} : \eve{n}{j} = i_0 \} = \intvect{1}{\N}$. As a consequence, eventually, $\varest[2]{\chunk{z}{0}{n - 1}}(h) = 0$ for all $h \in \bmf{\Xfd}$, which makes the estimator impractical. In the next section, we propose a simple modification of the CLE that stabilises numerically the same at the cost of a negligible, controllable bias. 

\subsection{Our estimator}
\label{sec:our:estimator}

The estimator that we propose is based on the simple idea of stabilising numerically the CLE by tracing, backwards in time, only a few generations of the particle genealogy, rather than tracing the history all the way back to the time-zero ancestors. In our approach, the Eve indices will be replaced by
\emph{Enoch indices}\footnote{Two figures named Enoch appear in the 2nd as well as the 6th generations of the Genealogies of Genesis, as the son of Cain and the son-son-son-son-son of Seth, respectively.} defined, for all $i \in \intvect{1}{\N}$ and $m \in \nset$, recursively as  
\begin{equation} \label{eq:def:Enoch}
\enoch{m}{n}{i} \eqdef 
\begin{cases}
i & \mbox{for } n = m, \\
\enoch{m}{n - 1}{\ind{n}{i}}  & \mbox{for } n > m.  
\end{cases}
\end{equation}
In other words, for all $n \in \nset$, $m \in \intvect{1}{n}$, and $i \in \intvect{1}{\N}$, $\epart{m}{\enoch{m}{n}{i}}$ is the ancestor of $\epart{n}{i}$ at time $m$. Now, let $\lag \in \nset$ be some fixed number, referred to as the lag, and define $\lagtime{n}{\lag} \eqdef (n - \lambda) \vee 0$; then, we propose $\varest[2]{\chunk{z}{0}{n - 1}}[\lambda](h)$, with  
\begin{equation} \label{eq:estimator}
\varest[2]{\chunk{z}{0}{n - 1}}[\lambda] : \bmf{\Xfd} \ni h \mapsto \frac{1}{\N} \sum_{i = 1}^\N \left( \sum_{j : \enoch{\lagtime{n}{\lambda}}{n}{j} = i} \left\{ h(\epart{n}{j}) - \predpart[\chunk{z}{0}{n - 1}] h \right\} \right)^2,
\end{equation}
as an estimator of the variance $\variance[2]{\chunk{z}{0}{n - 1}}(h)$ for all $n \in \nset$, $\chunk{z}{0}{n - 1} \in \Zsp^n$, and $h \in \bmf{\Xfd}$. 
Online computation of the Enoch indices $\{ \enoch{\lagtime{n}{\lambda}}{n}{i} \}_{i =Ê1}^\N$ requires the propagation of a window $\{Ê\enoch{\lagtime{n}{\lag}}{n}{i}, \ldots,  \enoch{n}{n}{i} \}_{i = 1}^\N$ of indices; see Algorithm~\ref{alg:fixed-lag:SMC} for a pseudo-code.  As the length of the window is bounded by $\lag + 1$, the memory demand of the estimator is $\ordo(\lag \N)$ independently of $n$. Moreover, since genealogical tracing has a linear complexity in $\N$, the total complexity of the estimator is $\ordo(\lag \N)$, again independently of $n$. 

\bigskip
\begin{algorithm}[H] \label{alg:fixed-lag:SMC}
    \KwData{$\{ \epart{n}{i} \}_{i = 1}^\N$, $\{Ê\enoch{\lagtime{n}{\lag}}{n}{i}, \ldots,  \enoch{n}{n}{i} \}_{i = 1}^\N$, $z_n$}
    \KwResult{$\{ \epart{n + 1}{i} \}_{i = 1}^\N$, $\{Ê\enoch{\lagtime{(n + 1)}{\lag}}{n + 1}{i}, \ldots,  \enoch{n + 1}{n + 1}{i} \}_{i = 1}^\N$}
    set $\wgtsum{n} \gets 0$\;
    \For{$i = 1 \to \N$}{
        set $\wgt{n}{i} \gets \pot[z_n](\epart{n}{i})$\;
        set $\wgtsum{n} \gets \wgtsum{n} + \wgt{n}{i}$\;
    }
    \For {$i = 1 \to \N$}{
        draw $\ind{n + 1}{i} \sim \cat(\{ \wgt{n}{\ell} / \wgtsum{n} \}_{\ell = 1}^N)$\;
        draw $\epart{n + 1}{i} \sim \mk(\epart{n}{\ind{n + 1}{i}}, \cdot)$\;
        \For{$m = \lagtime{(n + 1)}{\lambda} \to n$}{
            set $\enoch{m}{n + 1}{i} \gets \enoch{m}{n}{\ind{n + 1}{i}}$\;
        }
        set $\enoch{n + 1}{n + 1}{i} \gets i$\;
    }
    \caption{SMC particle and Enoch-index update}
\end{algorithm}
\bigskip

For $n = 0$, Algorithm~\ref{alg:fixed-lag:SMC} is initialised by drawing $\{ \epart{0}{i} \}_{i = 1}^\N \sim \init^{\varotimes \N}$ and setting $\enoch{0}{0}{i} \gets i$ for all $i \in \intvect{1}{\N}$. At the end of the algorithm, after the second \textbf{for}-loop, an estimate 
$$
    \varest[2]{\chunk{z}{0}{n}}[\lambda](h) = \frac{1}{\N} \sum_{i = 1}^\N \left( \sum_{j : \enoch{\lagtime{(n + 1)}{\lambda}}{n + 1}{j} = i} \{ h(\epart{n + 1}{j}) - \predpart[\chunk{z}{0}{n}] h \} \right)^2
$$
of $\variance[2]{\chunk{z}{0}{n}}[\lambda](h)$ may be formed for all $h \in \bmf{\Xfd}$. 

\subsection{Variance estimators for flows of updated distributions}
\label{sec:updated:measures}

Some applications involve approximation of the \emph{updated} measures 
\begin{equation} \label{eq:def:filt}
    \filt[\chunk{z}{k}{m}] : \Xfd \ni A 
    \mapsto \frac{\init \uk[\chunk{z}{k}{m - 1}] (\pot[z_m] \1_A)}
    {\init \uk[\chunk{z}{k}{m - 1}] (\pot[z_m] \1_\Xsp)},
\end{equation}
for $\chunk{z}{k}{m} \in \Zsp^{m - k + 1}$, rather than the measures defined by \eqref{eq:def:pred}.   
\begin{example}[partially dominated state-space models, revisited]
In the case of the partially dominated state-space models discussed in Example~\ref{example:state:space:model}, the updated measures $\{ \filt[\chunk{y}{0}{n}] \}_{n \in \nset}$ defined through \eqref{eq:def:filt} are the \emph{filter distributions}; more precisely, in this context, for all $n \in \nset$, $\filt[\chunk{y}{0}{n}]$ is the conditional distribution of the state $X_n$ given the realised observations $\chunk{y}{0}{n} \in \Ysp^{n + 1}$ up to time $n$ (i.e., \emph{including} the last observation $y_n$). 
\end{example}
Since for all $h \in \bmf{\Xfd}$, by normalisation, 
$$
    \filt[\chunk{z}{k}{m}] h = \frac{\pred[\chunk{z}{k}{m - 1}](\pot[z_m] h)}{\pred[\chunk{z}{k}{m - 1}] \pot[z_m]},
$$
the flow $\{ \filt[\chunk{z}{0}{n}] \}_{n \in \nset}$ of updated distributions is naturally approximated by the flow of weighted empirical measures 
\begin{equation} \label{eq:def:particle:filter}
    \filtpart[\chunk{z}{0}{n}] : A \ni \Xfd \mapsto \frac{\predpart[\chunk{z}{k}{m - 1}](\pot[z_m] \1_A)}{\predpart[\chunk{z}{k}{m - 1}] \pot[z_m]} = \sum_{i = 1}^\N \frac{\wgt{n}{i}}{\wgtsum{n}} \1_A(\epart{n}{i}),
\end{equation}
for some given sequence $\{ z_n \}_{n \in \nset}$ of perturbations, where the weights $\{ \wgt{n}{i} \}_{i = 1}^\N$ and the weight sum $\wgtsum{n}$ are computed in Algorithm~\ref{alg:SMC}. By the normality \eqref{eq:CLT} and the consistency \eqref{eq:particle:filter:consistency} one obtains, using Slutsky's theorem, for all $\chunk{z}{0}{n} \in \Zsp^{n + 1}$, the central limit theorem 
\begin{equation} \label{eq:CLT:updated:measures}
    \sqrt{\N} \left( \filt[\chunk{z}{0}{n}] h - \filt[\chunk{z}{0}{n}] h \right) \dlim \filtvariance{\chunk{z}{0}{n}}(h) Z,  
\end{equation}
as $\N$ tends to infinity, where $Z$ is standard normally distributed and the asymptotic variance is given by $\filtvariance[2]{\chunk{z}{0}{n}}(h) = \filtvariance[2]{\chunk{z}{0}{n}}[0](h)$ with 
\begin{equation} \label{eq:def:as:var:updated:measures}
    \filtvariance[2]{\chunk{z}{0}{n}}[\ell] : \bmf{\Xfd} \ni h \mapsto \frac{\variance[2]{\chunk{z}{0}{n - 1}}[\ell](\pot[z_n] \{ h - \filt[\chunk{z}{0}{n}] h \})}{(\pred[\chunk{z}{0}{n - 1}] \pot[z_n])^2}
\end{equation}
(where $\variance{\chunk{z}{0}{n - 1}}[\ell]$ is defined in \eqref{eq:def:as:var} for the original Feynman-Kac particle model). In the case $\ell = 0$, the expression \eqref{eq:def:as:var:updated:measures} is found also in \cite[Eqn.~(17)]{douc:moulines:olsson:2014}. In the light of \eqref{eq:def:as:var:updated:measures}, casting our fixed-lag approach into the framework of updated Feynman-Kac models yields the estimator 
\begin{multline} \label{eq:def:var:est:updated:measures}
    \varestfilt[2]{\chunk{z}{0}{n}}[\lambda] : \bmf{\Xfd} \ni h \mapsto \frac{\varest[2]{\chunk{z}{0}{n - 1}}[\lambda](\pot[z_n] \{h - \filtpart[\chunk{z}{0}{n}] h\})}{(\predpart[\chunk{z}{0}{n - 1}] \pot[z_n])^2} \\ 
    = \N \sum_{i = 1}^\N \left( \sum_{j : \enoch{\lagtime{n}{\lambda}}{n}{j} = i} \frac{\wgt{n}{i}}{\wgtsum{n}} \left\{ h(\epart{n}{j}) - \filtpart[\chunk{z}{0}{n}] h \right\} \right)^2
\end{multline}
for some suitable lag $\lag \in \nset$ (where the equality stems from the fact that $\predpart[\chunk{z}{0}{n - 1}] \{Ê\pot[z_n](h -  \filtpart[\chunk{z}{0}{n}] h) \} = 0$).  





