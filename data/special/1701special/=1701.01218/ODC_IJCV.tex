%f%%%%%%%%%%%%%%%%%%%%%%% file template.tex %%%%%%%%%%%%%%%%%%%%%%%%%
%
% This is a general template file for the LaTeX package SVJour3
% for Springer journals.          Springer Heidelberg 2010/09/16
%
% Copy it to a new file with a new name and use it as the basis
% for your article. Delete % signs as needed.
%
% This template includes a few options for different layouts and
% content for various journals. Please consult a previous issue of
% your journal as needed.
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% First comes an example EPS file -- just ignore it and
% proceed on the \documentclass line
% your LaTeX will extract the file if required
\begin{filecontents*}{example.eps}
%!PS-Adobe-3.0 EPSF-3.0
%%BoundingBox: 19 19 221 221
%%CreationDate: Mon Sep 29 1997
%%Creator: programmed by hand (JK)
%%EndComments
gsave
newpath
  20 20 moveto
  20 220 lineto
  220 220 lineto
  220 20 lineto
closepath
2 setlinewidth
gsave
  .4 setgray fill
grestore
stroke
grestore
\end{filecontents*}
%
\RequirePackage{fix-cm}
%
%\documentclass{svjour3}                     % onecolumn (standard format)
%\documentclass[smallcondensed]{svjour3}     % onecolumn (ditto)
%\documentclass[smallextended,final]{svjour3}       % onecolumn (second format)
\documentclass[twocolumn]{svjour3}          % twocolumn
\usepackage[toc,page]{appendix}
%
\smartqed  % flush right qed marks, e.g. at end of proof
%
\let\proof\relax 
\let\endproof\relax
\usepackage{times}
%\usepackage{natbib}
\usepackage{helvet}
\usepackage{courier}
\usepackage{comment}
\usepackage{subcaption}
\captionsetup{compatibility=false}
\usepackage{wrapfig}
\usepackage{float}
%\usepackage{subcaption}
\usepackage{times}

\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{wrapfig}
\usepackage{amssymb}
\usepackage{comment}
%\usepackage{caption}
\usepackage[font=small,labelfont=bf]{caption}
\usepackage{enumi tem}
\newcommand{\ignore}[1]{}
\usepackage[noend]{algorithm2e}
\usepackage{algorithmicx,algpseudocode}
\usepackage{booktabs,tabularx}%
\usepackage{multirow} 
\usepackage{amsthm}
\theoremstyle{plain}
\linespread{1.0}
\theoremstyle{definition}
\newtheorem{thm}{Theorem}[section]
\newtheorem{lem}[thm]{Lemma}
\newtheorem{prop}[thm]{Proposition}
\newtheorem*{cor}{Corollary}
\theoremstyle{definition}
\newtheorem{defn}{Definition}[section]
\newtheorem{conj}{Conjecture}[section]
\newtheorem{exmp}{Example}[section]
\theoremstyle{remark}
\newtheorem*{rem}{Remark}
%\graphicspath{{../BMVC15_camera_ready/}{../BMVC15_camera_ready/images/}{../BMVC15_camera_ready/supp2015_camera_ready/}}
%\usepackage{graphicx}
%\usepackage{amsmath,amssymb} % define this before the line numbering.
%\usepackage{ruler}
%\usepackage{color}
%\usepackage{expl3}
%\renewcommand\baselinestretch{0.98}
%\usepackage[width=122mm,left=12mm,paperwidth=146mm,height=193mm,top=12mm,paperheight=217mm]{geometry}

\def\ignore#1{}

%\ExplSyntaxOn
%\newcommand\latinabbrev[1]{
%  \peek_meaning:NTF . {% Same as \@ifnextchar
%    #1\@}%
%  { \peek_catcode:NTF a {% Check whether next char has same catcode as \'a, i.e., is a letter
 %     #1.\@ }%
%    {#1.\@}}}
%\ExplSyntaxOff

\def\onedot{.}
\def\eg{\emph{e.g}\onedot~} \def\Eg{\emph{E.g}\onedot}
\def\ie{\emph{i.e}\onedot~} \def\Ie{\emph{I.e}\onedot}
\def\cf{\emph{c.f}\onedot} \def\Cf{\emph{C.f}\onedot}
\def\etc{\emph{etc}\onedot} \def\vs{\emph{vs}\onedot}
\def\wrt{w.r.t\onedot} \def\dof{d.o.f\onedot}
\def\etal{\emph{et al}~}

\let\oldemptyset\emptyset
\let\emptyset\varnothing

%\AtBeginDocument{%
%  \setlength{\oddsidemargin}{\dimexpr(\paperwidth-\textwidth)/2-1in}%
%  \setlength{\evensidemargin}{\oddsidemargin}%
%  \setlength{\topmargin}{%
%    \dimexpr(\paperheight-\textheight)/2-\headheight-\headsep-1in}%
%}
%
% \usepackage{mathptmx}      % use Times fonts if available on your TeX system
%
% insert here the call for the packages your document requires
%\usepackage{latexsym}
% etc.
%
% please place your own definitions here and don't use \def but
% \newcommand{}{}
%
% Insert the name of "your journal" with
% \journalname{myjournal}
%

\begin{document}

%\title{Overlapping Domain Cover for  Scalable  and Accurate Regression Kernel Machines }


\title{Overlapping Cover Local Regression Machines }

%\thanks{Grants or other notes
%about the article that should go on the front page should be
%placed here. General acknowledgments should be placed at the end of the article.}

%\subtitle{from Pure Text or weak Attributes}

\author{Mohamed Elhoseiny    \and
        Ahmed Elgammal     %etc.
}

\authorrunning{M Elhoseiny et al.}
%\authorrunning{Short form of author list} % if too long for running head

\institute{Mohamed Elhoseiny$^{1}$ and Ahmed Elgammal$^{2}$ \at
	       $^{1}$Facebook AI Research\\
	       $^{2}$Department of Computer Science, Rutgers University\\
             % 110 Frelinghuysen Road, Piscataway, NJ 08854-8019, USA \\
              %USA\\
              %Tel.: +1-732-208-9712\\	
              \email{elhoseiny@fb.com, elgammal@cs.rutgers.edu}           %  \\
%             \emph{Present address:} of F. Author  %  if needed
       %    \and
        %   Ahmed Elgammal \at
         %  110 Frelinghuysen Road, \\ 
         %  Piscataway, NJ 08854-8019 \\
         %  USA\\
         %  \email{elgammal@cs.rutgers.edu}          
}


\date{Received: date / Accepted: date}
% The correct dates will be entered by the editor


\maketitle


\input{abstract_v2}
\section{Introduction}
\label{sec:1}
\input{intro2_v2}

\section{Background on Full GPR and TGP Models}
\label{sec:2}
\input{relatedwork_v2}


\section{Related Work on Approximation  Methods}
\label{sec:relappmethod}
\input{connRelwork_v2}

%------------------------------------------------------------------------- 
\section{ODC Framework}%
\label{sec:3}
\input{DDPrediction_v2}


\subsection{Training}
\label{sec:4}
\input{training2_v2}


\subsection{Prediction}
\label{sec:pred}
\input{prediction_v2}

\section{Experimental Results}
\label{sec:6}
\input{experiments_v2}

\section{Conclusion}
\label{sec:7}
\input{conclusion_v2}

%\begin{abstract}
%\input{abstract}
%\keywords{Text Visualization \and Multi-level MindMap Automation}
%\end{abstract}



%%%%%%%%% BODY TEXT

%\section{Kernel Conclusion}
%\input{Kconclusion}

%\bibliographystyle{ACM-Reference-Format-Journals}



%\begin{acknowledgements}
%If you'd like to thank anyone, place your comments here
%and remove the percent signs.
%\end{acknowledgements}

% BibTeX users please use one of
%\bibliographystyle{spbasic}      % basic style, author-year citations
\bibliographystyle{spmpsci}      % mathematics and
%\bibliography{acmsmall-sample-bibfile}
%\bibliography{write_a_classifier}
\bibliography{egbib.bib}

\begin{appendices}


\section{IWTGP-ODC Experiments}
Tables ~\ref{tab:poserw} and ~\ref{tab:hevaresw} details the results of IWTGP-ODC experiments on Poser and HumanEva datasets in terms of error and speedup in prediction time.


\begin{table}[htbp]
  \centering
    \scalebox{0.8}
  {
     \begin{tabular}{|c|cc|}
      \hline
    \multirow{2}[4]{*}{\textbf{}} & \textbf{IWTTGP} & \multicolumn{1}{c|}{\textbf{IWTGP-ODC}} \\
     	& ($M = 800, M_{tst} = 418$)  & ($M = 800, M_{tst} = 418$)	\\ \hline
    \textbf{error (deg)} & 6.1 &  5.32 \\
    \textbf{err reduction (deg)} & -     &  \textbf{0.783 }\\
    \textbf{err reduction \%} & -     & \textbf{12.836\%} \\
    \textbf{Prediction Time (sec)} & 360.0  & 26.61 \\
    \textbf{speedUp} & -      & \textbf{13.5} \\ \hline
    \end{tabular}%     
    }
  \caption{POSER dataset IWTGP-NN vs  IWTGP-ODC}
 \label{tab:poserw}%
\end{table}%

\begin{table}[htbp]
  \centering
  \scalebox{0.9}
  {
\begin{tabular}{|c|cc|}
\hline
    \multirow{2}[4]{*}{\textbf{}} & \textbf{IWTGP} & \multicolumn{1}{c|}{\textbf{IWTTGP-ODC}} \\
    & ($M = M_{tst} = 800$)& ($M=M_{tst}=800$)\\  \hline
          & \textbf{}  & \textbf{ } \\
    \textbf{error (mm)} & 39.1  & 39.3 \\
    \textbf{err reduction (mm)} & -     & \textbf{\textbf{-0.2}} \\
    \textbf{err reduction \%} & -   & \textbf{\textbf{-0.512\%}} \\
    \textbf{Prediction Time (sec)} & 7938.15 &  569.66 \\
    \textbf{speedUp} & -     &  \textbf{13.92}\\ \hline
    \end{tabular}%
    }
 \caption{Humen Eva dataset: IWTGPKNN vs IWTGP-ODC}
  \label{tab:hevaresw}
\end{table}% 




%\section{More Figures and Results}
%figure ~\ref{fig:ODCAnalysis400} shows our analysis on ODC for M =400. 

\ignore{
\begin{figure}[h!]
\centering
\begin{tabular}{c}
\bmvaHangBox{{  \includegraphics[width=1.0\textwidth,height=0.34\textwidth]{figTGP400HEva2.eps}}}\\
(a) TGP-ODC (M=400) \\
\bmvaHangBox{{  \includegraphics[width=1.0\textwidth,height=0.34\textwidth]{figGPR400HEva2.eps}}}\\
(a) GPR-ODC (M=400) \\
\end{tabular}
\vspace{2mm}
\caption{Overlapping Domain Cover Parameter Analysis of GPR and TGP  on Human Eva Dataset (best seen in color) (M=400)}
\label{fig:ODCAnalysis400}
%\label{fig:teaser}
\end{figure}}


\begin{comment}
\begin{table*}[htbp!]
  \centering
      \vspace{4pt}
  \caption{Error and Time for Poser and Human Eva datasets (on 2.6GHZ intel core i7), M = 800}
    \vspace{3pt}
    \scalebox{0.8}{
    \begin{tabular}{|l|l|lll|lll|}%ccc}
    \toprule
          &   & \textbf{Poser} & \textbf{} &       & \textbf{HumanEva} &       &      \\% & \textbf{Human 3.6} &       &  \\
    \midrule
         & & \textbf{Error (deg)} & \textbf{Training Time} & \textbf{Prediction Time} & \textbf{Error (mm) } & \textbf{Training Time} & \textbf{Prediction Time} \\%& \textbf{Error} & \textbf{Training Time} & \textbf{Prediction Time} \\
 \textbf{TGP}   & \textbf{NN} & 5.43       &  -     &    188.99 sec   & \textbf{38.1}      &  -     & 6364 sec \\%       &       &       &  \\
  & \textbf{ODC ($p= 0.9, t=1, K'=1$)-Ekmeans} & \textbf{5.4 }     &      (3.7 +25.1 ) sec  &   \textbf{16.5}  sec  &    \textbf{38.99 }  &  (2001 + 45.4) sec    &  \textbf{298} sec\\%       &       &       &  \\
    & \textbf{ODC ($p= 0.9, t=1, K'=2$)-Ekmeans} & {5.53}     &      (3.7 +29.4 ) sec  &   47.04  sec  &    {39.2 }  &  (2001 + 45.24) sec    &  569.6946  sec\\%       &       &       &  \\
      & \textbf{ODC ($p= 0.9, t=1, K'=3$)-Ekmeans} & {5.4}     &      (3.7 +28.8 ) sec  &   71.4 sec  &    {40.9 }  &  (2001 +  45.7) sec    &  721.0 sec\\%       &       &       &  \\
    & \textbf{ODC ($p= 0, t=1, K'=1$)-Ekmeans} &   7.6    &    (3.9 + 1.33) sec   &   14.8 sec  &    41.87   & (240 + 4.9832 ) sec       &  256.7 \\%      &       &       &  \\
       & \textbf{ODC ($p= 0, t=1, K'=2$)-Ekmeans} &   12.3    &    (3.9 + 2.69) sec   &   42.25 sec  &    136.52 & (240 + 4.7790  ) sec       &  514.93 \\%      &       &       &  \\
       & \textbf{ODC ($p= 0, t=1, K'=3$)-Ekmeans} &   12.52    &    (3.9 + 1.86) sec   &    72.38 sec  &    187.72   & (240 + 4.75 ) sec       &  771 \\%      &       &       &  \\
      &  \textbf{ODC ($p= 0.9, t=1, K'=1$)-RPC} & 5.6      &      (0.23 +41.6 ) sec  &   15.8 sec  &   39.9  &    ( 0.45 + 49.05) sec     & 277.25 sec\\%       &       &       &  \\
         &  \textbf{ODC ($p= 0.9, t=1, K'=2$)-RPC} & 5.52      &      (0.23 +43.80 ) sec  &   43.802 sec  &   40.41  &    ( 0.45 + 46.77) sec     & 677.52 sec\\%       &       &       &  \\
        &  \textbf{ODC ($p= 0.9, t=1, K'=3$)-RPC} & 5.59  &      (0.23 +43.05 ) sec  &   67.11 sec  &    41.21&    ( 0.45 + 47.63) sec     &  883 sec\\%       &       &       &  \\
  &  \textbf{ODC ($p= 0, t=1, K'=1$)-RPC} &   7.7    &   (0.15 + 1.7) sec   &   13.89 sec  &  42.32    &  (0.19 + 5.3)      sec &  241.64 sec\\%      &       &       &  \\
    &  \textbf{ODC ($p= 0, t=1, K'=2$)-RPC} &   9.29 &   (0.15 + 1.8) sec   &   41.86 sec  &  58.99    &  (0.19 + 5.16)      sec &  475.14 sec\\%      &       &       &  \\
      &  \textbf{ODC ($p= 0, t=1, K'=3$)-RPC} &   12.47    &   (0.15 + 1.80) sec   &   66.42 sec  &  136    &  (0.19 + 5.2)      sec &  721.49 sec\\%      &       &       &  \\
      \hline
  \textbf{GPR}  & \textbf{NN} & 6.77      &   -    &  24 sec     &   54.8    &      - &    618  sec \\%&       &       &  \\
   & \textbf{ODC ($p= 0.9, t=1 , K'=1$)-Ekmeans} &  \textbf{6.27}      &  (3.7 +11.1 ) sec  &       \textbf{0.56}  sec & \textbf{49.3}  &   (2001 + 42.85)sec & \textbf{78.85} sec \\%       &       &       &  \\
   & \textbf{ODC($p= 0.0, t=1 , K'=1$)-Ekmeans} & 7.54      &   ( 3.9 + 1.38 sec) &    0.35 sec   & 49.6  &  (240 + 6.4) sec  &  48.1 sec\\%      &       &       &  \\
  & \textbf{ODC ($p= 0.9, t=1 , K'=1$)-RPC} &  6.45      &  (0.23 +17.3 ) sec  &       0.52  sec & 52.8  & (0.49 + 46.06) sec     &  64.13 sec\\%       &       &       &  \\
    & \textbf{ODC ($p= 0.0, t=1 , K'=1$)-RPC  = ~\cite{Chalupka:2013}} &   7.46    &   (0.15 + 1.47) sec &    0.27 sec   & 54.6  &  (0.261 + 4.58 ) sec & 43.52 sec\\%      &       &       &  \\
    & \textbf{FITC ~\cite{fic06}} &   7.63 (+/- 0.4)  &   (- + 20.63)   &    0.3106     &   68.36(+/- 0.84)    &  -     & 101.5442 (+/- 1.36) sec\\%      &       &       &  \\
    \bottomrule
    \end{tabular}}%
  \label{tab:tblRes_detailed}%
\end{table*}%
\end{comment}


\section{More figures on AB Ekmeans}





 Figure~\ref{fig:ekmeans} shows the clustering performance on 300000 random 2D point (K=5). Figure  ~\ref{fig:hevaVis3} shows the clustering output of our algorithm visualized on using the first three principal components of Human Eva training hog features. The figures shows that the cluster are spatially cohesive but not necessarily circular. This makes the elliptic distribution of the data captured by Mode 3 gives more accuracy membership measure me to the subdomains. %\ignore{That's why Mode 3 gives a better accuracy than Mod 1. It also gives an accuracy better than Mode 2. A problem in Mode 2 is how to choose K so that it relects the correct associativity of the test point to the closest subdomans.}
 
 \ignore{\begin{figure}[h!]
\begin{tabular}{ccc}
\bmvaHangBox{\fbox{\includegraphics[width=4.6cm]{Ekmeans5.png}}}&
\bmvaHangBox{\fbox{\includegraphics[width=5.2cm]{Ekmeans57.png}}}\\
(a)&(b)
\end{tabular}
\caption{Applying our Assign and Balance variant of Kmeans on 300,000 random 2D points:  5 clusters}
\label{fig:ekmeans}
%\label{fig:teaser}
\end{figure}}
 

 \begin{figure}[h!]
 \centering
\begin{subfigure}[b]{0.5\textwidth}
\includegraphics[width=6.6cm]{Ekmeans5.png}
  \caption{5 clusters}
\end{subfigure}
%\begin{subfigure}[b]{0.5\textwidth}
%\includegraphics[width=5.2cm]{Ekmeans57.png}
  %\caption{57 clusters}
%\end{subfigure}
\caption{Applying our Assign and Balance variant of Kmeans on 300,000 random 2D points}
\label{fig:ekmeans}
%\label{fig:teaser}
\end{figure}
\begin{figure}[h!]
\centering
 \includegraphics[width=0.5\textwidth]{HumanEVAPCAVis.jpg}
\caption{Human Eva clustering first three Pricipal Components }
\label{fig:hevaVis3}
\end{figure}


%\subsection{Equal Size Assignment Algorithms Pseudocode}
%As presented in the pape, our k-means variant algorithms modifies only the assignment step of  the standard k-means algorithm. Algorithm ~\ref{alg:ddclusterALg2} and ~\ref{alg:ddclusterALg1} shows the pseudo-code of the assignments steps on IMDA-k-means and AB-k-means algorithms respectively. We attach the MATLAB implementation of both algorithms in "Ekmeans-assign" folder, We plan to release the whole implementation of our paper as well as soon as we well-document of the code. 




\section{Overlapping Domain Cover(ODC) Generation-Algorithm}

Algorithm ~\ref{alg:sdgen} shows how the overlapping sub-domains are generated form the the equal size clusters from the closest $r$ clusters. \ignore{ if the retrieved nearest neighbor points  belongs to more than $^OC_C$ clusters.}
\begin{algorithm}
{\textbf{Input:} Clusters ${\{C_k\}}_{k=1}^{K} $}
\KwOut{Overlapping subdomains ${\{D_k\}}_{k=1}^{K}$}
\ForEach{Cluster $C_k$}{
Compute the closest $r$ clusters ${\{{{C^{'}}_i}\}}_{i=1}^{r}$ based on $DK_i = \| \mu_k- \mu_i \|$ , $i\neq k$\\
Let $LK_i = 1/DK_i,  {WK_i} =  \frac{LK_i}{\sum_{l=1}^{^OC_C} LK_l}$  ${i=1 : r}$\\
Let ${NPK_i} =  floor(WK_i * OPC)$, ${i=1 : r}$ \\
Let $ExKPts = (1-p) M - \sum_{l=1}^{r} NPK_l$ \\
Let ${NPK_i}$ =  $NPK_i +1$ , $ i=1 : ExKPts $\\
$D_k =  C_k$ \\
Let $overflow = 0$\\
  \Comment{The following for loop goes over the $r$ clusters on an increasing order of $DK_i$ }\\
\For{i=1 : $r$} {
  \If{${NPK_i}$> $|C_i|$} { $overflow = overflow+ {NPK_i} -|C_i|$ \\  $NPK_i = |C_i|$   } 
  \If{${NPK_i}$< $|C_i|$} { $G_i =min(overflow, |C_i| -NPK_i$ ) \\  $NPK_i = NPK_i +G_i $ \\  $overflow =overflow-G_i$   } 
  %\ELSE{} {}\\
 $Ps_i = KNN({OVC_K}_j,NPK_i )$ 
 \\ $D_k = D_k \cup Ps_i$ 
 }


\For{i=1 : $r$} { $Ps_i = KNN({OVC_K}_j,NPK_i )$ \\ $D_k = D_k \cup Ps_i$ }

\Comment{where KNN is the K-nearest neighbors algorithms. For high performance calculation of $KNN$, we use FLANN \cite{flann09} to calculate $KNN$.}
}
\caption{Subdomains Generation (Note: All ${\{D_k\}}_{k=1}^{K}$ are stored as indices to $X$).  }
\label{alg:sdgen}
\end{algorithm}





%
%
%
%\subsection{Prediction} 
%From the above discussion, the prediction for each subdomain is computed as follows
%
%\begin{equation}
%\begin{split}
%\hat{Y^i_{x_*}} =  \underset{Y^i_{x_*}}{\operatorname{argmin       }}[ & k_Y(\textbf{Y}^i_{x_*},\textbf{Y}^i_{x_*}) -2 
%k_y(\textbf{Y}^i_{x_*})^T \textbf{u}_w -\\ & \eta_w  log (K_Y(\textbf{Y}^i_{x_*},\textbf{Y}^i_{x_*}) -\\& 
%k_y(\textbf{Y}^i_{x_*})^T {\textbf{W}^i}^\frac{1}{2} ({\textbf{W}^i}^\frac{1}{2} \textbf{K}_Y {\textbf{W}^i}^\frac{1}{2} +  \lambda_y I)^{-1} \\&{\textbf{W}^i}^\frac{1}{2} k_y(\textbf{Y}^i_{x_*}) ) ]
%\end{split}
%\end{equation}
%
%where $\textbf{u}_w = \textbf{W}^\frac{1}{2}  (\textbf{W}^\frac{1}{2} \textbf{K}_X \textbf{W}^\frac{1}{2} + \lambda_x I)^{-1} \textbf{W}^\frac{1}{2} k_x(\textbf{x})$, $\eta_w = k_X(\textbf{x},\textbf{x}) - k_x(\textbf{x})^T \textbf{u}_w$, $({\textbf{W}^i}^\frac{1}{2} \textbf{K}_X^i {\textbf{W}^i}^\frac{1}{2} + \lambda_x \textbf{I})^{-1}$,  $({{\textbf{W}^i}}^\frac{1}{2} \textbf{K}_X {\textbf{W}^i}^\frac{1}{2} + \lambda_x \textbf{I})^{-1}$ could be computed in quadratic time given  $\mathcal{M}^i$ and $W$. Hence,  the $\hat{Y^i_{x_*}}_j$ has  $O(iters \cdot M)^2$ complexity, where $iters$ is the number of iterations. 



\section{Local Kernel Machines hyper-parameters on each dataset}
The hyper parameters were learnt using cross validation on the training set for  GPR, TGP and IWTGP that we are interested in. The following subsection present the learnt hyper-parameters and the error measures on each dataset in case of TGPs.
\subsection{Poser Dataset}
The parameters $2 \rho_x^2$, $2 \rho_y^2$, $\lambda_X$, and $\lambda_Y$ were assigned to  $5$, $5000$, $10^{-4}$, and $10^{-4}$, respectively. 

\subsection{HumanEva Dataset}

The parameters $2 \rho_x^2$, $2 \rho_y^2$, $\lambda_X$, and $\lambda_Y$ were assigned to  $5$, $500000$, $10^{-3}$, and $10^{-3}$, respectively. 


\subsection{Human 3.6 Dataset}

The parameters $2 \rho_x^2$, $2 \rho_y^2$, $\lambda_X$, and $\lambda_Y$ were assigned to  $5$, $500000$, $10^{-3}$, and $10^{-3}$, respectively.




%\section{Equal Size Kmeans (EKmeans): More Details}



\end{appendices}

\clearpage
%begin{comment}
\begin{wrapfigure}{l}{0.2\textwidth}
     \includegraphics[width=0.2\textwidth]{elhoseiny.jpg}
\end{wrapfigure}
\textbf{Mohamed Elhoseiny } is a PostDoc Researcher at Facebook Research.
His primary research interest is in computer vision, machine learning, intersection between natural language and vision, language guided visual-perception,  and visual reasoning, art \& AI. He received his PhD degree from Rutgers University, New Brunswick, in 2016 under Prof. Ahmed Elgammal. Mohamed received an NSF Fellowship in 2014 for the Write-a-Classifier project (ICCV13), best intern award at SRI International 2014, and the Doctoral Consortium award at CVPR 2016.
\vspace{2mm}
 \begin{wrapfigure}{l}{0.2\textwidth}  
\vspace{-5mm} 
         \includegraphics[width=0.2\textwidth]{elgammal.jpg}
         \vspace{-10mm} 
\end{wrapfigure}\,\,\;

 \textbf{Ahmed Elgammal} is a professor at the Department of Computer Science, Rutgers, the State University of New Jersey Since Fall 2002. Dr. Elgammal is also a member of the Center for Computational Biomedicine Imaging and Modeling (CBIM). His primary research interest is computer vision and machine learning. His research focus includes human activity recognition, human motion analysis, tracking, human identification, and statistical methods for computer vision. Dr. Elgammal received the National Science Foundation CAREER Award in 2006. Dr. Elgammal has been the Principal Investigator and Co-Principal Investigator of several research projects in the areas of Human Motion Analysis, Gait Analysis, Tracking, Facial Expression Analysis and Scene Modeling; funded by NSF and ONR. Dr. Elgammal is Member of the review committee/board in several of the top conferences and journals in the computer vision field. Dr. Elgammal received his Ph.D. in 2002 from the University of Maryland, College Park. He is a senior IEEE member. 

%\end{comment}
\begin{comment}
\begin{biography}[me.eps]{Mohamed Elhoseiny}
 is a PostDoc Researcher at Facebook Research.
His primary research interest is in computer vision, machine learning, intersection between natural language and vision, language guided visual-perception,  and visual reasoning, art \& AI. He received his PhD degree from Rutgers University, New Brunswick, in 2016 under Prof. Ahmed Elgammal. Mohamed received an NSF Fellowship in 2014 for the Write-a-Classifier project (ICCV13), best intern award at SRI International 2014, and the Doctoral Consortium award at CVPR 2016.
\end{biography}

\begin{biography}[elgammal2.eps]{Ahmed Elgammal}
is an associate professor at the Department of Computer Science, Rutgers, the State University of New Jersey Since Fall 2002. Dr. Elgammal is also a member of the Center for Computational Biomedicine Imaging and Modeling (CBIM). His primary research interest is computer vision and machine learning. His research focus includes human activity recognition, human motion analysis, tracking, human identification, and statistical methods for computer vision. Dr. Elgammal received the National Science Foundation CAREER Award in 2006. Dr. Elgammal has been the Principal Investigator and Co-Principal Investigator of several research projects in the areas of Human Motion Analysis, Gait Analysis, Tracking, Facial Expression Analysis and Scene Modeling; funded by NSF and ONR. Dr. Elgammal is Member of the review committee/board in several of the top conferences and journals in the computer vision field. Dr. Elgammal received his Ph.D. in 2002 from the University of Maryland, College Park.
\end{biography}
\end{comment}

\end{document}
% end of file template.tex

