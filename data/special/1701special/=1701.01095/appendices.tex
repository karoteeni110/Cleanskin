%!TEX root = /Users/audrey/Dropbox/PhD/MOMAB/ArXiv/Latex/paper.tex

\section{Technical Tools}
\label{app:technical_tools}

\begin{fact}[$d$-dimensional Chernoff]
\label{fac:chernoffd}
    Let $X_1, \dots, X_N$ be i.i.d. $\sigma$-sub-Gaussian variables with values in such that $\Esp[X] = \mu$. Let $\hat \mu_N = \frac{1}{N} \sum_{i=1}^N X_i$. Then, as shown by \cite{Rigollet2015}, for any $a \geq 0$,
    \begin{align*}
        \Pr[|\hat \mu_N - \mu| \geq a] \leq 2 e^{-\frac{N a^2}{2 \sigma^2}}.
    \end{align*}
    Now consider the the multivariate setting where $\bsX_1, \dots, \bsX_N$ are i.i.d. $d$-dimensional $\sigma$-sub-Gaussian variables such that $\Esp[\bsX] = \bsmu$ and $\hat \bsmu_N = \frac{1}{N} \sum_{i=1}^N \bsX_i$. Then for any $a \geq 0$,
    \begin{align*}
        \Pr[\hat \bsmu_N \succeq \bsmu + a]
        & = \Pr[(\hat \mu_{N, 1} \geq \mu_1 + a) \wedge \dots \wedge (\hat \mu_{N, d} \geq \mu_d + a)]
        \leq e^{-\frac{d N a^2}{2 \sigma^2}}, \\
        \Pr[\hat \bsmu_N \not\preceq \bsmu + a]
        & \leq \Pr[(\hat \mu_{N, 1} \geq \mu_1 + a) \vee \dots \vee (\hat \mu_{N, d} \geq \mu_d + a)]
        \leq d e^{-\frac{N a^2}{2 \sigma^2}}, \\
        \Pr[\hat \bsmu_N \not\in B(\bsmu, a)]
        & \leq \Pr[(|\hat \mu_{N, 1} - \mu_1| \geq a) \vee \dots \vee (|\hat \mu_{N, d} - \mu_d| \geq a)]
        \leq 2 d e^{-\frac{N a^2}{2 \sigma^2}}.
    \end{align*}
\end{fact}

\begin{fact}[$d$-dimensional Gaussian concentration]
\label{fac:concentration}
    Let $X$ be a Gaussian random variable with mean $\mu$ and standard deviation $\sigma$. The following concentration is derived~\cite{Agrawal2013} from \cite{Abramowitz1964} for $z \geq 1$:
    \begin{align*}
        \Pr[|X - \mu| > z \sigma] \leq \frac{1}{2} e^{-z^2/2}.
    \end{align*}
    Now consider the multivariate setting where $\bsX$ denotes a $d$-dimensional Gaussian random variable with mean $\bsmu$ and diagonal covariance $\bsSigma$. Then for $z \geq 1$,
    \begin{align*}
        \Pr[\bsX \succ \bsmu + z \sqrt{\diag(\bsSigma)}]
        & = \Pr[(X_1 > \mu_1 + z \sigma_1) \wedge \dots \wedge (X_d > \mu_d + z \sigma_d)]
        \leq \bigg( \frac{1}{4} e^{-z^2/2} \bigg)^d, \\
        \Pr[\bsX \not\prec \bsmu + z \sqrt{\diag(\bsSigma)}]
        & \leq \Pr[(X_1 \geq \mu_1 + z \sigma_1) \vee \dots \vee (X_d \geq \mu_d + z \sigma_d)]
        \leq \frac{d}{4} e^{-z^2/2}, \\
        \Pr[\bsX \not\in B(\bsmu, z \sqrt{\diag(\bsSigma)})]
        & \leq \Pr[(|X_1 - \mu_1| \geq z \sigma_1) \vee \dots \vee (|X_d - \mu_d| \geq z \sigma_d)]
        \leq \frac{d}{2} e^{-z^2/2}.
    \end{align*}
\end{fact}

\begin{fact}[$d$-dimensional Gaussian anti-concentration]
\label{fac:anti_concentration}
    Let $X$ be a Gaussian random variable with mean $\mu$ and standard deviation $\sigma$. The following concentration is derived~\cite{Agrawal2013} from \cite{Abramowitz1964} for $z \geq 1$:
    \begin{align*}
        \Pr[X > \mu + z \sigma] \geq \frac{z}{\sqrt{2 \pi} (z^2 + 1)} e^{-z^2/2}.
    \end{align*}
    Now consider the multivariate setting where $\bsX$ denotes a $d$-dimensional Gaussian random variable with mean $\bsmu$ and diagonal covariance $\bsSigma$. Then for $z \geq 1$,
    \begin{align*}
        \Pr[\bsX \succ \bsmu + z \sqrt{\diag(\bsSigma)}]
        & = \Pr[(X_1 > \mu_1 + z \sigma_1) \wedge \dots \wedge (X_d > \mu_d + z \sigma_d)]
        \geq \bigg( \frac{z}{\sqrt{2 \pi} (z^2 + 1)} e^{-z^2/2} \bigg)^d.
    \end{align*}
\end{fact}


\section{Proof of Lemma~\ref{lem:counts_before_succ}}
\label{app:proof:counts_before_succ}

\begin{proof}
    Let $\Theta_j$ denote a $\Normal_d (\hat \bsmu_\star(\tau_j+1), (I_d + N_\star(\tau_j+1)I_d)^{-1})$ distributed multivariate normal random variable. Let $G_j$ be a geometric variable denoting the number of consecutive independent trials until $\Theta_j \succ \bsmu_\star - \rho_\star$. Then observe that 
    \begin{align*}
         \Esp \bigg[ \sum_{t=\tau_k+1}^{\tau_{k+1}} \Pr[\bstheta_\star(t) \not\succ \bsmu_\star - \rho_\star | \cF_t] \bigg]
         \leq \Esp[G_j]
         = \sum_{i = 1}^\infty \Pr[G_j \geq i].
    \end{align*}
    We want to bound the expected value of $G_j$ by a constant for all $j$. Consider any integer $i \geq 1$, let $z = \sqrt{\ln i^{1/d}}$, and let $\mathrm{MAX}_i$ denote the \emph{maximum preference} of $i$ independent samples of $\Theta_j$, that is $\max_{1 \leq i \leq j} f(\Theta_j)$. We abbreviate $\hat \bsmu_\star(\tau_j+1)$ as $\hat \bsmu_\star$ and $N_\star(\tau_j+1)$ as $N_\star$ in the following. Then
    \begin{align*}
        \Pr[G_j < i]
        & \geq \Pr[\mathrm{MAX}_i \succ \bsmu_\star - \rho_\star] \\
        & \geq \Pr \Big[ \mathrm{MAX}_i \succ \hat \bsmu_\star + \frac{z}{\sqrt{N_\star}} \Big| \hat \bsmu_\star + \frac{z}{\sqrt{N_\star}} \succeq \bsmu_\star - \rho_\star \Big]
        \cdot \Pr \Big[ \hat \bsmu_\star + \frac{z}{\sqrt{N_\star}} \succeq \bsmu_\star - \rho_\star \Big].
    \end{align*}
    Using Fact~\ref{fac:anti_concentration}, this gives
    \begin{align*}
        \Pr \Big[ \mathrm{MAX}_i \succ \hat \bsmu_\star + \frac{z}{\sqrt{N_\star}} \Big| \hat \bsmu_\star + \frac{z}{\sqrt{N_\star}} \succeq \bsmu_\star - \rho_\star \Big]
        & \geq 1 - \Bigg( 1 - \bigg( \frac{1}{\sqrt{2\pi}} \frac{z}{z^2 + 1} e^{-z^2/2} \bigg)^d \Bigg)^i \\
        & = 1 - \Bigg( 1 - \bigg( \frac{1}{\sqrt{2\pi}} \frac{\sqrt{\ln i^{1/d}}}{(\ln i^{1/d} + 1)} \frac{1}{\sqrt{i^{1/d}}} \bigg)^d \Bigg)^i \\
        & \geq 1 - \Bigg( 1 - \bigg( \frac{1}{\sqrt{18 \pi d i^{1/d} \ln i}} \bigg)^d \Bigg)^i \\
        & \geq 1 - e^{-\frac{\sqrt{i}}{\sqrt{18 \pi d \ln i}^d}},
    \end{align*}
    where the second inequality uses that $\ln i^{1/d} + 1 < 3 \ln i$ and the last inequality uses that $1 - x < e^{-x}$. Also, using Fact~\ref{fac:chernoffd}, we have
    \begin{align*}
        \Pr[\hat \bsmu_\star \succeq \bsmu_\star - \frac{z}{\sqrt{N_\star}}]
        \geq 1 - d e^{-\frac{z^2}{2\sigma^2}}
        = 1 - \frac{d}{i^{1/(2d\sigma^2)}}.
    \end{align*}
    Substituting, we obtain
    \begin{align*}
        \Pr[G_j < i]
        \geq \Big( 1 - e^{-\frac{\sqrt{i}}{\sqrt{4 \pi \ln i}^d}} \Big) \cdot \Big( 1 - \frac{d}{i^{1/(2d\sigma^2)}} \Big)
        \geq 1 - \frac{d}{i^{1/(2d\sigma^2)}} - e^{-\frac{\sqrt{i}}{\sqrt{18 \pi d \ln i}^d}}
    \end{align*}
    and
    \begin{align*}
        \Esp[G_j]
        & = \sum_{i \geq 1} (1 - \Pr[G_j < i]) \\
        & \leq \sum_{i \geq 1} \Big( \frac{d}{i^{1/(2d\sigma^2)}} + e^{-\frac{\sqrt{i}}{\sqrt{18 \pi d \ln i}^d}} \Big) \\
        & \leq C(d) + 2 d \sum_{i \geq 1} \frac{1}{i^{1/(2d\sigma^2)}},
    \end{align*}
    where $C(d)$ is such that $e^{-\frac{\sqrt{i}}{\sqrt{18 \pi d \ln i}^d}} \leq \frac{d}{i^{1/(2d\sigma^2)}}$ for $i \geq C(d)$. We observe that $\sigma^2 \leq 1/(4d)$ is required in order for the sum to converge.
\end{proof}

