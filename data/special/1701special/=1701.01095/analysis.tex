%!TEX root = /Users/audrey/Dropbox/PhD/MOMAB/ArXiv/Latex/paper.tex

\section{Theoretical Analysis}
\label{sec:analysis}

In this section we start by proving Prop.~\ref{prop:mvn_ts} that provides a regret bound for TS with MVN priors that is independent from the preference function. Then we use the relations between the gap and the preference radius in three preference function families to obtain Theorem~\ref{thm:mvn_ts}.

\subsection{Proof of Prop.~\ref{prop:mvn_ts}}

The following analysis extends the work for the 1-dimensional setting~\cite{Agrawal2013} to the $d$-dimensional setting. We rewrite Eq.~\ref{eqn:regret} as
\begin{align*}
    \kR(T) = \sum_{a \in \cA, a \neq \star} \Delta_a \sum_{t = 1}^T \Pr[a(t) = a],
\end{align*}
where we control $\sum_{t = 1}^T \Pr[a(t) = a]$. The proof relies on several facts (see Appendix~\ref{app:technical_tools}) that extend Chernoff's inequalities and (anti-)concentration bounds from the $1$-dimensional setting to the $d$-dimensional setting using the concepts of Pareto-domination and preference radius. We introduce the following quantities and events to control the quality of mean estimations and the quality of samples.

\begin{definition}[Quantities $r_a$]
    For each suboptimal action $a$, we choose a quantity $r_a < \rho_a$, where $\rho_a$ is a preference radius. By definition of the preference radii, we have $\bsmu_a \prec \bsmu_a + r_a \prec \bsmu_a + \rho_a$. Recall that $f(\bsx) < f(\bsy)$ if $\bsx \prec \bsy$. Hence we have $f(\bsmu_a) < f(\bsmu_a + r_a) < f(\bsmu_a + \rho_a) < f(\bsmu_\star - \rho_\star)$.
\end{definition}

\begin{definition}[Events $E_a^\mu(t)$, $E_a^\theta(t)$]
    For each suboptimal action $a$, define $E_a^\mu(t)$ as the event that $\hat \bsmu_a(t) \prec \bsmu_a + r_a$, and define $E_a^\theta(t)$ as the event that $\bstheta_a(t) \prec \bsmu_a + \rho_a$. More specifically, they are the event that suboptimal action $a$ is well estimated and well sampled, respectively.
\end{definition}

\begin{definition}[Filtration $\cF_t$]
    Define filtration $\cF_t = \{ a(s), \bsz(s) \}_{s = 1, \dots, t-1 }$.
\end{definition}

For suboptimal action $a$, we decompose
\begin{align*}
    \sum_{t=1}^T \Pr[a(t) = a]
    % & = \sum_{t=1}^T \Pr[a(t) = a] \\
    & = \underbrace{\sum_{t=1}^T \Pr[a(t) = a, E_a^\mu(t), E_a^\theta(t)]}_{\text(A)}
    + \underbrace{\sum_{t=1}^T \Pr[a(t) = a, E_a^\mu(t), \overline{E_a^\theta(t)}]}_{\text(B)}
    + \underbrace{\sum_{t=1}^T \Pr[a(t) = a, \overline{E_a^\mu(t)}]}_{\text(C)}
\end{align*}
and control each part separately. In (A), $a$ is played while being well estimated and well sampled. We control this by bounding poor estimation and poor samples for the optimal action. In (B), $a$ is played while being well estimated but poorly sampled. We control this using Gaussian concentration inequalities. In (C), $a$ is played while being poorly estimated. We control this using Chernoff inequalities. Gathering the following results together
% , we obtain
% \begin{align*}
%     \Delta_a \Esp[N_a(T)]
%     & \leq \Delta_a (2C(d) + 8d) (1 + \sigma)^2 \frac{\ln(d T \Delta_a^2)}{\rho_\star^2} + \frac{4}{\Delta_a}
%     + \Delta_a 2 \frac{\ln(d T \Delta_a^2)}{(\rho_a - r_a)^2} \\
%     & \quad + \Delta_a 2 \sigma^2 \frac{\ln(d T \Delta_a^2)}{r_a^2}
% \end{align*}
% for $\sigma^2 \leq 1/(4d)$, where $C(d)$ is such that $e^{-\frac{\sqrt{i}}{\sqrt{18 \pi d \ln i}^d}} \leq \frac{d}{i^2}$ for $i \geq C(d)$.
and summing over all suboptimal actions, we obtain Prop.~\ref{prop:mvn_ts}.


\subsubsection{Bounding (A)}

By definition of TS, for suboptimal $a$ to be played on episode $t$, we must (at least) have $f(\bstheta_a(t)) \geq f(\bstheta_\star(t))$. By definition of event $E_a^\theta(t)$ and the preference radii, we have $f(\bstheta_a(t)) < f(\bstheta_\star(t))$ if $\bstheta_\star(t) \succ \bsmu_\star - \rho_\star$. Let $\tau_k$ denote the time step at which action $\star$ is selected for the $k^\mathrm{th}$ time for $k \geq 1$, and let $\tau_0 = 0$. Note that for any action $a$, $\tau_k > T$ for $k > N_a(T)$ and $\tau_T \geq T$. Then

\begin{align}
\label{eqn:a}
    (A)
    & = \sum_{t=1}^T \Pr[a(t) = a, E_a^\mu(t), E_a^\theta(t) | \cF_t] \nonumber \\
    & \leq \sum_{t=1}^T \Pr[f(\bstheta_a(t)) > f(\bstheta_\star(t)), E_a^\mu(t), E_a^\theta(t) | \cF_t] \nonumber \\
    & \leq \sum_{t=1}^T \Pr[\bstheta_\star(t) \not\succ \bsmu_\star - \rho_\star | \cF_t] \nonumber \\
    & \leq \sum_{k=0}^{L} \Esp \bigg[ \sum_{t=\tau_k+1}^{\tau_{k+1}} \ind[\bstheta_\star \not\succ \bsmu_\star - \rho_\star | \cF_t] \bigg]
    + \sum_{t=\tau_L+1}^T \Pr[\bstheta_\star(t) \not\succ \bsmu_\star - \rho_\star, N_\star(t) > L | \cF_t].
\end{align}
The second inequality uses the fact that the sampling of $\bstheta_\star(t)$ is independent from the events $E_a^\mu(t)$ and $E_a^\theta(t)$. The last inequality uses the observation that $\Pr[\bstheta_\star(t) \not\succ \bsmu_\star - \rho_\star | \cF_t]$ is fixed given $\cF_t$ and that it changes only when $\pi_\star(t)$ changes, that is only when action $\star$ is played. The first sum counts the number of episodes required before action $\star$ has been played $L$ times. The second counts the number of episodes where $\star$ is badly sampled after having been played $L$ times. We use the following Lemma to control the first summation, see Appendix~\ref{app:proof:counts_before_succ}.

\begin{lemma}[Based on Lemma~6 from \cite{Agrawal2013}]
\label{lem:counts_before_succ}
    Let $\tau_k$ denote the time of the $k^\mathrm{th}$ selection of action $\star$. Then, for any $d \in \Nat$ and $\sigma^2 \leq 1/(4d)$,
    \begin{align*}
        \Esp \bigg[ \sum_{t=\tau_k+1}^{\tau_{k+1}} \Pr[\bstheta_\star(t) \not\succ \bsmu_\star - \rho_\star | \cF_t] \bigg]
        \leq C(d) + 4d,
    \end{align*}
    where $C(d)$ is such that $e^{-\frac{\sqrt{i}}{\sqrt{18 \pi d \ln i}^d}} \leq \frac{d}{i^2}$ for $i \geq C(d)$.
\end{lemma}

Now we bound the second summation in Eq.~\ref{eqn:a} by controlling the probability of poorly sampling $\bstheta_\star(t)$ when $N_\star(t) > L$. Let $E_\star(t)$ denote the event that $\hat \bsmu_\star(t) \succ \bsmu_\star - \sigma \rho_\star / (1 + \sigma)$. Then we have
\begin{align*}
    \Pr[\bstheta_\star(t) \not\succ \bsmu_\star - \rho_\star, N_\star(t) > L | \cF_t]
    & \leq \Pr \Big[ \bstheta_\star(t) \not\succ \hat \bsmu_\star(t) - \frac{\rho_\star}{1 + \sigma}, E_\star(t), N_\star(t) > L | \cF_t \Big] \\
    & \qquad + \Pr[\overline{E_\star(t)}, N_\star(t) > L | \cF_t] \\
    & \leq \Pr \Big[ \bstheta_\star(t) \not\in B \Big( \hat \bsmu_\star(t), \frac{\rho_\star}{1 + \sigma} \Big), E_\star(t), N_\star(t) > L | \cF_t \Big] \\
    & \qquad + \Pr \Big[ \hat \bsmu_\star(t) \not\in B \Big( \bsmu_\star - \frac{\sigma \rho_\star}{1 + \sigma} \Big), N_\star(t) > L | \cF_t \Big] \\
    & \leq \frac{d}{2} e^{-\frac{L \rho_\star^2}{2(1 + \sigma)^2}} + 2 d e^{-\frac{L \rho_\star^2}{2(1 + \sigma)^2}}.
\end{align*}
The last inequality uses Facts~\ref{fac:chernoffd} and~\ref{fac:concentration}. With $L = 2 (1 + \sigma)^2 \frac{\ln(d T \Delta_a^2)}{\rho_\star^2}$ we obtain
\begin{align}
\label{eqn:a:prob_bad_theta_star}
    \Pr[\bstheta_\star(t) \not\succ \bsmu_\star - \rho_\star, N_\star(t) > L | \cF_t]
    \leq \frac{5}{2 T \Delta_a^2}.
\end{align}
We use Lem.~\ref{lem:counts_before_succ} and Eq.~\ref{eqn:a:prob_bad_theta_star} in Eq.~\ref{eqn:a} to obtain
\begin{align*}
    (A) \leq (2C(d) + 8d) (1 + \sigma)^2 \frac{\ln(d T \Delta_a^2)}{\rho_\star^2} + \frac{5}{2 \Delta_a^2}
\end{align*}
for $\sigma^2 \leq 1/(4d)$, where $C(d)$ is such that $e^{-\frac{\sqrt{i}}{\sqrt{18 \pi d \ln i}^d}} \leq \frac{d}{i^2}$ for $i \geq C(d)$.


\subsubsection{Bounding (B)}

We control the probability of badly sampling suboptimal action $a$ given that it has been played at least $L$ times. Recall that filtration $\cF_t$ is such that $E_a^\mu(t)$ holds. To that extent we decompose
\begin{align*}
    (B)
    & = \sum_{t=1}^T \Pr[a(t) = a, \overline{E_a^\theta(t)}, E_a^\mu(t), N_a(t) \leq L | \cF_t]
    + \sum_{t=1}^T \Pr[a(t) = a, \overline{E_a^\theta(t)}, E_a^\mu(t), N_a(t) > L | \cF_t] \\
    & \leq \Esp \bigg[ \sum_{t=1}^T \ind[a(t) = a, N_a(t) \leq L | \cF_t] \bigg]
    + \sum_{t=1}^T \Pr[\bstheta_a(t) \not\prec \bsmu_a + \rho_a, N_a(t) > L | \cF_t] \\
    & \leq L + \sum_{t=1}^T \Pr[\bstheta_a(t) \not\prec \hat \bsmu_a(t) + (\rho_a - r_a), N_a(t) > L | \cF_t] \\
    & \leq L + T \frac{d}{2} e^{-\frac{L(\rho_a - r_a)^2}{2}}.
\end{align*}
The first inequality uses the observation that $\Pr[a(t) = a | \cF_t]$ is fixed given $\cF_t$ and the definition of event $\overline{E_a^\theta(t)}$. The second inequality uses the fact that event $E_a^\mu(t)$ holds. The last inequality uses Fact~\ref{fac:concentration}.
% with $\sigma^2 = \frac{1}{N_a(t)+1} \leq \frac{1}{L}$.
With $L = 2 \frac{\ln(d T \Delta_a^2)}{(\rho_a - r_a)^2}$ we obtain
\begin{align*}
    (B) \leq 2 \frac{\ln(d T \Delta_a^2)}{(\rho_a - r_a)^2} + \frac{1}{2 \Delta_a^2}.
\end{align*}


\subsubsection{Bounding (C)}

Similarly to what has been done previously with (B), we can control the probability of badly estimating suboptimal action $a$ given that it has been played at least $L$ times. Then we have
\begin{align*}
    (C)
    & \leq \sum_{t=1}^{T} \Pr[a(t) = a, \overline{E_a^\mu(T)}, N_a(t) \leq L | \cF_t]
    + \sum_{t=1}^T \Pr[a(t) = a, \overline{E_a^\mu(T)}, N_a(T) > L | \cF_t] \\
    & \leq \Esp \bigg[ \sum_{t=1}^T \ind[a(t) = a, N_a(t) \leq L] \bigg] + \sum_{t=1}^T \Pr[\overline{E_a^\mu(T)}, N_a(T) \geq L] \\
    & \leq L + T d e^{-\frac{L r_a^2}{2\sigma^2}}.
\end{align*}
The second inequality uses the observation that $\Pr[a(t) = a | \cF_t]$ is fixed given $\cF_t$. The last inequality uses Fact~\ref{fac:chernoffd}. With $L = 2 \sigma^2 \frac{\ln(d T \Delta_a^2)}{r_a^2}$ we obtain
\begin{align*}
    (C) \leq 2 \sigma^2 \frac{\ln(d T \Delta_a^2)}{r_a^2} + \frac{1}{\Delta_a^2}.
\end{align*}


\subsection{Proof of Theorem~\ref{thm:mvn_ts}}

By definition of the preference radii, given a linear (Ex.~\ref{ex:linear}), Chebyshev (Ex.~\ref{ex:chebyshev}), or $\epsilon$-constraint preference function (Ex.~\ref{ex:epsilon-constraint}), one can take $\rho_\star = \rho_a = \frac{\Delta_a}{2}$, $r_a = \frac{\Delta_a}{6}$. Using these values in Prop.~\ref{prop:mvn_ts}, we obtain Theorem~\ref{thm:mvn_ts}:
\begin{align*}
    \kR(T)
    & \leq \sum_{a \in \cA, a \neq \star} \bigg[
    (8C(d) + 24d + 18 + 72\sigma^2) (1 + \sigma)^2 \frac{\ln(d T \Delta_a^2)}{\Delta_a} + \frac{4}{\Delta_a} \bigg].
\end{align*}
Let $\Delta_a = \delta_a \sqrt{\frac{dN \ln N}{T}}$, for $\delta_a \in (0, \sqrt{\frac{T}{d N \ln N}}]$. The regret is bounded by
\begin{align*}
    \kR(T)
    & \leq (8C(d) + 24d + 18 + 72 \sigma^2) (1 + \sigma)^2  \frac{\sqrt{N T} \ln(d^2 N \ln N)}{\delta_a \sqrt{d \ln N}} + \frac{4 \sqrt{N T}}{\delta_a \sqrt{d \ln N}}
\end{align*}
with $\sigma^2 \leq 1/(4d)$, that is of order $\cO(\sqrt{dNT}\ln d + \sqrt{dNT \ln N})$. More specifically, for $d \leq \ln N$, the regret bound is of order $\cO(\sqrt{dNT \ln N})$.
