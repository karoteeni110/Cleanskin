Approach to build SBM comprises of:(1)~Network Structure Learning, (2)~Parameter Learning, \& (3)~Bayesian Inference. For structure learning, we propose a novel technique of learning Tree based Bayesian Networks(TBN), whereas for parameter learning and Bayesian inference, we use the idea of \cite{yadav2015business} that performs probabilistic queries using SQL queries on the database of conditional probability tables.

\textbf{TBN Structure Learning:}\label{sec:struct}
Bayesian networks are associated with parameters known as conditional probability tables (CPT), where a CPT of a node indicates the probability that each value of a node can take given all combinations of values of its parent nodes. In CPTs, the number of bins grows exponentially as the number of parents increases leaving fewer data instances in each bin for estimating the parameters. Thus, sparser structures often provide better estimation of the underlying distribution~\cite{koller2009probabilistic}. Also, if the number of states of each node becomes high and the learned model is complex, Bayesian inferencing becomes conceptually and computationally intractable~\cite{lam1994learning}.
Hence, tree-based structures can be useful for density estimation from limited data and in the presence of higher number of states for facilitating faster inferencing. We employ a greedy search, and score based approach for learning TBN structure.

Given the global characteristic $G_j$ and $M$ local characteristics, we find set of top $\eta$ most relevant local characteristics w.r.t. $G_j$ using mutual information. We denote these $\eta$ local characteristics by $Y^j(L)$. Further, we learn a \textit{Tree based Bayesian Network(TBN)} on random variables $X = \{X_r: r=1,2,...,\eta+1\}$, where each $X_r \in X$ is either local characteristic $L_i \in Y^j(L)$ or global characteristic $G_j$ 

Chow et al. in \cite{chow1968approximating} state that cross-entropy between the tree structures distributions and the actual underlying distribution is minimized when the structure is a maximum weight spanning tree(MST). So, in order to learn TBN structure, we first learn MST for the characteristics in the set $X$. We find the mutual information between each pair characteristics, denoted by $W(X_r,X_s)$. Further, we use the mutual information as the weight between each pair of characteristics and learn MST using Kruskal's algorithm.

\vspace{-15pt}
\begin{equation}
\scriptsize
\label{eq:cross_ent}
Total Weight(TW) = \sum_{r=1,Pa(X_{r}) \neq 0}^{\eta+1} W(X_{r},Pa(X_{r}))
\vspace{-5pt}
\end{equation}

By learning MST, order of search space of possible graphs is reduced to $2^{O(\eta)}$, from $2^{O((\eta)^2)}$. Using this MST we search for the directed graph with least cross-entropy, by flipping each edge directions sequentially to obtain $2^{\eta}$ directed graphs along with their corresponding TW calculated using Eq.~\ref{eq:cross_ent}. Graph with maximum TW (minimum cross-entropy)~\cite{lam1994learning} is chosen as the best graphical structure representative of underlying distribution.

\textbf{Parameter Learning and Inference:} To learn the parameters of Bayesian Network(CPTs), for every product $l$ in $L$ we compute the probabilities $p^l_{j,1}, p^l_{j,2},..., p^l_{j,m_j}$, for every state of $G_j$, given the observed values of local characteristics in the Bayesian network, using an approach described in\cite{yadav2015business}. Here, CPTs are learned from the data stored in RDBMS and all queries are also answered using SQL.