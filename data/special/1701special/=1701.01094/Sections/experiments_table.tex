% !TEX root = ../KDD.tex
We now present the results of using our approach for predicting global characteristics on real-life dataset of global market research organization. We 
shall present the accuracy of our predictions using each model BGM and TIR individually and also with the Ensemble approach. Further, we 
also measure the accuracies of our confidence bounds (CoP) while predicting target values, for each global characteristics $g_j$,
which is used practice for routing uncertain records for human annotation.

\textbf{Real-life data}: We have data for carbonated drinks of twenty six thousands products. There are three datasets available:
\begin{itemize}
 \item \textbf{Local characteristics}: Each product has around 49 local characteristics, where cardinality of local characteristics varies from tens to thousands.
 \item \textbf{Retailer description}: Each product have descriptions given by retailers of that product, where number of descriptions of a single product in one geography varies from tens to hundreds.
 \item\textbf{Master dictionary}: It has all possible values of local and global characteristics. Cardinality of global characteristics  varies from tens to thousands.
\end{itemize}

We applied our approach for predicting four global characteristics $g_1$, $g_2$, $g_3$, and $g_4$ for the following two cases:
\begin{itemize}
 \item\textbf{Case-1 (60-20-20)}: Training, validation, and testing data is taken randomly in the ratio of 60:20:20.
 \item\textbf{Case-2 (20-20-60)}: We divide the data into more realistic scenario where training data contain only about 10\% of total possible values of global characteristics. 
 
 \end{itemize}
\textbf{NOTE:} While Case-1 uses a traditional split of training vs testing data, Case-1 is more realistic, since in practice preparing a training data by manual data labeling is costly: For example, we would like to `onboard' a data from a particular dataset 
by manually annotating only a small fraction (e.g. 20\%) of records and automate the remainder. Further, we might like to bring on board 
data from one organization (e.g. retailer or distributor) in a particular geography in the hope that data from remaining sources in that
geography share similar local attributes, eliminating manual annotation for a large volume of data. 
In the latter scenario, there is no guarantee that products from the first organization are representative of those from the remainder. 
To simulate this practical scenario, we used the first few records from the local dataset, which happened to contain only 10\% or so of the
total possible values that each global attribute could take.

In BGM approach, for each $g_j$, we first choose top $k$ local characteristics $l_i$ based on mutual information of $g_j$ with each $l_i$ and then we learn the Tree based Bayesian structure. For experiments, we choose the value of $k$ manually. Figure~\ref{fig:networks}, shows the four network structure learned for each global characteristic(for both cases). It also shows the number of possible values of each characteristic in the network. Since the cardinality of characteristics is in the order of thousands, specially for the networks of $g_1$ and $g_2$, bayesian inferencing become computationally intractable for complex networks. Figure~\ref{fig:g1net}, shows another Bayesian network structure of the global characteristic $g_1$, learned on the same set of characteristics. This structure is learned using an open source python library Pebl~\cite{shah2009python}. Clearly, it is more complex as compared to the one shown in Figure~\ref{fig:networks}(a). For example, the maximum number of rows in CPTs of the network shown in Figure~\ref{fig:g1net}, goes up to order $1200\times1400\times700\times643$ as compared to the order $1200\times1400$ for the network shown in Figure~\ref{fig:networks}(a). 
\begin{figure}
\centering
\includegraphics[width = 85mm]{Figures/networks}
\caption{Tree based Bayesian Networks learned for each global characteristics}
\label{fig:networks}
\end{figure}

\begin{figure}
\centering
\includegraphics[width = 55mm]{Figures/g1-net}
\caption{Bayesian Network of $g_1$ learned using Pebl}
\label{fig:g1net}
\end{figure}

Once we learn the structure of network, we learn the CPTs and then predict the global characteristics using BGM approach as explained in Section~\ref{sec:BGM}. We also predict global characteristics using TIR and Ensemble approach. Figure \ref{fig:case-1} and \ref{fig:case-2}, summarize our results by showing the prediction accuracy of four global characteristic for Case-1 and Case-2 respectively. Here, the accuracy is the ratio of correctly predicted products to the total number of products. In Case-1, accuracy of Ensemble model is in the range of 85 to 99\%  and it outperform both BGM and TIR for all four global characteristics. 

Now consider Case-2 (Figure \ref{fig:case-2}), where the training data contains 10\% of possible states of each global characteristic.
This naturally renders the BGM less accurate, since the model does not even include global attribute values that were not present in the
training data. However, this is compensated by the performance TIR, which searches the target set of global attribute values from the
reference data definition. Combining both approaches using our Ensemble model the accuracy of four global characteristics ranges from 78 to 93\%.

\begin{figure}
\centering
\includegraphics[width = 85mm]{Figures/case-1}
\caption{Predictive accuracy of global characteristics for Case-1}
\label{fig:case-1}
\end{figure}

\begin{figure}
\centering
\includegraphics[width = 85mm]{Figures/case-2}
\caption{Predictive accuracy of global characteristics for Case-2}
\label{fig:case-2}
\end{figure}

Recall that as described in Section \ref{sec:ensemble}, our approach produces a confidence value called Confidence of prediction (CoP) to each prediction of global characteristics. The CoP helps to decide whether the predicted value of $g_j$ is trustworthy or not.  If $R$ is the total number of records in the test data and $M$ is the number of records in test data for which the confidence of our predictions, CoP, is greater than $\tau$, we define three categories:
\begin{itemize}
 \item \textbf{P-C:} Number of products which we choose to predict, i.e., having CoP greater than $\tau$ and also predicted correctly by our approach.
 \item \textbf{P-I:} Number of products having CoP greater than $\tau$  and predicted incorrectly by our approach.
 \item \textbf{NP:} Products which we choose not to predict. i.e., products having CoP less than $\tau$.
\end{itemize}

In practice, we would like to choose $\tau$ in order to maximize P-C and minimize P-I category, while not increasing NP so much that
exercise becomes almost entirely manual.

{\color{red} We learn the threshold $\tau$ on the validation set by maximizing category P-C while not reducing NP too much. 
Since products in the category P-I are more costly for a company as compared to the products in NP category, 
we give more weight to P-I while learning $\tau$. 

Table~\ref{table:categories}, shows the percentage of products in each category (P-C, P-I, NP) on on both validation
and test sets along with the threshold $\tau$ value for both cases. It shows that the for some $\tau$, percentage of products in P-C category is in the range of 75-96\% for Case-1, whereas, it ranges from 63 to 96\% for Case-2. Also, the average percentage of products in P-I category, which is a costly category for a company, is only around 6\%. These numbers establish that the CoP is a good measure to decide whether the predicted value is reliable or not. 
\renewcommand{\arraystretch}{1.5}
\begin{table*}[!t]
\caption{Percentage of products in each category(P-C, P-I, NP)}
\label{table:categories}
  \centering
  \begin{tabular}{|c|c|c|c|c|c|c|c|c|}
    \hline
    \multirow{2}{*}{Global} &
      \multicolumn{4}{c|}{Case-1} &
      \multicolumn{4}{c|}{Case-2} \\
      %\cline{2-9}
    & $\tau$ & P-C & P-I & NP & $\tau$ & P-C & P-I & NP \\
   \hline
   $g_1$ & 0.65 & 82\% & 5\% & 13\% & 0.6 & 78\% & 8\% & 14\% \\
   \hline
   $g_2$ & 0.6 & 74\% & 12\% & 14\% & 0.65 & 63\% & 16\% & 21\% \\
   \hline
   $g_3$ & 0.7 & 96\% & 1\% & 3\% & 0.7 & 96 & 1\% & 3\% \\
   \hline
   $g_4$ & 0.8 & 86\% & 3\% & 11\% & 0.8 & 85\% & 4\% & 11\% \\
   \hline
  \end{tabular}
\end{table*}

\begin{figure}
\centering
\includegraphics[width = 85mm]{Figures/PC-PI-NP}
\caption{Predictive accuracy of global characteristics for Case-2}
\label{fig:PC}
\end{figure}
Figure~\ref{fig:PC}, shows the variation in the percentage of products in each category(P-C, P-I, and NP) with respect to threshold value $\tau$, for the prediction of global characteristics $g_1$ in Case-2. In practice, we would like to choose $\tau$ in order to maximize P-C and minimize P-I category, 
while keeping NP under, say, 20\%, leading to 0.6 is an appropriate value for $\tau$.  }


