train$x[1:2,]
?mean
mean(train$x[1,])
mean(train$x[2,])
mean(train$x[,1])
mean(train$x[,2])
?scale
haha <- matrix(1:10, ncol = 2)
haha
mean(haha)
scale(haha)
mean(scale(haha))
cov(scale(haha))
haha
apply(haha, fun=scale)
apply(haha, margin=1,fun=scale)
?applyl
?apply
train[["x"]]
X <- t(t(train$x) - rep(colMeans(train$x), n))
em_cov <- t(X)@X/train$n
View(X)
X <- t(t(train$x) - rep(colMeans(train$x), train$n))
em_cov <- t(X)@X/train$n
colMeans(train$x)
rep(colMeans(train$x), train$n)
t(X)@X
t(X)
cov\(X)
cov(X)
rep(False or 11, 3)
rep((False or 11), 3)
divide_by_sd <- function(colm) {
colm <- colm/sd(colm)
}
apply(X,2,divide_by_sd)
X <- t(t(train$x) - rep(colMeans(train$x), train$n))
divide_by_sd <- function(colm) {
if (sd(colm) == 0) {sdd <- 1};
else {sdd <- sd(colm)} ;
X <- t(t(train$x) - rep(colMeans(train$x), train$n))
divide_by_sd <- function(colm) {
if (sd(colm) == 0) {sdd <- 1}
else {sdd <- sd(colm)}
colm <- colm/sdd
}
X <- apply(X,2,divide_by_sd)
em_cov <- (t(X)@X)/train$n
X
X <- t(t(train$x) - rep(colMeans(train$x), train$n))
divide_by_sd <- function(colm) {
if (sd(colm) == 0) {sdd <- 1}
else {sdd <- sd(colm)}
colm <- colm/sdd
}
X <- apply(X,2,divide_by_sd)
em_cov <- (t(X)@X)/rep(train$n, train$n)
X <- t(t(train$x) - rep(colMeans(train$x), train$n))
divide_by_sd <- function(colm) {
if (sd(colm) == 0) {sdd <- 1}
else {sdd <- sd(colm)}
colm <- colm/sdd
}
X <- apply(X,2,divide_by_sd)
em_cov <- (t(X)*X)/rep(train$n, train$n)
X <- t(t(train$x) - rep(colMeans(train$x), train$n))
X
X <- train$x
source('loadmnist.r')
load_mnist()
train$n <- 5000
train$x <- train$x[1:train$n,]
train$y <- train$y[1:train$n]
test$n <- 1000
test$x <- test$x[1:test$n,]
test$y <- test$y[1:test$n]
library(proxy)
dists <- dist(train$x, test$x)
dists[1]
# Keep an accuracy table
acc <- rep(NA,50)
n <- test$n
for (k in 1:50) {
# initialize neighbors and the predicted class
nearest <- matrix(0, ncol = k, nrow = n)
knn.class <- rep(-1, n)
for (i in 1:n) {
# k nearest points; represented by points' idx in dataset
nearest[i,] <- order(dists[,i])[1:k]
# Sort the classes by decreasing frequency, and
# pick the most frequent labels
nearest_labels <- train$y[nearest[i,]]
knn.class[i] = names(sort(table(nearest_labels), decreasing=TRUE))[1]
}
# count the number of	correct classifications and divide it by test set size
acc[k] = sum(knn.class == test$y) / test$n
}
plot(acc, type = 'l', xlab = 'k', ylab = 'test acc')
X <- train$x
X <- train$x
View(X)
X <- train$x
X <- t(t(X) - rep(colMeans(X), train$n))
divide_by_sd <- function(colm) {
if (sd(colm) == 0) {sdd <- 1}
else {sdd <- sd(colm)}
colm <- colm/sdd
}
X <- apply(X,2,divide_by_sd)
em_cov <- (t(X)*X)/rep(train$n, train$n)
X*X
t(X)*t(X)
t(X)@t(X)
type(X)
typeof\(X)
typeof(X)
class(X)
rep(train$n, train$n)
X <- train$x
X <- t(t(X) - rep(colMeans(X), train$n))
divide_by_sd <- function(colm) {
if (sd(colm) == 0) {sdd <- 1}
else {sdd <- sd(colm)}
colm <- colm/sdd
}
X <- apply(X,2,divide_by_sd)
em_cov <- (t(X)@X)/matrix(rep(train$n, train$n), nrow=train$n)
(t(X)/matrix(rep(train$n, train$n), nrow=train$n)
)
(t(X)/matrix(rep(train$n, train$n), nrow=train$n)
)
t(X)/matrix(rep(train$n, train$n), nrow=train$n)
X/matrix(rep(train$n, train$n), nrow=train$n)
?multiply
?mult
?matrix
cov(X)
X <- train$x
X <- t(t(X) - rep(colMeans(X), train$n))
divide_by_sd <- function(colm) {
if (sd(colm) == 0) {sdd <- 1}
else {sdd <- sd(colm)}
colm <- colm/sdd
}
X <- apply(X,2,divide_by_sd)
diag(cov(X))
?eign
?eigen
eigen(haha)
haha
haha<- matrix(1:25, nrow=5)
haha
eigen(haha)
order(eigen(haha)$values)[1:2]
6.864208e+01+0.000000e+00i
x_eig <- eigen(X)
x_eig <- eigen(cov(X))
largest <- order(x_eig$values[])[1:2]
large_vecs <- x_eig$vectors[largest,]
largest
large_vecs
View(large_vecs)
28*28
x_eig <- eigen(cov(X))
largest <- order(x_eig$values[])[1:2]
large_vecs <- x_eig$vectors[largest,]
show_digit(large_vecs[1])
show_digit(matrix(large_vecs[1], nrow=28))
matrix(large_vecs[1], nrow=28)
show_digit(matrix(large_vecs[1,], nrow=28))
show_digit(matrix(large_vecs[1,], nrow=28))
show_digit(matrix(large_vecs[2,], nrow=28))
x_eig <- eigen(cov(X))
largest <- order(x_eig$values[])[1:2]
large_vecs <- x_eig$vectors[largest,]
largest
x_eig <- eigen(cov(X))
largest <- order(x_eig$values)[1:2]
large_vecs <- x_eig$vectors[largest,]
largest
x_eig$values[784]
x_eig$values[783]
x_eig$values[780]
x_eig$values[78]
?order
x_eig <- eigen(cov(X))
largest <- order(x_eig$values,decreasing=T)[1:2]
large_vecs <- x_eig$vectors[largest,]
largest
show_digit(matrix(large_vecs[1,], nrow=28))
show_digit(matrix(large_vecs[2,], nrow=28))
plot(matrix(large_vecs[1,], nrow=28))
plot(matrix(large_vecs[2,], nrow=28))
show_digit(matrix(large_vecs[1,], nrow=28))
show_digit(matrix(large_vecs[2,], nrow=28))
x_eig$values[1]
x_eig$values[2]
x_eig$values[3]
order(x_eig$values,decreasing=T)[1:5]
order(x_eig$values,decreasing=T)[1:9]
x_eig <- eigen(cov(X))
largest <- sort(table(x_eig$values),decreasing=T)[1:2]
large_vecs <- x_eig$vectors[largest,]
largest
x_eig <- eigen(cov(X))
largest <- sort(x_eig$values,decreasing=T)[1:2]
large_vecs <- x_eig$vectors[largest,]
largest
sort(x_eig$values,decreasing=T)[:5]
sort(x_eig$values,decreasing=T)[1:5]
order(x_eig$values,decreasing=T)[1:5]
x_eig$values[1:5]
x_eig <- eigen(cov(X))
x_eig$values[1:5]
x_eig$values[1:10]
x_eig$values[1:30]
nrows(cov(X))
nrow(cov(X))
ncol(cov(X))
large_vecs[1,]
?project
lower_X1 <- t(t(X) @ large_vecs[1,])
lower_X1 <- t(t(X) * large_vecs[1,])
lower_X2 <- t(t(X) * large_vecs[2,])
plot(lower_X1,lower_X2)
lower_X1 <- t(t(X) * large_vecs[1,])
lower_X2 <- t(t(X) * large_vecs[2,])
plot(lower_X1,lower_X2,  pch = 20, cex = .5, asp = 1, col = train$y+1)
?norm
lower_X1 <- t(t(X) * large_vecs[1,] / rep(sqrt(t(X) * large_vecs[1,]), 784))
length(t(X) * large_vecs[1,])
matult
?matmult
matmult(t(X),large_vecs[1,])
lower_X1 <- t(t(X) %*% large_vecs[1,] / rep(sqrt(t(X) * large_vecs[1,]), 784))
length(t(X) %*% large_vecs[1,])
length(t(X)
)
length(X)
length(X[1,])
plot(X, pch=19)
?plot
plot(X[,1],X[,2], pch=19)
plot(X[,1],X[,3], pch=19)
x_eig <- eigen(cov(X))
largest <- sort(x_eig$values,decreasing=T)[1:2]
large_vecs <- x_eig$vectors[largest,]
large_vecs
x_eig <- eigen(cov(X))
largest <- sort(x_eig$values,decreasing=T)[1:2]
large_vecs <- x_eig$vectors[largest,]
large_vecs[1,1:10]
large_vecs[2,10:10]
x_eig <- eigen(cov(X))
largest <- sort(x_eig$values,decreasing=T)[1:2]
large_vecs <- x_eig$vectors[largest,]
large_vecs[1,1:10]
large_vecs[2,1:10]
largest
x_eig <- eigen(cov(X))
largest <- order(x_eig$values,decreasing=T)[1:2]
large_vecs <- x_eig$vectors[largest,]
large_vecs[1,1:10]
large_vecs[2,1:10]
largest
x_eig <- eigen(cov(X))
largest <- order(x_eig$values,decreasing=T)[1:2]
large_vecs <- x_eig$vectors[largest,]
large_vecs[1,500:510]
large_vecs[2,500:510]
large_vecs[2,]
x_eig$vectors
x_eig$vectors[1]
x_eig$vectors[1,]
order(x_eig$values,decreasing=T)[1:2]
x_eig$vectors[1]
x_eig$vectors[1,]
x_eig$vectors[2,]
x_eig$vectors[9,]
View(x_eig)
x_eig
max.row(x_eig$values)[1:2]
?max.col
max(x_eig$values)
x_eig <- eigen(cov(X))
large_vecs <- x_eig$vectors[1:2,]
large_vecs[1,500:510]
large_vecs[2,500:510]
x_eig$vectors
x_eig$vectors[4]
which.max(x_eig$vectors)
which.min(x_eig$vectors)
lower_X1 <- t(t(X) %*% large_vecs[,1] / rep(sqrt(t(X) * large_vecs[1,]), 784))
lower_X1 <- t(t(X) %*% large_vecs[,1] / rep(sqrt(t(X) * large_vecs[,1]), 784))
lower_X1 <- t(t(X) * large_vecs[,1] / rep(sqrt(t(X) * large_vecs[,1]), 784))
lower_X1 <- t(t(X) * large_vecs[,1])
lower_X2 <- t(t(X) * large_vecs[,2])
plot(lower_X1,lower_X2, pch = 20, cex = .5, asp = 1, col = train$y+1)
lower_X1 <- t(t(X) * large_vecs[,1])
lower_X2 <- t(t(X) * large_vecs[,2])
data.frame(lower_X1, lower_X2)
plot(lower_X1,lower_X2, pch = 20, cex = .5, asp = 1, col = train$y)
source('loadmnist.r')
load_mnist()
train$n <- 5000
train$x <- train$x[1:train$n,]
train$y <- train$y[1:train$n]
test$n <- 1000
test$x <- test$x[1:test$n,]
test$y <- test$y[1:test$n]
library(proxy)
dists <- dist(train$x, test$x)
dists[1]
# Keep an accuracy table
acc <- rep(NA,50)
n <- test$n
for (k in 1:50) {
# initialize neighbors and the predicted class
nearest <- matrix(0, ncol = k, nrow = n)
knn.class <- rep(-1, n)
for (i in 1:n) {
# k nearest points; represented by points' idx in dataset
nearest[i,] <- order(dists[,i])[1:k]
# Sort the classes by decreasing frequency, and
# pick the most frequent labels
nearest_labels <- train$y[nearest[i,]]
knn.class[i] = names(sort(table(nearest_labels), decreasing=TRUE))[1]
}
# count the number of	correct classifications and divide it by test set size
acc[k] = sum(knn.class == test$y) / test$n
}
X <- train$x
X <- t(t(X) - rep(colMeans(X), train$n))
divide_by_sd <- function(colm) {
if (sd(colm) == 0) {sdd <- 1}
else {sdd <- sd(colm)}
colm <- colm/sdd
}
X <- apply(X,2,divide_by_sd)
diag(cov(X))
plot(X[,1],X[,3], pch=19)
x_eig <- eigen(cov(X))
large_vecs <- x_eig$vectors[1:2,]
show_digit(matrix(large_vecs[1,], nrow=28))
show_digit(matrix(large_vecs[2,], nrow=28))
lower_X1 <- t(t(X) * large_vecs[,1])
lower_X2 <- t(t(X) * large_vecs[,2])
plot(lower_X1,lower_X2, pch = 20, cex = .5, asp = 1, col = train$y+1)
?nnorm
?pnorm
?rnomr
?rnorm
samples <- rnorm(1000, mean=0,sd=10)
0 in samples
samples
0<=13 && 0>=9\
0<=13 && 0>=9
m <- 1000
n <- 100
mean_in_sample <- rep(0, m)
for (i in 1:m) {
samples <- rnorm(n, mean=0,sd=10)
xbar <- mean(samples)
ci <- 1.96*sd(samples)/sqrt(n)
if (0<=xbar+ci && 0>=xbar-ci) {mean_in_sample[i] <- 1}
}
sum(mean_in_sample) / m
?sample
D_j <- sample(samples, k, replace = T)
D_j
D_j <- sample(samples, 1000, replace = T)
D_j
?pnorm
?znorm
?z
m <- 1000
n <- 100
k <- 1000
mean_in_sample <- rep(0, m)
for (i in 1:m) {
samples <- rnorm(n, mean=0,sd=10)
D_j <- sample(samples, k, replace = T)
xbar <- mean(D_j)
ci <- 1.96*sd(D_j)/sqrt(k)
if (0<=xbar+ci && 0>=xbar-ci) {mean_in_sample[i] <- 1}
}
sum(mean_in_sample) / m
m <- 1000
n <- 100
k <- 1000
mean_in_sample <- rep(0, m)
for (i in 1:m) {
# samples <- rnorm(n, mean=0,sd=10)
D_j <- sample(samples, k, replace = T)
xbar <- mean(D_j)
ci <- 1.96*sd(D_j)/sqrt(k)
if (0<=xbar+ci && 0>=xbar-ci) {mean_in_sample[i] <- 1}
}
sum(mean_in_sample) / m
m <- 1000
n <- 100
k <- 1000
mean_in_sample <- rep(0, m)
for (i in 1:m) {
# samples <- rnorm(n, mean=0,sd=10)
D_j <- sample(samples, k, replace = T)
xbar <- mean(D_j)
ci <- 1.96*sd(D_j)/sqrt(k)
if (0<=xbar+ci && 0>=xbar-ci) {mean_in_sample[i] <- 1}
}
sum(mean_in_sample)
xbar
ci
m <- 1000
n <- 100
k <- 1000
mean_in_sample <- rep(0, m)
samples <- rnorm(n, mean=0,sd=10)
for (i in 1:m) {
D_j <- sample(samples, k, replace = T)
xbar <- mean(D_j)
ci <- 1.96*sd(D_j)/sqrt(k)
if (0<=xbar+ci && 0>=xbar-ci) {mean_in_sample[i] <- 1}
}
sum(mean_in_sample) / m
integer(0.8)
integer(0.9)
integer(1.9)
integer(3.9)
int(3.9)
library(ggplot2)
summarySE <- function(data=NULL, measurevar, groupvars=NULL, na.rm=FALSE, conf.interval=.95, .drop=TRUE) {
library(plyr)
# New version of length which can handle NA's: if na.rm==T, don't count them
length2 <- function (x, na.rm=FALSE) {
if (na.rm) sum(!is.na(x))
else       length(x)
}
# This does the summary. For each group's data frame, return a vector with
# N, mean, and sd
datac <- ddply(data, groupvars, .drop=.drop,
.fun = function(xx, col) {
c(N    = length2(xx[[col]], na.rm=na.rm),
mean = mean   (xx[[col]], na.rm=na.rm),
sd   = sd     (xx[[col]], na.rm=na.rm)
)
},
measurevar
)
# Rename the "mean" column
datac <- rename(datac, c("mean" = measurevar))
datac$se <- datac$sd / sqrt(datac$N)  # Calculate standard error of the mean
# Confidence interval multiplier for standard error
# Calculate t-statistic for confidence interval:
# e.g., if conf.interval is .95, use .975 (above/below), and use df=N-1
ciMult <- qt(conf.interval/2 + .5, datac$N-1)
datac$ci <- datac$se * ciMult
return(datac)
}
abstract <- read.csv('../../data/100_1_fulltext_composition.txt', header=T)
abstract <- read.csv('100_1_fulltext_composition.txt', header=T)
abstract2 <- summarySE(abstract, measurevar = 'kld', groupvars = c('category'))
abstract <- read.csv('100_1_abstract_composition.txt', header=T)
abstract2 <- summarySE(abstract, measurevar = 'kld', groupvars = c('category'))
pdf("../../results/100kdata_200tp.pdf", h=7, w=6)
ggplot(abstract2, aes(x=reorder(category, kld, FUN=mean), y=kld)) + geom_errorbar(aes(ymin=kld-ci, ymax=kld+ci)) + geom_point() + coord_flip() + ylab('KL divergence (bits)') + xlab('') + scale_y_continuous(limits=c(5.5,7.5)) + theme_bw()# ,breaks=c(2.0,2.2,2.4,2.6,2.8,3.0)) + theme_bw()
dev.off()
abstract <- read.csv('100_1_abstract_composition.txt', header=T)
abstract2 <- summarySE(abstract, measurevar = 'kld', groupvars = c('category'))
pdf("gensim_100tpc_abst_rank.pdf", h=7, w=6)
ggplot(abstract2, aes(x=reorder(category, kld, FUN=mean), y=kld)) + geom_errorbar(aes(ymin=kld-ci, ymax=kld+ci)) + geom_point() + coord_flip() + ylab('KL divergence (bits)') + xlab('') + scale_y_continuous(limits=c(5.5,7.5)) + theme_bw()# ,breaks=c(2.0,2.2,2.4,2.6,2.8,3.0)) + theme_bw()
dev.off()
ggplot(abstract2, aes(x=reorder(category, kld, FUN=mean), y=kld)) + geom_errorbar(aes(ymin=kld-ci, ymax=kld+ci)) + geom_point() + coord_flip() + ylab('KL divergence (bits)') + xlab('') + scale_y_continuous(limits=c(5.5,7.5)) + theme_bw()# ,breaks=c(2.0,2.2,2.4,2.6,2.8,3.0)) + theme_bw()
abstract <- read.csv('100_1_abstract_composition.txt', header=T)
abstract2 <- summarySE(abstract, measurevar = 'kld', groupvars = c('category'))
library(ggplot2)
summarySE <- function(data=NULL, measurevar, groupvars=NULL, na.rm=FALSE, conf.interval=.95, .drop=TRUE) {
library(plyr)
# New version of length which can handle NA's: if na.rm==T, don't count them
length2 <- function (x, na.rm=FALSE) {
if (na.rm) sum(!is.na(x))
else       length(x)
}
# This does the summary. For each group's data frame, return a vector with
# N, mean, and sd
datac <- ddply(data, groupvars, .drop=.drop,
.fun = function(xx, col) {
c(N    = length2(xx[[col]], na.rm=na.rm),
mean = mean   (xx[[col]], na.rm=na.rm),
sd   = sd     (xx[[col]], na.rm=na.rm)
)
},
measurevar
)
# Rename the "mean" column
datac <- rename(datac, c("mean" = measurevar))
datac$se <- datac$sd / sqrt(datac$N)  # Calculate standard error of the mean
# Confidence interval multiplier for standard error
# Calculate t-statistic for confidence interval:
# e.g., if conf.interval is .95, use .975 (above/below), and use df=N-1
ciMult <- qt(conf.interval/2 + .5, datac$N-1)
datac$ci <- datac$se * ciMult
return(datac)
}
abstract <- read.csv('100_1_abstract_composition.txt', header=T)
abstract2 <- summarySE(abstract, measurevar = 'kld', groupvars = c('category'))
abstract <- read.csv('100_1_kld.txt', header=T)
abstract2 <- summarySE(abstract, measurevar = 'kld', groupvars = c('category'))
pdf("gensim_100_1.pdf", h=7, w=6)
ggplot(abstract2, aes(x=reorder(category, kld, FUN=mean), y=kld)) + geom_errorbar(aes(ymin=kld-ci, ymax=kld+ci)) + geom_point() + coord_flip() + ylab('KL divergence (bits)') + xlab('') + scale_y_continuous(limits=c(5.5,7.5)) + theme_bw()# ,breaks=c(2.0,2.2,2.4,2.6,2.8,3.0)) + theme_bw()
dev.off()
dev.off()
