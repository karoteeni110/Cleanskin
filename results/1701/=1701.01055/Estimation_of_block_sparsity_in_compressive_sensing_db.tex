
 %% bare_jrnl.tex
%% V1.4b
%% 2015/08/26
%% by Michael Shell
%% see http://www.michaelshell.org/
%% for current contact information.
%%
%% This is a skeleton file demonstrating the use of IEEEtran.cls
%% (requires IEEEtran.cls version 1.8b or later) with an IEEE
%% journal paper.
%%
%% Support sites:
%% http://www.michaelshell.org/tex/ieeetran/
%% http://www.ctan.org/pkg/ieeetran
%% and
%% http://www.ieee.org/

%%*************************************************************************
%% Legal Notice:
%% This code is offered as-is without any warranty either expressed or
%% implied; without even the implied warranty of MERCHANTABILITY or
%% FITNESS FOR A PARTICULAR PURPOSE!
%% User assumes all risk.
%% In no event shall the IEEE or any contributor to this code be liable for
%% any damages or losses, including, but not limited to, incidental,
%% consequential, or any other damages, resulting from the use or misuse
%% of any information contained here.
%%
%% All comments are the opinions of their respective authors and are not
%% necessarily endorsed by the IEEE.
%%
%% This work is distributed under the LaTeX Project Public License (LPPL)
%% ( http://www.latex-project.org/ ) version 1.3, and may be freely used,
%% distributed and modified. A copy of the LPPL, version 1.3, is included
%% in the base LaTeX documentation of all distributions of LaTeX released
%% 2003/12/01 or later.
%% Retain all contribution notices and credits.
%% ** Modified files should be clearly indicated as such, including  **
%% ** renaming them and changing author support contact information. **
%%*************************************************************************


% *** Authors should verify (and, if needed, correct) their LaTeX system  ***
% *** with the testflow diagnostic prior to trusting their LaTeX platform ***
% *** with production work. The IEEE's font choices and paper sizes can   ***
% *** trigger bugs that do not appear when using other class files.       ***                          ***
% The testflow support page is at:
% http://www.michaelshell.org/tex/testflow/



\documentclass[journal,onecolumn]{IEEEtran}
%
% If IEEEtran.cls has not been installed into the LaTeX system files,
% manually specify the path to it like:
% \documentclass[journal]{../sty/IEEEtran}





% Some very useful LaTeX packages include:
% (uncomment the ones you want to load)


% *** MISC UTILITY PACKAGES ***
%
%\usepackage{ifpdf}
% Heiko Oberdiek's ifpdf.sty is very useful if you need conditional
% compilation based on whether the output is pdf or dvi.
% usage:
% \ifpdf
%   % pdf code
% \else
%   % dvi code
% \fi
% The latest version of ifpdf.sty can be obtained from:
% http://www.ctan.org/pkg/ifpdf
% Also, note that IEEEtran.cls V1.7 and later provides a builtin
% \ifCLASSINFOpdf conditional that works the same way.
% When switching from latex to pdflatex and vice-versa, the compiler may
% have to be run twice to clear warning/error messages.






% *** CITATION PACKAGES ***
%
%\usepackage{cite}
% cite.sty was written by Donald Arseneau
% V1.6 and later of IEEEtran pre-defines the format of the cite.sty package
% \cite{} output to follow that of the IEEE. Loading the cite package will
% result in citation numbers being automatically sorted and properly
% "compressed/ranged". e.g., [1], [9], [2], [7], [5], [6] without using
% cite.sty will become [1], [2], [5]--[7], [9] using cite.sty. cite.sty's
% \cite will automatically add leading space, if needed. Use cite.sty's
% noadjust option (cite.sty V3.8 and later) if you want to turn this off
% such as if a citation ever needs to be enclosed in parenthesis.
% cite.sty is already installed on most LaTeX systems. Be sure and use
% version 5.0 (2009-03-20) and later if using hyperref.sty.
% The latest version can be obtained at:
% http://www.ctan.org/pkg/cite
% The documentation is contained in the cite.sty file itself.






% *** GRAPHICS RELATED PACKAGES ***
%
\ifCLASSINFOpdf
  % \usepackage[pdftex]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../pdf/}{../jpeg/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.pdf,.jpeg,.png}
\else
  % or other class option (dvipsone, dvipdf, if not using dvips). graphicx
  % will default to the driver specified in the system graphics.cfg if no
  % driver is specified.
  % \usepackage[dvips]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../eps/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.eps}
\fi
% graphicx was written by David Carlisle and Sebastian Rahtz. It is
% required if you want graphics, photos, etc. graphicx.sty is already
% installed on most LaTeX systems. The latest version and documentation
% can be obtained at:
% http://www.ctan.org/pkg/graphicx
% Another good source of documentation is "Using Imported Graphics in
% LaTeX2e" by Keith Reckdahl which can be found at:
% http://www.ctan.org/pkg/epslatex
%
% latex, and pdflatex in dvi mode, support graphics in encapsulated
% postscript (.eps) format. pdflatex in pdf mode supports graphics
% in .pdf, .jpeg, .png and .mps (metapost) formats. Users should ensure
% that all non-photo figures use a vector format (.eps, .pdf, .mps) and
% not a bitmapped formats (.jpeg, .png). The IEEE frowns on bitmapped formats
% which can result in "jaggedy"/blurry rendering of lines and letters as
% well as large increases in file sizes.
%
% You can find documentation about the pdfTeX application at:
% http://www.tug.org/applications/pdftex





% *** MATH PACKAGES ***
%
%\usepackage{amsmath}
% A popular package from the American Mathematical Society that provides
% many useful and powerful commands for dealing with mathematics.
%
% Note that the amsmath package sets \interdisplaylinepenalty to 10000
% thus preventing page breaks from occurring within multiline equations. Use:
%\interdisplaylinepenalty=2500
% after loading amsmath to restore such page breaks as IEEEtran.cls normally
% does. amsmath.sty is already installed on most LaTeX systems. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/pkg/amsmath





% *** SPECIALIZED LIST PACKAGES ***
%
%\usepackage{algorithmic}
% algorithmic.sty was written by Peter Williams and Rogerio Brito.
% This package provides an algorithmic environment fo describing algorithms.
% You can use the algorithmic environment in-text or within a figure
% environment to provide for a floating algorithm. Do NOT use the algorithm
% floating environment provided by algorithm.sty (by the same authors) or
% algorithm2e.sty (by Christophe Fiorio) as the IEEE does not use dedicated
% algorithm float types and packages that provide these will not provide
% correct IEEE style captions. The latest version and documentation of
% algorithmic.sty can be obtained at:
% http://www.ctan.org/pkg/algorithms
% Also of interest may be the (relatively newer and more customizable)
% algorithmicx.sty package by Szasz Janos:
% http://www.ctan.org/pkg/algorithmicx




% *** ALIGNMENT PACKAGES ***
%
%\usepackage{array}
% Frank Mittelbach's and David Carlisle's array.sty patches and improves
% the standard LaTeX2e array and tabular environments to provide better
% appearance and additional user controls. As the default LaTeX2e table
% generation code is lacking to the point of almost being broken with
% respect to the quality of the end results, all users are strongly
% advised to use an enhanced (at the very least that provided by array.sty)
% set of table tools. array.sty is already installed on most systems. The
% latest version and documentation can be obtained at:
% http://www.ctan.org/pkg/array


% IEEEtran contains the IEEEeqnarray family of commands that can be used to
% generate multiline equations as well as matrices, tables, etc., of high
% quality.




% *** SUBFIGURE PACKAGES ***
%\ifCLASSOPTIONcompsoc
%  \usepackage[caption=false,font=normalsize,labelfont=sf,textfont=sf]{subfig}
%\else
%  \usepackage[caption=false,font=footnotesize]{subfig}
%\fi
% subfig.sty, written by Steven Douglas Cochran, is the modern replacement
% for subfigure.sty, the latter of which is no longer maintained and is
% incompatible with some LaTeX packages including fixltx2e. However,
% subfig.sty requires and automatically loads Axel Sommerfeldt's caption.sty
% which will override IEEEtran.cls' handling of captions and this will result
% in non-IEEE style figure/table captions. To prevent this problem, be sure
% and invoke subfig.sty's "caption=false" package option (available since
% subfig.sty version 1.3, 2005/06/28) as this is will preserve IEEEtran.cls
% handling of captions.
% Note that the Computer Society format requires a larger sans serif font
% than the serif footnote size font used in traditional IEEE formatting
% and thus the need to invoke different subfig.sty package options depending
% on whether compsoc mode has been enabled.
%
% The latest version and documentation of subfig.sty can be obtained at:
% http://www.ctan.org/pkg/subfig




% *** FLOAT PACKAGES ***
%
%\usepackage{fixltx2e}
% fixltx2e, the successor to the earlier fix2col.sty, was written by
% Frank Mittelbach and David Carlisle. This package corrects a few problems
% in the LaTeX2e kernel, the most notable of which is that in current
% LaTeX2e releases, the ordering of single and double column floats is not
% guaranteed to be preserved. Thus, an unpatched LaTeX2e can allow a
% single column figure to be placed prior to an earlier double column
% figure.
% Be aware that LaTeX2e kernels dated 2015 and later have fixltx2e.sty's
% corrections already built into the system in which case a warning will
% be issued if an attempt is made to load fixltx2e.sty as it is no longer
% needed.
% The latest version and documentation can be found at:
% http://www.ctan.org/pkg/fixltx2e


%\usepackage{stfloats}
% stfloats.sty was written by Sigitas Tolusis. This package gives LaTeX2e
% the ability to do double column floats at the bottom of the page as well
% as the top. (e.g., "\begin{figure*}[!b]" is not normally possible in
% LaTeX2e). It also provides a command:
%\fnbelowfloat
% to enable the placement of footnotes below bottom floats (the standard
% LaTeX2e kernel puts them above bottom floats). This is an invasive package
% which rewrites many portions of the LaTeX2e float routines. It may not work
% with other packages that modify the LaTeX2e float routines. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/pkg/stfloats
% Do not use the stfloats baselinefloat ability as the IEEE does not allow
% \baselineskip to stretch. Authors submitting work to the IEEE should note
% that the IEEE rarely uses double column equations and that authors should try
% to avoid such use. Do not be tempted to use the cuted.sty or midfloat.sty
% packages (also by Sigitas Tolusis) as the IEEE does not format its papers in
% such ways.
% Do not attempt to use stfloats with fixltx2e as they are incompatible.
% Instead, use Morten Hogholm'a dblfloatfix which combines the features
% of both fixltx2e and stfloats:
%
% \usepackage{dblfloatfix}
% The latest version can be found at:
% http://www.ctan.org/pkg/dblfloatfix




%\ifCLASSOPTIONcaptionsoff
%  \usepackage[nomarkers]{endfloat}
% \let\MYoriglatexcaption\caption
% \renewcommand{\caption}[2][\relax]{\MYoriglatexcaption[#2]{#2}}
%\fi
% endfloat.sty was written by James Darrell McCauley, Jeff Goldberg and
% Axel Sommerfeldt. This package may be useful when used in conjunction with
% IEEEtran.cls'  captionsoff option. Some IEEE journals/societies require that
% submissions have lists of figures/tables at the end of the paper and that
% figures/tables without any captions are placed on a page by themselves at
% the end of the document. If needed, the draftcls IEEEtran class option or
% \CLASSINPUTbaselinestretch interface can be used to increase the line
% spacing as well. Be sure and use the nomarkers option of endfloat to
% prevent endfloat from "marking" where the figures would have been placed
% in the text. The two hack lines of code above are a slight modification of
% that suggested by in the endfloat docs (section 8.4.1) to ensure that
% the full captions always appear in the list of figures/tables - even if
% the user used the short optional argument of \caption[]{}.
% IEEE papers do not typically make use of \caption[]'s optional argument,
% so this should not be an issue. A similar trick can be used to disable
% captions of packages such as subfig.sty that lack options to turn off
% the subcaptions:
% For subfig.sty:
% \let\MYorigsubfloat\subfloat
% \renewcommand{\subfloat}[2][\relax]{\MYorigsubfloat[]{#2}}
% However, the above trick will not work if both optional arguments of
% the \subfloat command are used. Furthermore, there needs to be a
% description of each subfigure *somewhere* and endfloat does not add
% subfigure captions to its list of figures. Thus, the best approach is to
% avoid the use of subfigure captions (many IEEE journals avoid them anyway)
% and instead reference/explain all the subfigures within the main caption.
% The latest version of endfloat.sty and its documentation can obtained at:
% http://www.ctan.org/pkg/endfloat
%
% The IEEEtran \ifCLASSOPTIONcaptionsoff conditional can also be used
% later in the document, say, to conditionally put the References on a
% page by themselves.




% *** PDF, URL AND HYPERLINK PACKAGES ***
%
%\usepackage{url}
% url.sty was written by Donald Arseneau. It provides better support for
% handling and breaking URLs. url.sty is already installed on most LaTeX
% systems. The latest version and documentation can be obtained at:
% http://www.ctan.org/pkg/url
% Basically, \url{my_url_here}.




% *** Do not adjust lengths that control margins, column widths, etc. ***
% *** Do not use packages that alter fonts (such as pslatex).         ***
% There should be no need to do such things with IEEEtran.cls V1.6 and later.
% (Unless specifically asked to do so by the journal or conference you plan
% to submit to, of course. )


% correct bad hyphenation here
\hyphenation{op-tical net-works semi-conduc-tor}
\usepackage{amsfonts}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{color}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{amsbsy}
\usepackage{indentfirst}

\begin{document}
%
% paper title
% Titles are generally capitalized except for words such as a, an, and, as,
% at, but, by, for, in, nor, of, on, or, the, to and up, which are usually
% not capitalized unless they are the first or last word of the title.
% Linebreaks \\ can be used within to get better formatting as desired.
% Do not put math or special symbols in the title.
\title{Estimation of block sparsity in \\ compressive sensing}
%
%
% author names and IEEE memberships
% note positions of commas and nonbreaking spaces ( ~ ) LaTeX will not break
% a structure at a ~ so this keeps an author's name from being broken across
% two lines.
% use \thanks{} to gain access to the first footnote area
% a separate \thanks must be used for each paragraph as LaTeX2e's \thanks
% was not built to handle multiple paragraphs
%

\author{Zhiyong~Zhou %,~\IEEEmembership{Member,~IEEE,}
        and~Jun~Yu %,~\IEEEmembership{Member,~IEEE}% <-this % stops a space
\thanks{The authors are with the Department
of Mathematics and Mathematical Statistics, Ume{\aa} University,  Ume{\aa},
901 87, Sweden (e-mail: zhiyong.zhou@umu.se, jun.yu@umu.se).}}% <-this % stops a space
%\thanks{J. Doe and J. Doe are with Anonymous University.}% <-this % stops a space
%\thanks{Manuscript received April 19, 2005; revised August 26, 2015.}}

% note the % following the last \IEEEmembership and also \thanks -
% these prevent an unwanted space from occurring between the last author name
% and the end of the author line. i.e., if you had this:
%
% \author{....lastname \thanks{...} \thanks{...} }
%                     ^------------^------------^----Do not want these spaces!
%
% a space would be appended to the last name and could cause every name on that
% line to be shifted left slightly. This is one of those "LaTeX things". For
% instance, "\textbf{A} \textbf{B}" will typeset as "A B" not "AB". To get
% "AB" then you have to do: "\textbf{A}\textbf{B}"
% \thanks is no different in this regard, so shield the last } of each \thanks
% that ends a line with a % and do not let a space in before the next \thanks.
% Spaces after \IEEEmembership other than the last one are OK (and needed) as
% you are supposed to have spaces between the names. For what it is worth,
% this is a minor point as most people would not even notice if the said evil
% space somehow managed to creep in.



% The paper headers
%\markboth{Journal of \LaTeX\ Class Files,~Vol.~14, No.~8, August~2015}%
%{Shell \MakeLowercase{\textit{et al.}}: Bare Demo of IEEEtran.cls for IEEE Journals}
% The only time the second header will appear is for the odd numbered pages
% after the title page when using the twoside option.
%
% *** Note that you probably will NOT want to include the author's ***
% *** name in the headers of peer review papers.                   ***
% You can use \ifCLASSOPTIONpeerreview for conditional compilation here if
% you desire.




% If you want to put a publisher's ID mark on the page you can do it like
% this:
%\IEEEpubid{0000--0000/00\$00.00~\copyright~2015 IEEE}
% Remember, if you use this you must call \IEEEpubidadjcol in the second
% column for its text to clear the IEEEpubid mark.



% use for special paper notices
%\IEEEspecialpapernotice{(Invited Paper)}




% make the title area
\maketitle

% As a general rule, do not put math, special symbols or citations
% in the abstract or keywords.
\begin{abstract}
Explicitly using the block structure of the unknown signal can achieve better recovery performance in compressive censing. An unknown signal with block structure can be accurately recovered from underdetermined linear measurements provided that it is sufficiently block sparse. However, in practice, the block sparsity level is typically unknown. In this paper, we consider a soft measure of block sparsity, $k_\alpha(\mathbf{x})=\left(\lVert\mathbf{x}\rVert_{2,\alpha}/\lVert\mathbf{x}\rVert_{2,1}\right)^{\frac{\alpha}{1-\alpha}},\alpha\in[0,\infty]$ and propose a procedure to estimate it by using multivariate isotropic symmetric $\alpha$-stable random projections without sparsity or block sparsity assumptions. The limiting distribution of the estimator is given. Some simulations are conducted to illustrate our theoretical results.
\end{abstract}

% Note that keywords are not normally used for peerreview papers.
\begin{IEEEkeywords}
Block sparsity; Multivariate isotropic symmetric $\alpha$-stable distribution; Compressive sensing; Characteristic function.
\end{IEEEkeywords}






% For peer review papers, you can put extra information on the cover
% page as needed:
% \ifCLASSOPTIONpeerreview
% \begin{center} \bfseries EDICS Category: 3-BBND \end{center}
% \fi
%
% For peerreview papers, this IEEEtran command inserts a page break and
% creates the second title. It will be ignored for other modes.
\IEEEpeerreviewmaketitle



\section{Introduction}
% The very first letter is a 2 line initial drop letter followed
% by the rest of the first word in caps.
%
% form to use if the first word consists of a single letter:
% \IEEEPARstart{A}{demo} file is ....
%
% form to use if you need the single drop letter followed by
% normal text (unknown if ever used by the IEEE):
% \IEEEPARstart{A}{}demo file is ....
%
% Some journals put the first two words in caps:
% \IEEEPARstart{T}{his demo} file is ....
%
% Here we have the typical use of a "T" for an initial drop letter
% and "HIS" in caps to complete the first word.

\IEEEPARstart{S}{ince} its introduction a few years ago \cite{crt,ct1,ct2,d}, Compressive Sensing (CS) has attracted considerable interests (see the monographs \cite{ek,fr} for a comprehensive view). Formally, one considers the standard CS model, \begin{align}
\mathbf{y}=A\mathbf{x}+\boldsymbol{\varepsilon}, \label{1.1}
\end{align}
where $\mathbf{y}\in\mathbb{R}^{m\times 1}$ is the measurements, $A\in\mathbb{R}^{m\times N}$ is the measurement matrix, $\mathbf{x}\in\mathbb{R}^N$ is the unknown signal, $\boldsymbol{\varepsilon}$ is the measurement error, and $m\ll N$. The goal of CS is to recover the unknown signal $\mathbf{x}$ by using only the underdetermined measurements $\mathbf{y}$ and the matrix $A$. Under the assumption of sparsity of the signal, that is $\mathbf{x}$ has only a few nonzero entries, and the measurement matrix $A$ is properly chosen, $\mathbf{x}$ can be recovered from $\mathbf{y}$ by certain algorithms, such as the Basis Pursuit (BP), or $\ell_1$-minimization approach, the Orthogonal Matching Pursuit (OMP) \cite{tg}, Compressive Sampling Matching Pursuit (CoSaMP) \cite{nt} and the Iterative Harding Thresholding algorithm \cite{bd1}. Specifically, when the sparsity level of the signal $\mathbf{x}$ is $s=\lVert\mathbf{x}\rVert_0=\mathrm{card}\{j:x_j\neq 0\}$, if $m\geq Cs\ln(N/s)$ with some universal constant $C$, and $A$ is subgaussian random matrix, then accurate or robust recovery can be guaranteed with high probability.

The sparsity level parameter $s$ plays a fundamental role in CS, as the number of measurements, the properties of measurement matrix $A$, and even some recovery algorithms all involve it. However, the sparsity level of the signal is usually unknown in practice. To fill the gap between theory and practice,
very recently \cite{l1,l2} proposed a numerically stable measure of sparsity $s_\alpha(\mathbf{x})=\left(\frac{\lVert\mathbf{x}\rVert_{\alpha}}{\lVert\mathbf{x}\rVert_{1}}\right)^{\frac{\alpha}{1-\alpha}}$ with $\alpha\in[0,\infty]$,
which is in ratios of norms. By random linear projections using i.i.d univariate symmetric $\alpha$-stable random variables, the author constructed the estimation equation for $s_{\alpha}(\mathbf{x})$ with $\alpha\in(0,2]$ by adopting the characteristic function method and obtained the asymptotic normality of the estimator.

As a natural extension of the sparsity with nonzero entries arbitrarily spread throughout the signal, we can consider the sparse signals exhibit additional structure in the form of the nonzero entries occurring in clusters. Such signals are referred to as block sparse \cite{de,ekb,em}. Block sparse model appears in many practical scenarios, such as when dealing with multi-band signals \cite{me2}, in measurements of gene expression levels \cite{pvmh}, and in colour imaging \cite{mw}. Moreover, block sparse model can be used to treat the problems of multiple measurement vector (MMV) \cite{ch,crek,em,me1} and sampling signals that lie in a union of subspaces \cite{bd2,em,me2}.

To make explicit use of the block structure to achieve better sparse recovery performance, the corresponding extended versions of sparse representation
algorithms have been developed, such as mixed $\ell_2/\ell_1$-norm recovery algorithm \cite{ekb,em,sph}, group lasso \cite{yl} or adaptive group lasso \cite{lbw}, iterative reweighted $\ell_2/\ell_1$ recovery algorithms \cite{zb}, block version of OMP algorithm \cite{ekb} and the extensions of the CoSaMP algorithm and of the Iterative Hard Thresholding to the model-based setting \cite{bcdh}, which includes block sparse model as a special case. It was shown in \cite{em} that if the measurement matrix $A$ has small block-restricted isometry constants which generalizes the
conventional RIP notion, then the mixed $\ell_2/\ell_1$-norm recovery algorithm is guaranteed to recover any block sparse signal, irrespectively of the
locations of the nonzero blocks. Furthermore, recovery will be
robust in the presence of noise and modeling errors (i.e., when
the vector is not exactly block sparse). \cite{bcdh} showed that the block versions of CoSaMP and Iterative Hard Thresholding exhibit
provable recovery guarantees and robustness properties. In addition, with the block-coherence of $A$ is small, the robust recovery of mixed $\ell_2/\ell_1$-norm method, and the block version of the OMP algorithm are guaranteed in \cite{ekb}.

The block sparsity level plays the same central role in recovery for block sparse signals as the sparsity level in recovery for sparse signals. Namely, the required number of measurements, properties of the recovery measurement matrix (Block RIP), and some recovery algorithms for signals with block structure all depend on the block sparsity level. However, in reality, the block sparsity level of the signals are also unknown. To obtain its estimator is very important from both the theoretical and practical views.

\subsection{Contributions}
 First, as a extension of the soft sparsity measure $s_\alpha(\mathbf{x})=\left(\frac{\lVert\mathbf{x}\rVert_{\alpha}}{\lVert\mathbf{x}\rVert_{1}}\right)^{\frac{\alpha}{1-\alpha}}$ with $\alpha\in[0,\infty]$ in \cite{l1,l2}, we propose a soft measure of block sparsity, that is $k_\alpha(\mathbf{x})=\left(\lVert\mathbf{x}\rVert_{2,\alpha}/\lVert\mathbf{x}\rVert_{2,1}\right)^{\frac{\alpha}{1-\alpha}}$.

 Second, we obtain an estimator for the block sparsity by using multivariate isotropic symmetric $\alpha$-stable random projections. When the block size is 1, our estimation procedure reduces to the case considered in \cite{l2}. The asymptotic distributions of the estimators are obtained, similar to the results presented in \cite{l2}.

 Finally, a series of simulation experiments are conducted to illustrate our theoretical results.

\subsection{Organization and Notations}

The remainder of the paper is organized as follows. In Section II, we introduce the definition of block sparsity and a soft measure of block sparsity. In Section III, we present the estimation procedure for the block sparsity measure and obtain the asymptotic properties for the estimators. In Section IV, we conduct some simulations to illustrate the theoretical results. Section V is devoted to the conclusion. Finally, the proofs are postponed to the Appendix.

Throughout the paper, we denote vectors by boldface lower letters e.g., $\mathbf{x}$, and matrices by upper letters e.g., $A$. Vectors are columns by default. $\mathbf{x}^T$ is the transpose of the vector $\mathbf{x}$. The notation $x_j$ denotes the $j$-th component of $\mathbf{x}$. For any vector $\mathbf{x}\in\mathbb{R}^N$, we denote the $\ell_p$-norm  $\lVert\mathbf{x}\rVert_p=(\sum_{j=1}^N|x_j|^p)^{1/p}$ for $p>0$. $I(\cdot)$ is the indicator function. $E$ is the expectation function. $\lfloor\cdot\rfloor$ is the bracket function, which takes the maximum integer value. $\mathrm{Re}(\cdot)$ is the real part function. $i$ is the unit imaginary number. $\langle\cdot,\cdot\rangle$ is the inner product of two vectors. $\overset{p}\longrightarrow$ indicates convergence in probability, while $\overset{d}\longrightarrow$ is convergence in distribution.


% An example of a floating figure using the graphicx package.
% Note that \label must occur AFTER (or within) \caption.
% For figures, \caption should occur after the \includegraphics.
% Note that IEEEtran v1.7 and later has special internal code that
% is designed to preserve the operation of \label within \caption
% even when the captionsoff option is in effect. However, because
% of issues like this, it may be the safest practice to put all your
% \label just after \caption rather than within \caption{}.
%
% Reminder: the "draftcls" or "draftclsnofoot", not "draft", class
% option should be used if it is desired that the figures are to be
% displayed while in draft mode.
%
%\begin{figure}[!t]
%\centering
%\includegraphics[width=2.5in]{myfigure}
% where an .eps filename suffix will be assumed under latex,
% and a .pdf suffix will be assumed for pdflatex; or what has been declared
% via \DeclareGraphicsExtensions.
%\caption{Simulation results for the network.}
%\label{fig_sim}
%\end{figure}

% Note that the IEEE typically puts floats only at the top, even when this
% results in a large percentage of a column being occupied by floats.


% An example of a double column floating figure using two subfigures.
% (The subfig.sty package must be loaded for this to work.)
% The subfigure \label commands are set within each subfloat command,
% and the \label for the overall figure must come after \caption.
% \hfil is used as a separator to get equal spacing.
% Watch out that the combined width of all the subfigures on a
% line do not exceed the text width or a line break will occur.
%
%\begin{figure*}[!t]
%\centering
%\subfloat[Case I]{\includegraphics[width=2.5in]{box}%
%\label{fig_first_case}}
%\hfil
%\subfloat[Case II]{\includegraphics[width=2.5in]{box}%
%\label{fig_second_case}}
%\caption{Simulation results for the network.}
%\label{fig_sim}
%\end{figure*}
%
% Note that often IEEE papers with subfigures do not employ subfigure
% captions (using the optional argument to \subfloat[]), but instead will
% reference/describe all of them (a), (b), etc., within the main caption.
% Be aware that for subfig.sty to generate the (a), (b), etc., subfigure
% labels, the optional argument to \subfloat must be present. If a
% subcaption is not desired, just leave its contents blank,
% e.g., \subfloat[].


% An example of a floating table. Note that, for IEEE style tables, the
% \caption command should come BEFORE the table and, given that table
% captions serve much like titles, are usually capitalized except for words
% such as a, an, and, as, at, but, by, for, in, nor, of, on, or, the, to
% and up, which are usually not capitalized unless they are the first or
% last word of the caption. Table text will default to \footnotesize as
% the IEEE normally uses this smaller font for tables.
% The \label must come after \caption as always.
%
%\begin{table}[!t]
%% increase table row spacing, adjust to taste
%\renewcommand{\arraystretch}{1.3}
% if using array.sty, it might be a good idea to tweak the value of
% \extrarowheight as needed to properly center the text within the cells
%\caption{An Example of a Table}
%\label{table_example}
%\centering
%% Some packages, such as MDW tools, offer better commands for making tables
%% than the plain LaTeX2e tabular which is used here.
%\begin{tabular}{|c||c|}
%\hline
%One & Two\\
%\hline
%Three & Four\\
%\hline
%\end{tabular}
%\end{table}


% Note that the IEEE does not put floats in the very first column
% - or typically anywhere on the first page for that matter. Also,
% in-text middle ("here") positioning is typically not used, but it
% is allowed and encouraged for Computer Society conferences (but
% not Computer Society journals). Most IEEE journals/conferences use
% top floats exclusively.
% Note that, LaTeX2e, unlike IEEE journals/conferences, places
% footnotes above bottom floats. This can be corrected via the
% \fnbelowfloat command of the stfloats package.



\section{Block Sparsity Measures}

\subsection{Definitions}
 We firstly introduce some basic concepts for block sparsity and propose a new soft measure of block sparsity.

With $N=\sum_{j=1}^{p}d_j$, we define the $j$-th block $\mathbf{x}[j]$ of a length-$N$ vector $\mathbf{x}$ over $\mathcal{I}=\{d_1,\cdots,d_p\}$. The $j$-th block is of length $d_j$, and the blocks are formed sequentially so that
\begin{align}
\mathbf{x}=(\underbrace{x_1\cdots x_{d_1}}_{\mathbf{x}^{T}[1]}\underbrace{x_{d_1+1}\cdots x_{d_1+d_2}}_{\mathbf{x}^{T}[2]}\cdots\underbrace{x_{N-d_p+1}\cdots x_N}_{\mathbf{x}^{T}[p]})^T. \label{signal}
\end{align}
Without loss of generality, we assume that $d_1=d_2=\cdots=d_p=d$, then $N=pd$. A vector $\mathbf{x}\in\mathbb{R}^N$ is called block $k$-sparse over $\mathcal{I}=\{d,\cdots,d\}$ if $\mathbf{x}[j]$ is nonzero for at most $k$ indices $j$. In other words, by denoting the mixed $\ell_2/\ell_0$ norm
$$
\lVert\mathbf{x}\rVert_{2,0}=\sum_{j=1}^{p}I(\lVert\mathbf{x}[j]\rVert_2>0),
$$
a block $k$-sparse vector $\mathbf{x}$ can be defined by $\lVert\mathbf{x}\rVert_{2,0}\leq k$.

Despite the important theoretical role of the parameter $\lVert\mathbf{x}\rVert_{2,0}$, it has a severe practical drawback of being not sensitive to small entries of $\mathbf{x}$. For instance, if $\mathbf{x}$ has $k$ large blocks and $p-k$ small blocks, then $\lVert\mathbf{x}\rVert_{2,0}=p$ as soon as they are nonzero. To overcome this drawback, it is desirable to replace the mixed $\ell_2/\ell_0$ norm with a soft version. Specifically, we generalize the sparsity measure based on entropy to the block sparsity measure. For any non-zero signal $\mathbf{x}$ given in (\ref{signal}), it induces a distribution $\pi(\mathbf{x})\in\mathbb{R}^p$ on the set of block indices $\{1,\cdots,p\}$, assigning mass $\pi_j(\mathbf{x})=\lVert\mathbf{x}[j]\rVert_2/\lVert\mathbf{x}\rVert_{2,1}$ at index $j\in\{1,\cdots,p\}$, where $\lVert\mathbf{x}\rVert_{2,1}=\sum_{j=1}^{p}\lVert\mathbf{x}[j]\rVert_2$. Then the entropy based block sparsity goes to
\begin{align}
k_{\alpha}(\mathbf{x})=\begin{cases}
\exp(H_{\alpha}(\pi(\mathbf{x}))) & \text{if $\mathbf{x}\neq \mathbf{0}$}\\
0 & \text{if $\mathbf{x}=\mathbf{0}$},
\end{cases}
\end{align}
where $H_\alpha$ is the R\'{e}nyi entropy of order $\alpha\in[0,\infty]$ \cite{l2,pv,v}. When $\alpha\notin\{0,1,\infty\}$, the R\'{e}nyi entropy is given explicitly by $H_{\alpha}(\pi(\mathbf{x}))=\frac{1}{1-\alpha}\ln(\sum_{j=1}^p\pi_{j}(\mathbf{x})^\alpha)$, and the cases of $\alpha\in\{0,1,\infty\}$ are defined by evaluating limits, with $H_1$ being the ordinary Shannon entropy. Then, for $\mathbf{x}\neq \mathbf{0}$ and $\alpha\notin\{0,1,\infty\}$, we have the measure of block sparsity written conveniently in terms of mixed $\ell_2/\ell_\alpha$ norm as $$
k_\alpha(\mathbf{x})=\left(\frac{\lVert\mathbf{x}\rVert_{2,\alpha}}{\lVert\mathbf{x}\rVert_{2,1}}\right)^{\frac{\alpha}{1-\alpha}},
$$
where the mixed $\ell_2/\ell_\alpha$ norm $\lVert\mathbf{x}\rVert_{2,\alpha}=\left(\sum_{j=1}^{p}\lVert\mathbf{x}[j]\rVert_2^{\alpha}\right)^{1/\alpha}$ for $\alpha>0$. The cases of $\alpha\in\{0,1,\infty\}$ are evaluated as limits: $k_0(\mathbf{x})=\lim\limits_{\alpha\rightarrow 0}k_{\alpha}(\mathbf{x})=\lVert\mathbf{x}\rVert_{2,0}$, $k_1(\mathbf{x})=\lim\limits_{\alpha\rightarrow 1}k_{\alpha}(\mathbf{x})=\exp(H_1(\pi(\mathbf{x})))$, and $k_\infty(\mathbf{x})=\lim\limits_{\alpha\rightarrow \infty}k_{\alpha}(\mathbf{x})=\frac{\lVert\mathbf{x}\rVert_{2,1}}{\lVert\mathbf{x}\rVert_{2,\infty}}$, where $\lVert\mathbf{x}\rVert_{2,\infty}=\max\limits_{1\leq j\leq p}\lVert\mathbf{x}[j]\rVert_{2}$. When the block size $d$ equals 1, our block sparsity measure $k_{\alpha}(\mathbf{x})$ reduces to the nonblock sparsity measure $s_\alpha(\mathbf{x})=\left(\frac{\lVert\mathbf{x}\rVert_{\alpha}}{\lVert\mathbf{x}\rVert_{1}}\right)^{\frac{\alpha}{1-\alpha}}$ given by \cite{l2}.

The fact that $k_{\alpha}(\mathbf{x})$ ($\alpha=2$) is a sensible measure of the block sparsity for non-idealized signals is illustrated in Figure \ref{fig_1}. In the case that $\mathbf{x}$ has $k$ large blocks and $p-k$ small blocks, we have $\lVert\mathbf{x}\rVert_{2,0}=p$, whereas $k_{2}(\mathbf{x})\approx k$.

In addition, the quantity $k_{\alpha}(\mathbf{x})$ has some important properties similar as $s_{\alpha}(\mathbf{x})$.
\begin{itemize}
\item Continuity:\,Unlike the mixed $\ell_2/\ell_0$ norm, the function $k_{\alpha}(\cdot)$ is continuous on $\mathbb{R}^N\setminus 0$ for all $\alpha>0$. Thus, it is stable with respective to small perturbations of the signal.
\item Range equal to $[0,p]$:\,For all $\mathbf{x}\in\mathbb{R}^N$ given as (\ref{signal}) and all $\alpha\in [0,\infty]$, we have $$
0\leq k_{\alpha}(\mathbf{x})\leq p.
$$
\item Scale-invariance:\,For all $c\neq 0$, it holds that $k_{\alpha}(c\mathbf{x})=k_{\alpha}(\mathbf{x})$. Scale-invariance encodes the idea that block sparsity should also be based on relative (rather than absolute) magnitudes of the entries of the signal as the sparsity.
\item Non-increasing in $\alpha$:\,For any $\alpha'\geq\alpha\geq 0$, we have $$
k_\infty(\mathbf{x})=\frac{\lVert\mathbf{x}\rVert_{2,1}}{\lVert\mathbf{x}\rVert_{2,\infty}}\leq k_{\alpha'}(\mathbf{x})\leq k_{\alpha}(\mathbf{x})\leq k_{0}(\mathbf{x})=\lVert\mathbf{x}\rVert_{2,0}.
$$
\end{itemize}
\begin{figure}
	\centering
	\includegraphics[width=0.8\textwidth,height=0.5\textheight]{vectorplot.jpeg}
	\caption{Three vectors (red, green, blue) in $\mathbb{R}^{100}$ are plotted with the $\ell_2$ norm of blocks in decreasing order. We set $d=5$ and compare the values $k_2(\mathbf{x})$ with $\lVert\mathbf{x}\rVert_{2,0}$.}
	\label{fig_1}
\end{figure}


\subsection{Recovery results in terms of $k_2(\mathbf{x})$}

Before presenting the estimation procedure for the $\lVert\mathbf{x}\rVert_{2,\alpha}^{\alpha}$ and $k_{\alpha}(\mathbf{x})$ with $\alpha\in(0,2]$, we give the block sparse signal recovery results in terms of $k_2(\mathbf{x})$ by using mixed $\ell_2/\ell_1$-norm optimization algorithm.

To recover the block sparse signal in CS model (\ref{1.1}), we use the following mixed $\ell_2/\ell_1$-norm optimization algorithm proposed in \cite{ekb,em}: \begin{align}
\widehat{\mathbf{x}}=\arg\min_{\mathbf{e}\in\mathbb{R}^N}\lVert\mathbf{e}\rVert_{2,1},\,\,\,\text{subject to}\,\,\,\lVert\mathbf{y}-A\mathbf{e}\rVert_2\leq\delta,
\label{2.1}
\end{align}
where $\delta\geq 0$ is a upper bound on the noise level $\lVert\boldsymbol{\varepsilon}\rVert_2$. Then, we have the following result concerning on the robust recovery for block sparse signals.\\

\noindent
{\bf Lemma 1}\,(\cite{em}). Let $\mathbf{y}=A\mathbf{x}+\boldsymbol{\varepsilon}$ be noisy measurements of a vector $\mathbf{x}$ and fix a number $k\in\{1,\cdots,p\}$. Let $\mathbf{x}^k$ denote the best block $k$-sparse approximation of $\mathbf{x}$, such that $\mathbf{x}^k$ is block $k$-sparse and minimizes $\lVert\mathbf{x}-\mathbf{f}\rVert_{2,1}$ over all the block $k$-sparse vectors $\mathbf{f}$, and let $\widehat{\mathbf{x}}$ be a solution to (\ref{2.1}), a random Gaussian matrix $A$ of size $m\times N$ with entries $A_{ij}\sim N(0,\frac{1}{m})$, and block sparse signals over $\mathcal{I}=\{d_1=d,\cdots,d_p=d\}$, where $N=pd$ for some integer $p$. Then, there are constants $c_0,c_1,c_2,c_3>0$, such that the following statement is true. If $m\geq c_0 k\ln(eN/kd)$, then with probability at least $1-2\exp(-c_1 m)$,  we have\begin{align}
\frac{\lVert\widehat{\mathbf{x}}-\mathbf{x}\rVert_2}{\lVert\mathbf{x}\rVert_2}\leq c_2\frac{\lVert\mathbf{x}-\mathbf{x}^k\rVert_{2,1}}{\sqrt{k}\lVert\mathbf{x}\rVert_2}+c_3\frac{\delta}{\lVert\mathbf{x}\rVert_2}.\label{2.2}
\end{align}

\noindent
{\bf Remark 1.} Note that the first term in (\ref{2.2}) is a result of the fact that $\mathbf{x}$ is not exactly block $k$-sparse, while the second term quantifies the recovery error due to the measurement noise. When the block size $d=1$, this Lemma goes to the conventional CS result for sparse signals. Explicit use of block sparsity reduces the required number of measurements from $\mathrm{O}(kd\ln(eN/kd))$ to $\mathrm{O}(k\ln(eN/kd))$ by $d$ times.\\

The limitation of the previous bound is that the ratio term $\frac{\lVert\mathbf{x}-\mathbf{x}^k\rVert_{2,1}}{\sqrt{k}\lVert\mathbf{x}\rVert_2}$ is typically unknown. Thus, it is not clear how large $m$ should be chosen to guarantee that the relative $\ell_2$-error $\frac{\lVert\widehat{\mathbf{x}}-\mathbf{x}\rVert_2}{\lVert\mathbf{x}\rVert_2}$ã€€is small with high probability. Next, we present an upper bound of the relative $\ell_2$-error by an explicit function of $m$ and the new proposed block sparsity measure $k_2(\mathbf{x})$, which is estimable. The following result is an extension of Proposition 1 in \cite{l2}. Its proof is left to Appendix.\\

\noindent
{\bf Lemma 2.}  Let $\mathbf{y}=A\mathbf{x}+\boldsymbol{\varepsilon}$ be noisy measurements of a vector $\mathbf{x}$, and let $\widehat{\mathbf{x}}$ be a solution to (\ref{2.1}), a random Gaussian matrix $A$ of size $m\times N$ with entries $A_{ij}\sim N(0,\frac{1}{m})$, and block sparse signals over $\mathcal{I}=\{d_1=d,\cdots,d_n=d\}$, where $N=pd$ for some integer $p$. Then, there are constants $\kappa_0,\kappa_1,\kappa_2,\kappa_3>0$, such that the following statement is true. If $m$ and $N$ satisfy $\kappa_0\ln(\kappa_0\frac{eN}{m})\leq m\leq N$, then with probability at least $1-2\exp(-\kappa_1 m)$, we have \begin{align}
\frac{\lVert\widehat{\mathbf{x}}-\mathbf{x}\rVert_2}{\lVert\mathbf{x}\rVert_2}\leq \kappa_2\sqrt{\frac{k_2(\mathbf{x})d\ln(\frac{eN}{m})}{m}}+\kappa_3\frac{\delta}{\lVert\mathbf{x}\rVert_2}.
\end{align}\\

\section{Estimation Method for $\lVert\mathbf{x}\rVert_{2,\alpha}^{\alpha}$ and $k_{\alpha}(\mathbf{x})$}

In this section, we mainly focus on the estimation of $k_{\alpha}(\mathbf{x})$ with $\alpha\in(0,2]$. There are two reasons to consider this interval. One is that small $\alpha$ is usually a better block sparsity measure than very large $\alpha$ in applications. And we can approximate $\lVert \mathbf{x}\rVert_{2,0}$ with very small $\alpha$ as will be shown later. The other reason is that our estimation method relies on the $\alpha$-stable distribution, which requires $\alpha$ to lie in $(0,2]$. The core idea to obtain the estimators for $\lVert\mathbf{x}\rVert_{2,\alpha}^{\alpha}$ and $k_{\alpha}(\mathbf{x})$ with $\alpha\in(0,2]$ is using random projections. Contrast to the conventional sparsity estimation by using projections with univariate symmetric $\alpha$-stable random variables \cite{l2,z}, we use projections with the multivariate centered isotropic symmetric $\alpha$-stable random vectors \cite{n,p} for the block sparsity estimation.

\subsection{Multivariate Isotropic Stable Distribution}
 We firstly give the definition of the multivariate centered isotropic symmetric $\alpha$-stable distribution.\\

\noindent
{\bf Definition 1.} For $d\geq 1$, a $d$-dimensional random vector $\mathbf{v}$ has a centered isotropic symmetric $\alpha$-stable distribution if there are constants $\gamma>0$ and $\alpha\in(0,2]$ such that its characteristic function has the form \begin{align}
E[\exp(i\mathbf{u}^{T}\mathbf{v})]=\exp(-\gamma^{\alpha}\lVert\mathbf{u}\rVert_2^\alpha),\,\,\,\text{for all $\mathbf{u}\in\mathbb{R}^d$}.
\end{align}
We denote the distribution by $\mathbf{v}\sim S(d,\alpha,\gamma)$, and $\gamma$ is referred to as the scale parameter.\\

\noindent
{\bf Remark 2.} The most well-known example of multivariate isotropic symmetric stable distribution is the case of $\alpha=2$ (Multivariate Independent Gaussian Distribution), and in this case, the components of the Multivariate Gaussian random vector are independent. Another case is $\alpha=1$ (Multivariate Spherical Symmetric Cauchy Distribution \cite{p}), unlike Multivariate Independent Gaussian case, the components of Multivariate Spherical Symmetric Cauchy are uncorrelated, but dependent. The perspective and contour plots for the densities of these two cases are illustrated in Figure \ref{fig_2}. The multivariate centered isotropic symmetric $\alpha$-stable random vector is a direct extension of the univariate symmetric $\alpha$-stable random variable, which is the special case when the dimension parameter $d=1$. In applications, to simulation a $d$-dimensional random vector $\mathbf{v}$ from the multivariate centered isotropic symmetric $\alpha$-stable distribution $S(d,\alpha,\gamma)$, we can adopt the fact that $\mathbf{v}=D^{1/2}\mathbf{q}$, where $D\sim \tilde{S}(1,\alpha/2,2\gamma^2[\cos(\pi\alpha/4)]^{2/\alpha})$ is a independent univariate positive
$(\alpha/2)$-stable random variable and $\mathbf{q}\sim N(0,I_d)$ is a standard $d$-dimensional Gaussian random vector, see \cite{n} for more details.
\begin{figure}[!t]
\centering
\includegraphics[width=0.8\textwidth,height=0.4\textheight]{multidensity.jpeg}
\caption{Perspective and Contour Plots for the Bivariate Centered Isotropic Symmetric Stable Densities. The top ones are for the Bivariate Independent Gaussian Distribution, while the bottom ones are for the Bivariate Spherical Symmetric Cauchy Distribution.}
\label{fig_2}
\end{figure}

\subsection{Estimation Procedure}

By random projections using i.i.d multivariate centered isotropic symmetric $\alpha$-stable random vectors, we can obtain the estimators for $\lVert\mathbf{x}\rVert_{2,\alpha}^{\alpha}$ and $k_{\alpha}(\mathbf{x})$ with $\alpha\in(0,2]$, which is presented as follows.

We estimate the $\lVert\mathbf{x}\rVert_{2,\alpha}^{\alpha}$ by using the random linear projection measurements: \begin{align}
y_i=\langle \mathbf{a}_i,\mathbf{x}\rangle+\sigma\varepsilon_i, \,\,\,i=1,2,\cdots,n,
\end{align}
where $\mathbf{a}_i\in\mathbb{R}^N$ is i.i.d random vector, and $\mathbf{a}_i=(\mathbf{a}_{i1}^T,\cdots,\mathbf{a}_{ip}^T)^T$ with $\mathbf{a}_{ij},j\in\{1,\cdots,p\}$ i.i.d drawn from $S(d,\alpha,\gamma)$. The noise term $\varepsilon_i$ are i.i.d from a distribution $F_0$ and assume its characteristic function is $\varphi_0$, the sets $\{\varepsilon_1,\cdots,\varepsilon_n\}$ and $\{\mathbf{a}_1,\cdots,\mathbf{a}_n\}$ are independent. $\{\varepsilon_i, i=1,2,\cdots,n\}$ are assumed to be symmetric about $0$, with $0<E|\varepsilon_1|<\infty$, but they may have infinite variance. The assumption of symmetry is only for convenience, it was explained how to drop it in Section III-B.e of \cite{l2}. A minor technical condition we place on $F_0$ is that the roots of its characteristic function $\varphi_0$ are isolated (i.e. no limit points). This
condition is satisfied by many families of distributions, such
as Gaussian, Student's $t$, Laplace, uniform$[a, b]$, and stable
laws. And we assume that the noise scale parameter $\sigma\geq0$ and the distribution $F_0$ are treated as being known for simplicity.\\

Since our work involves different choices of $\alpha$, we will write $\gamma_\alpha$ instead of $\gamma$. Then the link with the norm  $\lVert\mathbf{x}\rVert_{2,\alpha}^{\alpha}$ hinges on the following basic lemma.\\

\noindent
{\bf Lemma 3.} Let $\mathbf{x}=(\mathbf{x}[1]^T,\cdots,\mathbf{x}[p]^T)^T\in\mathbb{R}^N$ be fixed, and suppose $\mathbf{a}_1=(\mathbf{a}_{11}^T,\cdots,\mathbf{a}_{1p}^T)^T$ with $\mathbf{a}_{1j}, j\in\{1,\cdots,p\}$ i.i.d drawn from $S(d,\alpha,\gamma_\alpha)$ with $\alpha\in(0,2]$
and $\gamma_\alpha>0$. Then, the random variable $\langle \mathbf{a}_1,\mathbf{x}\rangle$ has the distribution $S(1,\alpha,\gamma_\alpha\lVert\mathbf{x}\rVert_{2,\alpha})$.\\

\noindent
{\bf Remark 3.} When $\mathbf{x}=(\mathbf{x}[1]^T,\cdots,\mathbf{x}[p]^T)^T\in\mathbb{R}^N$ has different block lengths which are $\{d_1,d_2,\cdots,d_p\}$ respectively, then we need choose the projection random vector $\mathbf{a}_1=(\mathbf{a}_{11}^T,\cdots,\mathbf{a}_{1p}^T)^T$ with $\mathbf{a}_{1j}, j\in\{1,\cdots,p\}$ i.i.d drawn from $S(d_j,\alpha,\gamma_\alpha)$. In that case, the conclusion in our Lemma and all the results in the followings still hold without any modifications. This Lemma is an extension of Lemma 1 in \cite{l2} from i.i.d univariate symmetric $\alpha$-stable projection to i.i.d multivariate isotropic symmetric $\alpha$-stable projection. \\

By using this result, if we generate a set of i.i.d measurement random vectors $\{\mathbf{a}_1,\cdots,\mathbf{a}_n\}$ as given above and let $\tilde{y}_i=\langle\mathbf{a}_i,\mathbf{x}\rangle$, then $\{\tilde{y}_1,\cdots,\tilde{y}_n\}$ is an i.i.d sample from the distribution $S(1,\alpha,\gamma_\alpha\lVert\mathbf{x}\rVert_{2,\alpha})$. Hence, in the special case of random linear measurements without noise, estimating the norm $\lVert\mathbf{x}\rVert_{2,\alpha}^{\alpha}$ reduces to estimating the scale parameter of a univariate stable distribution from an i.i.d sample.\\

Next, we present the estimation procedure by using the characteristic function method \cite{l2,mh,mhl}. We use two separate sets of measurements to estimate $\widehat{\lVert\mathbf{x}\rVert_{2,1}}$ and
$\widehat{\lVert\mathbf{x}\rVert_{2,\alpha}^{\alpha}}$. The respective sample sizes of each measurements are denoted by $n_1$ and $n_{\alpha}$.
To unify the discussion, we will describe just the procedure to obtain $\widehat{\lVert\mathbf{x}\rVert_{2,\alpha}^{\alpha}}$ for any $\alpha\in(0,2]$, since $\alpha=1$ is a special case. The two estimators are combined to obtain the estimator for $k_\alpha(\mathbf{x})$, which follows as:
\begin{align}
\hat{k}_{\alpha}(\mathbf{x})=\frac{\left(\widehat{\lVert\mathbf{x}\rVert_{2,\alpha}^{\alpha}}\right)^{\frac{1}{1-\alpha}}}
{\left(\widehat{\lVert\mathbf{x}\rVert_{2,1}}\right)^{\frac{\alpha}{1-\alpha}}}.
\end{align}
\\
In fact, the characteristic function of $y_i$ has the form: \begin{align}
\Psi(t)=E[\exp(ity_i)]=\exp(-\gamma_\alpha^{\alpha}\lVert\mathbf{x}\rVert_{2,\alpha}^{\alpha}|t|^{\alpha})\cdot\varphi_0(\sigma t),
\end{align}
where $t\in\mathbb{R}$. Then, we have
$$\lVert\mathbf{x}\rVert_{2,\alpha}^{\alpha}=-\frac{1}{\gamma_\alpha^{\alpha}|t|^\alpha}\log \left|\mathrm{Re}\left(\frac{\Psi(t)}{\varphi_0(\sigma t)}\right)\right|.
$$
By using the empirical characteristic function $$\hat{\Psi}_{n_\alpha}(t)=\frac{1}{n_\alpha}\sum\limits_{i=1}^{n_\alpha}e^{ity_i}$$
to estimate $\Psi(t)$, we obtain the estimator of $\lVert\mathbf{x}\rVert_{2,\alpha}^{\alpha}$ given by \begin{align}
\widehat{\lVert\mathbf{x}\rVert_{2,\alpha}^{\alpha}}=:\hat{v}_{\alpha}(t)=-\frac{1}{\gamma_\alpha^{\alpha}|t|^\alpha}\log \left|\mathrm{Re}\left(\frac{\hat{\Psi}_{n_\alpha}(t)}{\varphi_0(\sigma t)}\right)\right|,
\end{align}
when $t\neq 0$ and $\varphi_0(\sigma t)\neq 0$. \\

\subsection{Asymptotic Properties}
Then, similar to the Theorem 2 in \cite{l2}, we have the uniform central limit theorem (CLT) \cite{du} for $\hat{v}_\alpha(t)$. Before presenting the result, we introduce the noise-to-signal ratio constant $$
\rho_\alpha=\frac{\sigma}{\gamma_\alpha\lVert\mathbf{x}\rVert_{2,\alpha}}.
$$\\
\noindent
{\bf Theorem 1} (Uniform CLT for Mixed $\ell_2/\ell_{\alpha}$ Norm Estimator). Let $\alpha\in(0,2]$. Let $\hat{t}$ be any function of $\{y_1,\cdots,y_{n_\alpha}\}$ that satisfies \begin{align}
\gamma_\alpha\hat{t}\lVert \mathbf{x}\rVert_{2,\alpha}\overset{p}\longrightarrow c_\alpha,
\end{align}
as $(n_\alpha,N)\rightarrow \infty$ for some finite constant $c_\alpha\neq 0$ and $\varphi_0(\rho_\alpha c_\alpha)\neq 0$. Then, we have \begin{align}
\sqrt{n_\alpha}\left(\frac{\hat{v}_{\alpha}(\hat{t})}{\lVert\mathbf{x}\rVert_{2,\alpha}^{\alpha}}-1\right)\overset{d}\longrightarrow N(0,\theta_\alpha(c_\alpha,\rho_\alpha))
\end{align}
as $(n_{\alpha},N)\rightarrow\infty$, where the limiting variance $\theta_\alpha(c_\alpha,\rho_\alpha)$ is strictly positive and defined according to the formula \begin{align}
\theta_\alpha(c_\alpha,\rho_\alpha)=\frac{1}{|c_\alpha|^{2\alpha}}\Big(\frac{\exp(2|c_\alpha|^{\alpha})}{2\varphi_0(\rho_\alpha
	|c_\alpha|)^2}+\frac{\varphi_0(2\rho_\alpha|c_\alpha|)}{2\varphi_0(\rho_\alpha|c_\alpha|)^2}\exp((2-2^\alpha)|c_\alpha|^\alpha)-1\Big).
\end{align}
\\

For simplicity, we use the $\hat{t}_{\mathrm{pilot}}$ instead of the optimal $\hat{t}_{\mathrm{opt}}$ in \cite{l2}. Since it is simple to implement, and still gives a reasonably good estimator. To describe the pilot value, let $\eta_0>0$ be any number such that $\varphi_0(\eta)>\frac{1}{2}$ for all $\eta\in[0,\eta_0]$ (which exists for
any characteristic function). Also, we define the median absolute
deviation statistic $\hat{m}_\alpha=\mathrm{median}\{|y_1|,\cdots,|y_{n_{\alpha}}|\}$ and define $\hat{t}_{\mathrm{pilot}}=\min\{\frac{1}{\hat{m}_\alpha},\frac{\eta_0}{\sigma}\}$. Then we obtain the consistent estimator $\hat{c}_\alpha=\gamma_\alpha\hat{t}_{\mathrm{pilot}}[\hat{v}_{\alpha}(\hat{t}_{\mathrm{pilot}})]^{1/\alpha}$ of a constant $c_\alpha=\min\left(\frac{1}{\mathrm{median}(|S_1+\rho_\alpha\varepsilon_1|)},\frac{\eta_0}{\rho_\alpha}\right)$, where random variable $S_1\sim S(1,1,1)$ (see the Proposition 3 in \cite{l2}), and the consistent estimator of $\rho_\alpha$, $\hat{\rho}_\alpha=\frac{\sigma}{\gamma_\alpha[\hat{v}_{\alpha}(\hat{t}_{\mathrm{pilot}})]^{1/\alpha}}$. Therefore, the consistent estimator of the limiting variance $\theta_\alpha(c_\alpha,\rho_\alpha)$ is $\theta_\alpha(\hat{c}_\alpha,\hat{\rho}_\alpha)$. Thus, we immediately have the following corollary to obtain the confidence intervals for $\lVert\mathbf{x}\rVert_{2,\alpha}^{\alpha}$.\\

\noindent
{\bf Corollary 1} (Confidence Interval for $\lVert\mathbf{x}\rVert_{2,\alpha}^{\alpha}$). Under the conditions of Theorem 1, as $(n_\alpha, N)\rightarrow \infty$, we have \begin{align}
\sqrt{\frac{n_\alpha}{\theta_\alpha(\hat{c}_\alpha,\hat{\rho}_\alpha)}}\left(\frac{\hat{v}_{\alpha}(\hat{t}_{\mathrm{pilot}})}{\lVert\mathbf{x}\rVert_{2,\alpha}^{\alpha}}-1\right)\overset{d}\longrightarrow N(0,1).
\end{align}
Then, it follows that the asymptotic $1-\beta$ confidence interval for $\lVert\mathbf{x}\rVert_{2,\alpha}^{\alpha}$ is \begin{align}
\bigg[\Big(1-\sqrt{\frac{\theta_\alpha(\hat{c}_\alpha,\hat{\rho}_\alpha)}{n_\alpha}}z_{1-\beta/2}\Big)\hat{v}_{\alpha}(\hat{t}_{\mathrm{pilot}}),
\Big(1+\sqrt{\frac{\theta_\alpha(\hat{c}_\alpha,\hat{\rho}_\alpha)}{n_\alpha}}z_{1-\beta/2}\Big)\hat{v}_{\alpha}(\hat{t}_{\mathrm{pilot}})\bigg],
\end{align}
where $z_{1-\beta/2}$ is the $(1-\beta/2)$-quantile of the standard normal distribution.\\

As a consequence, we can obtain a CLT and a confidence interval for $\hat{k}_\alpha(\mathbf{x})$ by combining the estimators $\hat{v}_{\alpha}$ and $\hat{v}_1$ with their respective $\hat{t}_{\mathrm{pilot}}$. Before we present the main result, for each $\alpha\in(0,2]\setminus\{1\}$, we assume that there is a constant $\bar{\pi}_\alpha\in (0,1)$, such that $(n_1,n_\alpha,N)\rightarrow\infty$, $$
\pi_\alpha:=\frac{n_\alpha}{n_1+n_\alpha}=\bar{\pi}_\alpha+o(n_\alpha^{-1/2}).
$$

\noindent
{\bf Theorem 2} (Asymptotic Property for $\hat{k}_{\alpha}(\mathbf{x})$). Let $\alpha\in(0,2]\setminus\{1\}$ and the conditions of Theorem 1 hold. Then as $(n_1,n_\alpha,N)\rightarrow\infty$, \begin{align}
\sqrt{\frac{n_1+n_\alpha}{\hat{w}_\alpha}}\left(\frac{\hat{k}_\alpha(\mathbf{x})}{k_{\alpha}(\mathbf{x})}-1\right)\overset{d}\longrightarrow N(0,1),
\end{align}
where $\hat{w}_\alpha=\frac{\theta_\alpha(\hat{c}_\alpha,\hat{\rho}_{\alpha})}{\pi_\alpha}(\frac{1}{1-\alpha})^2+\frac{\theta_1(\hat{c}_1,\hat{\rho}_1)}{1-\pi_\alpha}(\frac{\alpha}{1-\alpha})^2$. And consequently, the asymptotic $1-\beta$ confidence interval for $k_{\alpha}(\mathbf{x})$ is \begin{align}
\bigg[\Big(1-\sqrt{\frac{\hat{w}_\alpha}{n_1+n_\alpha}}z_{1-\beta/2}\Big)\hat{k}_\alpha(\mathbf{x}),\Big(1+\sqrt{\frac{\hat{w}_\alpha}{n_1+n_\alpha}}z_{1-\beta/2}\Big)\hat{k}_\alpha(\mathbf{x})\bigg],
\end{align}
where $z_{1-\beta/2}$ is the $(1-\beta/2)$-quantile of the standard normal distribution.\\

\subsection{Estimating $\lVert\mathbf{x}\rVert_{2,0}$ with $\hat{k}_{\alpha}(\mathbf{x})$ and Small $\alpha$}

Next, we present the approximation of $\hat{k}_\alpha(\mathbf{x})$ to $\lVert\mathbf{x}\rVert_{2,0}$ when $\alpha$ is close to $0$. To state the theorem, we define the block dynamic range of a non-zero signal $\mathbf{x}\in\mathbb{R}^N$ given in (\ref{signal}) as \begin{align}
\mathrm{BDNR}(\mathbf{x})=\frac{\lVert\mathbf{x}\rVert_{2,\infty}}{|\mathbf{x}|_{2,\min}},
\end{align}
where $|\mathbf{x}|_{2,\min}$ is the smallest $\ell_2$ norm of the non-zero block of $\mathbf{x}$, i.e. $|\mathbf{x}|_{2,\min}=\min\{\lVert\mathbf{x}[j]\rVert_2:\mathbf{x}[j]\neq \mathbf{0}, j=1,\cdots,p\}$. When the block size $d=1$, our $\mathrm{BDNR}(\mathbf{x})$ goes to the $\mathrm{DNR}(\mathbf{x})$ defined in \cite{l2}. The following result involves no randomness and is applicable to any estimator $\tilde{k}_{\alpha}(\mathbf{x})$. \\

\noindent
{\bf Theorem 3.} Let $\alpha\in(0,1)$, $\mathbf{x}\in\mathbb{R}^N$ is non-zero signal given in (\ref{signal}), and let $\tilde{k}_{\alpha}(\mathbf{x})$ be any real number. Then, we have \begin{align}
\left|\frac{\tilde{k}_{\alpha}(\mathbf{x})}{\lVert\mathbf{x}\rVert_{2,0}}-1\right|\leq \left|\frac{\tilde{k}_{\alpha}(\mathbf{x})}{k_{\alpha}(\mathbf{x})}-1\right|+\frac{\alpha}{1-\alpha}\Big(\ln(\mathrm{BDNR}(\mathbf{x}))+\alpha\ln(\lVert\mathbf{x}\rVert_{2,0})\Big). \label{th3}
\end{align}
\\
\noindent
{\bf Remark 4.} This theorem is a direct extension of Proposition 5 in \cite{l2}, which corresponds the special case of the block size $d=1$. When choosing $\tilde{k}_{\alpha}(\mathbf{x})$ to be the proposed estimator $\hat{k}_{\alpha}(\mathbf{x})$, the first term in (\ref{th3}) is already controlled by Theorem 2. As pointed out in \cite{l2}, the second term is the approximation error that improves for smaller choices of $\alpha$. When the $\ell_2$ norms of the signal blocks are similar, the quantity of $\ln(\mathrm{BDNR}(\mathbf{x}))$ will not be too large. In this case, the bound behaves well and estimating $\lVert\mathbf{x}\rVert_{2,0}$ is of interest. On the other hand, if the $\ell_2$ norms of the signal blocks are very different, that is $\ln(\mathrm{BDNR}(\mathbf{x}))$ is large, then $\lVert\mathbf{x}\rVert_{2,0}$ may not be the best measure of block sparsity to estimate.
\section{Simulation}

In this section, we conduct some simulations to illustrate our theoretical results. We focus on choosing $\alpha=2$, that is we use $\hat{k}_2(\mathbf{x})$ to estimate the block sparsity measure $k_2(\mathbf{x})$. When estimating $k_2(\mathbf{x})$, we requires a set of $n_1$ measurements by using multivariate isotropic symmetric cauchy projection, and a set of $n_2$ by using multivariate isotropic symmetric normal projection.  We generated the samples $\mathbf{y}_1\in\mathbb{R}^{n_1}$ and $\mathbf{y}_2\in\mathbb{R}^{n_2}$ according to \begin{align}
\mathbf{y}_1=A_1\mathbf{x}+\sigma\boldsymbol{\varepsilon}_1\,\,\,\text{and}\,\,\,\mathbf{y}_2=A_2\mathbf{x}+\sigma\boldsymbol{\varepsilon}_2,
\end{align}
where $A_1=(\mathbf{a}_1,\cdots,\mathbf{a}_{n_1})\in\mathbb{R}^{n_1\times N}$, with  $\mathbf{a}_i\in\mathbb{R}^N$ is i.i.d random vector, and $\mathbf{a}_i=(\mathbf{a}_{i1}^T,\cdots,\mathbf{a}_{ip}^T)^T$ with $\mathbf{a}_{ij}, j\in\{1,\cdots,p\}$ i.i.d drawn from $S(d,1,\gamma_1)$, we let $\gamma_1=1$. Similarly,  $A_2=(\mathbf{b}_1,\cdots,\mathbf{b}_{n_2})\in\mathbb{R}^{n_2\times N}$, with  $\mathbf{b}_i\in\mathbb{R}^N$ is i.i.d random vector, and $\mathbf{b}_i=(\mathbf{b}_{i1}^T,\cdots,\mathbf{b}_{ip}^T)^T$ with $\mathbf{b}_{ij}, j\in\{1,\cdots,p\}$ i.i.d drawn from $S(d,2,\gamma_2)$, we let $\gamma_2=\frac{\sqrt{2}}{2}$. The noise terms $\boldsymbol{\varepsilon}_1$ and $\boldsymbol{\varepsilon}_2$ are generated with i.i.d entries from a standard normal distribution. We considered a sequence of pairs for the sample sizes $(n_1,n_2)=(50,50),(100,100),(200,200),\cdots,(500,500)$. For each experiments, we replicates 200 times. Consequently, we have 200 realizations of $\hat{k}_2(\mathbf{x})$ for each $(n_1,n_2)$. We then averaged the quantity $|\frac{\hat{k}_2(\mathbf{x})}{k_2(\mathbf{x})}-1|$ as an approximation of $E|\frac{\hat{k}_2(\mathbf{x})}{k_2(\mathbf{x})}-1|$.

\subsection{Exactly Block Sparse Case}
First, we let our signal $\mathbf{x}$ be a very simple exactly block sparse vector, that is
$$
\mathbf{x}=(\frac{1}{\sqrt{10}}\mathbf{1}_{10}^T, \mathbf{0}_{N-10}^T)^T,
$$
where $\mathbf{1}_q$ is a vector of length $q$ with entries all ones, $\mathbf{0}_q$ is the zero vector. Then it is obvious that $\lVert\mathbf{x}\rVert_{2,2}=\lVert\mathbf{x}\rVert_{2}=1$, while $\lVert\mathbf{x}\rVert_{2,1}$ and $k_2(\mathbf{x})$ depend on the block size $d$ that we choose.\\

a) We set $\eta_0=1$. The simulation is conducted under several choices of the parameters, $N$, $d$ and $\sigma$--with each parameter corresponding to a separate plot in Figure \ref{fig_3}. The signal dimension $N$ is set to 1000, except in the top left plot, where $N=20,100,500,1000$. We set $d=5$ in all cases, except in top right plot, where $d=1,2,5,10$, corresponding the real value $k_2(\mathbf{x})=10,5,2,1$, which also equals $\lVert\mathbf{x}\rVert_{2,0}$, the exact block sparsity level of our signal $\mathbf{x}$ with the block size to be $d$. In turn, $\sigma=0.1$ in all cases, except in the bottom plot where $\sigma=0,0.1,0.2,0.3$. In all three plots, the theoretical curves are computed in the following way. From Theorem 2, we have $|\frac{\hat{k}_2(\mathbf{x})}{k_2(\mathbf{x})}-1|\approx \frac{\sqrt{\omega_2}}{\sqrt{n_1+n_2}}|Z|$, where $Z$ is a standard Gaussian random variable, and we set $\omega_2=\frac{\theta_2(c_2,\rho_2)}{\pi_2}+4\frac{\theta_1(c_1,\rho_1)}{1-\pi_2}$. Since $E|Z|=\sqrt{2/\pi}$, the theoretical curves are simply $\frac{\sqrt{2\omega_2/\pi}}{\sqrt{n_1+n_2}}$, as a function of $n_1+n_2$. Note that $\omega_2$ depends on $\sigma$ and $d$, which is why there is only one theoretical curve in top left plot for error dependence on $N$.

From Figure \ref{fig_3}, we can see that the black theoretical curves agree well with the colored empirical ones. In addition, the averaged relative error has no observable dependence on $N$ or $d$ (when $\sigma$ is fixed), as expected from Theorem 2, and the dependence on the $\sigma$ is mild. \\

b) Next, a simulation study is conducted to illustrate the asymptotic normality of our estimators in Corollary 1 and Theorem 2. We have 1000 replications for these experiments, that is we have 1000 samples of the standardized statistics $\mathrm{res1}=\sqrt{\frac{n_1}{\theta_1(\hat{c}_1,\hat{\rho}_1)}}\left(\frac{\hat{v}_{1}(\hat{t}_{\mathrm{pilot}})}{\lVert\mathbf{x}\rVert_{2,1}}-1\right)$, $\mathrm{res2}=\sqrt{\frac{n_2}{\theta_2(\hat{c}_2,\hat{\rho}_2)}}\left(\frac{\hat{v}_{2}(\hat{t}_{\mathrm{pilot}})}{\lVert\mathbf{x}\rVert_{2,2}^{2}}-1\right)$ and $\mathrm{res}=\sqrt{\frac{n_1+n_2}{\hat{w}_2}}\left(\frac{\hat{k}_2(\mathbf{x})}{k_{2}(\mathbf{x})}-1\right)$. We consider four cases, with $(n_1,n_2)=(500,500), (1000,1000)$ and the noise is standard normal and $t(2)$ which has infinite variance. In all the cases, we set $N=1000$, $d=5$, and $\sigma=0.1$.

Figure \ref{fig_4} shows that the density curves of the standardized statistics all are very close to the standard normal density curve, which verified our theoretical results. And these results hold even when the noise distribution is heavy-tailed. Comparing the four plots, we see that it leads to improve the normal approximation by increasing the sample size $n_1+n_2$ and reducing the noise variance.

\begin{figure}[!t]
	\centering
	\includegraphics[width=0.7\textwidth,height=0.7\textheight]{dependplot.jpeg}
	\caption{The averaged relative error $|\frac{\hat{k}_2(\mathbf{x})}{k_2(\mathbf{x})}-1|$ depending on $N$, $d$ and $\sigma$ for the exactly block sparse case.}
	\label{fig_3}
\end{figure}


\begin{figure}[!t]
	\centering
	\includegraphics[width=0.8\textwidth]{densityplot.jpeg}
	\caption{The density plots of the stanardized statistics for the exactly block sparse case. The dashed black curve is the standard normal density in all four plots.}
	\label{fig_4}
\end{figure}


\subsection{Nearly Block Sparse Case}

Second, we consider our signal $\mathbf{x}\in\mathbb{R}^N$ to be not exactly block sparse but nearly block sparse, that is the entries of $j$-th block all equal $\frac{c}{\sqrt{d}}\cdot j^{-1}$, with $c$ chosen so that $\lVert \mathbf{x}\rVert_{2,2}=\lVert \mathbf{x}\rVert_{2}=1$. In this case, the $\ell_2$ norm of blocks decays like $j^{-1}$ for $j\in \{1,\cdots,p\}$. With the same settings as in the previous subsection, we obtain the similar simulation results as the exactly block sparse case in Figure \ref{fig_5} and Figure \ref{fig_6}.

\begin{figure}[!t]
	\centering
	\includegraphics[width=0.7\textwidth,height=0.7\textheight]{compressibledependplot.jpeg}
	\caption{The averaged relative error $|\frac{\hat{k}_2(\mathbf{x})}{k_2(\mathbf{x})}-1|$ depending on $N$, $d$ and $\sigma$ for the nearly block sparse case.}
	\label{fig_5}
\end{figure}

\begin{figure}[!t]
	\centering
	\includegraphics[width=0.8\textwidth]{compressibledensityplot.jpeg}
	\caption{The density plots of the stanardized statistics for the nearly block sparse case. The dashed black curve is the standard normal density in all four plots.}
	\label{fig_6}
\end{figure}
\subsection{Estimating $\lVert\mathbf{x}\rVert_{2,0}$ with $\hat{k}_{\alpha}(\mathbf{x})$ and Small $\alpha$}
Third, we consider the estimation of the mixed $\ell_2/\ell_0$ norm $\lVert\mathbf{x}\rVert_{2,0}$ by using $\hat{k}_{\alpha}(\mathbf{x})$ with $\alpha=0.06$. We consider the signals $\mathbf{x}\in\mathbb{R}^N$ of the form $$c'(\underbrace{\frac{1}{\sqrt{d}}\cdots\frac{1}{\sqrt{d}}}_{d}\underbrace{\frac{1/\sqrt{d}}{2}\cdots\frac{1/\sqrt{d}}{2}}_{d}\cdots
\underbrace{\frac{1/\sqrt{d}}{\lVert\mathbf{x}\rVert_{2,0}}\cdots\frac{1/\sqrt{d}}{\lVert\mathbf{x}\rVert_{2,0}}}_{d}0\cdots0)
$$
with $c'$ chosen so that $\lVert\mathbf{x}\rVert_{2,2}=\lVert\mathbf{x}\rVert_2=1$. In this experiment, we set $N=1000, d=5$. To obtain $\hat{k}_{\alpha}(\mathbf{x})$, we generate the samples $\mathbf{y}_1=A_1\mathbf{x}+\sigma\boldsymbol{\varepsilon_1}$ and $\mathbf{y}_{\alpha}=A_{\alpha}\mathbf{x}+\sigma\boldsymbol{\varepsilon_\alpha}$, where $A_1=(\mathbf{a}_1,\cdots,\mathbf{a}_{n_1})\in\mathbb{R}^{n_1\times N}$, with $\mathbf{a}_i\in\mathbb{R}^N$ is i.i.d random vector, and $\mathbf{a}_i=(\mathbf{a}_{i1}^T,\cdots,\mathbf{a}_{ip}^T)^T$ with $\mathbf{a}_{ij}, j\in\{1,\cdots,p\}$ i.i.d drawn from $S(d,1,\gamma_1)$, we let $\gamma_1=1$. Similarly,  $A_\alpha=(\mathbf{h}_1,\cdots,\mathbf{h}_{n_2})\in\mathbb{R}^{n_2\times N}$, with  $\mathbf{h}_i\in\mathbb{R}^N$ is i.i.d random vector, and $\mathbf{h}_i=(\mathbf{h}_{i1}^T,\cdots,\mathbf{h}_{ip}^T)^T$ with $\mathbf{h}_{ij}, j\in\{1,\cdots,p\}$ i.i.d drawn from $S(d,\alpha,\gamma_\alpha)$, we let $\gamma_\alpha=1$. The noise terms $\boldsymbol{\varepsilon_1}$ and $\boldsymbol{\varepsilon_2}$ are generated with i.i.d entries from a standard normal distribution. The noise level was set to $\sigma=0.1$. We considered a sequence of pairs for the sample sizes $(n_1,n_2)=(50,50),(100,100),(200,200),\cdots,(500,500)$. For each experiments, we replicates 200 times. Then, we have 200 realizations of $\hat{k}_\alpha(\mathbf{x})$ for each $(n_1,n_2)$. We varied $\lVert\mathbf{x}\rVert_{2,0}$ and  $\mathrm{BDNR}(\mathbf{x})$, and averaged the quantity $\left|\frac{\hat{k}_\alpha(\mathbf{x})}{\lVert\mathbf{x}\rVert_{2,0}}-1\right|$. Specifically, we considered the four cases $\lVert\mathbf{x}\rVert_{2,0}=\mathrm{BNDR}(\mathbf{x})=10,50,100,200$.

Figure \ref{fig_7} shows that $\hat{k}_{0.06}(\mathbf{x})$ estimates $\lVert\mathbf{x}\rVert_{2,0}$ accurately over a wide range of the parameters $\lVert\mathbf{x}\rVert_{2,0}$ and $\mathrm{BDNR}(\mathbf{x})$, and these parameters have a small effect on the relative estimate error $\left|\frac{\hat{k}_{0.06}(\mathbf{x})}{\lVert\mathbf{x}\rVert_{2,0}}-1\right|$, which is expected in Theorem 3.

\begin{figure}[!t]
\centering
\includegraphics[width=0.7\textwidth,height=0.5\textheight]{alphaplot.jpeg}
% where an .eps filename suffix will be assumed under latex,
% and a .pdf suffix will be assumed for pdflatex; or what has been declared
% via \DeclareGraphicsExtensions.
\caption{The average relative error $\left|\frac{\hat{k}_{\alpha}(\mathbf{x})}{\lVert\mathbf{x}\rVert_{2,0}}-1\right|$ with $\alpha=0.06$ depending on $\lVert\mathbf{x}\rVert_{2,0}$ and $\mathrm{BNDR}(\mathbf{x})$.}
\label{fig_7}
\end{figure}



\section{Conclusion}

In this paper, we proposed a new soft measure of block sparsity and obtained its estimator by adopting multivariate centered isotropic symmetric $\alpha$-stable random projections. The asymptotic properties of the estimators were presented. A series of simulation experiments illustrated our theoretical results.

There are some interesting issues left for future research. Throughout the paper, we assume that the noise scale parameter $\sigma$ and the characteristic function of noise $\psi_0$ are known. In practice, however, they are usually unknown and need to be estimated. Although \cite{l2} considered the effects of adopting their estimators in estimation procedure, how to estimate these parameters based on our random linear projection measurements $\mathbf{y}$ itself is still unknown. In addition, we have been considering the sparsity and block sparsity estimations for real-valued signals so far. It will be interesting to generalize the existing results to the case of complex-valued signals.





% if have a single appendix:
%\appendix[Proof of the Zonklar Equations]
% or
%\appendix  % for no appendix heading
% do not use \section anymore after \appendix, only \section*
% is possibly needed

% use appendices with more than one appendix
% then use \section to start each appendix
% you must declare a \section before using any
% \subsection or using \label (\appendices by itself
% starts a section numbered zero.)
%


\appendices
\section{Proofs}

Our main theoretical results Theorem 1 and Theorem 2 follow from Theorem 2 and Corollary 1 in \cite{l2}, since in both estimation procedures, the measurements without noise both have the univariate symmetric stable distribution but with different scale parameters after the random projection, $\gamma_\alpha\lVert \mathbf{x}\rVert_{\alpha}$ for sparsity estimation, $\gamma_\alpha\lVert \mathbf{x}\rVert_{2,\alpha}$ for block sparsity estimation. Therefore, the asymptotic results for the scale parameters estimators by using characteristic function method are rather similar. In order not to repeat, all the details are omitted. Next, we only present the proofs for Lemma 2, Lemma 3 and Theorem 3. \\

\noindent
{\bf Proof of Lemma 2.} \,\,\,The proof procedure follows from the proof of Proposition 1 in \cite{l2}ã€€with some careful modifications. Let $c_0$ be as in Lemma 1, and let $\kappa_0\geq 1$ be any number such that \begin{align}
\frac{2\ln(\kappa_0)+2}{d\kappa_0}+\frac{2}{\kappa_0}\leq \frac{1}{c_0}.
\end{align}
Define the positive number $t=\frac{m/\kappa_0}{d\ln(\frac{eN}{m})}$, and choose $k=\lfloor t\rfloor$ in Lemma 1. Note that when $m\leq N$, this choice of $k$ is clearly at most $p$, and hence lies in $\{1,\cdots,p\}$. Then we have \begin{align*}
k\ln(\frac{eN}{kd})&\leq (t+1)\ln(\frac{eN}{td}) \\
&=\left(\frac{m/\kappa_0}{d\ln(\frac{eN}{m})}+1\right)\cdot \ln\left(\frac{\kappa_0 eN}{m}\cdot\ln(\frac{eN}{m})\right)\\
&\leq \left(\frac{m/\kappa_0}{d\ln(\frac{eN}{m})}+1\right)\cdot\ln\left[(\kappa_0\frac{eN}{m})^2\right]\\
&=\frac{2m/\kappa_0}{d\ln(\frac{eN}{m})}\left(\ln(\kappa_0)+\ln(\frac{eN}{m})\right)+2\ln(\kappa_0\frac{eN}{m})\\
&\leq\left(\frac{2\ln(\kappa_0)+2}{d\kappa_0}+\frac{2}{\kappa_0}\right)m\leq \frac{m}{c_0}
\end{align*}
by using our assumption $N\geq m\geq \kappa_0\ln(\kappa_0\frac{eN}{m})$. Hence, our choice of $\kappa_0$ ensures $m\geq c_0k\ln(eN/kd)$. To finish the proof, let $\kappa_1=c_1$ be as in Lemma 1 so that the bound (\ref{2.2}) holds with probability at least $1-2\exp(-\kappa_1 m)$. Moreover, we have $$
\frac{1}{\sqrt{k}}\lVert\mathbf{x}-\mathbf{x}^k\rVert_{2,1}\leq \frac{1}{t}\lVert\mathbf{x}\rVert_{2,1}=\frac{\sqrt{\kappa_0}}{\sqrt{m}}\sqrt{d\lVert\mathbf{x}\rVert_{2,1}^2\ln(\frac{eN}{m})}.
$$
Let $c_2$ and $c_3$ be as in (\ref{2.2}), then we have \begin{align*}
\frac{\lVert\hat{\mathbf{x}}-\mathbf{x}\rVert_2}{\lVert\mathbf{x}\rVert_2}&\leq c_2\frac{\lVert\mathbf{x}-\mathbf{x}^k\rVert_{2,1}}{\sqrt{k}\lVert\mathbf{x}\rVert_2}+c_3\frac{\delta}{\lVert\mathbf{x}\rVert_2}\\
&\leq c_2\frac{\sqrt{\kappa_0}}{\sqrt{m}}\sqrt{d\frac{\lVert\mathbf{x}\rVert_{2,1}^2}{\lVert\mathbf{x}\rVert_2^2}\ln(\frac{eN}{m})}
+c_3\frac{\delta}{\lVert\mathbf{x}\rVert_2},
\end{align*}
then the proof is completed by setting $\kappa_2=c_2\sqrt{\kappa_0}$, $\kappa_3=c_3$ and noticing the fact that $\lVert\mathbf{x}\rVert_2=\lVert\mathbf{x}\rVert_{2,2}$. \\

\noindent
{\bf Proof of Lemma 3.}\,\,\, By using the independence of $\mathbf{a}_{1j}, j\in\{1,\cdots,p\}$, for $t\in \mathbb{R}$, the characteristic function of $\langle \mathbf{a}_1,\mathbf{x}\rangle$ has the form: \begin{align*}
E[\exp(it\langle \mathbf{a}_1,\mathbf{x}\rangle)]&=E\left[\exp\Big(it(\sum_{j=1}^p\mathbf{x}[j]^T\mathbf{a}_{1j})\Big)\right]\\
&=\prod_{j=1}^p E[\exp(it\mathbf{x}[j]^T\mathbf{a}_{1j})]\\
&=\prod_{j=1}^p \exp(-\gamma_\alpha^{\alpha}\lVert t\mathbf{x}[j]\rVert_2^\alpha)\\
&=\exp\left[-\gamma_\alpha^{\alpha}\left(\sum_{j=1}^p\lVert\mathbf{x}[j]\rVert_2^\alpha\right)|t|^{\alpha}\right]\\
&=\exp(-(\gamma_\alpha\lVert\mathbf{x}\rVert_{2,\alpha})^\alpha|t|^\alpha).
\end{align*}
Then, the Lemma follows from the Definition 1.\\

\noindent
{\bf Proof of Theorem 3.} The triangle inequality implies $$
\frac{|\tilde{k}_{\alpha}(\mathbf{x})-\lVert\mathbf{x}\rVert_{2,0}|}{\lVert\mathbf{x}\rVert_{2,0}}\leq \frac{|\tilde{k}_{\alpha}(\mathbf{x})-k_{\alpha}(\mathbf{x})|}{\lVert\mathbf{x}\rVert_{2,0}}+\frac{|k_{\alpha}(\mathbf{x})-\lVert\mathbf{x}\rVert_{2,0}|}{\lVert\mathbf{x}\rVert_{2,0}}.
$$
As $k_{\alpha}(\mathbf{x})\leq \lVert\mathbf{x}\rVert_{2,0}$, we have $$
\left|\frac{\tilde{k}_{\alpha}(\mathbf{x})}{\lVert\mathbf{x}\rVert_{2,0}}-1\right|\leq \left|\frac{\tilde{k}_{\alpha}(\mathbf{x})}{k_{\alpha}(\mathbf{x})}-1\right|+\frac{|k_{\alpha}(\mathbf{x})-\lVert\mathbf{x}\rVert_{2,0}|}{\lVert\mathbf{x}\rVert_{2,0}}.
$$

Thus, it suffices to bound the last term on the right side. Since $k_0(\mathbf{x})= \lVert\mathbf{x}\rVert_{2,0}$ and $k_{\alpha}(\mathbf{x})$ is a non-increasing function of $\alpha$, then $$
|k_{\alpha}(\mathbf{x})-\lVert\mathbf{x}\rVert_{2,0}|=\int_{0}^{\alpha}\left|\frac{d}{du}k_{u}(\mathbf{x})\right|du.
$$

We now derive a bound on $\left|\frac{d}{du}k_{u}(\mathbf{x})\right|$. For $u\in(0,\alpha]$ and $\alpha\in (0,1)$, if we define the probability vector $\omega_j=\frac{\pi_j(\mathbf{x})}{\lVert\pi(\mathbf{x})\rVert_{u}^u}$ with $j=1,\cdots,p$, $\pi_j(\mathbf{x})$ and $\pi(\mathbf{x})$ defined as Section II.A, then it holds that \begin{align*}
\left|\frac{d}{du}k_{u}(\mathbf{x})\right|&=-\frac{d}{du}k_{u}(\mathbf{x}) \\
&=-\frac{d}{du}\exp(H_{u}(\pi(\mathbf{x})))\\
&=-k_{u}(\mathbf{x})\frac{d}{du}H_{u}(\pi(\mathbf{x}))\\
&=-k_{u}(\mathbf{x})\bigg(\frac{-1}{(1-u)^2}\sum\limits_{j:\lVert\mathbf{x}[j]\rVert_2\neq 0}\omega_j(\mathbf{x})\log\Big(\frac{\omega_j(\mathbf{x})}{\pi_{j}(\mathbf{x})}\Big)\bigg)\\
&\leq \frac{\lVert\mathbf{x}\rVert_{2,0}}{(1-u)^2}\sum\limits_{j:\lVert\mathbf{x}[j]\rVert_2\neq 0}\omega_j(\mathbf{x})\log\Big(\frac{\omega_j(\mathbf{x})}{\pi_{j}(\mathbf{x})}\Big),
\end{align*}
by using $k_{u}(\mathbf{x})\leq\lVert\mathbf{x}\rVert_{2,0}$ and the formula for $\frac{d}{du}H_{u}(\pi(\mathbf{x}))$. Next, as $\frac{\omega_j(\mathbf{x})}{\pi_{j}(\mathbf{x})}=\frac{\pi_j(\mathbf{x})^{u-1}}{k_{u}(\mathbf{x})^{1-u}}$, then for $j$ with $\lVert\mathbf{x}[j]\rVert_2\neq 0$, we have \begin{align*}
\frac{\omega_{j}(\mathbf{x})}{\pi_{j}(\mathbf{x})}&\leq \frac{\pi_{j}(\mathbf{x})^{u-1}}{k_{\infty}(\mathbf{x})^{1-u}},\,\,\,\text{since $k_{\infty}(\mathbf{x})\leq k_{u}(\mathbf{x})$}\\
&\leq \frac{\pi_{j}(\mathbf{x})^{-1}}{k_{\infty}(\mathbf{x})}\cdot k_{\infty}(\mathbf{x})^u,\,\,\,\text{since $\pi_j(\mathbf{x})\in (0,1]$} \\
&=\frac{\lVert\mathbf{x}\rVert_{2,1}}{\lVert\mathbf{x}[j]\rVert_2}\frac{\lVert\mathbf{x}\rVert_{2,\infty}}{\lVert\mathbf{x}\rVert_{2,1}}\cdot k_{\infty}(\mathbf{x})^u,\,\,\,\text{since $k_{\infty}(\mathbf{x})=\frac{\lVert\mathbf{x}\rVert_{2,1}}{\lVert\mathbf{x}\rVert_{2,\infty}}$}\\
&\leq \mathrm{BDNR}(\mathbf{x})\cdot\lVert\mathbf{x}\rVert_{2,0}^u,\,\,\,\text{since $k_{\infty}(\mathbf{x})\leq \lVert\mathbf{x}\rVert_{2,0}$}.
\end{align*}
As this bound does not depend on $j$ and $\omega_j$ sum to 1, we obtain for all $u\in (0,\alpha]$ and $\alpha\in (0,1)$, \begin{align}
\left|\frac{d}{du}k_{u}(\mathbf{x})\right|&\leq \frac{\lVert\mathbf{x}\rVert_{2,0}}{(1-u)^2}\Big(\log \mathrm{BDNR}(\mathbf{x})+u\log(\lVert\mathbf{x}\rVert_{2,0})\Big)\nonumber \\
&\leq\frac{\lVert\mathbf{x}\rVert_{2,0}}{(1-u)^2}\Big(\log \mathrm{BDNR}(\mathbf{x})+\alpha\log(\lVert\mathbf{x}\rVert_{2,0})\Big).
\end{align}
Finally, the basic integral result $\int_0^{\alpha}\frac{1}{(1-u)^2}du=\frac{\alpha}{1-\alpha}$ completes the proof.
% use section* for acknowledgment
\section*{Acknowledgment}

This work is supported by the Swedish Research Council grant (Reg.No. 340-2013-5342).


% Can use something like this to put references on a page
% by themselves when using endfloat and the captionsoff option.
\ifCLASSOPTIONcaptionsoff
  \newpage
\fi



% trigger a \newpage just before the given reference
% number - used to balance the columns on the last page
% adjust value as needed - may need to be readjusted if
% the document is modified later
%\IEEEtriggeratref{8}
% The "triggered" command can be changed if desired:
%\IEEEtriggercmd{\enlargethispage{-5in}}

% references section

% can use a bibliography generated by BibTeX as a .bbl file
% BibTeX documentation can be easily obtained at:
% http://mirror.ctan.org/biblio/bibtex/contrib/doc/
% The IEEEtran BibTeX style support page is at:
% http://www.michaelshell.org/tex/ieeetran/bibtex/
%\bibliographystyle{IEEEtran}
% argument is your BibTeX string definitions and bibliography database(s)
%\bibliography{IEEEabrv,../bib/paper}
%
% <OR> manually copy in the resultant .bbl file
% set second argument of \begin to the number of references
% (used to reserve space for the reference number labels box)


% biography section
%
% If you have an EPS/PDF photo (graphicx package needed) extra braces are
% needed around the contents of the optional argument to biography to prevent
% the LaTeX parser from getting confused when it sees the complicated
% \includegraphics command within an optional argument. (You could create
% your own custom macro containing the \includegraphics command to make things
% simpler here.)
%\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{mshell}}]{Michael Shell}
% or if you just want to reserve a space for a photo:

%\begin{IEEEbiography}{Michael Shell}
%Biography text here.
%\end{IEEEbiography}

% if you will not have a photo at all:
%\begin{IEEEbiographynophoto}{Zhiyong Zhou}
%is currently a postdoctoral fellow in the Department of Mathematics and Mathematical Statistics at Ume{\aa} University, Sweden. He received his B.Sc. in 2011 and Ph.D in 2016 from Zhejiang University, China. His research interests include nonstationary time series, nonparametric and semiparametric regressions, high dimensional statistics with sparsity, and compressive sensing with applications in biomedical engineering.
%\end{IEEEbiographynophoto}

% insert where needed to balance the two columns on the last page with
% biographies
%\newpage

%{IEEEbiographynophoto}{Jun Yu}
%is currently the chaired professor in mathematical statistics at the Department of Mathematics and Mathematical Statistics at Ume{\aa} University, Sweden. He received the B.Sc. degree in mathematics from Hangzhou University, China, in 1982, and the Ph.D. degree in mathematical statistics from Ume{\aa} University in 1994.

%From 1994, he was a Senior Lecturer and Research Associate in the Department of Mathematical Statistics, Ume{\aa} University, where he became associate professor in 1999. In 1999, he joined the Centre of Biostochastics at The Swedish University of Agricultural Sciences and became professor in mathematical statistics in 2009. Since 2012, he has been working at the Department of Mathematics and Mathematical Statistics at Ume{\aa} University, Sweden.
%His current research interests include nonstationary biomedical signal/image analysis, wavelet theory applied to statistical problems, statistical classification and pattern recognition with applications in remote sensing, biomedical engineering and automated quality control, compressive sensing and high dimensional statistics with sparsity, and spatiotemporal statistics for big data.

%\end{IEEEbiographynophoto}

% You can push biographies down or up by placing
% a \vfill before or after them. The appropriate
% use of \vfill depends on what kind of text is
% on the last page and whether or not the columns
% are being equalized.

%\vfill

% Can be used to pull up biographies so that the bottom of the last one
% is flush with the other column.
%\enlargethispage{-5in}



% that's all folks
\end{document}

