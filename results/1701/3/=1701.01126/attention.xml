<?xml version="1.0" encoding="UTF-8"?>
<opml version="2.0">
  <head>
    <title></title>
    <abstract> </abstract>
  </head>
  <body>
<outline text="Formalization" _note="We assume both the premise tree and the hypothesis tree are binarized.  We use the premise tree and hypothesis tree in Figure   to demonstrate the process of our approach. The premise sentence is “two women are hugging one another”, and the hypothesis sentence is “the women are sleeping”.  Following the traditional approaches , we first find the alignments from hypothesis tree nodes to premise tree nodes (i.e., the dashed or dotted curves in Figure  ). Then we explore inducing the sentence-level entailment relations by 1) first computing the entailment relation at each node of the hypothesis tree based on the alignments, and then 2) composing the entailment relations at the internal hypothesis nodes from bottom up to the root in a recursive way. Our model resembles the work of Natural Logic in the spirit that the entailment relation is inferred [MODULARLY]{}, and composed [RECURSIVELY]{}.  We formalize this entailment task as a structured prediction problem similar to , , and . The inputs are two trees: premise tree , and hypothesis tree . The goal is to predict a label . Note that although the output label is not structured, we can still consider the problem as a structured prediction problem, because: 1) the input is a pair of trees; and 2) the internal alignments are structured.  More formally, we aim to minimize the negative log likelihood of the gold label given the two trees. The objective can be written in the online fashion as: \ where the structured latent variable represents an alignment. is the number of nodes in the tree. if and only if node in is aligned to node in , otherwise .  However, enumerating over all possible alignments takes exponential time, we need to efficiently approximate the above log expectation.  Fortunately, as point out, as long as the calculation only consists of linear calculation, simple nonlinearities like , and softmax, we can have following simplification via first-order Taylor approximation: \ which means instead of enumerating over all alignments and calculating the label probability for each alignment, we can use the label probability for the expected alignment as an approximation:[^1] \ Figure   shows an example of expected alignment calculation. The objective is simplified to \  [0.48]{}  With this observation, we split our calculation into two steps as the top two modules in Figure  . First in the Alignment module, we calculate the expected alignments using Equation   (Section  ). Then we calculate the node-wise entailment relation, propagate and compose the relation from bottom up to find out the final entailment relation (Equation  ) in the Entailment Composition module (Section  ). Both of these two modules rely on the composition of tree node meaning representations (Section  ).  [^1]: We use bold letter, , for binary alignments, and tilde version, ,     for the expected alignments in the real number space.">
</outline>
<outline text="Attention over Tree Nodes" _note="First we calculate the expected alignments between the hypothesis and the premise (Equation  ):  To simplify the calculation, we further approximate the global (binary) alignment to be consisted of the alignment of each tree node independently. is the th row of : is the probability of the node being aligned to node , which is defined as: are vectors representing the semantic meanings of node , , respectively, whose calculation will be described in Section  . is an affine transformation from to . This formulation essentially is equivalent to the widely used attention calculation in neural networks , i.e., for each node , we find the relevant nodes and use the softmax of the relevances as a probability distribution. In the rest of the paper, we use “expected alignment” and “attention” interchangeably.  The expected alignment of node being aligned to node , by definition, is:">
</outline>
<outline text="Entailment Composition" _note="Now we can calculate the entailment relation at each tree node and propagate the entailment relation following the hypothesis tree from bottom up, assuming the expected alignment is given (Equation  ):  Let vector denote the entailment relation in a latent relation space at hypothesis tree node . At the root of the hypothesis tree. We can induce the final entailment relation from entailment relation vector . We use a simple layer to project the entailment relation to the 3 relations defined in the task, and use a softmax layer to calculate the probability for each relation:  At each hypothesis node , is calculated recursively given the meaning representation at this tree node , the meaning representation of every node in the premise tree , and the entailment from ’s children, :  Note the resemblance between the above function and the definition of Binary-Tree LSTM transition function (Equation  ), this suggests that we can directly use a Binary-Tree LSTM layer to calculate the entailment in a way similar to . That is, using as the input, and , as the hidden states passed from children.[^1] Figure   illustrates the calculation of the entailment composition. We will discuss in Section  .  [0.49]{}  [0.49]{}  [^1]: We also need to add two vectors representing the memories from the     children.">
</outline>
<outline text="Dual-attention Over Tree Nodes" _note="We can further improve our alignment approximation in Section  , which does not consider any structural information of current tree, nor any alignment information from the premise tree.  We can take a closer look at our conceptual example in Figure  . Note that the alignments have, to some extent, a symmetric property: if a premise node is most relevant to a hypothesis node , then the hypothesis node should also be most relevant to premise node . For example, in Figure  , the premise phrase “hugging one another” contradicts the hypothesis word “sleeping”. In the perspective of the premise tree, the hypothesis word “sleeping” contradicts by the known claim “hugging one another”. This suggests us to calculate the alignments from both side, and eliminate the unlikely alignment if it only exists in one side. This technique is similar to the widely used forward and reversed alignment technique in the machine translation area.  In detail, we calculate the expected alignments from hypothesis to premise, and also the expected alignments from premise to hypothesis, and use their element-wise product as the attention to feed into the Entailment Composition module.[^1] This element-wise product is a mimic of the intersection of two alignments in machine translation. Figure   shows an example. In addition to our dual-attention, also explore to use the structural information to improve the alignment. However, their approach requires introducing some extra terms in the objective function, and is not straightforward to integrate into our model. We leave adding more structural constraints to further improve the attention as an open problem to explore in the future.  [^1]: We need to normalize at each row to make each row a probability     distribution.">
</outline>
  </body>
</opml>