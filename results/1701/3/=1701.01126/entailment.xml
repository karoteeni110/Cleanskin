<?xml version="1.0" encoding="UTF-8"?>
<opml version="2.0">
  <head>
    <title></title>
    <abstract> </abstract>
  </head>
  <body>
<outline text="Entailment Relation Composition" _note="Composing entailment relations induced from fractals of the sentence to form the sentence-level entailment relation is highly non-trivial. For the Natural Logic framework provided a matrix to handle all the possible combinations for the entailment relation composition. However the composition result still can not be deterministically decided. Recent research of propose to learn the entailment relation composition matrix with neural networks, but it still seems difficult.  Here we propose a simple model for entailment representation. In our model, each entailment relation is represented as a vector , where is the length of the entailment relation representation. The composition of two entailment relations and is modeled by the outer product between the two vectors: , which gives a result matrix of . We then treat this matrix as a vector of and use this vector as an input for a non-linear layer and map it to a vector of as the composition result.  A formal summarization of the above procedure is where is the composition result, and is the matrix of that maps the tensor dot product result to a vector of length .[^1]  The intuition behind our design is that, we consider each dimension of the entailment vector as a feature value, and the outer product and conversion from matrix to vector is just to collect feature bigrams of the composition. At last we use a non-linear layer to learn a simple classifier to get the correct entailment from these bigram features.  As a special case of our design when and there is no non-linear projection instead of a linear projection, we can consider each element of the entailment vector as an indicator of how likely the entailment belongs to one of the three entailment relation candidates (entailment, neutral, and contradiction). Our composition operation is just a linear classifier based on the bigrams of the indicators of the entailment relations being composed.  [^1]: We abuse the mathematical notation and assume an automatic     conversion from a matrix of to a vector of .  ">
</outline>
<outline text="Structured Entailment Model" _note="Now we combine the entailment composition with the attention model in Section  .  The combined model is similar to the attention model (Equations  - ) that it calculates the entailments following the hypothesis tree in a bottom-up order.  The transitions at each tree node is summarized by Equations   and  .  Equation   is exactly the same as Equation   in the attention model, which calculates the relatedness between the given hypothesis node and every premise node.  Equation   calculates the entailment relation with attention for the given hypothesis node . In matrix each row vector corresponds to a node in the premise tree, and the value of the elements in the row vector indicates the strongness of the alignment and the entailment relation.   is calculated using two terms. The first term considers the pairwise relatedness between and every premise node. The second term considers the attention and entailment relation propagated from ’s children in one step.  This matrix-tensor-matrix dot product might seem difficult to interpret. Here we can consider a simplified case where is a 3-d identity tensor, i.e., if.f. . The result is a tensor of .  If we take a look at th slice of , which essentially means .  This explains why this tensor product performs the operation of entailment relation composition. After this the result is mapped to a matrix of by a dot product with .[^1]  Finally at the root of the hypothesis tree, we induce the entailment relation by summing up the entailment matrix alone the dimension of the nodes, linearly project the result to a vector of length of the real number of relations in the dataset, and pass through a softmax layer to make it a probability distribution.  Again we use cross-entropy as the optimization objective in the training.  [^1]: Again we abuse the mathematical notations here and assume an     automatic conversion similar to Footnote   on the     last two dimensions of the dot product result. Then the converted     result is mapped by a linear projection.">
</outline>
  </body>
</opml>