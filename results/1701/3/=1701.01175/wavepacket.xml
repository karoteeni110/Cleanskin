<?xml version="1.0" encoding="UTF-8"?>
<opml version="2.0">
  <head>
    <title>Fast quantum computation at arbitrarily low energy</title>
    <abstract>One version of the energy-time uncertainty principle states that the
minimum time for a quantum system to evolve from a given state to any
orthogonal state is where is the energy uncertainty. A related bound
called the Margolus-Levitin theorem states that where is the expectation
value of energy and the ground energy is taken to be zero. Many
subsequent works have interpreted as defining a minimal time for an
elementary computational operation and correspondingly a fundamental
limit on clock speed determined by a system’s energy. Here we present
local time-independent Hamiltonians in which computational clock speed
becomes arbitrarily large relative to and as the number of computational
steps goes to infinity. We argue that energy considerations alone are
not sufficient to obtain an upper bound on computational speed, and that
additional physical assumptions such as limits to information density
and information transmission speed are necessary to obtain such a bound. </abstract>
  </head>
  <body>
<outline text="Introduction" _note="It is frequently argued that energy places a fundamental limit on the speed of computation, even if the computation is performed reversibly. Planck’s constant provides a conversion factor between energy and frequency, and on dimensional grounds one might expect that a quantum system at energy scale has maximum computational clock speed of order . More quantitatively, for a quantum system with energy uncertainty , the time to evolve to an orthogonal quantum state obeys the bound Several related energetic lower bounds on have been proven . In particular, the Margolus-Levitin theorem shows that for a state with energy expectation , evolving according to a time-independent Hamiltonian with zero ground energy, See for a recent review of such quantum speed limits and their applications. Many estimates of computational capacity of physical systems have their starting point in the assumption that can be interpreted as a maximum computational clock speed . Related, but distinct, arguments for limits on computational speed related to energy are given in . Another relationship between the energy-time uncertainty principle and computational complexity, which is not based on associating with the time necessary for a logical operation, was recently given in .  For several reasons, equating with a maximum computational clock speed seems quite plausible. In classical computers, each logic gate brings the system to an orthogonal state, and in quantum circuits, each logic gate typically[^1] brings the system to a near-orthogonal state. is the minimum time to flip a qubit from to and it would seem surprising to achieve logical gates in time less than . Furthermore, energy-time uncertainty principles suggest that the uncertainty of the timing of a logic operation scales as , and if the time between elementary logic operations is shorter than this then one would expect their ordering to be uncertain, thereby ruining the computation. (For a contrary conjecture, perhaps presaging the present work, see .)  However, we here show that these obstacles can be evaded. We give explicit constructions of quantum time-evolutions using time-independent Hamiltonians, which simulate the operation of a quantum circuit of elementary gates, while traversing only a constant number of orthogonal states, independent of . The Hamiltonians achieving this involve only 4-qubit interactions and can be made spatially local in two dimensions. The ratio of the computational clock speed to is unbounded, specifically growing linearly with . The same holds for . In other words, rather counterintuitively, the total time needed to simulate gates is on the same order as the minimum required time to flip a single bit. We conclude that energy alone does not present a fundamental limit to computational speed; to obtain such a limit one must invoke additional assumptions such as a limit to the spatial density at which qubits can be packed. The argument based on energy-time uncertainty principles is evaded because, although the uncertainty of the timing of each individual logic operation is very large in our construction, these timings are all correlated, leaving no ambiguity as to the ordering of the operations.  [^1]: As a rough argument, one could model a typical state at an     intermediate step of a quantum computation by a Haar random state on     qubits. In this case, the root-mean-square inner product between the     states before and after a one-qubit gate has been performed is .">
</outline>
<outline text="Basic Construction" _note="Here we show how to simulate an arbitrary quantum circuit of gates by evolving for some time according to a time-independent 4-local Hamiltonian. The computational clock speed achieved is such that the ratios and both diverge linearly with . This construction thus suffices to serve as a counterexample for the conjectures that computational clock speed is limited to some maximum rate proportional to either the energy uncertainty or the energy above the ground state. To those familiar with Feynman-Kitaev clock Hamiltonians, the essential idea of the construction can be concisely expressed: it is to initialize a Feynman-Kitaev-type Hamiltonian with a wavepacket whose breadth is comparable to the number of gates . Such broad wavepackets have small expected energy and low energy uncertainty. Furthermore, they maintain large overlap with previous states as they propagate, and thus the number of orthogonal states traversed during the time evolution is even as the number of computational steps is increased. In the remainder of this section, the construction is given in detail without assuming prior familiarity with the Feynman-Kitaev construction. Note that here we are using the Feynman-Kitaev Hamiltonian to execute quantum computation “ballistically” as in Feynman’s original construction not adiabatically as in .  Consider a quantum circuit consisting of a sequence of 2-qubit gates acting on qubits. For a given initial state the circuit proceeds through the states where The standard Feynman-Kitaev clock Hamiltonian acts on a register of computational qubits, together with a clock register, as where denotes the identity matrix, the first tensor factor represents the computational qubits, and the second tensor factor represents the clock register. The subspace is preserved by and the block of acting on this subspace looks like with all other entries zero. In other words, is the discretized second derivative on a one-dimensional lattice. Formally, if we think of as acting on a discretization of the unit interval one has Consequently, the dynamics induced by on the computational subspace can be intuitively thought of as the dynamics of a free particle on a line. A state will evolve according to[^1] To perform computation, one can prepare an initial wavepacket at small with momentum in the direction of increasing . Under Schr[ö]{}dinger time evolution according to [( )]{} this wavepacket will propagate to larger and broaden. Eventually, the wavepacket will reach the end of the line, and consist of a superposition with nonzero amplitude on , which is the output of the original quantum circuit.  One might be concerned about two apparent problems with the Feynman-Kitaev construction. Firstly, the initial wavepacket must have some width large compared to the lattice spacing in order for the continuum approximation to be valid. Thus it has support not only on the initial state but also on states in which some gates have already been applied. This seems like cheating; we are seeking to simulate the computation performed by the quantum circuit , but we start with an initial state in which some of this computation has already been done. This problem is easily solved by padding the circuit with sufficiently many initial identity gates so that the initial wavepacket only has support on states in which no nontrivial gates have been applied. Secondly, upon measuring the final state, the probability of obtaining the desired outcome will be much smaller than one because the final wavepacket has nonzero amplitude on states in which not all of the gates have yet been applied. This second problem can be similarly solved by padding the circuit with sufficiently many identity gates at the end.  So far, we have not specified the physical implementation of the clock register. We have simply labeled an orthonormal basis for its Hilbert space. We can implement this using qubits in the following encoding. This encoding is inefficient in the sense that only qubits are actually needed to store numbers in the range . However, the advantage of this encoding is that the operators such as appearing only act on two qubits, namely qubits and . Thus, if the original circuit is constructed from a universal gate set of one-qubit and two-qubit quantum gates, then is a 4-local Hamiltonian.  Let’s first treat broadening of the wavepacket as negligible and take the width of the wavepacket to be , as illustrated in figure  . (We defer analysis of broadening to [§ ]{}.) The original circuit should be padded with identity gates at the beginning and identity gates at the end, yielding a circuit with a total of gates and The initial state can be chosen as a wavepacket with width on the order of and support only on the initial identity gates. Let while retaining the notation defined in [( )]{} for the state obtained by the original unpadded circuit. Thus the initial wavepacket is: That is, the computational register is initialized to the same state that the original circuit starts with, which could be taken as without loss of generality[^2]. The wavepacket should be chosen with rightward momentum. Then it will propagate down the line, yielding a final state of Thus, in the final state, the computational register contains the output of the original circuit.  From figure   one sees that, as the wavepacket propagates, it passes through essentially three orthogonal states: the initial state supported on through , the state supported on through , and the final state supported on through . The energy uncertainty in a state of spatial width for the Hamiltonian scales as . The ground energy of the Feynman-Kitaev Hamiltonian is zero. One can verify this by noting that the state is an energy-zero eigenvector of and that, by [( )]{} and [( )]{}, is positive semidefinite since it is a sum of positive-semidefinite terms.  Let denote the number of gates in the padded quantum circuit. (If the wavepacket did not spread then we could take while still ensuring that the final superposition is only over states in which the computation is finished. In the present construction we will take slightly larger than , but still only linear in , as discussed in [§ ]{}.) Examining [( )]{} with , one sees that, in the limit of large , the wavepacket propagates like a nonrelativistic particle of mass on the unit interval. The velocity of such a particle (I.E. the group velocity of the wavepacket) is , where is the momentum of the wavepacket. The time needed to propagate to the end of the line is therefore . During this time, gates are simulated, so the clock speed is . As we show in [§ ]{}, it suffices to choose to be a constant multiple of . Thus, . The expectation value of the energy in the wavepacket state is . Hence, by choosing we can achieve a ratio of clock speed to energy of . As mentioned above, with wavepacket of width of order , the energy uncertainty is of order . Hence, the ratio of clock speed to energy uncertainty is also of order .  [^1]: For notational simplicity, we use units where here and throughout     the remainder of this paper.  [^2]: Quantum circuits acting on the all zeros string are     computationally universal because any classical input string can be     hard-coded via initial NOT gates.">
</outline>
<outline text="Dispersion" _note="Suppose the initial state is a Gaussian wavepacket where is a normalization constant. Under Schr[ö]{}dinger’s equation, , this evolves to[^1] where is a (real) phase, is a normalization factor, and  In the units being used, where the length of the line is 1 and the lattice spacing is , we have, in the continuum limit (I.E. ), an effective particle mass of and a propagation velocity Thus, the time to propagate down the line is and is O(1). In other words, the final superposition has width only a constant factor larger than the initial superposition. To obtain a wavepacket with finite support, one can truncate the Gaussian wavepacket at some multiple of away from the mean. That is, The inner product between this wavepacket and the untruncated Gaussian becomes exponentially close to one as the multiple is increased. By unitarity, the inner product between the ideal and truncated final states will be equal to the inner product between the ideal and truncated initial states. Because initial and final are , the initial truncated wavefunction has support only on clock values and can be completely accommodated by padding the circuit with initial identity gates. The final state will, to an exponentially good approximation for large , have all its amplitude within a range of clock values and can also be accommodated by padding with final identity gates.  As a concrete example, suppose we start with a circuit of gates, and we pad it with initial identity gates and final identity gates. Thus, . We set the initial state to be a Gaussian superposition of width and mean . Thus, we can take and have zero amplitude outside the initial pad, whose width is . This ensures that the initial state has zero amplitude for any of the computational gates to be already computed. An initial superposition of width means, in units where the length of the line is one, . We want the wavepacket to propagate from the middle of the initial pad to the middle of the final pad. Thus the total distance to propagate is . By [( )]{} this will take time . Choosing (with corresponding energy ) yields, by [( )]{}, a final wavepacket width of . Measurement of the clock register in the final wavepacket will yield a Gaussian probability distribution with mean and standard deviation[^2] . Almost all the probability thus lies in the final pad of identity gates covering clock values to . Outcomes in which the clock register has value less than correspond to events at least standard deviations below the mean, which have total probability about . Hence, including the errors from the truncation of the initial wavepacket, one finds that, upon measuring the final state, the computational register will with probability at least be in a state where all gates have been computed.  [^1]: An easy way to obtain this is by going to Fourier space, where the     Hamiltonian is diagonal.  [^2]: The 2 comes from squaring the amplitudes. The comes from .">
</outline>
<outline text="Distance Traversed Through Hilbert Space" _note="The question whether the number of orthogonal states traversed in a computation is a resource bearing on computational capacity is perhaps interesting independent of connection to energy. It could be asked in the context of computational models where energy may not play a manifest role, such as quantum cellular automata, quantum Turing machines, and the quantum circuit model. To ask this question in a precise way, we first note that the number of orthogonal states traversed during a computation is not a well-behaved metric. Instead, for a discrete-time quantum computation, such as the quantum circuit model, we can formalize the intuitive notion of distance traversed through Hilbert space as where is the quantum state obtained after the first gates have been applied, as in [( )]{}. The continuum analogue of this distance is which can be applied to Hamiltonian-based models of quantum computation. Note that the continuum version of remains invariant if we multiply the speed at which we traverse the path through Hilbert space by some factor, as is fitting for a metric of distance.  The construction of [§ ]{} demonstrates that it is possible to simulate gates while keeping . One way to see this is by the following calculation. The last line follows from the results of [§ ]{} stating that , , and .">
</outline>
<outline text="Dispersionless Discretization" _note="In the construction of [§ ]{}, the dispersion relation is quadratic and therefore the wavepacket spreads as it propagates. Here we give an alternative construction which avoids this complication by modifying the Feynman-Kitaev Hamiltonian to yield a linear dispersion. This modified construction achieves constant clock speed with energy uncertainty scaling as . This is thus a faster method of computation than the basic construction, which achieves clock speed at energy uncertainty[^1]. Furthermore, in the linear-dispersion construction is easy to calculate in complete quantitative detail. However, it uses wavepackets whose energy is not close to the ground energy. Depending on context this may or may not be relevant, which is why both constructions are presented in this manuscript. Roughly speaking, linear dispersion is achieved by discretizing a one-dimensional analogue of Dirac’s equation rather than discretizing the one-dimensional Schr[ö]{}dinger equation. Similar ideas have been used previously in .  Consider the Hamiltonian where is a “velocity.” This is Hermitian because is antihermitian. Schr[ö]{}dinger’s equation then reads Consequently, That is, and each obey the one-dimensional wave equation. A solution to the Schr[ö]{}dinger equation is therefore for any function , as can easily be verified. describes the shape of a wavepacket that rigidly propagates in the positive- direction without distortion.  We can now discretize and using finite differences while maintaining Hermiticity. To achieve this, we use a forward difference to discretize and a backward difference to discretize as illustrated by the following example on a lattice of four sites. Using as a basis for the subspace and for the subspace, one has with all other matrix elements zero other than the Hermitian conjugates of the above.  The linear-dispersion Feynman-Kitaev computational Hamiltonian corresponding to this discretization is We can use qubits to encode the clock state analogously to [( )]{}. That is, Then, the clock transitions are again 2-local operators. Thus, if the original circuit is built from gates that each act on at most two qubits, is a 4-local Hamiltonian.  Using this encoding, is an operator acting on a -dimensional Hilbert space. However, as with the Feynman-Kitaev Hamiltonian, the subspace is preserved by this Hamiltonian, and within , acts as illustrated in [( )]{}. Discretizing by a forward difference on a lattice of spacing corresponds to So, if we think of the clock register as discretizing the unit interval, the corresponding wave propagation speed is , up to higher order corrections in .  With we can achieve arbitrary length computations with constant-length paths through Hilbert space just as in the Feynman-Kitaev example, but now the evolution of the wavepacket is simpler and cleaner to analyze. We pad the circuit with initial identity gates and final identity gates. Then, we prepare the initial wavepacket state where is a normalized wavepacket and, without loss of generality, we take to be the initial state of the computational register.  We can now compute for this construction in the limit . In this continuum limit we have where and are the continuum analogues of and , respectively. Here we have chosen the normalization so that Thus, one finds where denotes the derivative of . In our construction padded with identity gates the propagation velocity is and the total duration is . So As a concrete example of a smooth normalized wavepacket with support only on , one could choose In this case, one finds by straightforward calculation that  Next we can compute . By [( )]{} Thus, by [( )]{}.  [^1]: These speeds and energy scales are determined by our choice of     normalization of the Hamiltonian, but this choice is not arbitrary.     Physically, one expects the individual 4-local terms to have norm.">
</outline>
<outline text="Spatially Local Construction" _note="The constructions of [§ ]{} and [§ ]{} are perhaps slightly unphysical in that, although the Hamiltonians are local in the sense of involving only 4-qubit interactions, they are not spatially local. In this section we describe how to use an idea from to modify the constructions so that they become spatially local in two dimensions. A detailed illustrative example is given in appendix  .  We arrange the gates of the original circuit into “layers” such that the gates within each layer act on distinct subsets of the qubits. The number of layers is called the circuit depth. Correspondingly, we have a sequence where is the state of the qubits after of the layers have been applied.  Next, we construct a new equivalent quantum circuit on qubits as follows. We lay out an square lattice of qubits in two dimensions. Each column (of qubits) is to be initialized to which, without loss of generality, is the all zeros state. The first stage in the new circuit is to obtain the state in the second column. This is done by applying the gates in the first layer of the original circuit in order from top to bottom on the qubits of the first column, and following each gate with a SWAP operation that brings the qubits it acted on into the second column.  Any qubit that was not acted on in the first layer of the circuit can be thought of as acted on by an identity gate. That is, it is swapped into the second column. Next, the same thing is done to implement the second layer of the circuit and swap the qubits into the third column, except the gates are implemented in order from bottom to top. Gates are implemented on successive layers alternating between bottom-to-top order, and top-to-bottom order, until all layers are complete. After the last layer there is no need for swap operations. This procedure ensures that, at the end of the computation, the last column contains state , which is the output from the original circuit. (See figures  -  in appendix  .)  The modified circuit can then be simulated using a clock Hamiltonian with bounded-range interactions in two-dimensions. Recall the clock encoding [( )]{}. Here, we use the same encoding, except the number of clock qubits will equal the total number of gates of the modified circuit including the swap gates. These clock qubits can then be “snaked” between the layers so that each clock bit geometrically neighbors the computational qubits acted on by the gate that the clock bit corresponds to, and the hopping terms that move the single 1 among the clock qubits are also spatially local. The construction is thus local in two spatial dimensions and involves 4-body interactions (and fewer). The wavepacket propagates down the snaking path of the clock qubits, which is of length at most , which is upper bounded by and often much smaller. (See figure   in appendix  .)">
</outline>
<outline text="Compressed Clock" _note="In the above clock-based constructions, the total number of qubits used is on the order of , whereas the original quantum circuit acted only on qubits. Such an increase in qubit requirement is undesirable, especially in the context of limited qubit density and signal propagation speed, as discussed in [§ ]{}. In this section we show how to modify the encoding of the clock so that only clock qubits are used, at the cost of requiring -local interactions. Achieving such clock compression together with spatial locality remains an open problem.  The most efficient encoding for the clock, in terms of qubit count, would be to store the number as a binary number in qubits. However, a Hamiltonian based on this clock would involve -qubit interactions, and is therefore, in most circumstances, not physically realistic. To achieve computational universality using a -local Hamiltonian of the sort described in section [§ ]{} one requires that the clock bit string encoding can be incremented to by flipping at most bits, and deciding whether a given clock bitstring encodes a given number requires examining at most qubits. The “pulse” encoding [( )]{} is optimal in this respect: incrementing the clock requires flipping two qubits, and deciding whether a string encodes requires only examining the qubit. However, as noted above, the pulse encoding is highly suboptimal in terms of number of qubits required, namely .  We can interpolate between these extremes as follows. We take all strings of bits in which the number of ones (Hamming weight) is . We number these from 0 to . In such an encoding, the operators and appearing in the clock Hamiltonian act nontrivially on at most bits, and therefore the clock Hamiltonian is -local. The number of bits needed for the clock register is the minimum such that , which scales as . We leave open the problem of finding the optimal tradeoff between locality and number of clock qubits.  To ensure that, given a quantum circuit, computing a description of the corresponding clock Hamiltonian is efficient, one must choose the numbering of the bit strings so that encoding of numbers into bitstrings and decoding of bitstrings into numbers are both efficient. That is, the map should admit polynomial-time classical encoding and decoding. The lexicographical numbering of the Hamming weight bitstrings admits simple methods for polynomial-time encoding and decoding for any as described for example in .">
</outline>
<outline text="Conclusion and Open Problems" _note="The examples given above show that energy limitations alone do not impose an upper bound on computational clock speed, even if we restrict our attention to Hamiltonians that are spatially local and involve only 4-body interactions. Should one conclude that our universe admits unlimited computation speed, in principle? This seems unlikely. By introducing more detailed assumptions about physics, beyond just a limit on energy, one may recover limits on computational speed.  Two goals one could consider are achieving constant clock speed with asymptically shrinking energy (as is done in [§ ]{}) or achieving asymptotically growing clock speed with constant energy ( and/or ). This latter goal could be achieved by the constructions in this manuscript but with the Hamiltonian rescaled by an appropriate factor, namely a factor of order for , and a factor of order for . However, these rescaled Hamiltonians would then be sums of 4-body interactions each with norm scaling as or . Whether a construction is possible achieving unbounded clock speed while keeping , , AND the strength of the local interactions bounded remains an open question. In other words, limiting the interaction strength to might be a sufficient additional physical assumption to recover a bound on computational speed.  If we keep the normalization used throughout this paper, where the interaction strengths are , we no longer obtain unbounded clock speed, but we still obtain unbounded ratio of clock speed to the energy scales and . Is this fully physically realistic? It seems the most fundamental aspect of this question is whether universal quantum computation by states of vanishing energy and energy uncertainty evolving according to time-indepentent Hamiltonians can be made fault tolerant against the influences of imperfect implementation and environmental noise. To our knowledge this is an open question.  One might also recover computational speed limits by bringing in spacetime considerations. In particular, assume a maximum speed for signal propagation, and a maximum density at which qubits can be packed. Then, for a computer of qubits in three-dimensional space, the distance between nearest-neighbor qubits is and the average distance between qubits is . Consequently, a two qubit gate must take time at least to act on neighboring qubits and on the order of to act on generic pairs of qubits.  The relevant limits to qubit density may appear at different scales depending on context. In a present-day practical context the limit on qubit density may be set by the atomic scale. In considering the computational complexity implications of relativistic quantum field theory, one can use consider a length scale as a cutoff[^1], where is the available energy. More concretely, the quantum simulation algorithms of demonstrate (in simple examples of quantum field theories) that scattering processes involving particles with total energy can be faithfully simulated by discretizing space onto a lattice of spacing and associating a register of logarithmically many qubits with each lattice site. This suggests that, effectively, the density of qubits accessible by experiments at energy scale is limited to in spatial dimensions. Perhaps the most fundamental limits to qubit density come from quantum gravity considerations such as the Bekenstein bound .  The practical limit on speed of information transmission often coincides, at least approximately, with the fundamental limit set by the speed of light. At the fundamental scale, it is thought that the maximum number of bits of entropy supportable within a region of spacetime faces a limit proportional to the surface area of the region, with the constant of proportionality approximately bits per square meter . It may be tempting therefore, for a given to compute from the Bekenstein bound a corresponding and consequently a maximum clock speed via [( )]{} with the speed of light taking the place of . However, it is not clear that this is a valid argument because in the regime where quantum gravity effects are significant the question of spatial locality may become subtle.\ \  **Acknowledgments:** I thank Scott Aaronson, Ning Bao, Michael Jarret, Manny Knill, Carl Miller, and Aaron Ostrander for useful discussions. I thank Ilya Bogdanov for suggesting the key idea behind [§ ]{} (via mathoverflow). This paper is a contribution of NIST, an agency of the US government, and is not subject to US copyright.  [^1]:  in units where explicit factors of and are kept">
</outline>
<outline text="Example of Spatially Local Construction" _note="">
</outline>
  </body>
</opml>