<?xml version="1.0" encoding="UTF-8"?>
<opml version="2.0">
  <head>
    <title></title>
    <abstract> </abstract>
  </head>
  <body>
<outline text="Matrix Factorization Model" _note="This section will present the matrix factorization model which is used for outlier detection. Before discussing the model in detail, we present the notations and definitions. We represent the corpus of text documents as a bag of words matrix. A lowercase or uppercase letter such as or , is used to denote a scalar. A boldface lowercase letter, such as , is used to denote a vector, and a boldface uppercase letter, such as , is used to denote a matrix. This is consistent with what is commonly used in much of the data mining literature. Indices typically start from , unless otherwise mentioned. For a , denotes its column, denotes its row and or or denote its element.  For greater expressibility, we have also borrowed certain notations from matrix manipulation scripts such as Matlab and Octave. For example, the notation returns the maximal element and returns a vector of maximal elements from each column . Similarly, denotes the -th row of the matrix and for -th column. For the readerâ€™s convenience, the notations used in the paper are summarized in Table  .  Let be the matrix representing the underlying data. In the context of a text collection, this corresponds to a term-document matrix, where terms correspond to rows and documents correspond to columns. In other words, denotes the number of times the term appears in document . Generally, we can write as follows:   []{}  Here, is a low rank matrix and represents the matrix of outlier entries. Typically, the matrix represents the documents created by a lower rank generative process (such as that modeled by pLSI), and the parts of the documents that do not correspond to the generative process are represented as part of the matrix . In real world scenarios, the outlier matrix contains entries which are very close to zero, and only a small number of entries have [SIGNIFICANTLY]{} non-zero values. These significantly nonzero entries are often present in only a small fraction of the columns. Columns which are fully representable in terms of factors are consistent with the low rank behavior of the data, and therefore [NOT]{} outliers. The rank of is not known in advance, and it can be expressed in terms of its underlying factors. Here, the two matrices have dimensions , , and . The matrices and are non-negative, and this provides interpretability in terms of being able to express a document as a non-negative linear combination of the relevant basis vectors, each of which in itself can be considered a frequency-annotated bag of words (topics) because of its non-negativity. Specifically, corresponds to the coefficients for the basis matrix . Intuitively, this corresponds to the case that every document , is represented as the linear combination of the topics. In cases, where this is [NOT]{} true, the document is an outlier, and those unrepresentable sections of the matrix are captured by the non-zero entries in the matrix. In real scenarios, the entries in this matrix are often extremely skewed, and the small number of non-zero entries very obviously expose the outliers. The decomposition of the matrix into different component is pictorially illustrated in Figure  .  In order to determine the best low rank factorization, one must try to optimize the aggregate values of the residuals in the matrix. This can of course be done in a variety of ways, depending upon the goals of the underlying factorization process. We model the determination of the matrices ,, and , as the following optimization problem:  The specific location of outliers in each column does not have a closed form solution, since the -norm penalty is applied to . The logic for applying the -norm in the context of the outlier detection problem is as follows. Each entry in the corresponds to a term in a document, whereas we are interested in the outlier behavior of entire document. This aggregate outlier behavior of the document can be modeled with the norm score of a particular column . In a real scenario, if a large segment of a document is not representable as the linear combination of the topics through , the corresponding column in the matrix will be compensated by having more entries in its column. In other words, we will have a higher value for the corresponding column , and this corresponds to a higher outlier score. Furthermore, the -norm penalty on defines the sum of the norm outlier scores over all the documents. Therefore, the optimization problem essentially tries to find the best model, an important component of which is to minimize the sum of the outlier scores over all documents. While a variety of different (and more commonly used) penalties such as the Frobenius norm are available for matrix factorization models, we have chosen the -norm penalty because of its intuitive significance in the context of the outlier detection problem, and its tendency to create skewed outlier scores across the columns of the matrix. As we will see in the next section, this comes at the expense of a formulation which is more difficult to solve algorithmically.  For high dimensional data, sparse coefficients are desirable for obtaining an interpretable low rank matrix . For this purpose, we add the -penalty on : The constant defines the weight for the outlier matrix over the recovery of the low rank space and the sparsity term. In the case of outlier detection in text documents, we give more weight for the outlier matrix over the low rank representation . This problem does not have a closed form solution, and therefore we cannot directly recover the low rank matrix in closed form. However, we can recover the column space. Without non-negativity constraints, this property is also known as the rotational invariant property . This particular formulation of the matrix factorization model is a bit different from the commonly used formulations, and off-the-shelf solutions do not directly exist for this scenario. Therefore, in a later section, we will carefully design an algorithm with the use of block coordinate descent for this problem.  In order to understand the modeling of the outliers better, we present the readers with a toy example from a real world data set, to show how skewed the typical values of the corresponding column may be in real scenarios. In this case, we used the [BBC]{} dataset[^1]. . We computed the matrix and generated the scores of the columns of outlier matrix . Figure   shows the outlier() scores of the documents. The -axis illustrates the index of the document, and the -axis illustrates the outlier score. It is evident that the scores for some columns are so close to zero, that they cannot even be seen on the diagram drawn to scale. These columns also happened to be the non-outlier/regular documents of the collection. Such documents correspond to the low rank space, and are approximately representable as a product of the basis matrix with the corresponding column vector of coefficients drawn from . However, the documents that are not representable in such a low rank space have a large outlier score. From the distribution of the outlier score, we can also observe that the scores of outlier documents against non-outliers are clearly separable, by using a simple statistical mean and standard deviation analysis. Therefore, while we use the scores to rank the documents in terms of their outlier behavior, the skew in the entries ensures that it is often easy to choose a cut-off in order to distinguish the outliers from the non-outliers.  In the following sections, we will analyze the property and performance of this model for outlier detection problems.  [^1]: &lt;http://mlg.ucd.ie/datasets/bbc.html&gt;">
</outline>
  </body>
</opml>