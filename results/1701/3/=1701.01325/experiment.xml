<?xml version="1.0" encoding="UTF-8"?>
<opml version="2.0">
  <head>
    <title></title>
    <abstract> </abstract>
  </head>
  <body>
<outline text="Experimental Results" _note="In this section, we present the experiments on text outlier analysis using matrix factorization. We used both real and synthetic data sets to test our algorithm. The real data sets correspond to the well known [RCV20]{}, [REUTERS]{} and [WIKI PEOPLE]{} data, whereas the synthetic data set was created using a well known market basket generator described later. It should be pointed out that these data sets were not originally designed for outlier analysis, and they have no ground truth information available. Therefore, some additional pre-processing needed to be applied to the real data sets, in order to isolate ground truth classes, and use them effectively for the outlier analysis problem. In this section, we will describe the data sets, their preparation, the performance criteria and the results obtained by our algorithm. At the end of this section, we will also present a discussion that provides interesting insights about the effectiveness of algorithm  .">
  <outline text="Data Sets" _note="The experiments were conducted with both labelled real and synthetic data sets. These are described below:\ [**RCV20 DATA SET:**]{} The [RCV20 DATA SET]{} [^1] is a collection of approximately 20,000 newsgroup documents, partitioned (nearly) evenly across 20 different newsgroups. We took all data points from two randomly chosen classes, which in this case corresponded to the [IBM]{} and [MAC HARDWARE]{} classes. In addition, 50 data points were chosen from one randomly chosen class, which corresponds to the [WINDOWS OPERATING SYSTEM (OS)]{} class. As it turns out, this is a rather hard problem for our algorithm because of some level of relationship between one of the rare classes and the base data. Specifically, [WINDOWS OPERATING SYSTEM]{} and [IBM HARDWARE]{} are both computer related subjects, and the former is often used with the latter. Therefore, some vocabulary is shared between the regular class and the rare class, and this makes the detection of outlier harder. We randomly permuted the position of the outliers and regular data points.\ [**REUTERS-21578 DATA SET:**]{} The documents in the [REUTERS-21578]{} collection [^2] appeared on the [REUTERS]{} newswire in 1987. It contains 21578 documents in 135 categories. Every document belongs to one or more categories. We selected those documents that belong to only one category. We chose totally 5768 documents that belong to the category [EARN]{} and [ACQ]{}. The outliers were 100 documents from category [INTEREST]{}. The vocabulary size of all the documents from these categories put together were 18933. We randomly permuted the position of the outliers and regular data points.\ \ [**MARKET BASKET DATA GENERATOR:**]{} We also wanted to understand the performance of our algorithm in some large sparse matrices that is similar to the bag of words matrix. Towards this end, we used the standard [IBM SYNTHETIC DATA GENERATION CODE FOR ASSOCIATIONS AND SEQUENTIAL PATTERNS]{} – market-basket data generator, that is packaged as part of [ILLIMINE]{}[^3] software. We set the average length of the transaction to be 300 and number of different items to be 50,000. Note that this generator uses a random seed, and by changing the seed, it is possible to completely change the transaction distribution, even if all other parameters remain the same. We generated 10,000 data points as a group of four different sets of 2500 data points with randomly chosen seed values. In addition, the rare class contained 250 data points from a single seed value. In addition, we randomly permuted the positions of the outliers and regular data points in the matrix representation, to avoid any unforeseen bias in the algorithm.  [^1]: &lt;http://qwone.com/~jason/20Newsgroups/&gt;  [^2]: &lt;http://archive.ics.uci.edu/ml/datasets/Reuters-21578+Text+Categorization+Collection&gt;  [^3]: &lt;http://illimine.cs.uiuc.edu/&gt;">
  </outline>
  <outline text="Performance Metrics" _note="The effectiveness was measured in terms of the ROC curve drawn on the outlier scores. We use the area under the Receiver Operating Characteristics(ROC) curve – the defacto metric for evaluation in outlier analysis. The idea of this curve is to evaluate a [RANKING]{} of outlier scores, by examining the tradeoff between the true positives and false positives, as the threshold on the outlier score is varied in a range. By using different thresholds, it is possible to obtain a relatively larger or smaller number of true positives with respect to the false positives.  Let be the set of outliers determined by using a threshold on the outlier scores. In this case, the [TRUE POSITIVE RATE]{} is graphed against the [FALSE POSITIVE RATE]{}. The true positive rate is defined in the same way as the metric of recall is defined in the IR literature. The false positive rate is the percentage of the falsely reported positives out of the ground-truth negatives. Therefore, for a data set with ground truth positives , these definitions are as follows: Note that the end points of the ROC curve are always at and , and a random method is expected to exhibit performance along the diagonal line connecting these points. The [LIFT]{} obtained above this diagonal line provides an idea of the accuracy of the approach. The area under the ROC curve provides a measure of the accuracy. A random algorithm would have an area of 0.5 under the ROC curve. The ROC curve was used to provide detailed insights into the tradeoffs associated with the method, whereas the area under the ROC curve was used in order to provide a summary of the performance of the method.">
  </outline>
  <outline text="Baseline Algorithms" _note="The baselines used by our approach were as follows:\ [**DISTANCE-BASED ALGORITHM:**]{} The first algorithm which was used was the -nearest neighbour algorithm, which is a classical distance-based algorithm frequently used for outlier detection . The outliers were ranked based on distances in order to create an ROC curve, rather than using a specific threshold as in . In addition, we gave the -nearest neighbour algorithm an advantage by picking a value of optimally based on area under ROC curve by sweeping from 1 to 50. Note that such an advantage would not be available to the baseline under real scenarios, since the ground-truth outliers in the data are unknown, and therefore the ROC curve cannot be optimized.\ [**SIMPLIFIED LOW RANK APPROXIMATION:**]{} We used a low rank approximation based on Singular Value Decomposition (). For a given matrix , a best -rank approximation is given by , where . That is, the trailing in the descending ordered singular values are set to . It is natural to understand that the outlier documents require linear combination of many basis vectors. Thus the norm on the can be used a score to determine the outliers. In the graphs, we use as the legend to represent this baseline. For the approach, we used the same low rank as our algorithm.">
  </outline>
  <outline text="Effectiveness Results" _note="We first present the ROC curves for the different data sets. The ROC curve for the [REUTERS]{} dataset is illustrated in Figure  . In this case, our algorithm shows a drastic improvement over both the baseline algorithms. This is evident from the rather large lift in the chart. Our algorithm had an area of 0.9340 under ROC. The -NN approach performed quite poorly, and had an area under the ROC curve of 0.5370. This is slightly better than random performance. The area under ROC for the method was 0.5816 and , which is better than the -NN method, but still significantly less than the proposed algorithm.  The comparison of our algorithm with baselines for the [RCV20]{} data set is shown in Figure  . As discussed in the data generation section, this is a particularly challenging data set, because of the similarity in the vocabulary distribution between the rare class, and the regular class. It is evident that our algorithm   performed better than the , and the -NN method. However, the lift in the ROC curve for all the methods is not particularly significant, because of the inherently challenging nature of the data set. The -NN method performed particularly poorly in this case. In a later section, we will provide some insights about the fact that some of this “poor” performance is because of the noise in the data set itself, where some of the points in the regular class should really be considered outliers. We generated a datasets in RCV20 where we just changed the outlier class to [CHRISTIAN RELIGION]{}. We received a best ROC of 0.9732 and it is not shown in Figure  .  The ROC comparison for the synthetic market basket data is illustrated in Figure  . In this case, the improvement of the algorithm   over the baseline methods was quite significant. Specifically, the algorithm  had an area under the ROC curve of 0.7598, which is a significant lift. This significantly outperformed the and . As in the case of the other data sets, the -NN algorithm performed very poorly with an area under the ROC curve of 0.5431. The consistently poor performance of the -NN approach over all algorithms is quite striking, and suggests that straightforward generalizations of outlier analysis techniques from other data domains are often not well suited to the text domain.  Clearly, conventional distance-based methods do not seem to work very well for text data.">
  </outline>
  <outline text="Parameter Sensitivity" _note="From ( ) in Section  , we can see that the parameters for our algorithm are and the low rank . We tested the algorithm for different variations in the parameters, and found that our algorithm was insensitive to changes in . In other words, for a given low rank and , the changes in the value of did not result in significant change in the area under ROC. Hence, in this paper, we provide the charts of the ROC area variation with the parameters and on the data sets.  The sensitivity results for the [REUTERS]{} data set are illustrated in Figure  . The value of is illustrated on the -axis, and different values of the low rank are graphed by different curves in the plot. It is evident in this case, that the area under the ROC increased with increase in low rank and . However the improvement started diminishing and changed very marginally at higher ranks .  The results for the [RCV20]{} and datasets are illustrated in Figure   . As in the previous case, the value of is illustrated on the -axis, and different values of the low rank are represented by different curves. In this case, the area under the ROC curve was relatively insensitive to the parameters. This implies that the algorithm can be used over a wide range of parameters, without affecting the performance too much. Finally, the results for the market basket data set are illustrated in Figure  . In this case, the area under the ROC curve decreases with increase in low rank and . This is because the market-basket data has inherently very low (implicit) dimensionality, and therefore, it is best to use a relatively low rank in order to mine the outliers.">
  </outline>
  <outline text="Further Insights" _note="In order to illustrate the inner workings of the matrix factorization approach, we provide some further insights about the statistics buried deep in the algorithm. We also present some interesting observations when outliers share the same vocabulary distribution as regular data points, as is the case for the [RCV20]{} data set. One observation is that the method of data generation implicitly assumes that all the documents within a “regular” class in a real data set are not outliers. This is of course not true in practice, since some of the documents within these classes will also be outliers, for reasons other than topical affinity. Our algorithm  was also able to detect such distinct documents, much better than the other baseline algorithms. We isolated those false positives of our algorithm  that were not detected in the baselines in the case of the [RCV20]{} data set. It was observed that while these outliers officially belonged to one of the regular classes, they did show different [KINDS]{} of distinctive characteristics. For example, while the average number of words in regular documents was 195, the “false positive” outliers chosen by our algorithm were typically either very lengthy with over 400 words, or were unusually short will less than 150 words. This behaviour was also generally reflected in the number of distinct words per document. Another observation is that these outlier documents typically had a significant vocabulary repetition over a small number of distinct words. Thus, the algorithm was also able to identify those natural outliers, which [OUGHT TO]{} have been considered outliers for reasons of statistical word distribution, as opposed to their topical behaviour.">
  </outline>
</outline>
  </body>
</opml>