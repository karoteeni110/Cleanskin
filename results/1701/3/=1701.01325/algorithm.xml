<?xml version="1.0" encoding="UTF-8"?>
<opml version="2.0">
  <head>
    <title></title>
    <abstract> </abstract>
  </head>
  <body>
<outline text="Algorithmic Solution" _note="As discussed earlier our technique is based on NMF, and this particular formulation , which is suited to outlier analysis, is relatively uncommon, and does not have a closed form solution. In order to address this issue we use a Block Coordinate Descent (BCD) framework and its application to solve the optimization problem . The BCD framework is a popular choice not only because of the ease in implementation, but also because it is scalable. First, we will lay the foundation for the basic BCD technique, as it generally applies to non-linear optimization problems. We will then relate it to our non-negative matrix factorization problem, and explain our algorithm () in detail.">
  <outline text="Block coordinate Descent" _note="In this section, we will see relevant foundation for using this framework. Consider a constrained non-linear optimization problem as follows: Here, is a closed convex subset of . An important assumption to be exploited in the BCD method is that the set is represented by a Cartesian product: where , , is a closed convex subset of , satisfying . Accordingly, the vector is partitioned as so that for . The BCD method solves for by fixing all other subvectors of in a cyclic manner. That is, if is given as the current iterate at the step, the algorithm generates the next iterate block by block, according to the solution of the following subproblem: Also known as a NON-LINEAR GAUSS-SEIDEL method , this algorithm updates one block each time, always using the most recently updated values of other blocks . This is important since it ensures that after each update, the objective function value does not increase. For a sequence where each is generated by the BCD method, the following property holds.    Suppose is continuously differentiable in , where , , are closed convex sets. Furthermore, suppose that for all and , the minimum of is uniquely attained. Let be the sequence generated by the block coordinate descent method as in Eq. . Then, every limit point of is a stationary point. The uniqueness of the minimum is not required for the case when .  The proof of this theorem for an arbitrary number of blocks is shown in Bertsekas . For a non-convex optimization problem, most algorithms only guarantee the stationarity of a limit point .  When applying the BCD method to a constrained non-linear programming problem, it is critical to wisely choose a partition of , whose Cartesian product constitutes . An important criterion is whether the sub-problems in Eq.  are efficiently solvable. For example, if the solutions of sub-problems appear in a closed form, each update can be computed fast. In addition, it is worth checking how the solutions of sub-problems depend on each other. The BCD method requires that the most recent values be used for each sub-problem in Eq. . When the solutions of sub-problems depend on each other, they have to be computed sequentially to make use of the most recent values. If solutions for some blocks are independent of each other, they can be computed simultaneously. We discuss how different choices of partitions lead to different NMF algorithms. The partitioning can be achieved in several ways, by using either matrix blocks, vector blocks or scalar blocks.">
    <outline text="BCD with Two Matrix Blocks - ANLS Method" _note="The most natural partitioning of the variables is to have two big blocks, and . In this case, following the BCD method in Eq. , we take turns solving the following:  Since the sub-problems are non-negativity constrained least squares (NLS) problems, the two-block BCD method has been called the alternating non-negative least square (ANLS) framework .">
    </outline>
    <outline text="BCD with 2k Vector Blocks - HALS/RRI Method" _note="We partition the unknowns into 2k blocks in which each block is a column/row of or . In this case, it is easier to consider the objective function in the following form: where and . The form in Eq.  represents the fact that can be approximated by the sum of rank-one matrices.  Following the BCD scheme, we can minimize by iteratively solving the following: for , and for .  The 2K-block BCD algorithm has been studied as Hierarchical Alternating Least Squares (HALS) proposed by Cichocki et al. and independently by Ho et al. as rank-one residue iteration (RRI).">
    </outline>
    <outline text="BCD with k(n + m) Scalar Blocks" _note="We can also partition the variables with the smallest element blocks of scalars, where every element of and is considered as a block in the context of  . To this end, it helps to write the objective function as a quadratic function of scalar or assuming all other elements in and are fixed:     where and denote the row and the column of , respectively.  In this paper for solving the optimization problem , we partition the matrices into vector blocks such as . The reasoning behind this partitioning is explained in the next section.">
    </outline>
  </outline>
  <outline text="()" _note="In this section, we propose an efficient algorithm for the outlier detection model .  To determine the for the aforementioned optimization problem , we use the block coordinate descent method. In other words, by fixing , we determine the optimal as vector blocks and vice versa. Due to -norm, this optimization corresponds to the two block non-smooth BCD framework.  Regarding , the minimization problem in has a separable structure: where . Therefore, we only need to define a solution with respect to one variable . Thus, we partition the matrix into vector blocks and construct as a set of vectors . Also, the blocks is independent of . That is, the closed form solution of is dependent only on . When all other blocks of , are fixed, every vector , can be solved to optimal in parallel. Thus, we adhere to BCD framework of solving the vector blocks of , to optimal, when all the other blocks are fixed.    The solution of the following minimization problem is the generalized shrinkage operator: where generalized shrinkage operator is defined as:   When , Therefore we have:  When , let then where Therefore, we get Now, utilizing the generalized shrinkage operator as defined in , where .  Now, we need to solve the following NMF model with sparsity constraints on : where . Let where and . For any , ( ) can be rewritten as The following is the framework of the block coordinate descent method with a separable regularizer such as the Frobenius norm. We iteratively minimize with respect to each column of and : where and  According to  , the solution of is independent of , and it enables us to solve the solution in parallel. This is very useful when computing for very large input matrices. Similarly, the vector blocks of can also be updated in parallel. Now, we have all the building blocks for the algorithm. We will be using   and the update for from . The Algorithm  , gives the outline of the and its complete implementation can be obtained from &lt;https://github.com/ramkikannan/outliernmf&gt; to try with any real world text dataset.  INITIALIZE **W**, **H**, **Z** AS A NONNEGATIVE RANDOM MATRIX">
  </outline>
</outline>
  </body>
</opml>