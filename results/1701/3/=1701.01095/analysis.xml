<?xml version="1.0" encoding="UTF-8"?>
<opml version="2.0">
  <head>
    <title></title>
    <abstract> </abstract>
  </head>
  <body>
<outline text="Theoretical Analysis" _note="In this section we start by proving Prop.   that provides a regret bound for TS with MVN priors that is independent from the preference function. Then we use the relations between the gap and the preference radius in three preference function families to obtain Theorem  .">
  <outline text="Proof of Prop. [prop:mvn_ts]" _note="The following analysis extends the work for the 1-dimensional setting  to the -dimensional setting. We rewrite Eq.   as where we control . The proof relies on several facts (see Appendix  ) that extend Chernoff’s inequalities and (anti-)concentration bounds from the -dimensional setting to the -dimensional setting using the concepts of Pareto-domination and preference radius. We introduce the following quantities and events to control the quality of mean estimations and the quality of samples.  For each suboptimal action , we choose a quantity , where is a preference radius. By definition of the preference radii, we have . Recall that if . Hence we have .  For each suboptimal action , define as the event that , and define as the event that . More specifically, they are the event that suboptimal action is well estimated and well sampled, respectively.  Define filtration .  For suboptimal action , we decompose and control each part separately. In (A), is played while being well estimated and well sampled. We control this by bounding poor estimation and poor samples for the optimal action. In (B), is played while being well estimated but poorly sampled. We control this using Gaussian concentration inequalities. In (C), is played while being poorly estimated. We control this using Chernoff inequalities. Gathering the following results together and summing over all suboptimal actions, we obtain Prop.  .">
    <outline text="Bounding (A)" _note="By definition of TS, for suboptimal to be played on episode , we must (at least) have . By definition of event and the preference radii, we have if . Let denote the time step at which action is selected for the time for , and let . Note that for any action , for and . Then  The second inequality uses the fact that the sampling of is independent from the events and . The last inequality uses the observation that is fixed given and that it changes only when changes, that is only when action is played. The first sum counts the number of episodes required before action has been played times. The second counts the number of episodes where is badly sampled after having been played times. We use the following Lemma to control the first summation, see Appendix  .    Let denote the time of the selection of action . Then, for any and , where is such that for .  Now we bound the second summation in Eq.   by controlling the probability of poorly sampling when . Let denote the event that . Then we have The last inequality uses Facts   and  . With we obtain We use Lem.   and Eq.   in Eq.   to obtain for , where is such that for .">
    </outline>
    <outline text="Bounding (B)" _note="We control the probability of badly sampling suboptimal action given that it has been played at least times. Recall that filtration is such that holds. To that extent we decompose The first inequality uses the observation that is fixed given and the definition of event . The second inequality uses the fact that event holds. The last inequality uses Fact  . With we obtain">
    </outline>
    <outline text="Bounding (C)" _note="Similarly to what has been done previously with (B), we can control the probability of badly estimating suboptimal action given that it has been played at least times. Then we have The second inequality uses the observation that is fixed given . The last inequality uses Fact  . With we obtain">
    </outline>
  </outline>
  <outline text="Proof of Theorem [thm:mvn_ts]" _note="By definition of the preference radii, given a linear (Ex.  ), Chebyshev (Ex.  ), or -constraint preference function (Ex.  ), one can take , . Using these values in Prop.  , we obtain Theorem  : Let , for . The regret is bounded by with , that is of order . More specifically, for , the regret bound is of order .">
  </outline>
</outline>
  </body>
</opml>