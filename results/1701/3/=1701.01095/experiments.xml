<?xml version="1.0" encoding="UTF-8"?>
<opml version="2.0">
  <head>
    <title></title>
    <abstract> </abstract>
  </head>
  <body>
<outline text="Experiments" _note="Given that the preference function is known a priori, one might be tempted to formalize the problem under the TRADITIONAL, single-objective, bandits setting. This would correspond to optimizing over the expected value of the preference function, , instead of . In the following experiments, we compare the performance of the TS algorithm from MVN priors (Alg.  ) in the multi-objective bandits scheme (Alg.  ) with the one-dimensional TS from Gaussian priors  applied to the multi-objective bandits problem formalized under the traditional bandits setting (Alg.  ).  sample play and observe  We randomly generate a 10-action setting with objectives, such that the objective space is . We consider settings where outcomes are sampled from multivariate normal distributions with covariance for all and from multi-Bernoulli distributions. A sample from a -dimensional multi-Bernoulli distribution with mean is such that . Experiments are conducted using the linear preference function and the -constraint preference function Tab.   gives the expected outcomes for all actions along with the associated preference value and gap given the preference function. Fig.   shows the expected outcomes and illustrates the preference function. We observe that the optimal action is different for the two preference functions. Each experiment is conducted over episodes and repeated 100 times. Repetitions have been made such that the noise is the same for all tested approaches on THE SAME REPETITION. Therefore we can compare the performance of different approaches on the same repetition. The goal is to minimize the cumulative regret (Eq.  ).     [0.41]{}  [0.41]{}  [0.49]{}  [0.49]{}  [0.49]{}  [0.49]{}  Fig.   shows the cumulative regret of TS from MVN priors and TS from Gaussian priors (in the traditional bandits formulation) for both outcome distributions and preference functions. We observe that the cumulative regret growth rate for TS from MVN priors appears to match the order of the provided theoretical bounds (Theorem  ). Results also show that, though it might be appealing to address a multi-objective problem as a single-objective bandits problem, it is not a good idea. Consider the -constraint preference function used in this experiment. It is evaluated as 0 if , otherwise to . With multi-Bernoulli outcomes, for example, this means that . Given that, . Since the action considered as optimal in the single-objective formulation is not the same as the optimal action in the multi-objective problem, TS with Gaussian priors converges to the WRONG action, hence the linear regret.">
</outline>
  </body>
</opml>