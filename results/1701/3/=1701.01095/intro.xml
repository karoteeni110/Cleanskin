<?xml version="1.0" encoding="UTF-8"?>
<opml version="2.0">
  <head>
    <title></title>
    <abstract> </abstract>
  </head>
  <body>
<outline text="Introduction" _note="Multi-objective optimization (MOO)  is a topic of great importance for real-world applications. Indeed, optimization problems are characterized by a number of conflicting, even contradictory, performance measures relevant to the task at hand. For example, when deciding on the healthcare treatment to follow for a given sick patient, a trade-off must be made between the efficiency of the treatment to heal the sickness, the side effects of the treatment, and the treatment cost. MOO is often tackled by combining the objective into a single measure (a.k.a. scalarization). Such approaches are said to be A PRIORI, as the preferences over the objectives is defined before carrying out the optimization itself. The challenge lies in the determination of the appropriate scalarization function to use and its parameterization. Another way to conduct MOO consists in learning the optimal trade-offs (the so-called Pareto-optimal set). Once the optimization is completed, techniques from the field of multi-criteria decision-making are applied to help the user to select the final solution from the Pareto-optimal set. These A POSTERIORI techniques may require a huge number of evaluations to have a reliable estimation of the objective values over all potential solutions. Indeed, the Pareto-optimal set can be quite large, encompassing a majority, if not all, of the potential solutions. In this work, we tackle the MOO problem where the scalarization function EXISTS a priori, but might be unknown, in which case a user can act as a black box for articulating preferences. Integrating the user to the learning loop, she can provide feedback by selecting her preferred choice given a set of options – the scalarization function lying in her head.  More specifically, we consider problems where outcomes are stochastic and costly to evaluate (e.g., involving a human in the loop). The challenge is therefore to identify the best solutions given random observations sampled from different (unknown) density distributions. We formulate this problem as multi-objective bandits, where we aim at finding the solution that maximizes the preference function while maximizing the performance of the solutions evaluated during the optimization. The Thompson sampling (TS)  technique is a typical approach for bandits problems, where potential solutions are tried based on a Bayesian posterior over their expected outcome. Here we consider TS from multivariate normal (MVN) priors for multi-objective bandits. We introduce the concept of preference radius providing the tolerance range over objective value estimations, such that the BEST OPTION given the preference function remains unchanged. We use this concept for providing a theoretical analysis of TS from MVN priors. Finally, we perform some empirical experiments to support the theoretical results and also highlight the importance of tackling multi-objective bandits problems as such instead of scalarizing those under the traditional bandit setting.">
</outline>
  </body>
</opml>