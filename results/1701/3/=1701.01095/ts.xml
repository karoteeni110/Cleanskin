<?xml version="1.0" encoding="UTF-8"?>
<opml version="2.0">
  <head>
    <title></title>
    <abstract> </abstract>
  </head>
  <body>
<outline text="Thompson Sampling" _note="The Thompson sampling (TS)  algorithm maintains a posterior distribution on the mean given a prior and the history of observations . On each episode , one option is sampled from each posterior distribution . The algorithm selects . Recall that . Therefore is proportionnal to the posterior probability that maximizes the preference function given the history . Let denote the number of times action has been played up to episode . Also let respectively denote the empirical mean and covariance, and let and denote priors. For MVN priors, the posterior over is given by a MVN distribution , where for the known covariance matrix . Since assuming that is known might be unrealistic in practice, one can consider the non-informative covariance . With non-informative priors and ,[^1] this corresponds to a direct extension of the one-dimensional TS from Gaussian priors . Alg.   shows the resulting TS procedure from MVN priors.  sample play and observe  The following proposition provides general regret bounds for TS from MVN priors. The next theorem specializes these regret bounds for three well known preference function families using the relation between preference radii and the gap, as discussed in previous examples.    Assuming -sub-Gaussian noise with , the expected regret of TS from MVN priors (Alg.  ) is bounded by where , are preference radii, , and is such that for (see Remark  ).    Assume either a linear (Ex.  ), Chebyshev (Ex.  ), or -constraint (Ex.  ) preference function. Assuming -sub-Gaussian noise with , the expected regret of TS from MVN priors (Alg.  ) is bounded by where is such that for (see Remark  ). This regret bound is of order , where . More specifically, for , it is of order .    For we can take . For we can take , for we can take , and so on for any .  For , the order of the regret bounds given by Theorem   match the order of the regret bounds for TS from Gaussian priors in the single-objective bandits setting , assuming -bounded outcomes. However we observe that the noise tolerance decreases linearly with the dimension of the objective space. This means that the more dimensions we have, the less noise we can bear in order for these bounds to hold, GIVEN THE PROVIDED ANALYSIS.  [^1]:  indicates a -elements column vector and indicates a identity     matrix.">
</outline>
  </body>
</opml>