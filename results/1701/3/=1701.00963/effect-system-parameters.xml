<?xml version="1.0" encoding="UTF-8"?>
<opml version="2.0">
  <head>
    <title></title>
    <abstract> </abstract>
  </head>
  <body>
<outline text="Setting the RADIUS Parameters" _note="        In the previous section, we presented the details of the Bayesian thresholding and the supporting techniques in RADIUS. We now study the impact of the aforementioned parameters required in each individual technique on the system performance. Specifically, we elaborate the effect of the Bayes threshold parameter and then explore the parameter space of all the parameters involved in supporting techniques. Based on detailed analysis, we give insights on the best parameter setting for an indoor office environment.  For this, we performed extensive experiments, collecting real data traces from an indoor testbed, whose details are reported in Section  . To capture different link conditions, we selected 8 sender-receiver pairs (either line-of-sight or non-line-of-sight) at different locations with various environment dynamics (e.g., human movements and obstacles). In each experiment, we simulated a good link turning into a weak link by decreasing the transmission power of the sender node from the maximum level gradually to the minimum with a packet sending rate of 5 Hz. The receiver node records the RSSI and PDR traces for more than 15 minutes. We repeated the experiment 10 times for each link. The minimum PDR that decides whether a link is a good link or a weak link is set to 80% throughout the whole analysis.">
  <outline text="Bayesian Thresholding" _note="According to Equation  , calculating the Bayes threshold requires a user-defined parameter: A PRIORI probability . Different from the general analysis depicted in Section  , we present here the detailed analysis of the impacts of on the false positive rate (FPR), false negative rate (FNR) and the total error rate.  Figure   shows the change of the error rates with varying ranging in   for 5 representative links out of the 8 analyzed links. We observe that the FPR always decreases with , while the FNR increases with . This is because a larger indicates a higher weight on the FPR in the computation of the Bayes error (see Equation  ). Hence, reducing FPR is more effective than reducing FNR to keep the Bayes error rate low for a larger .  Moreover, we can see in Figure   that the overall error rate mostly stays low regardless of the values of , except for the cases when the value of is extremely close to 0 or 1. The results confirm that the system performance with the Bayes threshold is insensitive to the setting of as long as extreme values are not considered. The reason for this is that the Bayesian thresholding approach always tries to balance between FPR and FNR for any setting.  From the figure, we can further see that a global setting from a wide range (any value not close to 0 or 1) may not be the best setting for each individual link. However, it can provide for all different links near optimal detection accuracy at the same time. In other words, with a coarse global setting of for all DAs, the Bayesian thresholding ensures RADIUS to deliver near optimal accuracy for different links under diverse link conditions without the need of tuning for each of them. For our case, we select the initial setting of at 0.8 because we assume that the probability of the links being good is generally higher than that of being weak, in our deployment environment. Additionally, to avoid the significant increase of FNR caused by the over-adjustment due to the A PRIORI probability refinement, we limit the MAXIMUM to 0.99 for our deployment environment.">
  </outline>
  <outline text="Estimating the Minimal Training Set Size" _note="As discussed in Section  , the first task of a DA before generating a Bayes threshold is to estimate the minimal training set size for each link. A proper needs to achieve a good tradeoff between detection accuracy and training latency. To apply Equation  , the computation of requires two parameters: (1) the number of first samples of RSSI for computing the standard deviation , and (2) the maximum error of the estimated mean . While the first samples of RSSI only give a quick indication, is the resultant minimal training set size, from which the DA estimates the RSSI mean and standard deviation for computing Bayes thresholds. is usually larger or at least equal to .  We first study the impact of . A small may result in a partial view of the complete channel variation, while overly large may only increase the training delay. To understand the impact of , we plot in Figure   the resultant standard deviation for various values of based on the RSSI traces of all 8 links. We observe that the values of initially have a larger variation and become more stable when is close to 250. The reason for this is that a small set of samples is insufficient to capture the overall temporal variations of RSSI, especially in an indoor environment where multi-path fading and interference are ubiquitous. Based on the result, we choose for our indoor environment. Then we focus on the impact of . According to Equation  , the choice of has a tradeoff: smaller indicates higher estimation accuracy of the RSSI mean and thus higher detection accuracy; a smaller , however, may also increase the training set size significantly. By varying the estimated errors , we plot the resultant training set size and error rates for 5 representative links in Figure  . The figure shows that the total error rate decreases significantly with a smaller at the expense of a rapidly increasing training set size. To balance between the detection error and the training time, we choose dBm for our indoor environment, which causes only a slight increase of the detection error compared to that of dBm while at the same time keeping within the scale of a few hundred samples, achieving a good tradeoff between the training latency (several minutes with a sending frequency of 5 Hz) and detection accuracy.">
  </outline>
  <outline text="Data Smoothing" _note="As mentioned in Section  , smoothing the noisy data during the detection phase requires a sliding window of size to reduce the detection error caused by the normal RSSI randomness. To see the impact of , we demonstrate how the error rate changes with different values of (window size from 1 to 15) under two representative values.  We observe from Figure   that, for both settings, increasing reduces FPR but increases FNR, which causes the total error rate to first decrease and then increase with a larger . The reason is that smoothing RSSI is effective to reduce false alarms. However, if keeps increasing, at some point, the real RSSI anomaly events are smoothed out, causing a significant increase in FNR. The impact of is also related to the setting of the minimal PDR () that defines a good link. In our case, as PDR is computed over a sliding window of 10 packets and is set to 80%, a small sliding window is preferred to avoid the significant increase in FNR. For our case, we choose , at which the total error rate is close to the lowest for both settings.">
  </outline>
  <outline text="Updating the Training Set" _note="To be adaptive to varying environment conditions, RADIUS updates the training set as discussed in Section  , dynamically generating new thresholds. For this, the relevant parameter is the update window size . We first show that the detection performance is enhanced with this updating technique, and then we discuss the impact of .  In Figure  (a), we present an RSSI trace with a valley of RSSI values (between 300 and 500 seconds) indicating an abnormal situation that causes the monitored PDR to fall below the expected performance as shown in Figure  (b). By adapting the training set and consequently the threshold (see Figure  (c)), we can see from Figure  (d) that in this experiment, the detection error can be reduced of 3%-4% with updated thresholds, down from 18% to 14% approximately.  Furthermore, we also observe from Figure  (d) that the impact of is not significant on the detection error (we set to be 50 for our example deployment). However, a larger may require a longer time to fill up the window making the threshold update less responsive in some cases. With such setting of , we observe the detection error can be reduced of 3% to 8% in all experiments. Considering that the total error rate in most of our experiments is less than 20%, such amount of reduction in the error rate is significant.">
  </outline>
  <outline text="Refinement of the A Priori Probability" _note="In addition to updating the training data set, one other situation that requires to generate a new threshold is when the detection accuracy degrades with an increasing number of false alarms, indicating the need of updating the A PRIORI probability. As described in Section  , we consider the maximum number of consecutive false alarms and the adjustment step of . We quantify the effects of these parameters in Figure  , where we compare the detection accuracy with and without the refinement of the A PRIORI probability. Specifically, we show the detection performance with varying and . We use the suggested values in the above sections for the other parameters.  From Figure  , we can see that a smaller can reduce FPR but it may also cause a significant increase in FNR due to over-adjustment. On the other hand, a larger makes the system conservative on the A PRIORI probability refinement and hence the refinement less effective. The optimal choice of falls at the location where the total error rate is lowest. In addition, the choice of the parameter needs to consider a tradeoff: larger indicates a more effective adjustment but a higher risk of over-adjustment. In our example, we choose and . With such parameter settings, the analysis of all data traces shows that based on the accuracy improvement achieved by the training set updating technique, refining can further reduce the error rate in a range from 2% to 5%.">
  </outline>
</outline>
  </body>
</opml>