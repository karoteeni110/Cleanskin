<?xml version="1.0" encoding="UTF-8"?>
<opml version="2.0">
  <head>
    <title></title>
    <abstract>.1cm This paper discusses minimum distance estimation method in the
linear regression model with dependent errors which are strongly mixing.
The regression parameters are estimated through the minimum distance
estimation method, and asymptotic distributional properties of the
estimators are discussed. A simulation study compares the performance of
the minimum distance estimator with other well celebrated estimator.
This simulation study shows the superiority of the minimum distance
estimator over another estimator. `KoulMde` (`R` package) which was used
for the simulation study is available online. See section for the
detail. </abstract>
  </head>
  <body>
<outline text="Introduction" _note="Consider the linear regression model   y\_[i]{} = \_i’ + \_i, where , with being non random design variables, and where is the parameter vector of interest. The methodology where the estimators are obtained by minimizing some dispersions or pseudo distances between the data and the underlying model is referred to as the minimum distance (m.d.) estimation method. In this paper we estimate regression parameter vector by the m.d.estimation method when the collection of in the model () is a dependent process.  Let be independent identically distributed (i.i.d.) random variables (r.v.’s) with distribution function (d.f.) where is unknown. The classical m.d.estimator of is obtained by minimizing following Cramr-von Mises (CvM) type -distance {G\_[n]{}(y) - G\_(y) }\^2 dH(y) where is an empirical d.f. of ’s and is a integrating measure. There are multiple reasons as to why CvM type distance is preferred, including the asymptotic normality of the corresponding m.d.estimator; see, e.g., Parr and Schucany (1980), Parr (1981) and Millar (1981). Many researchers have tried various ’s to obtain the m.d.estimators. Anderson and Darling (1952) proposed Anderson-Darling estimator obtained by using for . Another important example includes , giving a rise to Hodges - Lehmann type estimators. If and of the integrand are replaced with kernel density estimator and assumed density function of , the Hellinger distance estimators will be obtained; see Beran (1977).  Departing from one sample setup, Koul and DeWet (1983) extended the domain of the application of the m.d.estimation to the regression setup. On the assumption that ’s are i.i.d.r.v.’s with a known d.f., they proposed a class of the m.d.estimators by minimizing -distances between a weighted empirical d.f.and the error d.f.. Koul (2002) extended this methodology to the case where error distribution is unknown but symmetric around zero. Furthermore, it was shown therein that when the regression model has independent non-Gaussian errors the m.d.estimators of the regression parameters — obtained by minimizing -distance with various integrating measures — have the least asymptotic variance among other estimators including Wilcoxon rank, the least absolute deviation (LAD), the ordinary least squares (OLS) and normal scores estimators of : e.g.the m.d.estimators obtained with a degenerate integrating measure display the least asymptotic variance when errors are independent Laplace r.v’s.  However, the efficiency of the m.d.estimators depends on the assumption that errors are independent; with the errors being dependent, the m.d.estimation method will be less efficient than other estimators. Examples of the more efficient methods include the generalized least squares (GLS); GLS is nothing but regression of transformed on transformed . The most prominent advantage of using the GLS method is “decorrelation&quot; of errors as a result of the transformation. Motivated by efficiency of m.d.estimators — which was demonstrated in the case of independent non-Gaussian errors — and the desirable property of the GLS (decorrelation of the dependent errors), the author proposes generalized m.d.estimation method which is a mixture of the m.d.and the GLS methods: the m.d.estimation will be applied to the transformed variables. “Generalized&quot; means the domain of the application of the m.d.method covers the case of dependent errors; to some extent, the main result of this paper generalizes the work of Koul (2002). As the efficiency of the m.d.method is demonstrated in the case of independent errors, the main goal of this paper is to show that the generalized m.d.estimation method is still competitive when the linear regression model has dependent errors; indeed, the simulation study empirically shows that the main goal is achieved.  The rest of this article is organized as follows. In the next section, characteristics of dependent errors used through this paper is studied. Also, the CvM type distance and various processes — which we need in order to obtain the estimators of — will be introduced. Section describes the asymptotic distributions and some optimal properties of the estimators. Findings of a finite sample simulations are described in Section . All the proofs are deferred until Appendix. In the remainder of the paper, an Italic and boldfaced variable denotes a vector while a non-Italic and boldfaced variable denotes a matrix. An identity matrix will carry a suffix showing its dimension: e.g. denotes a identity matrix. For a function , let denote . For a real vector , denotes Euclidean norm. For any r.v., denotes . For a real matrix and , means that its entries are functions of .">
</outline>
<outline text="Strongly mixing process &amp;amp; CvM type distance" _note="Let be the -field generated by . The sequence is said to satisfy the strongly mixing condition if (k):={ |P(AB)-P(A)P(B)|:A[[F]{}]{}\_[-]{}\^[0]{}, B[[F]{}]{}\_[k]{}\^ }0, as . is referred to as mixing number. Chanda (1974), Gorodetskii (1977), Koul (1977), and Withers (1979) investigated the decay rate of the mixing number. Having roots in their works, Section defines the decay rate assumed in this paper; see, e.g., the assumption (a.8). Hereinafter the errors ’s are assumed to be strongly mixing with mixing number . In addition, is assumed to be stationary and symmetric around zero.  Next, we introduce the basic processes and the distance which are required to obtain desired result. Recall the model (). Let denote the design matrix whose th row vector is . Then the model () can be expressed as = **X**+, where and Let be any real matrix so that the inverse of is a positive definite symmetric matrix. Note that the diagonalization of positive definite symmetric matrix guarantees the existence of which is also a symmetric matrix. Let for denote the th row vector of . Define transformed variables \_[i]{} = \_[i]{}’,\_[2]{}’=\_[i]{}’**X**,\_[i]{} = \_[i]{}’,1in. As in the GLS method, obtained from covariance matrix of transforms dependent errors into uncorrelated ones, i.e., “decorrelates&quot; the errors. However, the GLS obtains in a slightly different manner. Instead of using , the GLS equates to the inverse of the covariance matrix, i.e., the GLS uses Cholesky decomposition. The empirical result in Section describes that from the diagonalization yields better estimators. Here we propose the class of the generalized m.d.estimators of the regression parameter upon varying . We impose Noether (1949) condition on . Now let and denote th column of . Let , , , be an matrix of real numbers and denote th column of . As stated in Koul (2002, p.60), if ( i.e., ), then under Noether condition, \_[i]{}\^[n]{}d\_[ik]{}\^[2]{}= 1, \_[1in]{}d\_[ik]{}\^[2]{}= o(1)1kp. Next, define CvM type distance from which the generalized m.d.estimator are obtained. Let and denote the density function and the d.f.of , respectively. Analogue of () — with and being replaced by empirical d.f.of and — will be a reasonable candidate. However, the d.f. is rarely known. Since the original regression error ’s are assumed to be symmetric, the transformed error ’s are also symmetric; therefore we introduce, as in Koul (2002; Definition 5.3.1), U\_[k]{}(y,; **Q**) &amp;:=&amp; d\_[ik]{}{I( \_[i]{}’-\_[i]{}’**X** y) - I(-\_[i]{}’+\_[i]{}’**X** &amp;lt; y) },\ (y,; **Q**)&amp;:=&amp; (U\_[1]{}(y,; **Q**),...,U\_[p]{}(y,; **Q**))’, y,\ [[L]{}]{}(; **Q**) &amp;:=&amp; (y,; **Q**)\^[2]{} d H(y), \^[p]{},\ &amp;=&amp; \_[k=1]{}\^[p]{}\^[2]{}, where is an indicator function, and is a finite measure on and symmetric around 0, i.e., . Subsequently, define as [[L]{}]{}(; **Q**)=\_[[L]{}]{}(; **Q**). Next, define \_[i]{}:=,:=, where is the th row vector of and ; observe that and are and matrices, respectively. Define a matrix so that its th entry is : e.g., th entry is for all and all other entries are zeros. Finally, define following matrices: \_[**D**]{}:=**I**\_[f]{}’(y)**DD**’**I**\_[f]{}(y)dH(y),:=**AX**’’\_[**D**]{}**XA**, which are needed for the asymptotic properties of . Let and . Note that =**AX**’ **XA**.">
</outline>
<outline text="Asymptotic distribution of" _note="In this section we investigate the asymptotic distribution of under the current setup. Note that minimizing does not have the closed form solutions; only numerical solutions can be tried, and hence it would be impracticable to derive asymptotic distribution of . To redress this issue, define for \^[\*]{}(;**Q**) = (y,;**Q**) + 2\_[**DA**]{}(y)**A**\^[-1]{}(-) \^[2]{}d H(y), where is a matrix. Next, define \^[\*]{}(;**Q**)=\_[[[L]{}]{}]{}\^[\*]{}(;**Q**). Unlike , minimizing has the closed form solution. Therefore, it is not unreasonable to approximate the asymptotic distribution of by one of if can be approximated by . This idea is plausible under certain conditions which are called UNIFORMLY LOCALLY ASYMPTOTICALLY QUADRATIC; see Koul (2002, p.159) for the detail. Under these conditions, it was shown that difference between and converges to zero in probability; see theorem 5.4.1. The basic method of deriving the asymptotic properties of is similar to that of sections 5.4, 5.5 of Koul (2002). This method amounts to showing that is uniformly locally asymptotically quadratic in belonging to a bounded set and To achieve these goals we need the following assumptions which in turn have roots in section 5.5 of Koul (2002).  (a.1)  :   The matrix is nonsingular and, with , satisfies \_[n]{}     n\_[1jp]{}\_[j]{}\^2&amp;lt;.  (a.2)  :   The integrating measure is finite and symmetric around 0, and     \_[0]{}\^ (1-F\_[i]{})\^[1/2]{}dH &amp;lt;,1in.  (a.3)  :   For any real sequences , , , \_[n]{}     \_[a\_[n]{}]{}\^[b\_[n]{}]{}f\_[i]{}(y+x)dH(y)dx=0,1in.  (a.4)  :   For , define , . Let . For all , , for all , and for all , \_[n]{}     \^[2]{}d H(y)c\^[2]{}, where does not depend on and .  (a.5)  :   For each and all , \^[2]{}d H(y)=o(1).  (a.6)  :    has a continuous density with respect to the Lebesgue measure on     for .  (a.7)  :   , for and .  (a.8)  :   The in the model () is strongly mixing with mixing number satisfying     \_[n]{} \_[k=1]{}\^[n-1]{}k\^[2]{}(k)&amp;lt;.  Note that (a.1) implies Noether condition and (a.2) implies . From Corollary 5.6.3 of Koul (2002), we note that in the case of i.i.d.errors, the asymptotic normality of was established under the weaker conditions: Noether condition and . The dependence of the errors now forces us to assume two stronger conditions (a.1) and (a.2).  Here we discuss examples of and that satisfy (A.2). Clearly it is satisfied by any finite measure . Next consider the -finite measure given by , a continuous d.f. symmetric around zero. Then and \_[0]{}\^ (1-F\_[i]{})\^[1/2]{}d H = \_[0]{}\^ dF\_[i]{} 2 \_[1/2]{}\^[1]{} (1-u)\^[-1/2]{} du &amp;lt; . Another useful example of a -finite measure is given by . For this measure, (a.2) is satisfied by many symmetric error d.f.s including normal, logistic, and Laplace. For example, for normal d.f., we do not have a closed form of the integral, but by using the well celebrated tail bound for normal distribution — see e.g., Theorem 1.4 of Durrett (2005) — we obtain \_[0]{}\^ {1-F\_[i]{}(y)}\^[1/2]{}dy (2)\^[-1/2]{}\_[0]{}\^ y\^[-1/2]{}(-y\^[2]{}/4) dy = (2/)\^[1/2]{}(1/4). Recall from Koul (2002) that the corresponding to is the extensions of the one sample Hodges-Lehmann estimator of the location parameter to the above regression model.  Consider condition (a.7). If ’s are bounded then implies the other two conditions in (a.7) for any -finite measure . For , when ’s are normal, logistic or Laplace densities. In particular, when and ’s are logistic d.f.’s, so that , this condition is also satisfied.  We are ready to state the needed results. The first theorem establishes the needed uniformly locally asymptotically quadraticity while the corollary shows the boundedness of a suitably standardized . Theorem 3.1 and Corollary 3.1 are counterparts of conditions (A) and (A5) in theorem 5.4.1 of Koul (2002), respectively. Note that condition (A4) in theorem 5.4.1 is met by () in the Appendix; condition (A6) in theorem 5.4.1 is trivial.  LET BE IN THE MODEL (). ASSUME THAT (A.1)-(A.8) HOLD. THEN, FOR ANY , E\_[A\^[-1]{}(b-)c]{} [[L]{}]{}(;**Q**)-[[L]{}]{}\^[\*]{}(;**Q**) =o(1).  **Proof.** See Appendix.  SUPPOSE THAT THE ASSUMPTIONS OF THEOREM HOLD. THEN FOR ANY , THERE EXISTS AN , AND SUCH THAT P(\_[A\^[-1]{}(b-)c\_]{}[[L]{}]{}(;**Q**)M )1-, nN\_.  **Proof.** See Appendix.  UNDER THE ASSUMPTIONS OF THEOREM , **A**\^[-1]{}( - )= - \^[-1]{} **AX**’’ **I**’\_[f]{}(y)**D** (y,)d H(y)+o\_[p]{}(1), WHERE IS AS IN ().  **Proof**. Note that the first term in the right-hand side is nothing but . Therefore, the proof follows from Theorem and Corollary , as in i.i.d.case illustrated in the theorem 5.4.1 of Koul (2002).  Next, define &amp;&amp;\_[i]{}(x):= \_[-]{}\^[-x]{}f\_[i]{}(y)d H(y) - \_[-]{}\^[x]{}f\_[i]{}(y)d H(y),\ &amp;&amp;**Z**\_[n]{} := **I**’\_[f]{}(y)**D** (y,)d H(y). Symmetry of the around 0 yields for . Let denote covariance matrix of . Define a matrix and write , , where \_[ij]{}=\_[l=1]{}\^[n]{}\_[h=1]{}\^[n]{}d\_[il]{}\^[\*]{}d\_[jh]{}\^[\*]{}E. Observe that &amp;&amp;\_[**ZZ**]{}= E(**AX**’’**Z**\_[n]{}**Z**\_[n]{}’**XA**) = **AX**’**Q**’\_**QXA**. Now, we are ready to state the asymptotic distribution of .  ASSUME IS POSITIVE DEFINITE FOR ALL . IN ADDITION, ASSUME THAT \_[u\^[p]{}, u=1]{} ’\^[-1]{}=O(1). THEN \_[**ZZ**]{}\^[-1/2]{} **X**’’**Z**\_[n]{}\_[d]{} N(,**I**\_[pp]{}), WHERE AND IS THE IDENTITY MATRIX.  **Proof.** To prove the claim, it suffices to show that for any , is asymptotically normally distributed. Note that ’\^[-1/2]{} **AX**’’**Z**\_[n]{} = , which is the sum as in the theorem 3.1 from Mehra and Rao (1975) with and . Note that \_[c]{}\^[2]{}:= c\_[ni]{}\^[2]{}= ’\_[**ZZ**]{}\^[-1]{},\_[n]{}\^[2]{}:= E{ }\^2 = \^[2]{}. Also, observe that \_[1in]{} c\_[ni]{}\^[2]{}/ \_[c]{}\^[2]{} \_[1in]{} =\_[1in]{} **AX**’\_[i]{}\^2 0, by assumption (A.1). Finally, we obtain \_[n]{}\_[n]{}\^[2]{}/\_[c]{}\^[2]{} \^[2]{}/(’\_[**ZZ**]{}\^[-1]{}) &amp;gt; 0, by the assumption that the terms in the denominator is . Hence, the desired result follows from the theorem 3.1 of Mehra and Rao (1975).  IN ADDITION TO THE ASSUMPTIONS OF THEOREM , LET THE ASSUMPTION OF LEMMA HOLD. THEN \_[**ZZ**]{}\^[-1/2]{} **A**\^[-1]{}( - )\_[d]{} 2\^[-1]{}N(, **I**\_[pp]{}).  **Proof**. Claim follows from Lemma upon noting that **Z**\_[n]{} = **I**’\_[f]{}(y)**D** (y,)d H(y).  Let denote the asymptotic variance of . Then we have Asym() &amp;=&amp; 4\^[-1]{}**A**\^[-1]{} \_[**ZZ**]{} \^[-1]{}**A**\ &amp;=&amp; 4\^[-1]{}**A** (**AX**’’\_[**D**]{}**XA**)\^[-1]{} (**AX**’**Q**’\_**QXA**) (**AX**’’\_[**D**]{}**XA**)\^[-1]{}**A** Observe that if all the transformed errors have the same distribution, i.e., , we have **AX**’’\_[**D**]{}**XA**=(|f\_[1]{}|\_[H]{}\^[2]{})\^[-1]{}**I**\_[pp]{}. Therefore, will be simplified as (2|f\_[1]{}|\_[H]{}\^[2]{})\^[-2]{}**A**(**AX**’**Q**’\_**QXA**)**A**. Moreover, if all the transformed errors are uncorrelated as a result of the transformation, can be simplified further as (2|f\_[1]{}|\_[H]{}\^[2]{})\^[-2]{} (**X**’**Q**\^[2]{}**X**)\^[-1]{}, where .">
</outline>
<outline text="Simulation studies" _note="In this section the performance of the generalized m.d.estimator is compared with one of the GLS estimators. Let and denote covariance matrix of the errors and its estimate, respectively. Consequently we obtain the GLS estimator of \_[GLS]{}=(**X**’\^[-1]{}**X**)(**X**’\^[-1]{}). In order to obtain the generalized m.d.estimator, we try two different ’s: and where **Q**\_[s]{}\^[2]{}=\^[-1]{},**Q**\_[c]{}’**Q**\_[c]{}=\^[-1]{}. We refer to the generalized m.d.estimators corresponding to and as GMD1 and GMD2 estimators, respectively.  In order to generate strongly mixing process for the dependent errors, the several restrictive conditions are required so that the mixing number decays fast enough — i.e., the assumption (a.8) is met. Withers (1981) proposed the upperbound and the decay rate of the mixing number . For the shake of completeness, we reproduce Theorem and Corollary 1 here.  LET BE INDEPENDENT R.V.S ON R WITH CHARACTERISTIC FUNCTIONS SUCH THAT (2)\^[-1]{}\_[i]{}|\_[i]{}(t)|dt&amp;lt;AND \_[i]{} E|\_[i]{}|\^&amp;lt;&amp;gt;0. LET BE A SEQUENCE OF COMPLEX NUMBERS SUCH THAT {S\_[t]{}((1,))}\^[(1,)]{}0 tWHERE S\_[t]{}()=\_[v=t]{}\^|g\_[v]{}|\^. ASSUME THAT g\_[v]{}=O(v\^[-]{})&amp;gt;1+\^[-1]{}+(1,\^[-1]{}). THEN THE SEQUENCE IS STRONGLY MIXING WITH MIXING NUMBER WHERE = (-(,1))(1+)\^[-1]{}-1 &amp;gt; 0.  To generate strongly mixing process by Lemma , we consider four independent ’s: normal, Laplace, logistic, and mixture of the two normals (MTN). Note that all the ’s have the finite second moments, and hence, we set at 2. It can be easily seen that for any we have , and hence the assumption (a.8) is satisfied. Then for \_[n]{}=\_[v=0]{}\^v\^[-(7+)]{}\_[n-v]{} satisfies the strongly mixing condition with . We let , or equivalently, .  The has a Laplace distribution if its density function is f\_[La]{}(x):=(2s\_[1]{})\^[-1]{}(-|x-\_[1]{}|/s\_[1]{}) while the density function of Logistic innovation is given by f\_[Lo]{}(x):= s\_[2]{}\^[-1]{}(-|x-\_[2]{}|/s\_[2]{})/(1+(-|x-\_[2]{}|/s\_[2]{}))\^[2]{}. When we generate , we set mean of normal, Laplace, and logistic innovations at 0 (i.e., ) since we assumed the , the sum of ’s, is symmetric. We set the standard deviation of normal at 2 while both and are set at 5 for Laplace and logistic, respectively. For MTN, we consider where . In each , we subsequently generate using ().  Next, we set the true , i.e., . For each , we obtain in as a random sample from the uniform distribution on ; is subsequently generated using models (). We estimate by the generalized m.d.and the GLS methods. We report empirical bias, standard error (SE), and mean squared error (MSE) of these estimators. We use the Lebesgue integrating measure, i.e., . To obtain the generalized m.d.estimators, the author used `R` package `KoulMde`. The package is available from Comprehensive `R` Archive Network (CRAN) at &lt;https://cran.r-project.org/web/packages/KoulMde/index.html&gt;. Table 1 and 2 report biases, SE’s and MSE’s of estimators for the sample sizes 50 and 100, each repeated 1,000 times. The author used High Performance Computing Center (HPCC) to accelerate the simulations. All of the simulations were done in the `R`-3.2.2.   N, La, Lo, and M denote normal, Laplace, logistic and MTN, respectively.  As we expected, both biases and SE’s of all estimators decrease as increases. First, we consider the normal ’s. When ’s are normal, the GLS and GMD1 estimators display the best performance; GLS and GMD1 show similar biases, SE’s, and hence MSE’s. GMD2 estimators show slightly worse performance than aforementioned ones; they display similar or smaller bias — e.g. estimators corresponding to and — while they always have larger SE’s which in turn cause larger MSE’s. Therefore, we conclude that GLS and GMD1 show similar performance to each other but better one than GMD2 when ’s are normal.  For non-Gaussian ’s, we come up with a different conclusion: the GMD1 estimators outperform all other estimators while The GLS and GMD2 estimators display the similar performance. Note that weighing the merits of the GLS, the GMD1, and the GMD2 estimators in terms of bias is hard. For example, for the Laplace when , the GLS and GMD1 estimators of all ’s show the almost same biases; the GMD2 estimator of () show smaller (larger) bias than the GLS and the GMD1 estimators. When we consider the SE, the GMD1 estimators display the least SE’s regardless of ’s and ’s. The GLS and the GMD2 estimators show somewhat similar SE’s when is Laplace or logistic; however, the GMD2 estimators have smaller SE’s than the GLS ones when is MTN. As a result, the GMD1 estimators display the least MSE for all non-Gaussian ’s and ’s; the GMD2 and the GLS — corresponding to Laplace or logistic ’s — show similar MSE’s while the GMD2 estimators show smaller MSE than the GLS ones when is MTN.">
</outline>
<outline text="Appendix" _note="**Proof of Theorem .** Section 5.5 of Koul (2002) illustrates () holds for independent errors. Proof of the theorem, therefore, will be similar to the one of Theorem 5.5.1 in that section. Define for , , J\_[k]{}(y,)&amp;:=&amp; d\_[ik]{}F\_[i]{}(y + \_[i]{}’**XA**),Y\_[k]{}(y,) :=d\_[ik]{}I(\_[i]{}’y + \_[i]{}’**XA** ),\ W\_[k]{}(y,) &amp;:=&amp; Y\_[k]{}(y,) - J\_[k]{}(y,).Rewrite [[L]{}]{}(+ **A**;**Q**) &amp;=&amp; \_[k=1]{}\^[p]{} \^[2]{}d H(y). where . Note that the last term of the integrand is the th coordinate of vector in . If we can show that suprema of norms of the first four terms of the integrand are , then applying Cauchy-Schwarz (C-S) inequality on the cross product terms in will complete the proof. Therefore to prove theorem it suffices to show that for all E|W\_[k]{}(y, )-W\_[k]{}(y,)|\^[2]{} d H(y)=o(1), | (J\_[k]{}(y,) - J\_[k]{}(y,)) - d\_[ik]{}\_[i]{}’**XA**f\_[i]{}(y)|\^[2]{} d H(y)=o(1), E | U\_[k]{}(y,) + 2d\_[ik]{}\_[i]{}’**XA**f\_[i]{}(y) |\^[2]{} d H(y) =O(1). where sup is taken over . Here we consider the proof of the case only. The similar facts will hold for the case .  Observe that (A.2) implies EU\_[k]{}(y,)\^[2]{}dH(y)2n\_[1in]{}\_[i]{}\^2 \_[1in]{} (1-F\_[i]{})dH&amp;lt;. Therefore, () immediately follows from (A.2) and (A.7). The proof of () does not involve the dependence of errors, and hence, it is the same as the proof of (5.5.11) of Koul (2002). Thus, we shall prove (), thereby completing the proof of theorem.  To begin with let , , and denote , and in when is replaced with so that , , and . Define for , , p\_[i]{}(y,; **X**)&amp;:=&amp; F\_[i]{}(y+\_[i]{}’**XA**)-F(y),\ B\_[ni]{}&amp;:=&amp; I( \_[i]{}’y + \_[i]{}’**XA**) -I(\_[i]{}’y ) - p\_[i]{}(y,; **X**). Rewrite W\_[ku]{}\^-W\_[k0]{}\^ = d\_[ik]{}\^{ I( \_[i]{}’y + \_[i]{}’**XA**) -I(\_[i]{}’y ) - p\_[i]{}(y,; **X**) }. Note that EB\_[ni]{}\^[2]{} F\_[i]{}(y+ \_[i]{})-F\_[i]{}(y) Recall a lemma from Deo(1973).  Suppose for each , are strongly mixing random variables with mixing number . Suppose and are two random variables respectively measurable with respect to and , . Assume and are such that , and and . Then for each |E(XY)-E(X)E(Y)|10\_[n]{}\^[1/r]{}(m)X\_[p]{}Y\_[q]{}. Consequently if then for and each |E(XY)-E(X)E(Y)|10\_[n]{}\^[1-1/q]{}(m)Y\_[q]{}.  In addition, consider following lemma.  For , n\^[-1]{}\_[i=1]{}\^[n-1]{}\_[k=1]{}\^[n-i]{}\^[1/r]{}(k)=O(1).  **Proof**. For given , let such that . Note that = &amp;gt;. Therefore, by Hlder’s inequality with and , we have n\^[-1]{}\_[i=1]{}\^[n-1]{}\_[k=1]{}\^[n-i]{}\^[1/r]{}(k) &amp;&amp; (\_[k=1]{}\^[n-1]{} )\^[1/p]{} (\_[k=1]{}\^[n-1]{}k\^[2]{}(k))\^[1/r]{} &amp;lt;. The last inequality follows from the assumption (A.8.6), thereby completing the proof of lemma.  Now, we consider the cross product terms of . &amp;&amp;|E \_[j=i+1]{}\^[n]{} d H(y)|\ &amp;&amp; \_[j=i+1]{}\^[n]{} d\_[ik]{}d\_[jk]{}| E B\_[ni]{}B\_[nj]{} |d H\ &amp;&amp; 10\_[j=i+1]{}\^[n]{} d\_[ik]{}d\_[jk]{}\^[1/2]{}(j-i)B\_[nj]{}\_[2]{} d H\ &amp;&amp; 10b\^[1/2]{}{n\_[i]{}d\_[ik]{}\^[2]{}}{\_[i]{}\_[i]{}}\^[1/2]{}n\^[-1]{}\_[i=1]{}\^[n-1]{}\_[m=1]{}\^[n-i]{} \^[1/2]{}(m) f\_[1]{}\^[1/2]{} d H 0.The second inequality follows from Lemma , and the convergence to zero follows from the Lemma with , (A.1), and (A.7). Consequently, by Fubini’s Theorem together with (A.3), we obtain, for every fixed , \_[n]{}E|W\_[ku]{}-W\_[k0]{}|\^[2]{}\_[H]{} &amp;&amp; \_[n]{} d\_[ik]{}\^[2]{}|F\_[i]{}(y+\_[i]{}’**XA**)-F(y) | d H(y)\ &amp;&amp;\_[n]{} {n\_[i]{}d\_[ik]{}\^[2]{}} \_[-a\_[n]{}]{}\^[a\_[n]{}]{}f\_[i]{}(y+s) d H(y)ds\ &amp;=&amp;0,where .  To complete the proof of (), it suffices to show that for all , there exists a such that for all , , \_[n]{} E\_[u-v]{}|[[K]{}]{}\_[ku]{}-[[K]{}]{}\_[kv]{}|, where [[K]{}]{}\_[ku]{}:=|W\_[ku]{}-W\_[k0]{}|\_[H]{}\^[2]{}, \^[p]{},1kp. () follows from (5.5.5) of Koul (2002), thereby completing the proof of theorem.  **Proof of Corollary .** The proof of () for independent errors can again be found in the section 5.5 of Koul (2002). The difference between the proof in the section 5.5 and one here arises only in the part which involves the dependence of the error. Thus, we present only the proof of an analogue of (5.5.27) in Koul (2002). Let L\_[k]{}&amp;:=&amp; f\_[1]{}(y)d H(y)\ &amp;=&amp; f\_[1]{}(y)d H(y),\ &amp;:=&amp; (L\_[1]{},...,L\_[p]{}).Note that . By the symmetry of and Fubini’s theorem, we obtain E{ I(\_[i]{}’y) - I(-\_[i]{}’ &amp;lt; y) }\^[2]{}d H(y)=4\_[0]{}\^(1-F\_[i]{})d H,1in. In addition, Lemma yields, for , &amp;&amp;E{ I(\_[i]{}’y) - I(-\_[i]{}’ &amp;lt; y) }{ I(\_[j]{}’y) - I(-\_[j]{}’ &amp;lt; y) }d H(y)\ &amp;&amp; 20 \^[1/2]{}(j-i)\_[1in]{}\_[0]{}\^(1-F\_[i]{})\^[1/2]{}d H.Together with the fact that , by (A.1), (A.2), (A.7), and Lemma , we obtain, for some , E\^[2]{}&amp;&amp; \_[k=1]{}\^[p]{}|f\_[1]{}|\_[H]{}\^[2]{}{ 4\_[i=1]{}\^[n]{}d\_[ik]{}\^[2]{}\_[0]{}\^(1-F\_[i]{})d H + 40n\_[1in]{}d\_[ik]{}\^[2]{}\ &amp; &amp; n\^[-1]{}\_[i=1]{}\^[n]{}\_[j=i+1]{}\^[n]{}\^[1/2]{}(j-i)\_[1in]{}\_[0]{}\^(1-F\_[i]{})\^[1/2]{}d H }\ &amp;&amp;lt;&amp;Cp|f\_[1]{}|\_[H]{}\^[2]{}\_[1in]{}\_[0]{}\^(1-F\_[i]{})\^[1/2]{}d H. Using for and Chebyshev inequality, for all there exists and such that P(c\_) 1- 1- /2,nN\_[1]{}. The rest of the proof will be the same as the proof of Lemma 5.5.4 of Koul (2002).  [17]{}  Beran, R. J. (1977). Minimum Hellinger distance estimates for parametric models. ANN. STATIST., **5** 445-463.  Deo, C. M. (1973). A note on empirical processes of strong mixing sequences. ANN. PROBAB., **1** 870-875.  Koul, H. L. (1977). Behavior of robust estimators in the regression model with dependent errors. ANN. STATIST., **5** 681-699.  Koul, H. L. (1985). Minimum distance estimation in linear regression with unknown error distributions. STATIST. PROBAB. LETT., **3** 1-8.  Koul, H. L. (1986). Minimum distance estimation and goodness-of-fit tests in first-order autoregression. ANN. STATIST., **14** 1194-1213.  Koul, H. L. (2002). WEIGHTED EMPIRICAL PROCESS IN NONLINEAR DYNAMIC MODELS. Springer, Berlin, Vol. 166.  Koul, H. L. and De Wet, T. (1983). Minimum distance estimation in a linear regression model. ANN. STATIST., **11** 921-932.  Mehra, K. L. and Rao, M. S. (1975). Weak convergence of generalized empirical processes relative to under strong mixing. ANN. PROBAB., **3** 979-991.  Millar, P. W. (1981). Robust estimation via minimum distance methods. ZEIT FUR WAHRSCHEINLICHKEITSTHEORIE., **55** 73-89.  Noether, G. E. (1949). On a theorem by Wald and Wolfowitz. ANN. MATH STATIST., **20** 445-458.  Parr, W. C. and Schucany, W. R. (1979). Minimum distance and robust estimation. J. AMER. STATIST. ASSOC., **75** 616-624.  Prescitt, E. C. and Schucany, W. R. (1987). Theory ahead of Business Cycle Measurement. Carnegie-Rochester Conference on Public Policy.">
</outline>
<outline text="Appendix" _note="**Proofs of Section**\ **Proof of Theorem .** Section 5.5 of Koul (2002) illustrates the claim holds for independent errors. Proof of the theorem, therefore, will be similar to the one of Theorem 5.5.1 in that section. Define for , , J\_[k]{}(y,u)&amp;:=&amp; d\_[ik]{}F(y + u’Ax\_[i]{}),Y\_[k]{}(y,u) :=d\_[ik]{}I(\_[i]{}y + u’Ax\_[i]{} ),\ W\_[k]{}(y,u) &amp;:=&amp; Y\_[k]{}(y,u) - J\_[k]{}(y,u),where are in the model , and . Note that , where is elementary vector whose th entry is 1. Therefore, rewrite T(+ Au) &amp;=&amp; \_[k=1]{}\^[p+1]{} \^[2]{}d H(y). Note that the last term of the integrand is the th coordinate of vector in . If we can show that suprema of norms of the first four terms of the integrand are , then applying Cauchy-Schwarz (C-S) inequality on the cross product terms in will complete the proof. Therefore to prove theorem it suffices to show that for all , &amp;&amp;E|W\_[k]{}(y,u)-W\_[k]{}(y,0)|\^[2]{} d H(y)=o(1),\ &amp;&amp;| (J\_[k]{}(y,u) - J\_[k]{}(y,0)) - u’e\_[k]{}f(y)|\^[2]{} d H(y)=o(1),\ &amp;&amp;E | U\_[k]{}(y,) + 2u’e\_[k]{}f(y)|\^[2]{} d H(y) =O(1). where sup is taken over .  First consider (). By the symmetry of and and Fubini’s theorem, we obtain E{ I(\_[i]{}y) - I(-\_[i]{} &amp;lt; y) }\^[2]{}d H(y)=4\_[0]{}\^(1-F)d H. In addition, Lemma yields, for , &amp;&amp;E{ I(\_[i]{}y) - I(-\_[i]{} &amp;lt; y) }{ I(\_[j]{}y) - I(-\_[j]{} &amp;lt; y) }d H(y)\ &amp;&amp; 20 \_[l]{}\^[1/2]{}(j-i)\_[0]{}\^(1-F)\^[1/2]{}d H.Together with the fact that , by (A.1), (A.2), (A.7), and Lemma , we obtain EU\_[k]{}(y,)\^[2]{}dH(y) &amp;&amp; 4 n\_[1in]{}d\_[ik]{}\^[2]{}\_[0]{}\^(1-F)d H + 40n\_[1in]{}d\_[ik]{}\^[2]{}\ &amp; &amp; n\^[-1]{}\_[i=1]{}\^[n]{}\_[j=i+1]{}\^[n]{}\_[l]{}\^[1/2]{}(j-i)\_[0]{}\^(1-F)\^[1/2]{}d H\ &amp;&amp;lt;&amp;. Therefore, () immediately follows.  The proof of () does not involve the dependence of errors, and hence, it is the same as the proof of (5.5.11) of Koul (2002). Thus, we shall prove (). Here we consider the proof for the case only. The similar facts will hold for the case .  To begin with let , , and denote , and in when is replaced with so that , , and . Define for , , p(y,u; x)&amp;:=&amp; F(y+u’Ax)-F(y),\ B\_[ni]{}&amp;:=&amp; I(\_[i]{}y + u’Ax\_[i]{}) -I(\_[i]{}y ) - p(y,u; x\_[i]{}). Rewrite W\_[ku]{}\^-W\_[k0]{}\^ = d\_[ik]{}\^{I(\_[i]{}y + u’Ax\_[i]{}) -I(\_[i]{}y ) - p(y,u; x\_[i]{}) }. Note that EB\_[ni]{}\^[2]{} F(y+ \_[i]{}u)-F(y) Recall a lemma from Deo(1973).  Suppose for each , are strongly mixing random variables with mixing number . Suppose and are two random variables respectively measurable with respect to and , . Assume and are such that , and and . Then for each |E(XY)-E(X)E(Y)|10\_[l]{}\^[1/r]{}(m)X\_[p]{}Y\_[q]{}. Consequently if then for and each |E(XY)-E(X)E(Y)|10\_[l]{}\^[1-1/q]{}(m)Y\_[q]{}.  In addition, consider following lemma.  For , n\^[-1]{}\_[i=1]{}\^[n-1]{}\_[k=1]{}\^[n-i]{}\_[l]{}\^[1/r]{}(k)=O(1).  **Proof**. Fix an . Let such that . Note that = &amp;gt;. Therefore, by Hlder’s inequality, we have n\^[-1]{}\_[i=1]{}\^[n-1]{}\_[k=1]{}\^[n-i]{}\_[l]{}\^[1/r]{}(k) &amp;&amp; (\_[k=1]{}\^[n-1]{} )\^[1/p]{} (\_[k=1]{}\^[n-1]{}k\^[2]{}\_[l]{}(k))\^[1/r]{} &amp;lt;. The last inequality follows from the assumption (A.8.6), thereby completing the proof of the lemma.  Now, we consider the cross product terms of . Then &amp;&amp;|E \_[j=i+1]{}\^[n]{} d H(y)|\ &amp;&amp; \_[j=i+1]{}\^[n]{} d\_[ik]{}d\_[jk]{}| E B\_[ni]{}B\_[nj]{} |d H\ &amp;&amp; 10b\^[1/2]{}{n\_[i]{}d\_[ik]{}\^[2]{}}{\_[i]{}\_[i]{}}\^[1/2]{}n\^[-1]{}\_[i=1]{}\^[n-1]{}\_[m=1]{}\^[n-i]{} \_[l]{}\^[1/2]{}(m) f\^[1/2]{} d H 0.The second inequality follows from Lemma , and the convergence to zero follows from the Lemma with , (A.1), and (A.7). Consequently, by Fubini’s Theorem together with (A.3), we obtain, for every fixed , \_[n]{}E|W\_[ku]{}-W\_[k0]{}|\^[2]{}\_[H]{} &amp;&amp; \_[n]{} \_[-a\_[n]{}]{}\^[a\_[n]{}]{}f(y+s) d H(y)ds =0,where .  To complete the proof of (), it suffices to show that for all , there exists a such that for all , , \_[n]{} E\_[u-v]{}|[[K]{}]{}\_[ku]{}-[[K]{}]{}\_[kv]{}|, where [[K]{}]{}\_[ku]{}:=|W\_[ku]{}-W\_[k0]{}|\_[H]{}\^[2]{},u\^[p+1]{},1kp+1. () follows from (5.5.5) of Koul (2002), thereby completing the proof of theorem.  **Proof of Corollary .** The proof of the claim for independent errors can again be found in the section 5.5 of Koul (2002). The difference between the proof in the section 5.5 and one here arises only in the part which involves the dependence of the error. Thus, we present only the proof of an analogue of (5.5.27) in Koul (2002). Let L\_[k]{}&amp;:=&amp; f(y)d H(y),\ L&amp;:=&amp; (L\_[1]{},...,L\_[p+1]{}).Note that . Therefore, using C-S inequality together with (), we obtain, for some EL\^[2]{} C(p+1)|f|\_[H]{}\^[2]{}\_[0]{}\^(1-F)\^[1/2]{}d H. Using for and Chebyshev inequality, for all there exists and such that P(Lc\_) 1- 1- /2,nN\_[1]{}. The rest of the proof is the same as that of Lemma 5.5.4 of Koul (2002).  \ **Proofs of Section**\ **Proof of Theorem** . Note that \_[k]{}(y,r)&amp;=&amp;S\_[k]{}(y,r)+ n\^[-1/2]{} \_[i-k]{}\ &amp;&amp; - n\^[-1/2]{} \_[n,i-k]{}(u) {I(\_[i]{} y + Z\_[i]{}’v/ ) - I(-\_[i]{} &amp;lt; y - Z\_[i]{}’v/ )}\ &amp;&amp; - n\^[-1/2]{} \_[n,i-k]{}(u)\ &amp;=&amp; S\_[r]{} + R1\_[uv]{}\^[k]{}- R2\_[uv]{}\^[k]{} - R3\_[uv]{}\^[k]{},(say). In what follows, the index runs from 1 to . In order to conserve the space, we write , , etc. for , , etc. Similar to the way as that used in Theorem , in order to prove Theorem , it suffices to show that for \_[ub,vb]{} |Rj\_[uv]{}\^[k]{}|\_[H]{}\^[2]{}=o\_[p]{}(1). Consider . First, define for w\_[k]{}(y,u,v)&amp;:=&amp; n\^[-1]{}\_[i-k]{}I( \_[i]{} y+Z\_[i]{}’v/ +\_[ni]{}(u,v)),\ \_[k]{}(y,u,v)&amp;:=&amp;n\^[-1]{}\_[i-k]{}F\_(y+Z\_[i]{}’v/ +\_[ni]{}(u,v)),\ [[W]{}]{}\_[k]{}(y,u,v)&amp;:=&amp; . Also consider the following processes: [[T]{}]{}\_[k]{}(y;u,v, )&amp;:=&amp;n\^[-1/2]{}\_[i-k]{}I( \_[i]{} y+Z\_[i]{}’v/ +\_[ni]{}(u,v)d\_[ni]{}(, u,v)),\ m\_[k]{}(y;u,v, )&amp;:=&amp; n\^[-1/2]{}\_[i-k]{}F\_(y+Z\_[i]{}’v/ +\_[ni]{}(u,v) d\_[ni]{}(, u,v)),\ [[Z]{}]{}\_[k]{}(y;u,v, ) &amp;:=&amp; [[T]{}]{}\_[k]{}(y;u,v, ) - m\_[k]{}(y;u,v, ). Note that . However, for the purpose of conserving the space, let [[T]{}]{}\_[k]{}(y;u,v, -):=n\^[-1/2]{}\_[i-k]{}I( \_[i]{} y+Z\_[i]{}’v/ +\_[ni]{}(u,v)-d\_[ni]{}(, u,v)). Similarly, we define and by subtracting term . Let and . Let any process with superscript denote the one with replaced by . Also drop the subscript , and let , etc. stand for , etc.  Now, consider where and . Then we obtain Z\_[i]{}’v/ +\_[ni]{}(u,v)-d\_[ni]{}(,u,v )Z\_[i]{}’t/ +\_[ni]{}(s,t)Z\_[i]{}’v/ +\_[ni]{}(u,v)+d\_[ni]{}(,u,v ). Note that and . Monotonicity of indicator function together with () in turn implies [[T]{}]{}\^(y;u,v, -)-[[T]{}]{}\^(y;u,v, 0) [[T]{}]{}\^(y;u,v, )-[[T]{}]{}\^(y;u,v, 0) for all , , , and . Using the fact that implies , appropriate centering of yields &amp;&amp;| w\_[st]{}\^-w\_[uv]{}\^ |\ &amp;&amp; | [[T]{}]{}\^(y;u,v, -)-[[T]{}]{}\^(y;u,v, 0) | + | [[T]{}]{}\^(y;u,v, )-[[T]{}]{}\^(y;u,v, 0) |\ &amp;&amp; |[[Z]{}]{}\^(y;u,v, -)-[[Z]{}]{}\^(y;u,v, 0) |+ |m\^(y;u,v, -)- m\^(y;u,v, 0) |\ &amp;&amp;+|[[Z]{}]{}\^(y;u,v, )-[[Z]{}]{}\^(y;u,v, 0) |+ |m\^(y;u,v, )- m\^(y;u,v, 0) |.  Assumption (B.2) implies E\^[2]{}d H(y)=o(1). Also, assumption (B.3) implies with and in (B.3) \_[n]{}P( n {\^(y,s,t)-\^(y,u,v) }\^[2]{}dH(y) c\^[2]{})=1 where supremum is taken over and . Moreover, assumptions (B.1)-(B.5) imply &amp;&amp;\_[ub,vb]{}n\^[-1]{}\^[2]{}d H(y)=o\_[p]{}(1).  **Proof**. Let . Define (y, u,v;z):= F\_(y+n\^[-1/2]{}v’Z\_[i]{}+ \_[ni]{}(u,v)+d\_[ni]{}(,u,v)) - F\_(y+n\^[-1/2]{}v’Z\_[i]{}+ \_[ni]{}(u,v)). Observe that th summand is conditionally centered r.v.’s, given , and hence, covariance of two summands is zero. Similar to (), the conditional variance of th summand, given , is bounded by E(\_[i-k]{}\^)\^[2]{}|(y,u,v;Z\_[i]{})| &amp;&amp; E(\_[i-k]{}\^)\^[2]{}| F\_(y+n\^[-1/2]{}v’Z\_[i]{}+ \_[ni]{}(u,v)+d\_[ni]{}(,u,v) ) - F\_(y)|\ &amp;&amp;+ E(\_[i-k]{}\^)\^[2]{}|F\_(y+n\^[-1/2]{}v’Z\_[i]{}+ \_[ni]{}(u,v))- F\_(y) |. Therefore, () follows from Fubini’s Theorem and repeated application of (B.2) with and . To prove (), observe that the monotonicity of and () imply &amp;&amp;m\^(y;u,v, -)m\^(y;s,t, 0)m\^(y;u,v, ),\ &amp;&amp;m\^(y;u,v, -)m\^(y;u,v, 0)m\^(y;u,v, ),and hence, m\^(y;u,v, -)-m\^(y;u,v, 0) n\^[1/2]{} m\^(y;u,v, )-m\^(y;u,v, 0). Using the fact that implies , we obtain n\^[1/2]{}|\_[st]{}\^-\_[uv]{}\^|&amp;&amp; n\^[-1/2]{}\_[i-k]{}\^{F\_(y+Z\_[i]{}’v/ +\_[ni]{}(u,v)+d\_[ni]{}(, u,v))\ &amp;&amp;- F\_(y+Z\_[i]{}’v/ +\_[ni]{}(u,v)-d\_[ni]{}(, u,v)) }.As a consequence of () and the assumption (B.3), we finally obtain &amp;&amp;\_[n]{} P( | {\^(y,s,t)-\^(y,u,v)}|\_[H]{}\^[2]{} c\^[2]{})\ &amp;&amp;\_[n]{} P( | n\^[-1/2]{}\_[i-k]{}\^{F\_(y+Z\_[i]{}’v/ +\_[ni]{}(u,v)+d\_[ni]{}(, u,v))\ &amp;&amp;- F\_(y+Z\_[i]{}’v/ +\_[ni]{}(u,v)-d\_[ni]{}(, u,v)) }|\_[H]{}\^[2]{} c\^[2]{})\ &amp;=&amp;1,where the supremums are taken over and , and hence, follows. Next, write and for and . Let [[L]{}]{}\_[uv]{}=|m\_[uv]{}\^-m\^ - n\^[-1/2]{}\_[i-k]{}\^f\_{ n\^[-1/2]{}v’Z\_[i]{}+\_[ni]{}(u,v)} |\_[H]{}\^[2]{}. In order to prove , by using (B.4) and compactness of and (see Theorem 9.1.1 of Koul (2002)), it suffices to show that for all there exists a such that \_[n]{}P(|[[L]{}]{}\_[st]{}-[[L]{}]{}\_[uv]{}|&amp;gt;)=0, where sup is taken over , , and . Using monotonicity of , , triangle inequality, , and (B.3), we obtain for with and \_[n]{} P(|m\_[st]{}-m\_[uv]{}|\_[H]{}\^[2]{}4c\^[2]{}) = 0 where the equality follows from (). Next, observe that &amp;&amp;| [[L]{}]{}\_[st]{}-[[L]{}]{}\_[uv]{} |\ &amp;&amp;\ &amp;&amp;d H(y)&amp;&amp; 2 |m\_[st]{}-m\_[uv]{}|\_[H]{}\^[2]{}+2|n\^[-1/2]{}\_[i-k]{}d\_[ni]{}(,u,v)f\_|\_[H]{}\^[2]{}+2|m\_[st]{}-m\_[uv]{}|\_[H]{}\ &amp;&amp;|n\^[-1/2]{}\_[i-k]{}d\_[ni]{}(,u,v)|\_[H]{} + 2|m\_[uv]{}-m- n\^[-1/2]{}\_[i-k]{}(Z\_[i]{}’v/ +\_[ni]{}(u,v)) f\_|\_[H]{}\ &amp;&amp;{ |m\_[st]{}-m\_[uv]{}|\_[H]{}+|n\^[-1/2]{}\_[i-k]{}d\_[ni]{}(,u,v)f\_|\_[H]{} }.In the first inequality, we use the fact that ; () implies that supremum of the first term of the last equation is ; the assumption (B.1) and the fact that imply that supremum of the second term is , and hence, supremum of the third term is ; the assumption (B.4) ensures that supremum of the last term is . Therefore, by making as small as desired, () follows, thereby completing the proof of ().  \_[ub,vb]{}\^[2]{}d H(y)=o\_[p]{}(1).  **Proof**. Note that . Together with this fact, Fubini’s theorem yields &amp;&amp;\_[n]{} E\_[ub,vb]{}n\^[-1]{}\_[j=1]{}\^[n]{}\_[i-k]{}\_[j-k]{}\_[ni]{}(u,v)\_[nj]{}(u,v) f\_\^[2]{}(y)d H(y)\ &amp;&amp; \_[n]{}C|f\_|\_[H]{}\^[2]{}n\^[-2]{}\_[j=1]{}\^[n]{}\_[l]{}\^[1/2]{}(j-i) \_[1]{}\_[4]{}\^[2]{} = 0,where . The inequality follows from Lemma ; the last equality follows from (B.1.a) and Lemma . Consequently, E| n\^[-1/2]{}\_[i-k]{}\_[ni]{}(u,v) f\_ |\_[H]{}\^[2]{} 0, where the supremum is taken over and , thereby completing the proof of lemma.  \_[ub,vb ]{}|m\_[uv]{}-m\_[0v]{}|\_[H]{}\^[2]{}=o\_[p]{}(1).  **Proof**. Note that &amp;&amp; m(y,u,v,0 )-m(y,0,v,0 )\ &amp;=&amp;{ m(y,u,v,0 )-m(y,0,0,0 )- n\^[-1/2]{}\_[i-k]{} ( Z\_[i]{}’v/ +\_[ni]{}(u,v))f\_(y) }\ &amp;&amp; - { m(y,0,v,0 )-m(y,0,0,0 )- v’n\^[-1]{} \_[i-k]{}Z\_[i]{}f\_(y) } + n\^[-1/2]{}\_[i-k]{} \_[ni]{}(u,v)f\_(y). Hence the proof of corollary will be completed by simply applying C-S inequality on the cross product terms after we show that &amp;&amp;| m\_[uv]{}-m- n\^[-1/2]{}\_[i-k]{} ( Z\_[i]{}’v/ +\_[ni]{}(u,v))f\_ |\_[H]{}\^[2]{}=o\_[p]{}(1),\ &amp;&amp;| m\_[0v]{}-m - v’ n\^[-1]{} \_[i-k]{}Z\_[i]{}f\_|\_[H]{}\^[2]{}=o\_[p]{}(1),\ &amp;&amp; | n\^[-1/2]{}\_[i-k]{}\_[ni]{}(u,v) f\_ |\_[H]{}\^[2]{}=o\_[p]{}(1), where the supremums are taken over and . The first and the second follow from () of Lemma ; the last follows from Lemma .  \_[ub,vb ]{}{ [[W]{}]{}\^(y,u,v)-[[W]{}]{}\^(y,0,0) }\^[2]{}d H(y)=o\_[p]{}(1).  **Proof**. Similar to the square root of the integrand in (), is sum of conditionally centered r.v.’s. Thus, by repeated application of (B.2) with and , we easily obtain for fixed , | [[W]{}]{}\_[uv]{}\^-[[W]{}]{}\^|\_[H]{}\^[2]{}=o\_[p]{}(1). Let . Similar to the proof of , in order to complete the proof, it suffices to show that for every there exists such that for every , \_[n]{} P(| [[C]{}]{}\_[st]{} - [[C]{}]{}\_[uv]{}|&amp;gt;)=0, where sup is taken over , , and . Note that | [[C]{}]{}\_[st]{}-[[C]{}]{}\_[uv]{} | |[[W]{}]{}\_[st]{}\^-[[W]{}]{}\_[uv]{}\^|\_[H]{}\^[2]{}+2|[[W]{}]{}\_[st]{}\^-[[W]{}]{}\_[uv]{}\^|\_[H]{}|[[W]{}]{}\_[uv]{}\^-[[W]{}]{}\^ |\_[H]{}. By the monotonicity of together with we obtain that |m\^(y;u,v, )-m\^(y;u,v, 0)|m\^(y;u,v, ) - m\^(y;u,v, -). Recall definition of . Then triangle inequality yields that |[[W]{}]{}\_[st]{}-[[W]{}]{}\_[uv]{}|&amp;&amp; |[[W]{}]{}\_[st]{}\^[+]{}-[[W]{}]{}\_[uv]{}\^[+]{}|+ |[[W]{}]{}\_[st]{}\^[-]{}-[[W]{}]{}\_[uv]{}\^[-]{}|,\ |[[W]{}]{}\_[st]{}\^-[[W]{}]{}\_[uv]{}\^|&amp;&amp; {|w\_[st]{}\^-w\_[uv]{}\^|+ |\_[st]{}\^-\_[uv]{}\^| }.Therefore, using repeatedly and , we have |[[W]{}]{}\_[st]{}\^-[[W]{}]{}\_[uv]{}\^|\_[H]{}\^[2]{}&amp;&amp; 16 { \^[2]{}d H(y)\ &amp;&amp; +\^[2]{}d H(y)\ &amp;&amp; +\^[2]{}d H(y)\ &amp;&amp; + |(\_[st]{}\^-\_[uv]{}\^) |\_[H]{}\^[2]{}}for all , , . Finally, assumption (B.3), , , and prove , thereby completing the proof of the lemma. .15cm Define SS\_[k]{}(y,u,v)&amp;:=&amp; n\^[-1/2]{}\_[i-k]{} {I( \_[i]{} y+Z\_[i]{}’v/ +\_[ni]{}(u,v))\ &amp;&amp;-I( -\_[i]{}&amp;lt; y-Z\_[i]{}’v/ -\_[i]{}(u,v))}.Rewrite R1\_[uv]{}\^[k]{}&amp;=&amp; -\ &amp;&amp;+ -\ &amp;&amp; + - .Using quadratic expansion and applying C-S inequality involving norm on the cross product, the claim that will be true if we can show that &amp;&amp;\_[ub,vb]{}|[[W]{}]{}\_[uv]{}- [[W]{}]{}|\_[H]{}\^[2]{}=o\_[p]{}(1),\_[vb]{}|[[W]{}]{}\_[0v]{}- [[W]{}]{}|\_[H]{}\^[2]{}=o\_[p]{}(1),\ &amp;&amp;\_[ub,vb]{}| m\_[uv]{}-m\_[0v]{}|\_[H]{}\^[2]{}=o\_[p]{}(1). (a) and (b) follow from Lemma ; (c) follows from Corollary .  Next, consider . To begin with, note that by C-S inequality and Fubini’s theorem together, we obtain for , and &amp;&amp;E{F\_(y+v’Z\_[1]{}/)-F\_(y-v’Z\_[1]{}/)}\^[2]{}d H(y)\ &amp;&amp; 4b\^[2]{} n\^[-1]{} (2b/)\^[-1]{}\_[-b/]{}\^[b/]{}E\^[2]{}d H(y)ds.Moreover, by the assumptions (B.1.a) and (B.1.c) and bounded convergence theorem, we have &amp;&amp;\^[1/2]{}d H(y)\ &amp;&amp;n\^[-1/4]{}\^[1/2]{}d H(y) =O(n\^[-1/4]{}).Finally, consider and let . Using C-S inequality and Fubini’s theorem again, we have &amp;&amp; E{F\_(y+v\_[2]{}’Z\_[1]{}/)-F\_(y+v\_[1]{}’Z\_[1]{}/)}\^[2]{}d H(y)\ &amp;&amp; n\^[-1]{} ( v\_[2]{}+v\_[1]{})\^[2]{} {(v\_[2]{}+v\_[1]{})/}\^[-1]{}\_[-v\_[1]{}/]{}\^[v\_[2]{}/]{}E\^[2]{}d H(y)ds.Define (y,v;z):=F\_(y+n\^[-1/2]{}v’z)-F\_(y-n\^[-1/2]{}v’z). Next, rewrite R2\_[uv]{}\^[k]{}&amp;=&amp; n\^[-1/2]{} \_[u]{}\^[ik]{} {I(\_[i]{} y+n\^[-1/2]{}v’Z\_[i]{} ) - I(-\_[i]{} &amp;lt; y-n\^[-1/2]{}v’Z\_[i]{} ) - (y,v;Z\_[i]{}) }\ &amp;&amp; + n\^[-1/2]{} \_[u]{}\^[ik]{}(y,v;Z\_[i]{})\ &amp;=&amp; R21\_[uv]{}\^[k]{}+R22\_[uv]{}\^[k]{}, say.Due to the monotonicity of , we obtain \_[vb]{}|(y,v;Z\_[i]{})|F\_(y+n\^[-1/2]{}bZ\_[i]{})-F\_(y-n\^[-1/2]{}bZ\_[i]{}). Therefore, by C-S inequality, (), , and (), we have E\_[u,vb ]{}|R22\_[uv]{}\^[k]{}|\_[H]{}\^[2]{} 4b\^[4]{}\_[n]{}\^[2]{}(2b/)\^[-1]{}\_[-b/]{}\^[b/]{}E\^[2]{}d H(y)ds 0. Next, consider the cross product of . For fixed &amp;&amp; E\_[j=i+1]{}\^[n]{} d H(y)\ &amp;&amp; b\^[2]{}\_[n]{}\^[2]{}[n]{}\^[-1]{}\_[j=i+1]{}\^[n]{}|E{I(\_[i]{} y+n\^[-1/2]{}v’Z\_[i]{} ) - I(-\_[i]{} &amp;lt; y-n\^[-1/2]{}v’Z\_[i]{} )\ &amp;&amp; -(y,v;Z\_[i]{}) } {I(\_[j]{} y+n\^[-1/2]{}v’Z\_[j]{} ) - I(-\_[j]{} &amp;lt; y-n\^[-1/2]{}v’Z\_[j]{} )\ &amp;&amp;- (y,v;Z\_[j]{}) } |d H(y)\ &amp;&amp; 10b\^[2]{}\_[n]{}\^[2]{}n\^[-1]{}\_[j=i+1]{}\^[n]{}\_[l]{}\^[1/2]{}(j-i)\ &amp;&amp; \^[1/2]{}d H(y)\ &amp;&amp; 0.The first inequality follows from Fubini’s theorem; the second inequality follows from Lemma ; the convergence to zero follows from , with , and . Therefore, the sum of the covariance of two summands in will tend to zero. Consequently, for fixed and , by Fubini’s theorem, stationarity of , and (B.5), we obtain &amp;&amp;\_[n]{}E|R21\_[uv]{}\^[k]{}|\_[H]{}\^[2]{}\ &amp;&amp;\_[n]{} 2b\^[3]{}\_[n]{}\^[2]{}n\^[-1/2]{} (2b/n\^[1/2]{})\^[-1]{}\_[-b/n\^[1/2]{}]{}\^[b/n\^[1/2]{}]{}EZ\_[1]{}f\_(y+sZ\_[1]{})d H(y)ds\ &amp;=&amp;0.  Finally, we shall show that . Let and for where and , and are unit vectors in and . Let -b=x\_[0]{}x\_[1]{}x\_[r]{}=b,-b=w\_[0]{}w\_[1]{}w\_[r]{}=b so that and as . Note that and rewrite R21\_[uv]{}\^[k]{}&amp;=&amp; n\^[-1/2]{} u’c\_[i-k]{} {I(\_[i]{} y+n\^[-1/2]{}v’Z\_[i]{} )- I(-\_[i]{} &amp;lt; y-n\^[-1/2]{}v’Z\_[i]{} ) }\ &amp;&amp;- n\^[-1/2]{} u’c\_[i-k]{}(y,v;Z\_[i]{})\ &amp;=&amp; T1\_[uv]{}-T2\_[uv]{},say.Define , . Let , , stand for the , , with being replaced by . Observe that |R21\_[uv]{}\^[k]{}|\_[H]{}\^[2]{}2{ |T1\_[uv]{}\^[+]{}-T2\_[uv]{}\^[+]{}|\_[H]{}\^[2]{}+|T1\_[uv]{}\^[-]{}-T2\_[uv]{}\^[-]{}|\_[H]{}\^[2]{} }. Note that is a difference of two nondecreasing (nonincreasing) function of , and hence, for all and where and , R21\_[h-1,l-1]{}\^[+]{}-(T2\_[h,l]{}\^[+]{}-T2\_[h-1,l-1]{}\^[+]{})R21\_[uv]{}\^[+]{}R21\_[h,l]{}\^[+]{}+(T2\_[h,l]{}\^[+]{}-T2\_[h-1,l-1]{}\^[+]{}). Therefore, using the fact that implies and that repeatedly, we obtain |R21\_[uv]{}\^[+]{}|\_[H]{}\^[2]{} &amp;&amp; 8. Using and ergodicity of , &amp;&amp;E|T2\_[h,l-1]{}\^[+]{}-T2\_[h-1,l-1]{}\^[+]{}|\_[H]{}\^[2]{}\ &amp;&amp; 16b\^[4]{}\_[n]{}\^2 (2b/)\^[-1]{}\_[-b/]{}\^[b/]{}E\^[2]{}d H(y)ds0,where the inequality follows from (), and the convergence to zero follows from assumption (B.5) and . Similarly, using (), we have &amp;&amp;E|T2\_[h-1,l]{}\^[+]{}-T2\_[h-1,l-1]{}\^[+]{}|\_[H]{}\^[2]{}\ &amp;&amp; b\^[2]{}\_[n]{}\^2 (|w\_[l]{}|+|w\_[l-1]{}|)\^[2]{}{(|w\_[l]{}|+|w\_[l-1]{}|)/}\^[-1]{}\ &amp;&amp;{ \_[-|w\_[l-1]{}|/]{}\^[|w\_[l]{}|/]{}E\^[2]{}d H(y)ds\ &amp;&amp;+\_[-|w\_[l-1]{}|/]{}\^[|w\_[l]{}|/]{}E\^[2]{}d H(y)ds}\ &amp;&amp; 0.Consequently, , , , and imply E\_[u,v]{} |R21\_[uv]{}\^[+]{}|\_[H]{}\^[2]{}0. Similar facts hold for , and this completes the proof of .  To show is , note that is the same as except the fact that it has as a weight instead of random error. So, the proof is similar, but much simpler. Hence we do not present here.  [17]{}  Athreya, K. B. and Pantula, S. G. (1986). Mixing properties of Harris chanis and autoregressive processes. J. APPL. PROBAB., **23** 880-892.  Beran, R. J. (1977). Minimum Hellinger distance estimates for parametric models. ANN. STATIST., **5** 445-463.  Bloomfield, P. (1992). Trends in global temperature CLIMATIC CHANGE, **21** 1-16.  Deo, C. M. (1973). A note on empirical processes of strong mixing sequences. ANN. PROBAB., **1** 870-875.  Donoho, D. L. and Liu, R. C. (1988 a). The “automatic&quot; robustness of minimum distance functionals. ANN. STATIST., **16** 552-586. Donoho, D. L. and Liu, R. C. (1988 b). Pathologies of some minimum distance estimators. ANN. STATIST., **16** 587-608.  Durrett, R. (2005). PROBABILITY: THEORY AND EXAMPLES., Thomson, Brooks Cole.  Hampel, F. R. (1974). The influence curve and its role in robust estimation. J. AMER. STATIST. ASSOC., **69** 383-393.  Koul, H. L. (1977). Behavior of robust estimators in the regression model with dependent errors. ANN. STATIST., **5** 681-699.  Koul, H. L. (2002). WEIGHTED EMPIRICAL PROCESS IN NONLINEAR DYNAMIC MODELS. Springer, Berlin, Vol. 166.  Koul, H. L. and DeWet, T. (1983). Minimum distance estimation in a linear regression model. ANN. STATIST., **11** 921-932.  Mehra, K. L. and Rao, M. S. (1975). Weak convergence of generalized empirical processes relative to under strong mixing. ANN. PROBAB., **3** 979-991.  Millar, P. W. (1981). Robust estimation via minimum distance methods. ZEIT FUR WAHRSCHEINLICHKEITSTHEORIE., **55** 73-89.  Noether, G. E. (1949). On a theorem by Wald and Wolfowitz. ANN. MATH STATIST., **20** 445-458.  Parr, W. C. and Schucany, W. R. (1979). Minimum distance and robust estimation. J. AMER. STATIST. ASSOC., **75** 616-624.">
</outline>
  </body>
</opml>