<?xml version="1.0" encoding="UTF-8"?>
<opml version="2.0">
  <head>
    <title>Optimal Low-Rank Dynamic Mode Decomposition</title>
    <abstract>Dynamic Mode Decomposition (DMD) has emerged as a powerful tool for
analyzing the dynamics of non-linear systems from experimental datasets.
Recently, several attempts have extended DMD to the context of low-rank
approximations. This extension is of particular interest for
reduced-order modeling in various applicative domains, [E.G., ]{}for
climate prediction, to study molecular dynamics or
micro-electromechanical devices. This low-rank extension takes the form
of a non-convex optimization problem. To the best of our knowledge, only
sub-optimal algorithms have been proposed in the literature to compute
the solution of this problem. In this paper, we prove that there exists
a closed-form optimal solution to this problem and design an effective
algorithm to compute it based on Singular Value Decomposition (SVD). A
toy-example illustrates the gain in performance of the proposed
algorithm compared to state-of-the-art techniques. </abstract>
  </head>
  <body>
<outline text="Introduction" _note="In many fields of Sciences, one is interested in studying the [spatio-temporal]{} evolution of a state variable characterized by a partial differential equation. Numerical discretization in space and time leads to a high dimensional system of equations [of the form:]{} [where]{} each element of the sequence of state variables belongs to , with the initial condition . [Because may correspond to a very high-dimensional system in some applications, computing a trajectory given an initial condition ]{} may lead to a heavy computational load, which may prohibit the direct use of the original high-dimensional system.  The context of uncertainty quantification provides an appealing example. Assume we are interested in characterizing the distribution of random trajectories generated by with respect to the distribution of the initial condition. A straightforward approach would be to sample the initial condition and run the high-dimensional system. However, in many applicative contexts, it is impossible to generate enough trajectories to make accurate approximations with Monte-Carlo techniques.  As a response to this computational bottleneck, reduced-order models aim to approximate the trajectories of the system for a range of regimes determined by a set of initial conditions . A common approach is to assume that the trajectories of interest are well approximated in a sub-space of . In this spirit, many tractable low-rank approximations of high-dimensional systems have been proposed in the literature, the most familiar being proper orthogonal decomposition (POD) , balanced truncation , Taylor expansions  or reduced-basis techniques . Other popular sub-space methods, such as linear inverse modeling (LIM) , principal oscillating patterns (POP) , or more recently, dynamic mode decomposition (DMD) , are known as Koopman operator approximations.  In this paper, we consider the setting where system is a black-box. In other words, we assume that we do not know the exact form of in and we only have access to a set of representative trajectories , , so-called SNAPSHOTS, obtained by running the high-dimensional system for different initial conditions. Moreover, we focus on the low-rank DMD approximation problem studied in . In a nutshell, these studies provide a procedure for determining a matrix of rank , which substitutes [for]{} function in and generates the approximations with a low computational effort. Alternatively, given , and its non-zero eigenvalues and associated eigenvectors , trajectories of can be computed by using the reduced-order model as long as matrix is symmetric. We will assume it is always the case for simplification issues. In what follows, we will refer to the parameters and as the -TH LOW-RANK DMD MODE AND AMPLITUDE at time .  Matrix targets the solution of the following non-convex optimization problem, which we will refer to as the [LOW-RANK DMD APPROXIMATION PROBLEM]{} where refers to the norm. In order to compute [a]{} solution , the authors in propose to rely on the assumption of linear dependence of recent snapshots on previous ones. This assumption may not be reasonable, especially in the case of non-linear systems.  Beyond the reduced modeling context discussed above, there has been a resurgence of interest for low-rank solutions of linear matrix equations . This class of problems is very large and includes in particular problem . Problems in this class are generally nonconvex and do not admit explicit solutions. Howewer, important results have arisen at the theoretical and algorithmic level, enabling the characterization of the solution for this class of problems by convex relaxation . Applications concern scenarios such as low-rank matrix completion, image compression or minimum order linear system realization, see . Nevertheless, there exists certain instances with a very special structure, which admit closed-form solutions . This occurs typically when the solution can be deduced from the well-known Eckart-Young theorem .  The contribution of this paper is to show that the special structure of problem enables the characterization of an exact closed-form solution and an easily implementable solver based on singular value decomposition (SVD). In the case , the proposed algorithm computes the solution of . More interestingly, for , [I.E., ]{}in the constrained case, our approach enables to solve exactly the low-rank DMD approximation problem without 1) any assumption of linear dependence, 2) the use of an iterative solver, on the contrary to the approaches proposed in . The paper is organized as follows. In section  , we provide a brief review of state-of-the-art techniques to compute low-rank DMD of experimental data. Section   details our analytical solution and the algorithm solving . Given this optimal solution, it then presents the reduced-order model solving . Finally, a numerical evaluation of the method is presented in [Section]{}   and concluding remarks are [given]{} in a last section.">
</outline>
<outline text="State-Of-The-Art Overview" _note="In what follows, we assume that we have at our disposal trajectories of snapshots. We will need in the following some matrix notations. The symbol and the upper script will respectively refer to the Frobenius norm and the transpose operator. will denote the -dimensional identity matrix. Let consecutive elements of the -th snapshot trajectory between time and be gathered in a matrix and let two large matrices with be defined as Without loss of generality, this work will assume that and that . We introduce the SVD decomposition of a matrix with : with , and so that and is diagonal. The Moore-Penrose pseudo-inverse of a matrix will be defined as . With these notations, problem can be rewritten as In what follows, we begin by presenting two state-of-the-art methods which enable to compute an approximation of the solution of problem .">
  <outline text="Projected DMD and Low-Rank Formulation" _note="As detailed herafter, the original DMD approach first proposed in , so-called PROJECTED DMD in , assumes that columns of are in the span of . The assumption is written by the authors in as the existence of , the so-called COMPANION MATRIX of parametrized by coefficients, such that We remark that this assumption is in particular valid when the -th snapshot can be expressed as a linear combination of the columns of and when is linear. Using the SVD decomposition and noticing is full rank, we obtain from a projected representation of in the basis spanned by the columns of , where Therefore, the low-rank formulation in proposes to approach the solution of by determining the coefficients of matrix which minimize the Frobenius norm of the residual . This yields after some algebraic manipulations to solve the problem The Eckart-Young theorem then provides the optimal solution to this problem based on a rank- SVD approximation of matrix given by where is a diagonal matrix containing only the -largest singular values of and with zero entries [otherwise]{}. Exploiting the low-dimensional representation , a reduced-order model for trajectories can then be obtained by inserting in the low-rank approximation As an alternative, the authors propose a reduced-order model for trajectories relying on the so-called DMD modes and their amplitudes. These modes are related to the eigenvectors of the solution of . The amplitudes are given by solving a convex optimization problem with an iterative gradient-based method, see details in .">
  </outline>
  <outline text="Non-projected DMD" _note="If we remove the low-rank constraint, becomes a least-squares problem whose solution is Based on the approximation , DMD modes and amplitudes serve to design a model to reconstruct trajectories of . We note that the DMD modes are simply given by the eigendecomposition of , which can be [efficiently]{} [computed]{} using SVD, as proposed in . The [associated]{} DMD amplitudes can then easily be derived.  It is important to remark that truncating to a rank- the solution of the above unconstrained minimization problem will not necessarily yield the solution of . This approach will generally be sub-optimal. However surprisingly, the solution to problem remain to our knowledge overlooked in the literature, and no algorithms enabling non-projected low-rank DMD approximations have yet been proposed.">
  </outline>
</outline>
<outline text="The Proposed Approach">
  <outline text="Closed-form Solution to" _note="Let the columns of matrix be the real orthonormal eigenvectors associated to the largest eigenvalues of matrix    A solution of is  This theorem states that can be simply solved by computing the orthogonal projection of the unconstrained problem solution onto the subspace spanned by the first eigenvectors of . A detailed proof is provided in [the technical report associated to this paper .]{}">
  </outline>
  <outline text="&lt;span&gt;Efficient&lt;/span&gt; Solver" _note="The matrix is of size . [Since is typically very large, t]{}his prohibits the direct computation of an eigenvalue decomposition. The following well-know remark is useful to overcome this difficulty.\    The eigenvectors associated to the non-zero eigenvalues of matrix with can be obtained [from]{} the eigenvectors and eigenvalues of the smaller matrix . Indeed, the SVD of a matrix of rank is where the columns of matrix are the eigenvectors of . Since is unitary, we obtain that the sought vectors are the [first ]{} columns of , [I.E., ]{}of  In the light of this remark, it is straightforward to design Algorithm  , which will compute efficiently the solution of based on SVDs.  **input**: -sample 1) Form matrix and as defined in . 2) Compute the SVD of . 3) Compute the columns of using Remark  .**output**: matrix  **input**: matrices , with 1) Compute the SVD of matrix .2) Solve for the eigen equation where and denote eigenvectors and eigenvalues of  **output**: DMD modes and amplitudes">
  </outline>
  <outline text="Reduced-Order Models" _note="We now discuss the resolution of the reduced-order model given the solution of . Trajectories of are fully determined by a -dimensional recursion involving the projected variable : Then, by multiplying both sides by matrix , we obtain the sought low-rank approximation . Alternatively, we can employ reduced-order model . The parameters of this model, [I.E., ]{}low-rank DMD modes and amplitudes, are efficiently computed without any minimization procedure, in contrast to what is proposed by the author in []{}. Indeed, we rely on the following remark stating that DMD modes and amplitudes can be obtained by means of SVDs using Algorithm  . The remark is proved in the [technical report ]{}.    Each pair generated by Algorithm   is one of the eigenvector/eigenvalue pair of .">
  </outline>
</outline>
<outline text="Numerical Evaluation" _note="In what follows, we evaluate on a toy model the different approaches for solving the low-rank DMD approximation problem. We consider a high-dimensional space of dimensions, a low-dimensional subspace of dimensions and snapshots. Let be a matrix of rank generated randomly according to , where entries of ’s are independent samples of the standard normal distribution. Let the initial condition be randomly chosen according to the same distribution. The snapshots, gathered in matrices and , are generated using for three configurations of :  , s.t. satisfying ,  ,  .  Setting corresponds to a linear system satisfying the assumption , as made in the projected DMD approaches . Setting and , do not make this assumption and simulate respectively linear and non-linear dynamical systems. We assess three different methods for computing :  optimal rank- approximation given by Algorithm  ,  th-order SVD approximation of , [I.E., ]{}-th order approximation of the rank- non-projected DMD solution ,  rank- approximation by , corresponding to the projected DMD approach (or for ).  The performance is measured in terms of the error norm with respect to the rank . Results for the three settings are displayed in Figure  . As a first remark, we notice that the solution provided by Algorithm   (method ) yields the best results, in agreement with [T]{}heorem  .  Second, in setting , the experiments confirm that when the linearity assumption is valid, the low-rank projected DMD (method ) achieves the same performance as the optimal solution (method ). Moreover, truncating the rank- DMD solution (method ) induces as expected an increase of the error norm. This deterioration is however moderate in our experiments.  Then, in settings and we remark that the behavior of the error norms are analogous (up to an order of magnitude). The [performance]{} of the projected approach (method ) differs notably from the optimal solution. A significant deterioration is visible for . This is the consequence of the non-validity of the assumption made in method . Nevertheless, we notice that method  accomplishes a slight gain in performance compared to method  up to a moderate rank (). Besides, we also notice that the error norm of method  in the case is not optimal.  Finally, as expected, all methods succeed in properly characterizing the low-dimensional subspace as soon as .">
</outline>
<outline text="Conclusion" _note="Following recent attempts to characterize an optimal low-rank approximation based on DMD, this paper provides a closed-form solution to this non-convex optimization problem. To the best of our knowledge, state-of-the-art methods are all sub-optimal. The paper further proposes effective algorithms based on SVD to solve this problem and run reduced-order models. Our numerical experiments attest that the proposed algorithm is more accurate than state-of-the-art methods. In particular, we illustrate the fact that simply truncating the full-rank DMD solution, or exploiting too restrictive assumptions for the approximation subspace is insufficient.">
</outline>
<outline text="Acknowledgements" _note="This work was supported by the “Agence Nationale de la Recherche&quot; through the GERONIMO project (ANR-13-JS03-0002).">
</outline>
  </body>
</opml>