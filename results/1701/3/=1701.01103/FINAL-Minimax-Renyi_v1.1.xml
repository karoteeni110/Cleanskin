<?xml version="1.0" encoding="UTF-8"?>
<opml version="2.0">
  <head>
    <title>Minimax Rényi Redundancy</title>
    <abstract>The redundancy for universal lossless compression of discrete memoryless
sources in Campbell’s setting is characterized as a minimax Rényi
divergence, which is shown to be equal to the maximal -mutual
information via a generalized redundancy-capacity theorem. Special
attention is placed on the analysis of the asymptotics of minimax Rényi
divergence, which is determined up to a term vanishing in blocklength.

**Keywords:** Universal lossless compression, generalized
redundancy-capacity theorem, minimax redundancy, minimax regret,
Jeffreys’ prior, risk aversion, Rényi divergence, -mutual information. </abstract>
  </head>
  <body>
<outline text="Introduction" _note="variable length source coding, expected code length is the usual cost function that one aims to minimize. For discrete memoryless sources, asymptotically, the minimal achievable per-letter expected code length is equal to the entropy. However, if is a discrete memoryless source distribution with an unknown parameter and the encoding system assumes a distribution , then one needs to pay an extra penalty for the mismatch given by[^1] where stands for the relative entropy between the probability measures and . In light of , the conventional worst-case measure of redundancy in universal lossless compression is where the infimization is over all distributions on , and the supremum is over all possible values of the unknown parameter. In this zero-sum game, is chosen by the code designer, and is chosen by nature.  A relation between and the maximal mutual information is given by the REDUNDANCY-CAPACITY THEOREM (e.g., , and ) that states that where[^2] the supremization is over all probability distributions on the parameter space. Through , and , we see a pleasing relationship between entropy, relative entropy and mutual information in the context of lossless data compression.  Let , and note that where the RELATIVE INFORMATION between the discrete probability measures and is defined as[^3] A much more stringent performance guarantee than the average of relative information is its pointwise maximum. In particular, if one replaces with in , the resulting quantity, i.e., is called the MINIMAX REGRET, which has found applications in various settings[^4], e.g., . An analogy to the Redundancy-Capacity Theorem is given by where denotes the -mutual information of infinite order, whose definition is given in .  The average and pointwise formulations are two extremes of performance guarantees, which are not quite suitable for certain applications. For this reason, one seeks a compromise between those two. For example, in the economics literature, average and pointwise guarantees are referred as RISK-NEUTRAL and RISK-AVOIDING, respectively. Since the former is known to be too lenient and the latter is known to be too stringent for typical applications, the notion of RISK-AVERSION has been introduced to provide a more useful compromise between these two extremes , , which is known to be relevant for diverse applications . In this paper, we introduce the notion of risk-aversion within the universal source coding context and quantify its effect on the fundamental limit.  In the non-universal setting, i.e., when the source distribution is known, a classical result of Campbell introduces such a risk-averse cost function in a discrete memoryless setting. Specifically, proposes to generalize the conventional notion of minimizing the expected code length with the cost function where , denotes the code, and denotes the length function. In this case, for a discrete memoryless source , Campbell shows that the minimum per-letter cost asymptotically achievable by prefix codes is given by the RÉNYI ENTROPY . Notice that captures the notion of risk-aversion through the parameter since A natural way to introduce risk-aversion in universal source coding is to use Campbell’s formulation and characterize the penalty for the mismatch akin to . Indeed, about forty years after Campbell’s work, Sundaresan shows that if one uses as the cost function, the penalty paid for universality can be written as[^5] where denotes the RÉNYI DIVERGENCE OF ORDER , which is defined in , and denotes the SCALED DISTRIBUTION of : The distance measure is known as the SUNDARESAN DIVERGENCE of order between and . Following , the relevant measure of redundancy for universal lossless compression under Campbell’s performance criterion is The conventional minimax redundancy in corresponds to while the minimax regret in corresponds to . Although, in general, , we are able to establish a pleasing analog to the classical redundancy results such as , and , : where in is the -MUTUAL INFORMATION OF ORDER between and with , see , . Note that is analogous to with Rényi divergence replacing the relative entropy. Thus, we refer as the MINIMAX RÉNYI REDUNDANCY. Moreover, generalizes the Redundacy-Capacity Theorem to -mutual information thereby finding another operational meaning for the maximal -mutual information beyond those that have been shown in the literature on error probability bounds for data transmission (e.g. , ). Moreover, the -mutual information smoothly interpolates between two extremes, namely in and in . Finally, and , coupled with Campbell’s result , provide a pleasing relationship between Rényi entropy, Rényi divergence and -mutual information in the context of universal lossless data compression.  The asymptotic behaviors of the minimax redundancy and minimax regret have also received considerable attention in the literature (e.g., , , , ) since, in addition to compression, they are relevant in applications such as machine learning, finance, prediction, gambling, and so on. In particular, Xie and Barron in their key contributions , show that where and are the number of observations and the alphabet size, respectively, denotes the Gamma function, and vanishes as .  While Merhav gives , we quantify asymptotically the effect of the risk-aversion parameter on the fundamental limit in universal source coding by providing a pleasing interpolation[^6] between and :  In the remainder of the paper, Section   sets the basic notation and definitions. Section   states the main results and gives the outlines of their proofs, which are contained in Section  . In the Appendices, we prove several lemmas that are used in Section  .  [^1]: For prefix codes, is well known . On the other hand, the loss in     rate incurred due to the prefix condition is known to be     asymptotically negligible .  [^2]:  is the mutual information between and with .  [^3]: Unless otherwise stated, logarithms and exponentials are of     arbitrary basis.  [^4]: For example, in lossless compression with prefix codes, is often     viewed as a proxy for the mismatch penalty incurred by assuming that     is drawn from rather than the true distribution . Such an     approximation can be justified asymptotically.  [^5]: Campbell’s and Sundaresan’s results are still valid when .     However, such a formulation corresponds to a RISK-SEEKING scheme,     which falls outside the philosophy espoused in this paper.  [^6]: In a fundamentally different setup, Hayashi considers the     counterpart of the Clarke and Barron result replacing relative     entropy with Rényi divergence.">
</outline>
<outline text="Notation and Definitions" _note="Let and denote the -dimensional simplex of probability mass functions defined on by For each parameter , we define our observation model such that[^1] and the independent identically distributed (i.i.d.) extension of this model such that where denotes the number of times appears in the vector , and therefore It can be verified that the Fisher information matrix (in nats) of for the parameter vector is[^2] where denotes an matrix all of whose entries are equal to 1. The determinant of the Fisher information matrix in satisfies An important probability measure on is JEFFREYS’ PRIOR defined as where denotes a special form of the Dirichlet integrals of type 1 which can be written in terms of the Gamma function: In particular,  The source distribution we get by assuming Jeffreys’ prior on the parameter space is referred as JEFFREYS’ MIXTURE which is denoted by[^3]  For discrete probability measures and on the set such that dominates , i.e., , RÉNYI DIVERGENCE of order[^4] between and is defined as where . In particular, when , Rényi divergence of order between and can be expressed as Given , an analogous generalization can be made for mutual information resulting in the -MUTUAL INFORMATION[^5] : where , independent of , and we have used the conventional notation for INFORMATION DENSITY . As shown in Lemma   in Appendix  , the infimum in can be solved explicitly.  In parallel with the standard usage for relative entropy, it is common to define the conditional Rényi divergence as therefore, the unconditional Rényi divergence in can be written as .  [^1]: As a special case, when , we use the shorthand notation instead of     .  [^2]: Note that the Fisher information matrix is since there are free     parameters in the model. Nevertheless, it is notationally convenient     to denote the parameter vector as if it were -dimensional.  [^3]: Whenever it is informative to explicitly show the dimensionality     of the parameter space in the notation for Jeffreys’ mixture, we do     so by replacing with .  [^4]: We are not concerned with Rényi divergences of order . A more     general definition can be found in .  [^5]: The definition of -mutual information in dates back to Sibson’s     information radius . Although, it should be noted that Sibson’s     motivation in is not the generalization of mutual information. See     for a more thorough discussion.">
</outline>
<outline text="Statement of the Results" _note="Theorem   states that under the minimax operation in the Sundaresan divergence can be replaced by the Rényi divergence. We further show that this minimax operation can be written as the maximization of the -mutual information, thus, providing a generalization to the Redundancy-Capacity Theorem in . In Theorem  , we investigate the asymptotic behavior of the minimax Rényi redundancy between and , and we find its precise asymptotic expansion, thereby quantifying the effect of the risk-aversion parameter .    For any , and positive integer  As we show in the proof in Section  , is due to the fact that scaling a distribution is a one-to-one operation that preserves memorylessness while the minimax theorem for Rényi divergence is the gateway to showing the generalized redundancy-capacity theorem in .  Although Theorem   holds in great generality, we illustrate its use in the simple example below.  Consider the Z-CHANNEL with crossover probability, see, e.g., . In this case, which is a concave function of for every value of , and is maximized when After some elementary algebra, plugging into yields Observe that as , the right side of converges to the capacity of the channel, namely, . On the other hand, to compute the minimax Rényi redundancy, note that Let be the distribution such that Since through and , as enforced by generalized redundancy-capacity theorem, we observe that the maximal -mutual information matches the minimax Rényi divergence.    For any  We prove Theorem   in Section   by dividing it into two parts: converse and achievability. In both parts, Jeffreys’ prior plays a significant role. However, it is known that Jeffreys’ prior dramatically emphasizes the lower dimensional faces of the simplex. While this is not a problem in proving the converse bound, Jeffreys’ prior achieves a suboptimal minimax value (see Lemma   in Appendix  ). Similar issues arise in finding the exact asymptotic constant in minimax redundancy , and in minimax regret . To overcome this problem, we modify Jeffreys’ prior by placing masses near the faces of the simplex as in . Although this resolves the problem encountered in the minimax redundancy and minimax regret cases, the functional form of Rényi divergence becomes the second obstacle which forces us to show a uniform Laplace approximation thereby making the proof of achievability a much more involved task than that of the converse. For this reason, we start by presenting the achievability proof in the special case of binary alphabets, in which the notation is simplified considerably.">
</outline>
<outline text="Proofs">
  <outline text="Proof of Theorem [thm:Generalized Redundancy-Capacity Theorem]" _note="To establish , for any , define the bijection as where Then, for any and , the scaled version of the conditional distribution (see ) satisfies Therefore, for any given distribution on As a result of , where follows because every probability measure in is a scaled version of another probability measure in .  In order to establish , note that where the expectation in is with respect to , and follows from , which holds when is finite. The right side of is the maximal -mutual information of order[^1] in the sense of Csiszár, see and , which is known to equal maximal (see , and ) in the discrete parameter case. To see that holds even when the parameter space is continuous, recall the definition of -mutual information, , which can be written as and note that where follows from Jensen’s inequality, follows from the fact that the maximin value is always less than or equal to the minimax value, and is again due to .  [^1]: When both random variables are discrete, another generalization of     mutual information, whose maximum also coincides with , is put     forward by Arimoto . See for further discussion of the various     proposals of -mutual information.">
  </outline>
  <outline text="Proof of the Converse of Theorem [thm:Asymptotic Behavior of Minimax Renyi Redundancy]" _note="This section is devoted to the proof of for any . Define for any . Let , and consider the following where is due to Theorem  , follows from a more general result, although, for the sake of completeness, its proof is included in Lemma   in Appendix  , is due to the suboptimal choice of Jeffreys’ prior, and follows because .  Using Robbins’ sharpening of Stirling’s approximation, one can show that where the entropy is in nats and denotes the empirical distribution of the vector . Since , particularizes to With the aid of and we can express the integral in the right side of as The gamma function generalization of Stirling’s approximation (shown to be valid for positive real numbers by Whittaker and Watson ) yields where . In particular, for , where It follows from and that Combining and , we can write where is due to the definition of , , the fact that for any positive constant , is a monotone increasing function of , and the fact that the error terms (see and ) satisfy Uniting the lower bounds in , and , where Notice that where follows after noticing that each factor of goes to 1, and follows from the definition of the Riemann integral. Assembling , and , we obtain the desired bound in .">
  </outline>
  <outline text="Proof of the achievability of Theorem [thm:Asymptotic Behavior of Minimax Renyi Redundancy] when" _note="In this section, we prove in when , i.e., To that end, we modify Jeffreys’ prior by placing masses near the vertices of the simplex, i.e., , which, in turn, enables us to show that when the parameter[^1] takes values near the vertices of the simplex the value of the minimax Rényi redundancy grows strictly slower than . Thus, we focus on values of that are not close to the vertices of the simplex, thereby enabling us to argue that the minimax Rényi redundancy behaves as in .  Inspired by Xie and Barron’s modified Jeffreys’ prior, for and , consider the prior which differs from the one in in the location of the point masses. Because of the modification on Jeffreys’ prior, the corresponding marginal changes from in to In view of Theorem  , where The following result shows that the first and the third supremizations in the right side of are both dominated by .    If , then  Assume that . We have where follows from , follows because Rényi divergence is monotone decreasing in (see Lemma   in Appendix  ) and follows because, for , Using a symmetrical argument, one can show that the upper bound in still holds when .  It remains to investigate the behavior of the second supremization in the right side of . Let and note that which follows from . The following proposition gives an asymptotic upper bound on .    Let . For any ,  Let and . Without loss of generality, we may assume that , otherwise we may interchange the roles of and together with the roles of and below. Note that where Thanks to Lemma   in Appendix  , we know that for all sufficiently large satisfying we have where the explicit expressions for and are given in and , respectively. Hence, we may now focus attention on . Note that where and denote the binary entropy and the binary relative entropy functions in nats, respectively and the bound in follows from Stirling’s approximation, see . Note also that where also follows from an application of Stirling’s approximation, see .  By substituting , , and into the right side of , we get where and Note that we can find an asymptotically suboptimal upper bound on that depends only on by invoking Lemma   in Appendix  , which shows a non-asymptotic uniform upper bound on , and then by invoking Lemma   in Appendix  , which shows a non-asymptotic uniform upper bound on Finding the optimal upper bound, on the other hand, requires a uniform Laplace approximation on , which is introduced next. First, given , split as where In Lemmas  ,   and   in Appendix  , we show each of the following properties: Since the left side of does not depend on , – imply, by letting , that Finally, it follows from , , , and that Since , it also follows that Combining and gives us the promised result of Proposition  .  Invoking Proposition  , we see that the functions in and can be bounded by while thanks to and Proposition  , it follows that Since , we see that the right side of asymptotically dominates the right sides of and . Due to , and –, the desired result in follows by choosing an arbitrarily small in .  [^1]: Since , we have . To simplify the discussion, we prefer the     shorthand notation rather than .">
  </outline>
  <outline text="Proof of the achievability of Theorem [thm:Asymptotic Behavior of Minimax Renyi Redundancy] when" _note="In this section, we prove in when , i.e., To do so, we once again modify Jeffreys’ prior as in the previous section by placing masses near the lower dimensional faces of the simplex, i.e., , which, in turn, enables us to show that when the parameter vector takes values near the faces of the simplex, the value of the minimax Rényi redundancy grows strictly slower than . Hence, by focusing on the parameter values that are not close to the faces of the simplex, we show that the minimax Rényi redundancy behaves as in .  Following the idea in , let and, for , define Accordingly, we define the probability measure with respect to , the Lebesgue measure on , as Finally, for , we define the prior distribution on the probability simplex as where is Jeffreys’ prior. Because of the modification on Jeffreys’ prior in , the corresponding marginal changes from in to where Define, for , Note that denotes the vectors none of whose coordinates are within close proximity of zero in the sense of .  In view of Theorem  , The following result shows that the supremizations over for in are all dominated by .    If , then for each where the explicit value of is given in .  Thanks to the symmetry, it suffices to show the result for . To that end, define as and let denote the Jeffreys’ mixture when the underlying parameter space is the -dimensional simplex. Further define and note that where follows from Lemma   in Appendix  . For , where follows from , and is due to . Finally, the desired result follows because – imply  It remains to investigate the supremization over in . Observe that which follows from the definition of in . Parallel to Proposition  , Proposition   characterizes the behavior of the supremum in the right side of .    For any ,  We are only interested in . Therefore, for all , where is a constant. Since there is an index such that , it simplifies notation without loss of generality that . Otherwise, the proof remains identical. For a given positive integer , let be a proper subset and note that where Thanks to Lemma   in Appendix  , we know that for all sufficiently large satisfying it follows that where is a constant depending only on and , which is explicitly given in the proof of Lemma  , see . Hence, we may now focus attention on . Note that and where both the entropy and relative entropy are in nats and the bound in follows from Stirling’s approximation, see . Note also that where also follows from an application of Stirling’s approximation, see .  By substituting , and into the right side of , we get where and Observe once again that we can find an asymptotically suboptimal upper bound on that depends only on and by invoking Lemma   in Appendix  , which shows a non-asymptotic uniform upper bound on , and then by invoking Lemma   in Appendix  , which shows a non-asymptotic uniform upper bound on Finding the optimal upper bound, on the other hand, requires a uniform Laplace approximation on , which is introduced next. First, given , recall the set as defined in , let and split as where In Lemmas   and   in Appendix  , we show that the following properties hold: Since the left side of does not depend on , and imply, by letting , that Finally, it follows from ( ), ( ), ( ), and , that holds when as we wanted to show.  Invoking Proposition  , we see that for each while thanks to and Proposition  , it follows that Since , we see that, as , the right side of goes to whereas the right side of remains constant. In view of , and , the desired result in follows by choosing an arbitrarily small in .">
  </outline>
</outline>
<outline text="Explicit Evaluation of -Mutual Information" _note="In the case of finite collection of arbitrary distributions, explicit evaluation of is provided by Sibson . A more general result that allows non-discrete alphabets can be found in .    Let . Given an arbitrary input distribution on and a random transformation with finite output alphabet , the -mutual information of order induced by on satisfies  Define and recall that for any distribution on . Capitalizing on , note that By the definition of the -mutual information, see ; implies the result in .">
</outline>
<outline text="Monotonicity of Binary Rényi Divergence" _note="  Let denote a Bernoulli distribution with parameter . For any and , is a monotone decreasing function of on .  Fix . Let . It suffices to prove that is a monotone decreasing function of on . To that end, note that where follows because implies">
</outline>
<outline text="Uniform Upper Bound on" _note="  Let be an element in the -dimensional simplex and assume that we are given a discrete i.i.d. model . Then, for any and , the relative information between the model and Jeffreys’ mixture satisfies the following bound where Consequently, for any , where is given in .  Immediate consequence of .">
</outline>
<outline text="Edge Cases of" _note="  Let and for a given positive integer , let be a proper subset of . Then, for any satisfying and (defined in ) where is defined[^1] in and is a constant that only depends and .  Denote and note that Regarding the last term within the summation in the right side of , where follows from the definition of the Dirichlet integrals in , and follows from the fact that . Now, observe that where is the remainder in Stirling’s approximation of in , and is due to the following elementary bounds: It follows that where Since , where is because and for any we have . Let It follows from and that Note that where denotes the Jeffreys’ mixture when the underlying parameter space is the -dimensional simplex. Using the uniform upper bound on Rényi divergence in Lemma  , we get where is as defined in . Since , , and for any integer , we can upper bound As a result, and follows after setting  [^1]: The quantity defined in corresponds to the special case of where ,     .">
</outline>
<outline text="Uniform Upper Bound on" _note="The quantity defined[^1] in satisfies the following upper bound.    where is explicitly given in .  Define where Note that where follows from , and follows from Stirling’s approximations, and , as well as the fact that Regarding , one can check that Invoking Lemma   in Appendix   to upper bound the left side of and applying the bound in to results in .  [^1]: The quantity defined in corresponds to the special case of where ,     .">
</outline>
<outline text="Bounds on" _note="The quantity defined[^1] in satisfies the following non-asymptotic bound.    Given , In particular,  For , because the function in the left side of is an increasing function. On the other hand, because the function of the left side of is a decreasing function. Finally, follows from the fact that and .    Let , and be fixed and be an integer. Assume that (defined in ) satisfies . If for then where in satisfies Furthermore,  Note that since , for which, in turn, imply that Hence, inequality follows. It is straightforward to see the limit in .  [^1]: The quantity defined in corresponds to the special case of where ,     .">
</outline>
<outline text="Lemmas for the Proof in Section [sec:subsec:ach-k=2]" _note="In the proofs of Lemmas  ,   and  , we use the following bound: for and , in nats. In particular, when To show and , we rely on Taylor’s theorem: for some in between and .    Let and fix . where is defined in .  Assume that is a sufficiently large integer, let be given. Then where is due to , the uniform upper bound on given in Lemma   in Appendix  , and the fact that , follows because for , Since the supremum in is attained at , it follows that holds.    Let . where is defined in .  Assume that is a sufficiently large integer, let be given and define We have where is due to , the bound on for the given range of (see Lemma   in Appendix  ), and the fact that for , follows because for , In light of Lemma   in Appendix  , Moreover, the Riemann sum in can be upper bounded as It follows that holds.    Let and fix . where is defined in .  The proof of this lemma is more involved than that of Lemma  . To proceed, using Pinsker’s inequality (e.g., ), namely we first prove that where is a fixed constant. Then, we show that with the help of Lemma   in Appendix  . Fix a constant , and assume that is a sufficiently large integer.  First, let be arbitrary and note that where follows from Lemma   in Appendix  , Pinsker’s inequality as in , and the fact that , follows because and for , Thus, implies that Since , and it follows that holds.  Second, let be arbitrary and fix some constant . Further, separate into two sums as follows where Regarding , we have where follows from Lemma   in Appendix   and , follows because and for and . Hence, and Regarding , we have where follows from Lemma   in Appendix  . Let be the maximizer of the right side in .  Note that where follows after noticing that for any , the function is a decreasing function in and therefore the corresponding Riemann sum in the left side of can be upper bounded by the integral in its right side and follows from Lemma   in Appendix  . Hence, As a result of and , holds. The desired result follows since we have established and .">
</outline>
<outline text="Upper Bound for the Integral in" _note="  Let and . Fix , and . For any  Abbreviate Applying integration by parts yields For , we have because is a decreasing function and is an increasing convex function for the given range of . Hence, we see that where follows because implies and">
</outline>
<outline text="Lemmas for the Proof in Section [sec:subsec:ach-gen]" _note="In the proofs of Lemmas   and  , we use the following bound: for and , where denotes the Fisher information matrix, and To show , we rely on Taylor’s theorem: for some such that lies between and .    The function defined in satisfies  Assume that is a sufficiently large integer, and let with be given. Define We invoke with Hence, where is due to , the bound on when (see Lemma   in Appendix  ), and the fact that for , follows because implies In light of Lemma   in Appendix  , Since the multi-variable Riemann sum in can be upper bounded as we can conclude that holds.    The function defined in satisfies  Assume that is a sufficiently large integer, and let with be given. Recall the definition of in , and note that if then there must exist such that Moreover, by symmetry, we can write where ( ) is due to the uniform upper bound on in Lemma  , in ( ), and the function denoted by is defined in . By invoking Lemma   in Appendix  , we see that can be upper bounded by a constant depending only on and . On the other hand, the sum without the factor vanishes as (see Lemmas   and  ). Therefore, follows.">
</outline>
<outline text="Jeffreys’ Mixture is not Minimax" _note="The fact that Jeffreys’ prior is capacity achieving (or least favorable) follows from the converse proof of Theorem  . Therefore, Jeffreys’ mixture is maximin for Rényi redundancy. Parallel to the results in and , Lemma   below proves that Jeffreys’ mixture is NOT minimax.    For any , where the supremization is over all that are on the face of the simplex so that at most of its components are known to be non-zero.  Note that the third term in the right side of interpolates the extra constants when and when , shown in and , respectively.  Assuming without loss of generality that the last entries of are equal to zero simplifies the notation. Otherwise, the proof remains identical. Define where denotes the -th entry of . Note that where denotes the Jeffreys’ mixture when the underlying parameter space is the -dimensional simplex, follows from the fact that and follows from Stirling’s approximation which can be seen in . Since where the supremization in the left side of is over all whose last entries are zero, the converse result in Section   with , and the fact that along with routine algebraic manipulations yield the desired result in .">
</outline>
<outline text="Acknowledgments" _note="This work has been supported by ARO-MURI contract number W911NF-15-1-0479 and in part by the Center for Science of Information, an NSF Science and Technology Center under Grant CCF-0939370.  [Semih Yagli]{} received his Bachelor of Science degree in Electrical and Electronics Engineering in 2013, his Bachelor of Science degree in Mathematics in 2014 both from Middle East Technical University and his Master of Arts degree in Electrical Engineering in 2016 from Princeton University.  Currently, he is pursuing his Ph.D. degree in Electrical Engineering in Princeton University under the supervision of Sergio Verdú. His research interest include information theory, optimization, and machine learning.  [Yücel Altuğ]{} received the B.S. and M.S. degrees in electrical and electronics engineering from Boğaziçi University, Turkey, in 2006 and 2008, respectively and the Ph.D. degree in electrical and computer engineering from Cornell University, in 2013, where he has been awarded the ECE Director’s Ph.D. Thesis Research Award. After postdoctoral appointments at Cornell University and Princeton University, he is currently a senior data scientist at Natera Inc. His research interests include Shannon theory, feedback communications, and stochastic modeling and algorithm design for next-generation DNA sequencing and genetic testing.  [Sergio Verdú]{} received the Telecommunications Engineering degree from the Universitat Politècnica de Barcelona in 1980, and the Ph.D. degree in Electrical Engineering from the University of Illinois at Urbana-Champaign in 1984. Since then, he has been a member of the faculty of Princeton University, where he is the Eugene Higgins Professor of Electrical Engineering, and is a member of the Program in Applied and Computational Mathematics.  Sergio Verdú is the recipient of the 2007 Claude E. Shannon Award, and the 2008 IEEE Richard W. Hamming Medal. He is a member of both the National Academy of Engineering and the National Academy of Sciences. In 2016, Verdú received the National Academy of Sciences Award for Scientific Reviewing.  Verdú is a recipient of several paper awards from the IEEE: the 1992 Donald Fink Paper Award, the 1998 and 2012 Information Theory Paper Awards, an Information Theory Golden Jubilee Paper Award, the 2002 Leonard Abraham Prize Award, the 2006 Joint Communications/Information Theory Paper Award, and the 2009 Stephen O. Rice Prize from the IEEE Communications Society. In 1998, Cambridge University Press published his book [MULTIUSER DETECTION,]{} for which he received the 2000 Frederick E. Terman Award from the American Society for Engineering Education. He was awarded a Doctorate Honoris Causa from the Universitat Politècnica de Catalunya in 2005, and was elected corresponding member of the Real Academia de Ingeniería of Spain in 2013.  Sergio Verdú served as President of the IEEE Information Theory Society in 1997, and on its Board of Governors (1988-1999, 2009-2014). He has also served in various editorial capacities for the [IEEE TRANSACTIONS ON INFORMATION THEORY]{}: Associate Editor (Shannon Theory, 1990-1993; Book Reviews, 2002-2006), Guest Editor of the Special Fiftieth Anniversary Commemorative Issue (published by IEEE Press as “Information Theory: Fifty years of discovery&quot;), and member of the Executive Editorial Board (2010-2013). He co-chaired the Europe-United States [FRONTIERS OF ENGINEERING]{} program, of the National Academy of Engineering during 2009-2013. He is the founding Editor-in-Chief of [FOUNDATIONS AND TRENDS IN COMMUNICATIONS AND INFORMATION THEORY]{}. Verdú served as co-chair of the 2000 and 2016 [IEEE INTERNATIONAL SYMPOSIA ON INFORMATION THEORY]{}.  Sergio Verdú has held visiting appointments at the Australian National University, the Technion-Israel Institute of Technology, the University of Tokyo, the University of California, Berkeley, the Mathematical Sciences Research Institute, Berkeley, Stanford University, and the Massachusetts Institute of Technology.">
</outline>
  </body>
</opml>