<?xml version="1.0" encoding="UTF-8"?>
<opml version="2.0">
  <head>
    <title>An Evaluation Framework and Database
for MoCap-Based Gait Recognition Methods[^1]

[^1]: This is a companion paper to our papers .</title>
    <abstract>As a contribution to reproducible research, this paper presents a
framework and a database to improve the development, evaluation and
comparison of methods for gait recognition from motion capture (MoCap)
data. The evaluation framework comprises source codes of
state-of-the-art human-interpretable geometric features as well as our
own approaches where gait features are learned by a modification of
Fisher’s Linear Discriminant Analysis with the Maximum Margin Criterion,
and by a combination of Principal Component Analysis and Linear
Discriminant Analysis. It includes a description and source codes of
a mechanism for evaluating class separability coefficients of feature
space and four classifier performance metrics. This framework also
contains a tool for learning a custom classifier and for classifying a
custom probe on a custom gallery. We provide an experimental database
along with source codes for its extraction from the general CMU MoCap
database. </abstract>
  </head>
  <body>
<outline text="Introduction" _note="Gait (walk) pattern has several attractive properties as a soft biometric trait. From a surveillance perspective, gait pattern biometrics is appealing in that it can be performed at a distance without requiring body-invasive equipment or subject cooperation.  Many research groups investigate the discrimination power of gait pattern and develop models that are applied to the automatic recognition of walking people from MoCap data. A number of MoCap-based gait recognition methods have been introduced in the past few years and new ones continue to emerge. In order to move forward with this competitive research, it is necessary to compare their innovative approaches with the state-of-the-art and evaluate them against established evaluation metrics on a benchmark database. New frameworks and databases have been developed recently .  As a contribution to reproducible research, this paper focuses on our framework for evaluating MoCap-based gait recognition methods and our benchmark MoCap gait database. We provide a large experimental database together with its extraction-and-normalization drive from the general CMU MoCap database, as specified in Section  . Implementation details of thirteen relevant methods are summarized in Section  . In Section   we describe the evaluation mechanism and define four class separability coefficients and four rank-based classifier performance metrics. Finally, Section   consists of a manual and comments on reproducing the experiments.">
</outline>
<outline text="Data" _note="MoCap technology provides video clips of individuals walking which contain structural motion data. The format keeps an overall structure of the human body and holds estimated 3D positions of major anatomical landmarks as the person moves. These MoCap data can be collected online by a system of multiple cameras (Vicon) or a depth camera (Microsoft Kinect). To visualize MoCap data (see Figure  ), a simplified stick figure representing the human skeleton (graph of joints connected by bones) can be recovered from body point spatial coordinates in time. Recent rapid improvement in MoCap sensor accuracy has brought affordable MoCap technology to assist human identification in such applications as access control and video surveillance.  For evaluation purposes we have extracted a large number of gait samples from the MoCap database obtained from the CMU Graphics Lab , which is available under the Creative Commons license. It is a well-known and recognized database of structural human motion data and contains a considerable number of gait sequences. Motions are recorded with an optical marker-based Vicon system. People wear a black jumpsuit with 41 markers taped on. The tracking space of is surrounded by 12 cameras with a sampling rate of at heights ranging from 2 to 4 meters above ground thereby creating a video surveillance environment. Motion videos are triangulated to get highly accurate 3D data in the form of relative body point coordinates (with respect to the root joint) in each video frame and are stored in the standard ASF/AMC data format. Each registered participant is assigned with their respective skeleton described in an ASF file. Motions in the AMC files store bone rotational data, which is interpreted as instructions about how the associated skeleton deforms over time.  These MoCap data, however, contain skeleton parameters pre-calibrated by the CMU staff. Skeletons are unique to each walker and even a trivial skeleton check could result in  recognition. In order to fairly use the collected data, a prototypical skeleton is constructed and used to represent bodies of all subjects, shrouding the skeleton parameters. Assuming that all walking individuals are physically identical disables the skeleton check from being a potentially unfair classifier. Moreover, this is a skeleton-robust solution as all bone rotational data are linked to one specific skeleton. To obtain realistic parameters, it is calculated as mean of all skeletons in the provided ASF files.  The raw data are in the form of bone rotations or, if combined with the prototypical skeleton, 3D joint coordinates. The bone rotational data are taken from the AMC files without any pre-processing. We calculate the joint coordinates using the bone rotational data and the prototypical skeleton. One cannot directly use raw values of joint coordinates, as they refer to absolute positions in the tracking space, and not all potential methods are invariant to person’s position or walk direction. To ensure such invariance, the center of the coordinate system is moved to the position of root joint for each time  and the axes are adjusted to the walker’s perspective: the X axis is from right (negative) to left (positive), the Y axis is from down (negative) to up (positive), and the Z axis is from back (negative) to front (positive). In the AMC file structure notation it is achieved by setting the root translation and rotation to zero (`root 0 0 0 0 0 0`) in all frames of all motion sequences.  Since the general motion database contains all motion types, we extracted a number of sub-motions that represent gait cycles. First, an exemplary gait cycle was identified, and clean gait cycles were then filtered out using a threshold for their Dynamic Time Warping (DTW) distance on bone rotations in time. The distance threshold was explicitly set low enough so that even the least similar sub-motions still semantically represent gait cycles. Setting this threshold higher might also qualify sub-motions that do not resemble gait cycles anymore. Finally, subjects that contributed with less than 10 samples were excluded. The final database  has 54 walking subjects that performed 3,843 samples in total, which results in an average of about 71 samples per subject.">
</outline>
<outline text="Implementation Details of Algorithms" _note="Recognizing a person from their gait involves capturing and normalizing their walk sample , extracting gait features to compose a template , and finally querying the gallery for a set of similar templates  – based on a distance function  – to report the most likely identity. This work focuses on extracting robust and discriminative gait features from raw MoCap data.  Many geometric gait features have been introduced over the past few years. They are typically combinations of static body parameters (bone lengths, person’s height)  with dynamic gait features such as step length, walking speed, joint angles and inter-joint distances , along with various statistics (mean, standard deviation or maximum) of their signals . Clearly, these features are schematic and human-interpretable, which is convenient for visualizations and for intuitive understanding, but unnecessary for automatic gait recognition. Instead, our approach  prefers learning features in a supervised manner that maximally separate the identity classes and are not limited by such dispensable factors.  What follows is a detailed specification of the thirteen gait features extraction methods that we have reviewed in our work to date. Since the idea behind each method has some potential, we have implemented each of them for direct comparison.  \#1  **\#1**  Ahmed by Ahmed [ET AL.]{}  extracts the mean, standard deviation and skew during one gait cycle of horizontal distances (projected on the Z axis) between feet, knees, wrists and shoulders, and mean and standard deviation during one gait cycle of vertical distances (Y coordinates) of head, wrists, shoulders, knees and feet, and finally the mean area during one gait cycle of the triangle of root and two feet. Ali by Ali [ET AL.]{}  measures the mean areas during one gait cycle of lower limb triangles. Andersson by Andersson [ET AL.]{}  calculates gait attributes as mean and standard deviation during one gait cycle of local extremes of the signals of lower body angles, step length as a maximum of feet distance, stride length as a length of two steps, cycle time and velocity as a ratio of stride length and cycle time. In addition, they extract the mean and standard deviation during one gait cycle of each bone length, and height as the sum of the bone lengths between head and root plus the averages of the bone lengths between root and both feet. Ball by Ball [ET AL.]{}  measures mean, standard deviation and maximum during one gait cycle of lower limb angle pairs: upper leg relative to the Y axis, lower leg relative to the upper leg, and the foot relative to the Z axis. Dikovski by Dikovski [ET AL.]{}  selects the mean during one gait cycle of step length, height, all bone lengths, then mean, standard deviation, minimum, maximum and mean difference of subsequent frames during one gait cycle of all major joint angles, and the angle between the lines of the shoulder joints and the hip joints. Gavrilova by Gavrilova [ET AL.]{}  chooses 20 joint relative distance signals and 16 joint relative angle signals across the whole body, compared using the DTW. Jiang by Jiang [ET AL.]{}  measures angle signals between the Y axis and four major lower body (thigh and calf) bones. The signals are compared using the DTW. Krzeszowski by Krzeszowski [ET AL.]{}  observes the signals of rotations of eight major bones (humerus, ulna, thigh and calf) around all three axes, the person’s height and step length. These signals are compared using the DTW distance function. Kumar by Kumar [ET AL.]{}  extracts all joint trajectories around all three axes. Gait samples are compared by a distance function of their covariance matrices. Kwolek by Kwolek [ET AL.]{}  processes signals of bone angles around all axes, the person’s height and step length. The gait cycles are normalized to 30 frames. Preis by Preis [ET AL.]{}  takes height, length of legs, torso, both lower legs, both thighs, both upper arms, both forearms, step length and speed. Sedmidubsky by Sedmidubsky [ET AL.]{}  concludes that only the two shoulder-hand signals are discriminatory enough to be used for recognition. These temporal data are compared using the DTW distance function. Sinha by Sinha [ET AL.]{}  combines all features of Ball and Preis with mean areas during one gait cycle of upper body and lower body, then mean, standard deviation and maximum distances during one gait cycle between the centroid of the upper body polygon and the centroids of four limb polygons.  We are interested in finding an optimal feature space by maximizing its class separability, which is when gait templates are close to those of the same walker and far from those of other walkers. The method proposed in  learns gait features directly from joint coordinates by a modification of Fisher’s Linear Discriminant Analysis  with Maximum Margin Criterion. The framework allows learning from bone rotations as well.  Let the model of a human body have  joints and all samples be linearly normalized to their average length . Labeled learning data in the sample (measurement) space are in the form where is a sample (gait cycle) in which are 3D spatial coordinates of a joint at time normalized with respect to the person’s position and direction. See that has dimensionality . Learning on bone rotations is analogical. Each learning sample falls strictly into one of the learning identity classes determined by . A class has samples. The classes are complete and mutually exclusive. We say that learning samples and share a common walker if and only if they belong to the same class, i.e., .  Apart from Maximum Margin Criterion (MMC) we also investigated the fusion of Principal Component Analysis (PCA) with Linear Discriminant Analysis (LDA) that has been used for silhouette-based (2D) gait recognition by Su [ET AL.]{} . Feature extraction is given by a linear transformation (feature) matrix from a sample space of not necessarily labeled gait samples to a -dimensional feature space of gait templates where and gait samples are transformed into gait templates by .  On given labeled learning data , Algorithm   and Algorithm   are efficient ways of learning the transforms for MMC and PCA+LDA, respectively. Both algorithms  are of quadratic complexity with respect to the number of learning identity classes due to the singular value decomposition and eigenvalue decomposition.  split into classes of samples compute overall mean and individual class means compute compute compute compute eigenvectors and corresponding eigenvalues of through SVD of compute eigenvectors of through SVD of compute eigenvectors compute eigenvalues return transform as eigenvectors in that correspond to the eigenvalues of at least in  split into classes of samples compute overall mean and individual class means compute compute compute eigenvectors of that correspond to largest eigenvalues compute eigenvectors of return transform  In addition to the gait features extraction methods of our fellow researchers, we implemented our own methods as described below. Depending on whether the raw data are in the form of bone rotations or joint coordinates, the methods are referred to with BR or JC subscripts, respectively.  \_MMC learns gait features by MMC (Algorithm  ) and the gait templates are compared by the Mahalanobis distance . \_PCALDA learns gait features by PCA+LDA (Algorithm  ) and the gait templates are also compared by the Mahalanobis distance . \_Random has no features and classification is performed by picking a random identity that is present in the gallery. **\_Raw** takes all raw data. The template vector, normalized to the average of frames, results in a large feature space dimensionality , which is why the raw data cannot be directly used for recognition on large databases.">
</outline>
<outline text="Evaluation" _note="Learning data of identities and evaluation data of identity classes have to be disjunct at all times. In the following, we introduce two setups of data separation: homogeneous and heterogeneous. The homogeneous setup learns the transformation matrix on samples of identities and is evaluated on templates derived from the other samples of the same identities. The heterogeneous setup learns the transform on all samples in identities and is evaluated on all templates derived from other identities. An abstraction of this concept is depicted in Figure  . Note that unlike in the homogeneous setup, no walker identity is ever used for both learning and evaluation at the same time in the heterogeneous setup.  The homogeneous setup is parametrized by a single number of learning-and-evaluation identity classes, whereas the heterogeneous setup has the form specifying how many learning and how many evaluation identity classes are randomly selected from the database. The evaluation of each setup is repeated 3 times, selecting new random and identity classes each time and reporting the average result.  In the homogeneous setup, all results are estimated with nested cross-validation (see Figure  ) that involves the outer 3-fold cross-validation loop where templates in one fold are used for learning the features, while templates in the remaining two folds are used for evaluations. In the heterogeneous setup, the learning and evaluation parts are selected at random based on the given and , respectively. For both setups, this model is frozen and ready to be evaluated for class separability coefficients. Evaluation of rank-based classifier performance metrics advances to the inner 10-fold cross-validation loop taking one dis-labeled fold as a testing set and the other nine labeled folds as gallery. Test templates are classified by the winner-takes-all strategy, in which a test template gets assigned with the label of the gallery’s closest identity class.  Correct Classification Rate (CCR) is often perceived as the ultimate qualitative measure, however, if a method has a low CCR, we cannot directly say if the system is failing because of bad features or a bad classifier. It is more explanatory to provide an evaluation in terms of class separability of the feature space. The class separability measures give an estimate on the recognition potential of the extracted features and do not reflect an eventual combination with an unsuitable classifier:  \#1 (\#2)  Davies-Bouldin Index (DBI) where is the average distance of all elements in identity class to its centroid, and analogically for . Templates of low intra-class distances and of high inter-class distances have a low DBI. Dunn Index (DI) with from the above DBI. Since this criterion seeks classes with high intra-class similarity and low inter-class similarity, a high DI is more desirable. Silhouette Coefficient (SC) where is the average distance from to other samples within the same identity class and is the average distance of to the samples in the closest class. It is clear that and SC close to one means that classes are appropriately separated. Fisher’s Discriminant Ratio (FDR) High FDR is preferred for classes of low intra-class sparsity and high inter-class sparsity.  Apart from analyzing the distribution of templates in the feature space, it is schematic to combine the features with a rank-based classifier and to evaluate the system based on distance distribution with respect to a probe. For obtaining a more applied performance evaluation, we evaluate:  \#1 (\#2)     Cumulative Match Characteristic (CMC) Sequence of Rank- (for on X axis from 1 up to ) recognition rates (Y axis) for measuring ranking capabilities of a recognition method. Its headline Rank-1 is the well-known **CCR**. False Accept Rate vs. False Reject Rate (FAR/FRR) Two sequences of the error rates (Y axis) as functions of discrimination threshold (X axis). Each method has a value  of this threshold giving Equal Error Rate (**EER**=FAR=FRR). Receiver Operating Characteristic (ROC) Sequence of True Accept Rate (**TAR**) and False Accept Rate (**FAR**) with a varied discrimination threshold. For a given threshold the system signals both TAR (Y axis) and FAR (X axis). Area Under Curve (**AUC**) is computed as the integral of the ROC curve. Recall vs. Precision (RCL/PCN) Sequence of rates with a varied discrimination threshold. For a given threshold the system signalizes both RCL (X axis) and PCN (Y axis). The value of Mean Average Precision (**MAP**) is computed as the area under RCL/PCN curve.  These measures reflect how well the feature is class-separated and how much it takes to confuse the identities of two people. They do not, in fact, provide complementary information, however, a quality evaluation framework should be able to evaluate the most popular measures. Each measure is evaluated in the context of a particular application. For example, a hotel lobby authentication system could use a high Rank-3 at the CMC, while a city-level person tracking system is likely to need the ROC curve leaning towards the upper left corner.">
</outline>
<outline text="Reproducing the Experiments" _note="This section provides a description of the framework we implemented and the database we extracted. With this manual, a reader should be able to reproduce the evaluation and to use the implementation for recognizing people. All source codes including (1) database extraction drive, (2) implementations of the proposed and all relevant methods, (3) classifier learning and classification mechanisms and (4) evaluation mechanism and metrics, are available at our departmental Git repository . The original CMU MoCap database and extracted databases are available online at our research group web page .  `Executor.java` is the main execution class. Set all parameters of evaluation and file locations and `distanceThreshold`, then select which actions to perform and finally, select the evaluation setups. The class contains the `main(String[] args)` method. It contains four methods to select for execution:  `extractDatabase()`  :   for extracting an experimental database from the original CMU MoCap     database –- this is also available for download at our page as     `original.zip`. To run this method, unzip to get the files     `gaitcycle.amc` (exemplary gait cycle) and `skeleton.asf`     (prototypical skeleton) and the directory `amcOriginal` (original     AMC files). Extraction begins with normalization with respect to a     person’s position and walk direction as provided in the     `normalized.zip` file. Clean gait cycles are then filtered out by     the distance threshold (see last paragraph of Section  ) that     numerically expresses how much extracted motions resemble gait     cycles, that is, the lower the distance threshold, the fewer and     cleaner the gait cycles. Set a value for `distanceThreshold` to     produce a folder of the extracted database. Evaluations in  are set     with 302.0, extracting a database of 54 identities and     3[,]{}843 gait cycles. A higher distance threshold will qualify some     non-gait motions.  `learnClassifiers()`  :   for learning classifiers of all implemented methods on a     sub-database determined by the distance threshold. Set a value for     `distanceThreshold` (such as 302.0) and provide the corresponding     directory of the learning database (such as `amc302.0` in     `extracted-302.0.zip`) and the learned classifiers appear in the     `classifiers` folder.  `performClassification()`  :   for performing a classification of a custom probe/query on a custom     gallery with a custom classifier. Set file locations for the     classifier file `customClassifier`, the probe gait cycle     `customQueryFileAMC` and the gallery directory     `customGalleryDirectory`. Results are printed on the standard     output.  `evaluateMethods()`  :   for evaluating the implemented methods in homogeneous and     heterogeneous setups. To skip database extraction, one could supply     a provided extracted database (such as `amc302.0` in     `extracted-302.0.zip`) and a skeleton file (such as `skeleton.asf`     in any extracted database ZIP file). Our page provides additional     databases categorised according to various values of     `distanceThreshold`. The results are set to be printed on the     standard output but we suggest to redirect it to a CSV file. Results     of individual evaluation attempts vary slightly as different     learning, testing and gallery sets are randomly selected upon each     attempt.  Compile to obtain `Gait.jar`. The main project location should also contain the `lib` directory and all necessary files and directories depending on which actions are to be executed. Run command `$ java -jar Gait.jar &gt; output.csv`.  The output file (see structure in Table  ) contains the performance metrics as specified in Section   and the information about average distance computation time (**DCT**) in milliseconds and average template dimensionality (**TD**). The evaluation results are in the form of one value per coefficient (see results in Table  ) and the sequences CMC, FAR/FRR, ROC=TAR/FAR and RCL/PCN. The CMC sequence contains values, one for each in the Rank- recognition rate. The other three pairs of sequences are normalized to 30 values by `method.setFineness(30)`. The FAR/FRR sequences of all methods are normalized to the discrimination threshold with respect to the first value of FAR=0 and FRR=1, and to the middle value that represents EER where all sequences cross. The ROC sequences are normalized with respect to the first value of TAR=FAR=0 and to the last value of TAR=FAR=1. Finally, the RCL/PCN sequences are normalized with respect to the first value of RCL=0 and to the last value of RCL=1.  3.3pt  To reproduce the experiments in Table  , follow the instructions in the README file at  in the `reproduce` folder. Please note that some methods are slow even on a leading edge hardware. Learning and evaluation times in Table   were measured on a computer with Intel Xeon CPU E5-2650 v2 @ 2.60GHz and  RAM.  4.2pt  The goal of the MMC-based learning is to find a linear discriminant that maximizes the misclassification margin. This optimization technique appears to be more effective than designing geometric gait features. Table   indicates the best results for the MMC on bone rotational data: highest SC, EER and AUC, and competitive DBI, DI, FDR, CCR and MAP. In terms of the Correct Classification Rate metric, our MMC method was only outperformed by the Raw method, which is implemented here as a form of baseline. We interpret the high scores as a sign of robustness.  Apart from the performance merits, the MMC method is also efficient: relatively low-dimensional templates and Mahalanobis distance ensure fast distance computations and thus contribute to high scalability. Note that even if the Raw method has some of the best results, it can hardly be used in practice due to its extreme consumption of time and space resources. On the other hand, Random has no features but cannot be considered a serious recognition method. To illustrate the evaluation time, calculating the distance matrix (a matrix of distances between all evaluation templates) took a couple minutes for the MMC method, almost nothing for the Random method, and more than two weeks for the Raw method. To conclude, the MMC method on bone rotational data appears to be an optimal trade-off between effectiveness and efficiency, and thus the new state-of-the-art in feature extraction for MoCap-based gait recognition.">
</outline>
<outline text="Summary and Future Work" _note="As our contribution to reproducible research, we have provided implementation details and source codes  of our evaluation framework for gait recognition . The software implements the proposed method as well as all related methods. We include the evaluation database  together with source codes for its extraction from the general CMU MoCap database. We also attach the description and portable software for evaluating class separability coefficients of extracted features and classifier performance metrics. Finally, we provide documentation and installation instructions for easy and straightforward reproducibility of the experiments.  As demonstrated by outperforming other methods in four class separability coefficients and four classification metrics, the proposed features learning mechanism has a strong potential in gait recognition applications. Even though we believe that MMC is the most suitable criterion for optimizing gait features, we continue to research further potential optimality criteria and machine learning approaches.  We hope that the evaluation framework and database presented here will contribute to smooth development and evaluation of further novel MoCap-based gait recognition methods. All used data and source codes have been made available  under the Creative Commons Attribution license (CC-BY) for database and the Apache 2.0 license for software, which grant free use and allow for experimental evaluation. We encourage all readers and developers of MoCap-based gait recognition methods to contribute to the framework with new algorithms, data and improvements.">
  <outline text="Acknowledgments" _note="The authors thank to the anonymous reviewers and editor for their detailed commentary and suggestions. The data used in this project was created with funding from NSF EIA-0196217 and was obtained from &lt;http://mocap.cs.cmu.edu&gt; .">
  </outline>
</outline>
  </body>
</opml>