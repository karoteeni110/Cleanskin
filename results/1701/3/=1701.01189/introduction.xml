<?xml version="1.0" encoding="UTF-8"?>
<opml version="2.0">
  <head>
    <title></title>
    <abstract> </abstract>
  </head>
  <body>
<outline text="Introduction" _note="This paper studies the multisplit primitive for GPUs. [^1] Multisplit divides a set of items (keys or key-value pairs) into contiguous buckets, where each bucket contains items whose keys satisfy a programmer-specified criterion (such as falling into a particular range). Multisplit is broadly useful in a wide range of applications, some of which we will cite later in this introduction. But we begin our story by focusing on one particular example, the delta-stepping formulation of single-source shortest path (SSSP).  The traditional (and work-efficient) serial approach to SSSP is Dijkstra’s algorithm , which considers one vertex per iteration—the vertex with the lowest weight. The traditional parallel approach (Bellman-Ford-Moore ) considers all vertices on each iteration, but as a result incurs more work than the serial approach. On the GPU, the recent SSSP work of Davidson et al.  instead built upon the delta-stepping work of Meyer and Sanders , which on each iteration classifies candidate vertices into BUCKETS or BINS by their weights and then processes the bucket that contains the vertices with the lowest weights. Items within a bucket are unordered and can be processed in any order.  Delta-stepping is a good fit for GPUs. It avoids the inherent serialization of Dijkstra’s approach and the extra work of the fully parallel Bellman-Ford-Moore approach. At a high level, delta-stepping divides up a large amount of work into multiple buckets and then processes all items within one bucket in parallel at the same time. How many buckets? Meyer and Sanders describe how to choose a bucket size that is “large enough to allow for sufficient parallelism and small enough to keep the algorithm work-efficient” . Davidson et al. found that 10 buckets was an appropriate bucket count across their range of datasets. More broadly, for modern parallel architectures, this design pattern is a powerful one: expose just enough parallelism to fill the machine with work, then choose the most efficient algorithm to process that work. (For instance, Hou et al. use this strategy in efficient GPU-based tree traversal .)  Once we’ve decided the bucket count, how do we efficiently classify vertices into buckets? Davidson et al. called the necessary primitive MULTISPLIT. Beyond SSSP, multisplit has significant utility across a range of GPU applications. Bucketing is a key primitive in one implementation of radix sort on GPUs , where elements are reordered iteratively based on a group of their bits in their binary representation; as the first step in building a GPU hash table ; in hash-join for relational databases to group low-bit keys ; in string sort for singleton compaction and elimination ; in suffix array construction to organize the lexicographical rank of characters ; in a graphics voxelization pipeline for splitting tiles based on their descriptor (dominant axis) ; in the shallow stages of -d tree construction ; in Ashari et al.’s sparse-matrix dense-vector multiplication work, which bins rows by length ; and in probabilistic top- selection, whose core multisplit operation is three bins around two pivots . And while multisplit is a crucial part of each of these and many other GPU applications, it has received little attention to date in the literature. The work we present here addresses this topic with a comprehensive look at efficiently implementing multisplit as a general-purpose parallel primitive.  The approach of Davidson et al. to implementing multisplit reveals the need for this focus. If the number of buckets is 2, then a scan-based “split” primitive  is highly efficient on GPUs. Davidson et al. built both a 2-bucket (“Near-Far”) and 10-bucket implementation. Because they lacked an efficient multisplit, they were forced to recommend their theoretically-less-efficient 2-bucket implementation:  &gt; The missing primitive on GPUs is a high-performance MULTISPLIT that &gt; separates primitives based on key value (bucket id); in our &gt; implementation, we instead use a sort; in the absence of a more &gt; efficient multisplit, we recommend utilizing our Near-Far work-saving &gt; strategy for most graphs.   Like Davidson et al., we could implement multisplit on GPUs with a sort. Recent GPU sorting implementations  deliver high throughput, but are overkill for the multisplit problem: unlike sort, multisplit has no need to order items within a bucket. In short, sort does more work than necessary. For Davidson et al., reorganizing items into buckets after each iteration with a sort is too expensive: “the overhead of this reorganization is significant: on average, with our bucketing implementation, the reorganizational overhead takes 82% of the runtime.”    In this paper we design, implement, and analyze numerous approaches to multisplit, and make the following contributions:  On modern GPUs, “global” operations (that require global communication across the whole GPU) are more expensive than “local” operations that can exploit faster, local GPU communication mechanisms. Straightforward implementations of multisplit primarily use global operations. Instead, we propose a parallel model under which the multisplit problem can be factored into a sequence of local, global, and local operations better suited for the GPU’s memory and computational hierarchies.  We show that reducing the cost of global operations, even by significantly increasing the cost of local operations, is critical for achieving the best performance. We base our model on a hierarchical divide and conquer, where at the highest level each subproblem is small enough to be easily solved locally in parallel, and at the lowest level we have only a small number of operations to be performed globally.  We locally reorder input elements before global operations, trading more work (the reordering) for better memory performance (greater coalescing) for an overall improvement in performance.  We promote the warp-level privatization of local resources as opposed to the more traditional thread-level privatization. This decision can contribute to an efficient implementation of our local computations by using warp-synchronous schemes to avoid branch divergence, reduce shared memory usage, leverage warp-wide instructions, and minimize intra-warp communication.  We design a novel voting scheme using only binary ballots. We use this scheme to efficiently implement our warp-wide local computations (e.g., histogram computations).  We use these contributions to implement a high-performance multisplit targeted to modern GPUs. We then use our multisplit as an effective building block to achieve the following:  -   We build an alternate radix sort competitive with CUB (the current     fastest GPU sort library). Our implementation is particularly     effective with key-value sorts     (Section  ).  -   We demonstrate a significant performance improvement in the     delta-stepping formulation of the SSSP algorithm     (Section  ).  -   We build an alternate device-wide histogram procedure competitive     with CUB. Our implementation is particularly suitable for a small     number of bins (Section  ).  [^1]: This paper is an extended version of initial results published at     PPoPP 2016 . The source code is available at     &lt;https://github.com/owensgroup/GpuMultisplit&gt;.">
</outline>
  </body>
</opml>