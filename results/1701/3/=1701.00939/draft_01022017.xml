<?xml version="1.0" encoding="UTF-8"?>
<opml version="2.0">
  <head>
    <title></title>
    <abstract> </abstract>
  </head>
  <body>
<outline text="Introduction" _note="Deep neural networks are highly sensitive to small well-designed perturbations of the inputs, known as adversarial perturbations, which lead to misclassifications of these perturbed inputs. Consider images as an example of the data. A raw image, which is correctly classified by the neural network, can be modified in a small (and often imperceptible to our vision) way, so that the resulting deformed image is classified as a different class . A related problem is that DNN classify certain images as belonging to some class, although they are unrecognizable to humans as exemplars of any class . These rubbish or fooling images correspond to patches of the image space that have a small value of the objective function used in training and are located far away from any of the training data. These observations has challenged the integrity of DNN classification and has led to an opinion that their predictions are untrustworthy (see discussion of this issue in numerous blogs), though similar problems are shared by many other machine learning techniques, such as logistic regression, support vector machines, K-nearest neighbors and others .  Further investigation have shown that both adversarial images and rubbish images can be transferred between many different models having distinct architectures, different hyperparameters, and even trained on different training sets . Moreover, some of the adversarial and rubbish images can be transferred between a diverse set of models in machine learning . This opens up a possibility for a potential adversarial attack, when a hacker can train his own model and create a set of images that are misclassified by it, and then deploy this set of images against another victim model, which will also misclassify them. Realistic attacks were studied in . Often they do not require any internal knowledge of the victim model.  In addition to being a security issue, transferability suggests that various computational models learn very similar representations of the data. It also suggests that in order to address these problems, one might have to design training algorithms that learn a very different representation of the data compared to the existing methods. An ideal solution to these issues should be an algorithm that assigns small values of the objective function only to those areas of the image space that are recognizable by humans as images of the corresponding class. It should also require a substantial and recognizable by humans deformation of the initial correctly classified image towards a different target class before the label changes. In spite of a substantial amount of work on these problems , no algorithm have been identified so far which satisfies these requirements and at the same time is competitive to state of the art algorithms in terms of classification accuracy.  In a recent paper it was proposed that Dense Associative Memory (DAM) models with higher order interactions in the energy function learn representations of the data, which strongly depend on the power of the interaction vertex. The network extracts features form the data for small values of this power, but as the power of the interaction vertex is increased there is a gradual shift to a prototype-based representation, the two extreme regimes of pattern recognition known in cognitive psychology. Remarkably, there is a broad range of powers of the energy function, for which the representation of the data is already in the prototype regime, but the accuracy of classification is still competitive to the best available algorithms (based on DNN with ReLUs). This suggests that the DAM models might behave very differently compared to the standard methods used in deep learning with respect to adversarial deformations.  In the present paper we report three main results. First, using a gradient decent in the pixel space, a set of “rubbish” images is constructed that correspond to the minima of the objective function used in training. This is done on the MNIST dataset of handwritten digits using different values of the power of the interaction vertex, which is denoted by . For small values of the power these images indeed look like speckled rubbish noise, and do not have any semantic content for human vision, a result consistent with . However, as the power of the interaction vertex is increased the images gradually become less speckled and more semantically meaningful. In the limit of very large , these images are no longer rubbish at all. They represent plausible images of handwritten digits that could have possibly been produced by a human. Second, starting from clean images from the dataset a set of adversarial images is constructed in such a way that each image is placed exactly on the decision boundary between two label classes. For small powers these images look very similar to the initial clean image with a little bit of speckled noise added, but are misclassified by the neural network, a result consistent with . However, as the power of the interaction vertex is increased these adversarial images become less and less similar to the initial clean image. In the limit of very large powers these adversarial images look either like a morphed image of two digits (the initial clean image and another digit from the class that the deformation targets), or the initial digit superimposed on a “ghost” image from the target class. Either way the interpretation of the artificial patterns generated by the neural net on the decision boundary requires the presence of another digit from the target class in addition to the initial seed from the dataset, and cannot be explained by simply adding noise to the initial clean image. Third, adversarial and rubbish images generated by models with small can be transferred to and fool another model with small (but possibly different) value . However, they fail to transfer to models with large . Thus rubbish and adversarial images generated by models with small cannot fool models with large . In contrast, the “rubbish” images generated by models with large can be transferred to models with small , but this is not a problem since those “rubbish” images are actually not rubbish at all and look like credible handwritten digits. These results suggest that the DAMs with a large power of the interaction vertex in the energy function better mimic the psychology of human visual perception than DAMs with a small power. The latter are equivalent to DNNs with ReLUs .">
</outline>
<outline text="Data representation in  networks" _note="In order to illustrate these results in the simplest possible setting we trained a family of DAM networks for several values of the power of the energy function on the MNIST pixel permutation invariant task.  The model is defined by a set of weights and and the feedforward update rule where summation over repeated index is assumed. The functions are rectified polynomials  As we explained in , the argument of the update rule ( ) is the difference of two energies, corresponding to the initial and the final states of the neurons. For this reason the functions are called the energy functions in the rest of the paper, and the integer is called the power of the interaction vertex. The weights are learned using a backpropagation algorithm (see Appendix A of ) starting from random initial seeds by minimizing the objective function where is the target output ( for the wrong classes and for the correct class). Hyperparameter regulates the shape of the objective function.  In we investigated this model for varying powers of the energy functions and discovered that the network learns feature-based representations of the data for small and prototype-based representations for large . This is clear from the Fig. . For small each feature detector describes a feature which is useful for recognizing several different digit classes.  As the power is increased, most of the feature detectors specialize and become responsible for recognizing one possible prototype, which is a constructed representation and is not simply a copy of one image from the training set. Importantly, throughout the range the classification accuracy remains approximately the same, . This is comparable to the best result , achieved by the standard DNNs trained with backpropagation alone .">
</outline>
<outline text="Rubbish examples in  networks" _note="Once the training is complete, the neural network can be used to inspect the minima of the objective function. In order to do that one can define a set of objective functions each penalizing the deviations from the corresponding class with the target output A random image generated from a gaussian distribution can then be deformed into 10 images (sufficiently close to the initial one in the pixel space) corresponding to the 10 label classes by following the (negative) gradient of the objective functions according to the iterative rule where is a unit vector, which points in the direction of the gradient of the objective function , and is the size of the update step. This dynamics flows towards the minimum of the objective function, thus for a good learning algorithm we expect to see a recognizable image of the corresponding digit once it reaches the fixed point. The actual results are shown in Fig. .  For small the dynamics converges to local minima which are not recognizable as digits by humans. These are the rubbish minima of . The training algorithm has learned them while trying to minimize the classification error on the training set. In contrast, for large the dynamics flows towards a minimum of the objective function that corresponds to a recognizable image of the corresponding digit. Thus this simple experiment demonstrates that for DAM with large the minima of the objective function have semantic meaning in the image space, while for models with small these minima are semantically meaningless. All this is achieved by training the network in a purely supervised way on the pixel permutation invariant task.">
</outline>
<outline text="Adversarial deformations" _note="A clean image, which is classified by a neural network as belonging to a certain class, can be modified by a small perturbation so that the deformed image is classified as a different class. What makes this statement a problem is that the perturbation, which is sufficient for changing the label, is small and typically is a speckled pattern that lacks semantic meaning. The most common technique for generating adversarial images is the sign of the gradient method of . For our purposes it is necessary to generate images that are placed exactly on the decision boundary between the classes and not just in its vicinity. For this reason we use a slightly different method.  For each input image the vector of labels can be calculated by using ( ). The elements of this vector can be sorted from largest to smallest value. The largest value of the output corresponds to the top classification choice of the network, the second largest output corresponds to the second choice, etc.  The initial image can then be iteratively deformed along the (negative) gradient of the objective function corresponding to the second choice of the label made by the network. During this iterative process the first choice objective function will increase, and the second choice objective function will decrease (see top panel in Fig. ). The image itself should gradually change from the initial clean image to an image that the network thinks should correspond to the second choice label. At some iteration the two objective functions become equal. This is the mathematical definition of the decision boundary. From the point of view of the neural network the image that corresponds to this point on the deformation trajectory is exactly in the middle between the two classes. The question is whether a human observer would agree with this interpretation, in other words whether or not this image looks ambiguous to humans.  This method of generating the adversarial images has an advantage compared to since it guarantees that the image at the crossing point of the two objective functions is placed exactly on the decision boundary, and not just close to it. Also, in contrast to , it does not require a careful choice of the iteration step in ( ), provided that it is sufficiently small. The procedure will simply need more steps in order to find the correct image corresponding to the crossing point, if the step is too small. The drawback of this method compared to is that it is slower.  In Fig.  one can find a set of triplets of images generated by models with different values of power . In each triplet, the first image is the initial clean image from the dataset, the second image is the one that the neural network has generated on the decision boundary, and the third one is the image that the network has generated when the iterative procedure ( ) has converged to the second choice label of the initial clean image. Two observations can be made from this figure. First, as the power of the interaction vertex is increased, the third image in each triplet becomes more semantically meaningful. This is in accord with the results of Fig. . Second, as the power of the interaction vertex is increased, the adversarial image (the middle image) on the decision boundary becomes more meaningful as well. For or , the middle image looks almost the same as the initial clean image with a little bit of added speckled noise. The noise does not share much similarity with the second choice label. In contrast, for , the deformation does not look like noise at all. The middle image looks as if the two digits (the initial seed and a digit from the target class) were morphed together in one image, or as if the second digit was added as a “ghost” image to the initial digit. These results suggest that models with large powers not only learn semantically meaningful set of minima of the objective function, but also learn semantically meaningful deformations between the classes. This is achieved by training the network on the pixel permutation invariant classification task, not by training it as a generative model. In principle, the target class does not have to necessarily coincide with the second choice of the initial classification decision. This convention is used for convenience. To the best of our knowledge, the results do not change qualitatively if instead of the second choice label, the initial image is deformed towards any other label but the top one.">
</outline>
<outline text="Transferability between the models with various" _note="The most intriguing finding about rubbish and adversarial images is that they can be transferred between a diverse set of machine learning models. In order to test this phenomenon within the DAM framework we designed two experiments: one concerns the transfer of adversarial images and the other one concerns the transfer of rubbish images.  In the first experiment the MNIST test set was used to generate adversarial examples by the four models with , resulting in four datasets each having 10000 images. For each clean image from the test set the procedure described in section   was used to generate an artificial image placed one iteration step behind the decision boundary defined in Fig. . The resulting image is thus classified as the second choice of the network on the original image. These four adversarial datasets were used for the cross-classification by the same four models. The error rates are shown in Fig. . The diagonal elements of this table are close to , which is guaranteed by the design of the datasets[^1]. The most important aspect of this table is that the adversarial images generated by models with and do not transfer to the model with . These images share semantic similarity with the clean image that was used as an initial seed for crafting them, but do not generally have semantic features of the target class of the deformation. The model with can detect this similarity with the initial image and can still correctly classify of these cases. In contrast the adversarial images crafted using the model with can be transferred to models with . However, as we argued in the previous section this is expected since these images share semantic similarities with both the initial seed and the target class of the deformation. Thus, any machine learning algorithm or a human subject should misclassify a substantial fraction of them.  In the second experiment an artificial dataset was created, so that data correspond to the minima of the 10 objective functions . For each sample a random noise image was generated from a gaussian distribution, which was then iteratively changed in the direction of the negative gradient according to ( ) until convergence. The dataset has 100 images of each label class, 1000 images in total. This procedure was repeated for each value of . The resulting four datasets were used for cross-classification by the same four models. The results are shown in Fig. , but to discuss them we need to first introduce the notion of confidence.  [^1]: The reason why the error rate is around 99% instead of 100% is     because the classification error on the clean dataset is about 1.5%,     and in about 1% of the cases the second choice of the network is the     correct answer. Thus in these rare cases the adversarial deformation     used to generate the dataset actually turns the incorrect answer     into the correct one.">
  <outline text="Confidence" _note="The output of the network ( ) is a collection of numbers . For the following discussion it is convenient to define a measure of confidence that the neural network has in making a classification decision. If the actual outputs are approximately equal to the target output, the confidence is high. In contrast, if the outputs are all approximately zero, with one being slightly larger than the rest , the confidence is low. To quantify this effect, it is useful to pass the outputs through a softmax function to define a probability of each label given the input where the parameter is a number that regulates the steepness of the soft-max function. For each input image the confidence of the network is defined as the probability of the most probable label. Thus if the network is confident that the presented image belongs to a certain class , while if the network is unsure about what is shown in the image the distribution of labels is flat, and (in case of 10 classes for MNIST). The parameter should be chosen in such a way that the average confidence of the network on a dataset matches its error on the test set. If the outputs of the network were exactly equal to the target outputs for all inputs, then the average confidence on a dataset would be equal to (since there is one correct choice and nine incorrect ones) The error rate of all our models is of the order of , which defines the right hand side of the above equation. This results in . Since the outputs of the networks are in general slightly different from the target outputs, the actual values of are different for the four models that are discussed. They are: , , , . However, all the conclusions presented below remain true, for any values of as long as they stay within the range .  The images from the MNIST test set can be binned into groups having certain confidence and the classification error can be calculated on each bin. The results are shown in Fig. . The probability of error decreases as the confidence increases, a result suggesting that the confidence measure is a meaningful quantity. Now we return to the discussion of the second experiment pertaining to the analysis of “rubbish” images.  In Fig.  the mean confidence of the 16 cross-classification pairs is reported together with 10 sample images from each dataset, one for each class, for the four data sets. The bottom row corresponds to the data generated by the model with . The samples look like rubbish images, in accord with Fig. . These rubbish images are recognized by the network with at the average confidence , which is the result of the particular choice of the value . These rubbish images can be transferred to the model with , which is also very confident that they correspond to the correct labels. However, if one tries to transfer these rubbish images to the models with or , these higher order models immediately detect that these images do not correspond to any class and produce an average confidence . This accurately mimics the behavior of a human subject who would immediately detect that these rubbish images are semantically meaningless, thus should not belong to any class. The same conclusion applies to the dataset constructed by the model with . In contrast, the dataset constructed by the model with is composed of nice images of digits, which are recognizable by humans. These images can be transferred to any model with without loss in confidence. These results suggest that models with large better mimic human visual cognition, compared to models with small , both at generation and at testing.  From the security perspective these results suggest that the models with large can be used to detect and stop a potential hacker attack, which is devised by exploiting conventional machine learning techniques (for example DNNs with ReLUs). At the same time, if the adversary tries to use models with large for mounting the attack and deploys it against models with any (small or large) , this does not possess a security issue, because the interpretations that any model gives to these images are consistent with human visual perception.  From this perspective the models with large can be valuable even if their classification accuracy is slightly lower than that of models with small . They can be used in pair with another model that has a good classification accuracy, but that is vulnerable to adversarial/rubbish examples. The first model (with large ) detects a potential adversarial attack. In cases when it is confident, and its label disagrees with the other more accurate model, one can use the prediction of the more accurate model for the final classification decision. However if the large model is unconfident of a particular input, that input should be labeled as junk and not processed by the more accurate/vulnerable model.">
  </outline>
</outline>
<outline text="Discussion and conclusions" _note="Although modern machine learning techniques outperform humans on many classification tasks, there is a serious concern that they do not understand the structure of the training data. A clear demonstration of this lack of understanding was presented in , who showed two examples of nonsensical predictions of DNNs that contradict to human visual perception: adversarial images and rubbish images. In the present paper we propose that DAM with higher order interactions in the energy function produce more sensible interpretations (consistent with human vision) of adversarial and rubbish images. We argue that these models better mimic human visual perception than DNNs with ReLUs.  A possible explanation of adversarial examples, pertaining to neural networks being too linear, was given in . Our explanation follows the same line of thought with some differences. One result of is that DAMs with large powers of the interaction vertex in the energy function are dual to feed-forward neural nets with highly non-linear activation functions - the rectified polynomials of higher degrees. From the perspective of this duality, one might expect that by simply replacing the ReLUs in DNNs by the higher rectified polynomials one might solve the problem of adversarial and rubbish images for a sufficiently large power of the activation function. We tried that and discovered that although DNNs with higher rectified polynomials alone perform better than DNNs with ReLUs from the adversarial perspective, they are worse than DAMs with the update rule ( ). These observations need further comprehensive investigation. Thus, simply changing ReLUs to higher rectified polynomials is not enough to get rid of adversarial problems, and other aspects of the training algorithm presented in section   are important.  For thinking about neurobiology the energy functions with higher order interactions considered in and in this paper should be thought of as effective theories that arise after excluding the auxiliary variables from the microscopic description. The microscopic realization in terms of biological neurons will be discussed elsewhere.  There are two straightforward ideas for possible extensions of this work. First, it would be interesting to complement the proposed training procedure with adversarial training. In other words to train the network using the algorithm of section   but on a combination of clean images and adversarial images, along the lines of . We expect that this should further increase the robustness to adversarial examples and increase the classification accuracy on the clean images. Second, it would be interesting to investigate the proposed methods in the convolutional setting. Naively, one expects that the adversarial problems are more severe in the fully connected networks than in the convolutional networks. For this reason we used the fully connected networks for our experiments. We expect that the training algorithm of section   can be combined with convolutional layers to better describe images.  Although more work is required to fully resolve the problem of adversarial and rubbish images, we believe that the present paper has identified a promising computational regime of the neural networks that significantly mitigates the vulnerability of DNNs to adversarial and rubbish images and that remains little investigated.  [99]{} Szegedy, C., Zaremba, W., Sutskever, I., Bruna, J., Erhan, D., Goodfellow, I. and Fergus, R., 2013. Intriguing properties of neural networks. arXiv preprint arXiv:1312.6199.  Nguyen, A., Yosinski, J. and Clune, J., 2015, June. Deep neural networks are easily fooled: High confidence predictions for unrecognizable images. In 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (pp. 427-436). IEEE.  Papernot, N., McDaniel, P. and Goodfellow, I., 2016. Transferability in Machine Learning: from Phenomena to Black-Box Attacks using Adversarial Samples. arXiv preprint arXiv:1605.07277.  Papernot, N., McDaniel, P., Goodfellow, I., Jha, S., Berkay Celik, Z. and Swami, A., 2016. Practical Black-Box Attacks against Deep Learning Systems using Adversarial Examples. arXiv preprint arXiv:1602.02697.  Kurakin, A., Goodfellow, I. and Bengio, S., 2016. Adversarial examples in the physical world. arXiv preprint arXiv:1607.02533.  Miyato, T., Maeda, S.I., Koyama, M., Nakae, K. and Ishii, S., 2015. Distributional smoothing with virtual adversarial training. stat, 1050, p.25.  Gu, S. and Rigazio, L., 2014. Towards deep neural network architectures robust to adversarial examples. arXiv preprint arXiv:1412.5068.  Huang, R., Xu, B., Schuurmans, D. and Szepesvári, C., 2015. Learning with a strong adversary. CoRR, abs/1511.03034.  Nokland, A., 2015. Improving back-propagation by adding an adversarial gradient. arXiv preprint arXiv:1510.04189.  Wang, Q., Guo, W., Zhang, K., Xing, X., Giles, C.L. and Liu, X., 2016. Random Feature Nullification for Adversary Resistant Deep Architecture. arXiv preprint arXiv:1610.01239.  Krotov, D. and Hopfield, J.J., 2016. Dense Associative Memory for Pattern Recognition. Advances in Neural Information Processing Systems 2016, in press, arXiv preprint arXiv:1606.01164.  Simard, P.Y., Steinkraus, D. and Platt, J.C., 2003, August. Best practices for convolutional neural networks applied to visual document analysis. In ICDAR (Vol. 3, pp. 958-962).  Goodfellow, I.J., Shlens, J. and Szegedy, C., 2014. Explaining and harnessing adversarial examples. arXiv preprint arXiv:1412.6572.">
</outline>
  </body>
</opml>