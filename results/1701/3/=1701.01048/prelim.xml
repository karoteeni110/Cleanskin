<?xml version="1.0" encoding="UTF-8"?>
<opml version="2.0">
  <head>
    <title></title>
    <abstract> </abstract>
  </head>
  <body>
<outline text="Preliminaries" _note="This section provides a formal description of the representation language, the relational planning problem, and the description of the running example in this context.">
  <outline text="Relational Expressions and their Calculus of Operations" _note="The computation of SDP algorithms is facilitated by a representation that enables compact specification of functions over world states. Several such representations have been devised and used. In this chapter we chose to abstract away some of those details and focus on a simple language of relational expressions. This is closest to the GFODD representation of , but it resembles the case notation of .  We assume familiarity with basic concepts and notation in first order logic (FOL) . Relational expressions are similar to expressions in FOL. They are defined relative to a relational signature, with a finite set of predicates each with an associated arity (number of arguments), a countable set of variables , and a set of constants . We do not allow function symbols other than constants (that is, functions with arity ). A term is a variable (often denoted in uppercase) or constant (often denoted in lowercase) and an atom is either an equality between two terms or a predicate with an appropriate list of terms as arguments. Intuitively, a term refers to an object in the world of interest and an atom is a property which is either true or false.  We illustrate relational expressions informally by some examples. In FOL we can consider open formulas that have unbound variables. For example, the atom is such a formula and its truth value depends on the assignment of and to objects in the world. To simplify the discussion, we assume for this example that arguments are typed (or sorted) and ranges over “objects” and over “colors”. We can then quantify over these variables to get a sentence which will be evaluated to a truth value in any concrete possible world. For example, we can write expressing the statement that there is a color associated with all objects. Generalized expressions allow for more general open formulas that evaluate to numerical values. For example, is similar to the previous logical expression but returns non-binary values.  Quantifiers from logic are replaced with aggregation operators that combine numerical values and provide a generalization of the logical constructs. In particular, when the open formula is restricted to values 0 and 1, the operators and simulate existential and universal quantification. Thus, is equivalent to the logical sentence given above. But we can allow for other types of aggregations. For example, evaluates to the largest number of objects associated with one color, and the expression evaluates to the number of objects that have no color association. In this manner, a generalized expression represents a function from possible worlds to numerical values and, as illustrated, can capture interesting properties of the state.  Relational expressions are also related to work in statistical relational learning . For example, if the open expression given above captures probability of ground facts for the predicate and the ground facts are mutually independent then captures the joint probability for all facts for . Of course, the open formulas in logic can include more than one atom and similarly expressions can be more involved.  In the following we will drop the cumbersome if-then-else notation and instead will assume a simpler notation with a set of mutually exclusive conditions which we refer to as [CASES]{}. In particular, an expression includes a set of mutually exclusive open formulas in FOL (without any quantifiers or aggregators) denoted associated with corresponding numerical values . The list of cases refers to a finite set of variables . A generalized expression is given by a list of aggregation operators and their variables and the list of cases so that the last expression is canonically represented as .  The semantics of expressions is defined inductively exactly as in first order logic and we skip the formal definition. As usual, an expression is evaluated in an INTERPRETATION also known as a possible world. In our context, an interpretation specifies (1) a finite set of domain elements also known as objects, (2) a mapping of constants to domain elements, and (3) the truth values of all the predicates over tuples of domain elements of appropriate size to match the arity of the predicate. Now, given an expression , an interpretation , and a substitution of variables in to objects in , one can identify the case which is true for this substitution. Exactly one such case exists since the cases are mutually exclusive and exhaustive. Therefore, the value associated with is . These values are then aggregated using the aggregation operators. For example, consider again the expression and an interpretation with objects and where is associated with colors black and white and is associated with color black. In this case we have exactly 4 substitutions evaluating to 0.3, 0.3, 0.5, 0.3. Then the final value is .  Any binary operation over real values can be generalized to open and closed expressions in a natural way. If and are two closed expressions, represents the function which maps each interpretation to . This provides a definition but not an implementation of binary operations over expressions. For implementation, the work in showed that if the binary operation is [SAFE]{}, i.e., it distributes with respect to all aggregation operators, then there is a simple algorithm (the Apply procedure) implementing the binary operation over expressions. For example, is safe w.r.t.  aggregation, and it is easy to see that = , and the open formula portion of the result can be calculated directly from the open expressions and . Note that we need to standardize the expressions apart, as in the renaming of to for such operations. When and are open relational expressions the result can be computed through a cross product of the cases. For example, When the binary operation is not safe then this procedure fails, but in some cases, operation-specific algorithms can be used for such combinations.[^1]  As will become clear later, to implement SDP we need the binary operations , , and the aggregation includes in addition to aggregation in the reward function. Since , , are safe with respect to aggregation one can provide a complete solution when the reward is restricted to have aggregation. When this is not the case, for example when using sum aggregation in the reward function, one requires a special algorithm for the combination. Further details are provided in .  Relational expressions are closest to the GFODD representation of . Every case in a relational expression corresponds to a path or set of paths in the GFODD, all of which reach the same leaf in the graphical representation of the GFODD. GFODDs are potentially more compact than relational expressions since paths share common subexpressions, which can lead to an exponential reduction in size. On the other hand, GFODDs require special algorithms for their manipulation. Relational expressions are also similar to the case notation of . However, in contrast with that representation, cases are not allowed to include any quantifiers and instead quantifiers and general aggregators are globally applied over the cases, as in standard quantified normal form in logic.  [^1]: For example, a product of expressions that include only product     aggregations, which is not safe, can be obtained by scaling the     result with a number that depends on domain size, and is euqal to     when the domain has objects.">
  </outline>
  <outline text="Relational MDPs" _note="In this section we define MDPs, starting with the basic case with enumerated state and action spaces, and then providing the relational representation.  We assume familiarity with basic notions of Markov Decision Processes (MDPs) . Briefly, a MDP is a tuple given by a set of states , set of actions , transition probability , immediate reward function and discount factor . The solution of a MDP is a policy that maximizes the expected discounted total reward obtained by following that policy starting from any state. The Value Iteration algorithm (VI) informally introduced in Eq  , calculates the optimal value function by iteratively performing Bellman backups, , defined for each state as, Unlike Eq  , which was goal-oriented and had only a single reward at the terminal horizon, here we allow the reward R(S) to accumulate at all time steps as typically allowed in MDPs. If we iterate the update until convergence, we get the optimal infinite horizon value function typically denoted by and optimal stationary policy . For finite horizon problems, which is the topic of this chapter, we simply stop the iterations at a specific . In general, the optimal policy for the finite horizon case is not stationary, that is, we might make different choice in the same state depending on how close we are to the horizon.  RMDPs are simply MDPs where the states and actions are described in a function-free first order logical language. A state corresponds to an interpretation over the corresponding logical signature, and actions are transitions between such interpretations. A relational planning problem is specified by providing the logical signature, the start state, the transitions as controlled by actions, and the reward function. As mentioned above, one of the advantages of relational SDP algorithms is that they are intended to produce an abstracted form of the value function and policy that does not require specifying the start state or even the number of objects in the interpretation at planning time. This yields policies that generalize across domain sizes. We therefore need to explain how one can use logical notation to represent the transition model and reward function in a manner that does not depend on domain size.  Two types of transition models have been considered in the literature:  [**Endogenous Branching Transitions:**]{} In the basic form, state transitions have limited stochastic branching due to a finite number of action outcomes. The agent has a set of action types each parametrized with a tuple of objects to yield an action template and a concrete ground action (e.g. template and concrete action ). Each agent action has a finite number of action variants (e.g., action success vs. action failure), and when the user performs in state one of the variants is chosen randomly using the state-dependent action choice distribution . To simplify the presentation we follow and require that are given by open expressions, i.e., they have no aggregations and cannot introduce new variables. For example, in &lt;span style=&quot;font-variant:small-caps;&quot;&gt;BoxWorld&lt;/span&gt;, the agent action has success outcome and failure outcome with action outcome distribution as follows: where, to simplify the notation, the last case is shortened as to denote that it complements previous cases. This provides the distribution over deterministic outcomes of actions.  The deterministic action dynamics are specified by providing an open expression, capturing successor state axioms , for each variant and predicate template . Following we call these expressions TVDs, standing for truth value diagrams. The corresponding TVD, , is an open expression that specifies the truth value of [IN THE NEXT STATE]{} (following standard practice we use prime to denote that the predicate refers to the next state) when has been executed [ IN THE CURRENT STATE]{}. The arguments and are intentionally different logical variables as this allows us to specify the truth value of all instances of simultaneously. Similar to the choice probabilities we follow and assume that TVDs have no aggregations and cannot introduce new variables. This implies that the regression and product terms in the SDP algorithm of the next section do not change the aggregation function, thereby enabling analysis of the algorithm. Continuing our &lt;span style=&quot;font-variant:small-caps;&quot;&gt;BoxWorld&lt;/span&gt; example, we define the TVD for for and as follows: Note that each TVD has exactly two cases, one leading to the outcome 1 and the other leading to the outcome 0. Our algorithm below will use these cases individually. Here we remark that since the next state (primed) only depends on the previous state (unprimed), we are effectively logically encoding the Markov assumption of MDPs.  [**Exogenous Branching Transitions:**]{} The more complex form combines the endogenous model with an exogenous stochastic process that affects ground atoms independently. As a simple example in our &lt;span style=&quot;font-variant:small-caps;&quot;&gt;BoxWorld&lt;/span&gt; domain, we might imagine that with some small probability, each box in a city () may independently randomly disappear (falsify ) owing to issues with theft or improper routing — such an outcome is independent of the agent’s own action. Another more complicated example could be an inventory control problem where customer arrival at shops (and corresponding consumption of goods) follows an independent stochastic model. Such exogenous transitions can be formalized in a number of ways ; we do not aim to commit to a particular representation in this chapter, but rather to mention its possibility and the computational consequences of such general representations.  Having completed our discussion of RMDP transitions, we now proceed to define the reward , which can be any function of the state and action, specified by a relational expression. Our running example with existentially quantified reward is given by but we will also consider additive reward as in">
  </outline>
</outline>
  </body>
</opml>