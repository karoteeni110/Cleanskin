<?xml version="1.0" encoding="UTF-8"?>
<opml version="2.0">
  <head>
    <title></title>
    <abstract> </abstract>
  </head>
  <body>
<outline text="Introduction" _note="In this chapter we illustrate that stochastic planning can be viewed as a specific form of probabilistic inference and show that recent symbolic dynamic programming (SDP) algorithms for the planning problem can be seen to perform “generalized lifted inference”, thus making a strong connection to other chapters in this book. As we discuss below, although the SDP formulation is more expressive in principle, work on SDP to date has largely focused on algorithmic aspects of reasoning in open domain models with rich quantified logical structure whereas lifted inference has largely focused on aspects of efficient arithmetic computations over finite domain (quantifier free) template-based models. The contributions in these areas are therefore largely along different dimensions. However, the intrinsic relationships between these problems suggest a strong opportunity for cross-fertilization where the true scope of generalized lifted inference can be achieved. This chapter intends to highlight these relationships and lay out a paradigm for generalized lifted inference that subsumes both fields and offers interesting opportunities for future research.  To make the discussion concrete, let us introduce a running example for stochastic planning and the kind of generalized solutions that can be achieved. For illustrative purposes, we borrow a planning domain from Boutilier et. al.  that we refer to as &lt;span style=&quot;font-variant:small-caps;&quot;&gt;BoxWorld&lt;/span&gt;. In this domain, outlined in Figure  , there are several cities such as , etc., trucks , etc., and boxes , etc. The agent can load a box onto a truck or unload it and can drive a truck from one city to another. When any box has been delivered to a specific city, , the agent receives a positive reward. The agent’s planning task is to find a policy for action selection that maximizes this reward over some planning horizon.  Our objective in lifted stochastic planning is to obtain an abstract policy, for example, like the one shown in Figure  . In order to get some box to , the agent should drive a truck to the city where the box is located, load the box on the truck, drive the truck to , and finally unload the box in . This is essentially encoded in the symbolic value function shown in Fig.  , which was computed by discounting rewards time steps into the future by .  Similar to this example, for some problems we can obtain a solution which is described abstractly and is independent of the specific problem instance or even its size — for our example problem the description of the solution does not depend on the number of cities, trucks or boxes, or on knowledge of the particular location of any specific truck. Accordingly, one might hope that computing such a solution can be done without knowledge of these quantities and in time complexity independent of them. This is the computational advantage of symbolic stochastic planning which we associate with lifted inference in this chapter.  The next two subsections expand on the connection between planning and inference, identify opportunities for lifted inference, and use these observations to define a new setup which we call [GENERALIZED LIFTED INFERENCE]{} which abstracts some of the work in both areas and provides new challenges for future work.">
  <outline text="Stochastic Planning and Inference" _note="Planning is the task of choosing what actions to take to achieve some goals or maximize long-term reward. When the dynamics of the world are deterministic, that is, each action has exactly one known outcome, then the problem can be solved through logical inference. That is, inference rules can be used to deduce the outcome of individual actions given the current state, and by combining inference steps one can prove that the goal is achieved. In this manner a proof of goal achievement embeds a plan. This correspondence was at the heart of McCarthy’s seminal paper that introduced the topic of AI and viewed planning as symbolic logical inference. Since this formulation uses first-order logic, or the closely related situation calculus, lifted logical inference can be used to solve deterministic planning problems.  When the dynamics of the world are non-deterministic, this relationship is more complex. In particular, in this chapter we focus on the stochastic planning problem where an action can have multiple possible known outcomes that occur with known state-dependent probabilities. Inference in this case must reason about probabilities over an exponential number of state trajectories for some planning horizon. While lifted inference and planning may seem to be entirely different problems, analogies have been made between the two fields in several forms . To make the connections concrete, consider a finite domain and the finite horizon goal-oriented version of the &lt;span style=&quot;font-variant:small-caps;&quot;&gt;BoxWorld&lt;/span&gt; planning problem of Figure  , e.g., two boxes, three trucks, and four cities and a planning horizon of 10 steps where the goal is to get some box in . In this case, the value of a state, , corresponds to the probability of achieving the goal, and goal achievement can be modeled as a specific form of inference in a Bayesian network or influence diagram.  We start by considering the [CONFORMANT PLANNING PROBLEM]{} where the intended solution is an explicit sequence of actions. In this case, the sequence of actions is determined in advance and action choice at the th step does not depend on the actual state at the th step. For this formulation, one can build a Dynamic Bayesian Network (DBN) model where each time slice represents the state at that time and action nodes affect the state at the next time step, as in Figure  (a). The edges in this diagram capture , where is the current state, is the current action and is the next state, and each of is represented by multiple nodes to show that they are given by a collection of predicates and their values. Note that, since the world dynamics are known, the conditional probabilities for all nodes in the graph are known. As a result, the goal-based planning problem where a goal must hold at the last step, can be modeled using standard inference. The value of conformant planning is given by marginal MAP (where we seek a MAP value for some variables but take expectation over the remaining variables) : The optimal conformant plan is extracted using argmax instead of max in the equation.  The standard MDP formulation with a reward per time time step which is accumulated can be handled similarly, by normalizing the cumulative reward and adding a binary node whose probability of being true is a function of the normalized cumulative reward. Several alternative formulations of planning as inference have been proposed by defining an auxiliary distribution over finite trajectories which captures utility weighted probability distribution over the trajectories . While the details vary, the common theme among these approaches is that the planning objective is equivalent to calculating the partition function (or “probability of evidence&quot;) in the resulting distribution. This achieves the same effect as adding a node that depends on the cumulative reward. To simplify the discussion, we continue the presentation with the simple goal based formulation.  The same problem can be viewed from a Bayesian perspective, treating actions as random variables with an uninformative prior. In this case we can use to observe that and therefore one can alternatively maximize the probability conditioned on .  However, linear plans, as the ones produced by the conformant setting, are not optimal for probabilistic planning. In particular, if we are to optimize goal achievement then we must allow the actions to depend on the state they are taken in. That is, the action in the second step is taken with knowledge of the probabilistic outcome of the first action, which is not known in advance. We can achieve this by duplicating action nodes, with a copy for each possible value of the state variables, as illustrated in Figure  (b). This represents a separate policy associated with each horizon depth which is required because finite horizon problems have non-stationary optimal policies. In this case, state transitions depend on the identity of the current state and the action variables associated with that state. The corresponding inference problem can be written as follows: However, the number of random variables in this formulation is prohibitively large since we need the number of original action variables to be multiplied by the size of the state space.  Alternatively, the same desideratum, optimizing actions with knowledge of the previous state, can be achieved without duplicating variables in the equivalent formulation In fact, this formulation is exactly the same as the finite horizon application of the value iteration (VI) algorithm for (goal-based) Markov Decision Processes (MDP) which is the standard formulation for sequential decision making in stochastic environments. The standard formulation abstracts this by setting The optimal policy (at ) can be obtained as before by recording the argmax values. In terms of probabilistic inference, the problem is no longer a marginal MAP problem because summation and maximization steps are constrained in their interleaved order. But it can be seen as a natural extension of such inference questions with several alternating blocks of expectation and maximization. We are not aware of an explicit study of such problems outside the planning context.">
  </outline>
  <outline text="Stochastic Planning and Generalized Lifted Inference" _note="Given that planning can be seen as an inference problem, one can try to apply ideas of lifted inference to planning. Taking the motivating example from Figure  , let us specialize the reward to a ground atomic goal equivalent to for constants and . Then we can query to compute where is the concrete value of the current state.  Given that Figure   implies a complex relational specification of the transition probabilities, lifted inference techniques are especially well-placed to attempt to exploit the structure of this query to perform inference in aggregate and thus avoid redundant computations. However, we emphasize that, even if lifted inference is used, this is a standard query in the graphical model where evidence constrains the value of some nodes, and the solution is a single number representing the corresponding probability (together with a MAP assignment to variables).  However, Eq   suggests an explicit additional structure for the planning problem. In particular, the intermediate expressions include the values (the probability of reaching the goal in steps) for all possible concrete values of . Similarly, the final result includes the values for all possible start states. In addition, as in our running example we can consider more abstract rewards. This suggests a first generalization of the standard setup in lifted inference. Instead of asking about a ground goal and expecting a single number as a response, we can abstract the setup in two ways: first, we can ask about more general conditions such as and second we can expect to get a structured result that specifies the corresponding probability for every concrete state in the world. If we had two box instances and truck instances , the answer for , i.e., the value for the goal based formulation with horizon one, might take the form:  The significance of this is that the question can have a more general form and that the answer solves many problems simultaneously, providing the response as a case analysis depending on some properties of the state. We refer to this reasoning as [INFERENCE WITH GENERALIZED QUERIES AND ANSWERS]{}. In this context, the goal of lifted inference will be to calculate a structured form of the reply directly.  A second extension arises from the setup of generalized queries. The standard form for lifted inference is to completely specify the domain in advance. This means providing the number of objects and their properties, and that the response to the query is calculated only for this specific domain instantiation. However, inspecting the solution in the previous paragraph it is obvious that we can at least hope to do better. The same solution can be described more compactly as  Arriving at such a solution requires us to allow open domain reasoning over all potential objects (rather than grounding them, which is impossible in open domains), and to extend ideas of lifted inference to exploit quantifiers and their structure. Following through with this idea, we can arrive at a DOMAIN-SIZE INDEPENDENT VALUE FUNCTION AND POLICY as the one shown in Figure  . In this context, the goal of lifted inference will be to calculate an abstracted form of the reply directly. We call this problem [INFERENCE WITH GENERALIZED MODELS]{}. As we describe in this chapter, SDP algorithms are able to perform this type of inference.  The previous example had enough structure and a special query that allowed the solution to be specified without any knowledge of the concrete problem instance. This property is not always possible. For example, consider a setting where we get one unit of reward for every box in : . In addition, consider the case where, after the agent takes their action, any box which is not on a truck disappears with probability . In this case, we can still potentially calculate an abstract solution, but it requires access to more complex properties of the state, and in some cases the domain size (number of objects) in the state. For our example this gives:  Here we have introduced a new notation for count expressions where, for example, counts the number of boxes in Paris in the current state. To see this result note that any existing box in Paris disappears 20% of the time and that a box on a truck is successfully unloaded 90% of the time but remains and does not disappear only in 80% of possible futures leading to the value 7.2. This is reminiscent of the type of expressions that arise in existing lifted inference problems and solutions. Typical solutions to such problems involve parameterized expressions over the domain (e.g., counting, summation, etc.), and critically do not always require closed-domain reasoning (e.g., A PRIORI knowledge of the number of boxes). They are therefore suitable for inference with generalized models. Some work on SDP has approached lifted inference for problems with this level of complexity, including exogenous activities (the disappearing boxes) and additive rewards. But, as we describe in more detail, the solutions for these cases are much less well understood and developed.  To recap, our example illustrates that stochastic planning potentially enables abstract solutions that might be amenable to lifted computations. SDP solutions for planning problems have focused on the computational advantages arising from these expressive generalizations. At the same time, the focus in SDP algorithms has largely been on problems where the solution is completely independent of domain size and does not require numerical properties of the state. These algorithms have thus skirted some of the computational issues that are typically tackled in lifted inference. It is the combination of these aspects, as illustrated in the last example, which we call [**generalized lifted inference**]{}. As the discussion suggests, generalized lifted inference is still very much an open problem. In addition to providing a survey of existing SDP algorithms, the goal of this chapter is to highlight the opportunities and challenges in this exciting area of research.">
  </outline>
</outline>
  </body>
</opml>