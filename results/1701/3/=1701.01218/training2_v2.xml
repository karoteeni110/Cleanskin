<?xml version="1.0" encoding="UTF-8"?>
<opml version="2.0">
  <head>
    <title></title>
    <abstract> </abstract>
  </head>
  <body>
<outline text="Equal-size Clustering" _note="There are recent algorithms that deal with size constraints in clustering. For example,    formulated the problem of clustering with size constraints as a linear programming problem. However such algorithms are not computationally efficient, especially for large scale datasets (e.g., Human3.6M). We study two efficient ways to generate equal size clusters; see Table   (last row) for their ODC-complexity.  **Recursive Projection Clustering (RPC) .** In this method,  the training data is partitioned to perform GPR prediction.  Initially all data points are put in one cluster. Then, two points are chosen randomly  and orthogonal projection of all the data onto the line connecting them is computed. Depending on the median value of the projections, The data is then split into two equal size subsets. The same process is then applied to each cluster to generate clusters after repetitions. The iterations stops once . As indicated, the number of clusters in this method has to be a power of two and it might produce long thin clusters.  **Equal-Size K-means (EKmeans).** We propose a variant of k-means clustering  to generate equal-size clusters.  The goal is to obtain disjoint partitioning of into clusters , similar to the k-means objective, minimizing the within-cluster sum of squared Euclidean distances, ,  where is the mean of cluster , and is the squared distance. Optimizing this objective is NP-hard and k-means iterates between the assignment and update steps as a heuristic to achieve a solution; denotes number of iterations of kmeans. We add equal-size constraints .  In order to achieve this partitioning, we propose an efficient heuristic algorithm, denoted by [ASSIGN AND BALANCE (AB) EKMEANS]{}. It mainly modifies the assignment step of the k-means to bound the size of the resulting clusters. We first assign the points to their closest see center as typically done in the assignment step of k-means. We use to denote the cluster assignment of a given point . This results in three types of clusters: balanced, overfull, and underfull clusters. Then some of the points in the overfull clusters are redistributed to the underfull clusters by assigning each of these points to the closest underfull cluster. This is achieved by initializing a pool of overfull points defined as ; see Figure  .  Let us denote the set of underfull clusters by . We compute the distances . Iteratively, we pick the minimum distance pair and assign to cluster instead of cluster . The point is then removed from the overfull pool. Once an underfull cluster becomes full it is removed from the underfull pool, once an overfull cluster is balanced, the remaining points of that cluster are removed from overfull pool. The intuition behind this algorithms is that, the cost associated with the initial optimal assignment (given the computed means) is minimally increased by each swap since we pick the minimum distance pair in each iteration. Hence the cost is kept as low as possible while balancing the clusters. We denote the the name of this Algoirthm as Assign and Balance EKmeans. Algorithm   illustrates the overall assignment step and Fig.   visualizes the balancing step.     1- Assign the points initially to its closest center; this will put the clusters into 3 groups (1) balanced clusters (2) overflowed clusters (3) under-flowed clusters.\ 2- Create a matrix , where is the distance between the point to the cluster center; rows are restricted points belongs only to the overflowed clusters; columns are restricted to underflowed cluster centers\ 3- Get the coordinate that maps the smallest distance in .\ 4- Remove the row from matrix and mark it as assigned to the cluster\ 5- If the size of the cluster achieves the ideal size (i.e.   ), then remove the column from matrix .\ 6- Go to step 3 if there is still unassigned points">
</outline>
<outline text="Overlapping Domain Cover(ODC) Model" _note="Having generated the disjoint equal size clusters, we generate the ODC subdomains based on the overlapping ratio , such that points are selected from the neighboring clusters. Let’s assume that we select only the closest clusters to each cluster, is closer to than if . It is important to note that must be greater than in order to supply the required points; this is since number of points in each cluster is . Hence, the minimum value for is clusters. Hence, we parametrize as . We study the effect of in the experimental results section. Having computed from and , each subdomain is then created by merging the points in the cluster with points, retrieved from the neighboring clusters. Specifically, the points are selected by sorting the points in each of clusters by the distance to . The number of points retrieved for each of the neighboring clusters is inversely proportional to the distance of its center to . If a subset of the clusters are requested to retrieve more than its capacity (i.e., ), the set of the extra points are requested from the remaining clusters giving priority to the closer clusters (i.e., starting from the nearest neighboring cluster to the cluster on which the subdomain is created).  As and increases, all points that belong to the clusters tends to be merged with . In our framework, we used FLANN   for fast NN-retrieval; see pseudo-code of ODC generation in Appendix C.  After the ODC is generated, we compute the the sample normal distribution using the points that belong to each subdomain. Then, a local kernel machine is trained for each of the overlapping subdomains. We denote the point set normal distribution of the subdomains as ;   is precomputed during the training for later use during the prediction.  Finally, we factor out all the computations that does not depend on the test point (for GPR, TGP, IWTGP) and store them with each sub domain as its local kernel machine. We denote the training model for subdomain as , which is computed as follows for GPR and TGP respectively.  **[GPR]{}.**  Firstly, we precompute , where is an kernel matrix, defined on the input points in . Each dimension in the output could have its own hyper-parameters, which results in a different kernel matrix for each dimension . We also precompute for each dimension. Hence .  **[TGP.]{}**  The local kernel machine for each subdomain in TGP case is defined as , where and are kernel matrices defined on the input points and the corresponding output points respectively, which belong to domain .  **IWTGP.** It is not obvious how to factor out computations that does not depend on the test data in the case of IWTGP, since the computational extensive factor(i.e., , ) does depend on the test set since is computed on test time. To help factor out the computation, we used linear algebra to show that where is a diagonal matrix, is the identity matrix, and is the trace of matrix .  Kenneth Miller  proposed the following Lemma on Matrix Inverse. Applying Miller’s lemma, where and , leads directly to Eq.  .  Mapping to [^1], to either of or , we can compute . Having computed on test time, , could be computed in quadratic time given following equation   , since the inverse and the power of has linear computational complexity since it is diagonal.  [^1]:  is a diagonal matrix">
</outline>
  </body>
</opml>