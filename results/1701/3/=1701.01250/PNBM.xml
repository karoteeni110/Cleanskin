<?xml version="1.0" encoding="UTF-8"?>
<opml version="2.0">
  <head>
    <title>A Probabilistic View of Neighborhood-based Recommendation Methods</title>
    <abstract>Probabilistic graphic model is an elegant framework to compactly present
complex real-world observations by modeling uncertainty and logical flow
(conditionally independent factors). In this paper, we present a
probabilistic framework of neighborhood-based recommendation methods
(PNBM) in which SIMILARITY is regarded as an unobserved factor. Thus,
PNBM leads the estimation of user preference to maximizing a posterior
over SIMILARITY. We further introduce a novel multi-layer SIMILARITY
descriptor which models and learns the joint influence of various
features under PNBM, and name the new framework MPNBM. Empirical results
on real-world datasets show that MPNBM allows very accurate estimation
of user preferences. </abstract>
  </head>
  <body>
<outline text="Introduction" _note="Collaborative filtering, which leverages user history information to predict users’ unknown preference, is one of the most successful techniques to build recommender systems . Matrix factorization (MF) and neighborhood-based methods (NBMs) are two representative approaches. MF family attracts more attention due to its ability of modeling influence of various features (e.g. ), thus to improve accuracy. However, it is difficult to provide explainable recommendation results. NBM family, shown as Fig.  , is very popular mainly due to the fact that it naturally explains recommendation results (e.g. An item which is similar with what you bought before). SIMILARITY serves as the basis of weighting neighbors which is crucial to the accuracy of NBM recommender systems. However, existing SIMILARITY computation scheme is incapable of capturing influence from different features which hampers further polishing SIMILARITY to improve accuracy. In this paper, we first present a basic probabilistic framework of NBM family (PNBM) which leads learning SIMILARITY to a regression problem. Then we introduce a novel multi-layer SIMILARITY descriptor which models and learns the joint influence of different features under PNBM.">
  <outline text="Related Work" _note="Commonly, NBMs are divided into two classes . One is user-based approach which predicts the rating that a user will assign to an unrated item by referring to other users who are similar to this user. The other is item-based approach which estimates a user’s preference to an unrated item based on other items that are similar to this unrated item. The two approaches follow the same principle. With respect to NBM, researches have mainly focused on SIMILARITY computation schemes and neighbor selection strategies . SIMILARITY also serves as the basis for neighbor selection, thus we concentrate upon SIMILARITY in this paper. Generally, there are two main approaches to compute SIMILARITY. One introduces different kinds of correlation coefficients as SIMILARITY , such as Pearson and Cosine correlations. However, some researchers argue that such kind of methods isolate the relations between two items without leveraging global information. The other approach learns SIMILARITY via regression models. introduce a way to learn similarity by minimizing mean squared error between observed ratings and their corresponding estimation. factors similarity matrix via low-rank approximations. presents a weighted error function which gives more weight to the users who rated items most similar to the estimated item. simplify standard neighborhood-based models to a simple linear regression problem for top- recommendation based on binary databases.  A number of probabilistic models have been introduced to collaborative filtering. However, only a very small portion of them are NBM related. presents a generic Bayesian personalized ranking framework which is optimized for the area under ROC (AUC) metric. introduces a probabilistic memory-based collaborative filtering method in which they use a mixture Gaussian model built on the basis of a set of user profiles and use the posterior distribution of user ratings for prediction. builds a Markov network using Pearson-correlation NBM as basis. People also place probabilistic prior assumptions to observations to model uncertainty. Such as, places Dirichlet distribution on the absolute value of rating difference. uses different probabilistic density functions to sample neighbors from a predefined similarity matrix (vectors).  Unfortunately, these models are incapable of representing complex features, and none of them discusses NBM family itself from a Bayesian perspective.">
  </outline>
  <outline text="Contribution" _note="In this paper, we present a probabilistic (Bayesian) framework of NBM family, and our contribution is twofold.  First, we present a general graphical model of NBM family (PNBM) which leads the estimation of user preference to maximizing a posterior over SIMILARITY.  Then, we introduce a novel multi-layer SIMILARITY descriptor which is capable of modeling and learning the joint influence of various features (e.g. rating, text, genre) under PNBM, and we name the new framework as MPNBM.  MPNBM is evaluated on three popular real-world datasets via root-mean-square-error (RMSE) metric. Empirical results show that MPNBM consistently outperform state-of-art approaches on the datasets we choose.">
  </outline>
</outline>
<outline text="Preliminary" _note="Suppose we have a data set organized in form of matrix , it contains users and items. is item SIMILARITY matrix, denotes similarity between item and , we further assume . presents indicator matrix, and . if user rated item , otherwise . denotes all the observed ratings.  So far, many neighborhood-based methods have been proposed, as surveyed in . For simplicity, we take a variant of mean-centering NBM as instance throughout the paper. The predication formula is defined in Equation ( ). where is rating score that user gave to item . denotes the estimation of user ’s preference on item . is the mean value of all the ratings given to item . presents a set containing all the items.  For further simplicity, Equation ( ) is transformed into a vectorization form: where and . denotes SIMILARITY vector corresponding to item and represents rating vector of user . The multiplication denotes the inner product of the two vectors. is an indicator vector of user . The symbol means a vector that does not contain an item which is being predicted. For example, with regard to Equation ( ), denotes a vector does not contain . Moreover, we assume the testing set is excluded from the training set, when we predict in the testing set, in the training set is always zero.">
</outline>
<outline text="Probabilistic Framework of NBM" _note="In this section, we present a probabilistic graphical model of NBMs (PNBM), shown in Fig.  . It is a Bayesian network which describes the following factorization:  In our context, placing prior distribution on hyper-parameters does not significantly improve accuracy while dramatically increasing time complexity. For the sake of simplicity and reduction of time complexity, we simply let be constant, and is also constant. So we can simplify Equation ( ) to  We introduce a general Gaussian distribution (but not limited to, other distribution can be also applied to. It depends on real-world context.) to density function which naturally leads to a sum-of-square-error.  More specifically, assume that an item’s similarity vector is independent from those of other items, and is sampled from a mean-zero spherical Gaussian distribution. Thus we have where denotes the Gaussian distribution for with mean and precision . We also assume that ratings are independent with each other. Combine with Equation ( ), we have following">
</outline>
<outline text="Multi-layer SIMILARITY Descriptor" _note="In Section  , we introduced a general probabilistic (Bayesian) NBM framework which is simple and straightforward. However, like other similarity computation methods, PNBM falls short in feature representation which extremely limits the accuracy improvement. In this section, we present a multi-layer SIMILARITY descriptor (MLSD, shown in Fig.  ) which is able to model and learn the joint influence of various features (e.g. ratings, text, genre, time). MLSD is mathematically defined as  where denotes the SIMILARITY basis at -th layer. is ’s constraint matrix which presents an influence of observed features. For example, it can present the similarity of text description ( or time closeness ) between any two-item. Note that different influences may be generated from the same feature. denotes the importance of the feature-influence modeled at layer . is the number of layers (influence) employed to model SIMILARITY. In this paper, we don’t require , since we always have a normalization factor in the prediction equation, i.e. Equation ( ). denotes point-wise product operation (Hadamard product) on matrices and , e.g.  MLSD can be smoothly integrated into PNBM, shown in Fig.  , named MPNBM. The Bayesian network is mathematically describe as where . Follow the same assumptions in Section  , we define the prior of layer ( ) as And we have the conditional distribution over observed ratings defined as  ">
</outline>
<outline text="Maximum a Posterior" _note="PNBM is a specific case of MPNBM, which only has one layer with constraint-matrix set to 1. In this section, we take MPNBM as example to present how we optimize SIMILARITY via maximizing a posterior.  The log of Bayesian network defined in Equation ( ) is given by  In fact, it defines the posterior distribution over SIMILARITY. Combine it with Equation ( ) and Equation ( ), we have  Maximizing the above Bayesian network distribution with hyper-parameters being kept fixed is equivalent to minimizing an error function defined as  where is the regularization parameter for layer .  A simple linear Gaussian model sometimes makes prediction value fall out of the range of valid rating values. In order to force the predication values to fall into valid range, we pass the linear-Gaussian model through hyperbolic tangent function which makes prediction values be in range of  . We map the centralized ratings to range   with Equation ( ). where and are the max and min value of ratings, respectively. Since the ratings are centralized by their corresponding mean value, we always have and . As a result, the range of valid rating value align with the estimation produced by our models.  The conditional distribution of observed ratings becomes We adopt stochastic gradient descent (SGD) as learning algorithm to train latent factors, shown in Algorithm  .  rating matrix , error function. similarity basis , influence constraint-matrix , influence importance factor , learning rate , regular parameter . Note that . Training: For each layer (), point-wisely update the similarity basis : where . prediction using Equation ( ) with top- the most similar neighbors.">
</outline>
<outline text="Experiments">
  <outline text="Description of Data Sets" _note="In the experiments, we evaluate our models and state-of-art methods over three different data sets, summarized in Table  .     ML-20M, ML-10M are data sets provided by MovieLens . Netflix is a subset sampled from Netflix Prize data set such that each user rated 50-1500 movies, and each movie is rated by 5-1800 users. Yahoo-R4 is a subset of the movie-rating data set provided by the Yahoo Labs Webscope Team such that each movie are at least rated by 5 users.  We use ML-10M, Netflix and Yahoo-R4 to compare the models’ accuracy.  We also compare each model’s accuracy on data sets with different densities. In order to avoid the inherent differences of data sets from different originations, we extract 10 subsets from one single data set (ML-20M) based on the users’ rating number. Precisely, each subset has similar amount of users and items, the number of users and items are in range   and   respectively. The density of each data set is from 0.28% to 2.67%.  ">
  </outline>
  <outline text="Models for Comparison" _note="In this paper, the following models are compared:  [**RegSim:** ]{} Regression on SIMILARITY , a representative work which learns SIMILARITY via a regression method.  [**SLIM:** ]{} Sparse linear methods , a regression model for top- recommendation on binary data set. We extend it to a arbitrary real-value prediction model by placing a jointly Gaussian-Laplace prior on similarity vectors. It has a very similar error function as SLIM, where is a non-zero mean value of the Gaussian prior.  [**PCC:** ]{} NBM using Pearson correlation as SIMILARITY .  [**COS:** ]{} NBM using Cosine correlation as SIMILARITY .  [**PMF:** ]{} Probabilistic matrix factorization .  [**MPNBM:** ]{} In this paper, we exploit influence from ratings as an instance to demonstrate MLSD’s ability of modeling various features, thus to improve accuracy. We use a 3-layer SIMILARITY descriptor in which layer-1 treats latent influence equally with constraint-matrix set to 1; Layer-2 adopts Pearson correlation as constraint-matrix that stresses the influence from those items which either have significant positive correlation or strong negative correlation with the item under predication; Layer-3 employs Jaccard index to form a constraint-matrix that amplifies the influence from those items which have similar rating history, alleviates the divergence from infrequent-rated items and frequent-rated items. [**Time Complexity.**]{} The computational time is mainly taken by updating SIMILARITY. At a single epoch, approximately similarities are updated, where is the size of training set, is the average rating number per users and is the number of influence layers. Intuitively, a single epoch takes about 4, 260, 340 seconds on Yahoo-R4, Netflix, ML-10M respectively.  [**Tanh-MPNBM:** ]{} The model which we pass MPNBM through hyperbolic tangent function ( detailed in Section   ).  EXPERIMENT SETTING. All models are implemented with Matlab, and run on a single core of a Intel (R) Xeon(R) 3.50 GHz machine with 16 GB memory.">
  </outline>
  <outline text="Parameters Setting" _note="For RegSim, MPNBM, Tanh-MPNBM and SLIM, we empirically choose parameters for each model after a grid search in which . The finally chosen parameters are summarized in Table   ( indicates a model does not have such a parameter).     For PMF, we choose latent feature dimension and the momentum of mini-batch SGD . Regularized parameters ( for user latent factors and latent item factors respectively) and learning rate are set to  For Yahoo-R4, and .  For Netflix, and .  For ML-10M, and .  For the first two ML-20M subsets shown in the left panel of Fig.  , and .  For the other eight ML-20M subsets shown in the left panel of Fig.  , and .  For PCC and COS, we use top-200 the most similar neighbors for prediction.">
  </outline>
  <outline text="Comparison Results" _note="During the test, we randomly divide each data set into training set (85%), validation set (5%) and testing set (10%). We adopt RMSE for evaluation. We repeat the experiments 5 times.   ">
    <outline text="Accuracy" _note="The comparison is performed over :  Accuracy on different data sets.  Accuracy on different density.  Fig.   presents the detail of training on different data sets. Table   records the final accuracy comparison (the training process is conducted by validation set). RegSim is selected as baseline model, the accuracy improvement of each model is displayed in the INC % column. Fig.   shows the accuracy comparison on data sets with different density. MPNBM and Tanh-MPNBM consistently outperform outperform state-of-art models, especially on those extremely sparse data sets ( which have serious COLD START problem). For simplicity, we don’t draw SLIM on the graph, since SLIM has similar accuracy with RegSim.  We are also interested in that how the layer importance-factor affects the MPNBM (Tanh-MPNBM). We use two strategies to select parameters for each layer, 1) we consistently choose for all the three data sets, named TMPN-V1; 2) letting , we assign higher value to the which corresponding has lower RMSE, named TMPN-V2. The comparison is shown in Fig.  , and 1) the accuracy is not significantly influenced, MPNBM (Tanh-MPNBM) is able to balance the influence automatically. 2) Assigning proper weight to according to the RMSE of results in faster convergence.">
    </outline>
    <outline text="Stability" _note="Model based approach may easily over fit when increasing the number of parameters under training. The system can be beneficent from the stability of algorithms which is defined by  converge speed: the first epoch where a model converges to the local best solution, denoted as .  ability of models to maintain their best status: the number of epochs that a model stays in the local best solution, denoted as .  Table   shows the values of and of each model over different data sets.     In the comparison of stability, we treat RMSE values , if . Note that with regard to a model which does not over fit after 200 epochs (value of prefixed with ), if the lowest RMSE value appears at least 10 epochs, it is seen as the local best solution. \* in Table   means a model does not converge after 200 epochs on a data set. e.g, MPNBM does not converge on Netflix data set, also shown in Fig.  . The experimental results show that MPNBM and Tanh-MPNBM stay in the local best solution for many ( 40) epochs which is better than PMF. With regard to converge speed, as shown in Table  , it seems that sometimes MPNBM and Tanh-MPNBM do not converge as fast as PMF. In fact, they achieve a considerable accuracy at a much earlier epoch.">
    </outline>
  </outline>
</outline>
<outline text="Conclusion" _note="In this paper, we have presented a probabilistic framework of NBM family, and introduced a multi-layer SIMILARITY descriptor under PNBM which is capable of modeling and learning the joint influence of various features. Our experiments show that MPNBM and Tanh-MPNBM allow accurate and stable estimation of user preferences.  Privacy is a serious problem to recommender systems. Nowadays, applying differential privacy to recommendation algorithms attracts great attention. A common approach is adding noise to data set. Recently, people find that sampling from a posterior distribution achieves some extent of differential privacy “for free&quot; , and this idea has been already successfully applied to probabilistic matrix factorization . Following the same idea, our models can also provide such kind of “free privacy&quot;. We leave a detailed investigation as future work.">
</outline>
<outline text="Acknowledgments" _note="Both authors are supported by a CORE (junior track) grant from the National Research Fund, Luxembourg. Qiang Tang is also partially supported by an internal project from University of Luxembourg.">
</outline>
  </body>
</opml>