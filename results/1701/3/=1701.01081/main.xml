<?xml version="1.0" encoding="UTF-8"?>
<opml version="2.0">
  <head>
    <title>SalGAN: visual saliency prediction with adversarial networks</title>
    <abstract>Recent approaches for saliency prediction are generally trained with a
loss function based on a single saliency metric. This could lead to low
performance when evaluating with other saliency metrics. In this paper,
we propose a novel data-driven metric based saliency prediction method,
named SalGAN (Saliency GAN), trained with adversarial loss function.
SalGAN consists of two networks: one predicts saliency maps from raw
pixels of an input image; the other one takes the output of the first
one to discriminate whether a saliency map is a predicted one or ground
truth. By trying to make the predicted saliency map indistinguishable
with the ground truth, SalGAN is expected to generate saliency maps that
resembles the ground truth. Our experiments show that the adversarial
training allows our model to obtain state-of-the-art performances across
various saliency metrics. </abstract>
  </head>
  <body>
<outline text="Introduction" _note="Visual saliency describes the spatial locations in an image that attract the human attention. It is understood as a result of a bottom-up process where a human observer explores the image for a few seconds with no particular task in mind. Therefore, saliency prediction is indispensable for various machine vision tasks such as object recognition .  Visual saliency data are traditionally collected by eye-trackers , and more recently with mouse clicks or webcams . The salient points of the image are aggregated and convolved with a Gaussian kernel to obtain a saliency map. As a result, a gray-scale image or a heat map is generated to represent the probability of each corresponding pixel in the image to capture the human attention.  A lot of research effort has been made in designing an optimal loss function for saliency prediction. State-of-the-art methods adopt saliency based metrics while others use distance in saliency map space. How to choose or design a best training loss is still an open problem. In addition, different saliency metrics diverge at defining the meaning of saliency maps, and there exist inconsistency with the model comparison. For instance, it has been pointed that the optimal metric for model optimization may depend on the final application . To this end, instead of designing a tailored loss function, we introduce adversarial training for visual saliency prediction inspired by generative adversarial networks (GANs). We dub the proposed method as SalGAN. We focus on exploring the benefits of using such an adversarial loss to make the output saliency map not able to be distinguished from the real saliency maps. In GANs, training is driven by two competing agents: first, the GENERATOR synthesizing samples that match with the training data; second, the DISCRIMINATOR distinguishing between a real sample drawn directly from the training data and a fake one synthesized by the generator. In our case, this data distribution corresponds to pairs of real images and their corresponding visual saliency maps.  Specifically, SalGAN estimates the saliency map of an input image using a deep convolutional neural network (DCNN). As shown in the Figure   this network is initially trained with a binary cross entropy (BCE) loss over down-sampled versions of the saliency maps. The model is then refined with a discriminator network trained to solve a binary classification task between the saliency maps generated by SalGAN and the real ones used as ground truth. Our experiments show how adversarial training allows reaching state-of-the-art performance across different metrics when combined with a BCE content loss in a single-tower and single-task model. To summarize, we investigate the introduction of the adversarial loss to the visual saliency learning. By introducing adversarial loss to the BCE saliency prediction model, we achieve the state-of-the-art performance in MIT300 and SALICON dataset for almost all the evaluation metrics The remaining of the text is organized as follows. Section   reviews the state-of-the-art models for visual saliency prediction, discussing the loss functions they are based upon, their relations with the different metrics as well as their complexity in terms of architecture and training. Section   presents SalGAN, our deep convolutional neural network based on a convolutional encoder-decoder architecture, as well as the discriminator network used during its adversarial training. Section   describes the training process of SalGAN and the loss functions used. Section   includes the experiments and results of the presented techniques. Finally, Section   closes the paper by drawing the main conclusions.  Our results can be reproduced with the source code and trained models available at &lt;https://imatge-upc.github.io/saliency-salgan-2017/&gt;.">
</outline>
<outline text="Related work" _note="Saliency prediction has received interest by the research community for many years. Thus seminal works proposed to predict saliency maps considering low-level features at multiple scales and combining them to form a saliency map. , also starting from low-level feature maps, introduced a graph-based saliency model that defines Markov chains over various image maps, and treat the equilibrium distribution over map locations as activation and saliency values. presented a bottom-up, top-down model of saliency based not only on low but mid and high-level image features. combined low-level features saliency maps of previous best bottom-up models with top-down cognitive visual features and learned a direct mapping from those features to eye fixations.  As in many other fields in computer vision, a number of deep learning solutions have very recently been proposed that significantly improve the performance. For example, the Ensemble of Deep Networks (eDN) represented an early architecture that automatically learns the representations for saliency prediction, blending feature maps from different layers. In two convolutional neural networks trained ebd-to-end for saliency prediction are compared, a lighter one designed and trained from scratch, and a second and deeper one pre-trained for image classification. DCNN have shown better results even when pre-trained with datasets build for other purposes. DeepGaze provided a deeper network using the well-know AlexNet , with pre-trained weights on Imagenet and with a readout network on top whose inputs consisted of some layer outputs of AlexNet. The output of the network is blurred, center biased and converted to a probability distribution using a softmax. Huang et al. , in the so call SALICON net, obtained better results by using VGG rather than AlexNet or GoogleNet . In their proposal they considered two networks with fine and coarse inputs, whose feature maps outputs are concatenated.  Li et al. proposed a multi resolution convolutional neural network that is trained from image regions centered on fixation and non-fixation locations over multiple resolutions. Diverse top-down visual features can be learned in higher layers and bottom-up visual saliency can also be inferred by combining information over multiple resolutions. These ideas are further developed in they recent work called DSCLRCN , where the proposed model learns saliency related local features on each image location in parallel and then learns to simultaneously incorporate global context and scene context to infer saliency. They incorporate a model to effectively learn long-term spatial interactions and scene contextual modulation to infer image saliency. Deep Gaze II sets the state of the art in the MIT300 dataset by combining features trained for image recognition with four layer of 1x1 convolutions. Both DSCLRCN and Deep Gaze II obtain excellent results in the benchmarks when combined with a center bias, which is not considered in SalGAN as the results are purely the results at inference time. MLNET proposes an architecture that combines features extracted at different levels of a DCNN. They introduce a loss function inspired by three objectives: to measure similarity with the ground truth, to keep invariance of predictive maps to their maximum and to give importance to pixels with high ground truth fixation probability. In fact choosing an appropriate loss function has become an issue that can lead to improved results. Thus, another interesting contribution of   lies on minimizing loss functions based on metrics that are differentiable, such as NSS, CC, SIM and KL divergence to train the network (see and for the definition of these metrics. A thorough comparison of metrics can be found in ). In KL divergence gave the best results. also tested loss functions based on probability distances, such as [X2]{} divergence, total variation distance, KL divergence and Bhattacharyya distance by considering saliency map models as generalized Bernoulli distributions. The Bhattacharyya distance was found to give the best results. In our work we present a network architecture that takes a different approach. By incorporating the high-level adversarial loss into the conventional saliency prediction training approach, the proposed method achieves the state-of-the-art performance in both MIT300 and SALICON datasets by a clear margin.">
</outline>
<outline text="Architecture" _note="The training of SalGAN is the result of two competing convolutional neural networks: a generator of saliency maps, which is SalGAN itself, and a discriminator network, which aims at distinguishing between the real saliency maps and those generated by SalGAN. This section provides details on the structure of both modules, the considered loss functions, and the initialization before beginning adversarial training. Figure    shows the architecture of the system.">
  <outline text="Generator" _note="The generator network, SalGAN, adopts a convolutional encoder-decoder architecture, where the encoder part includes max pooling layers that decrease the size of the feature maps, while the decoder part uses upsampling layers followed by convolutional filters to construct an output that is the same resolution as the input.  The encoder part of the network is identical in architecture to VGG-16 , omitting the final pooling and fully connected layers. The network is initialized with the weights of a VGG-16 model trained on the ImageNet data set for object classification . Only the last two groups of convolutional layers in VGG-16 are modified during the training for saliency prediction, while the earlier layers remain fixed from the original VGG-16 model. We fix weights to save computational resources during training, even at the possible expense of some loss in performance.  The decoder architecture is structured in the same way as the encoder, but with the ordering of layers reversed, and with pooling layers being replaced by upsampling layers. Again, ReLU non-linearities are used in all convolution layers, and a final convolution layer with sigmoid non-linearity is added to produce the saliency map. The weights for the decoder are randomly initialized. The final output of the network is a saliency map in the same size to input image.  The implementation details of SalGAN are presented in Table  .">
  </outline>
  <outline text="Discriminator" _note="Table   gives the architecture and layer configuration for the discriminator. In short, the network is composed of six 3x3 kernel convolutions interspersed with three pooling layers (2), and followed by three fully connected layers. The convolution layers all use ReLU activations while the fully connected layers employ activations, with the exception of the final layer, which uses a sigmoid activation.">
  </outline>
</outline>
<outline text="Training" _note="The filter weights in SalGAN have been trained over a perceptual loss resulting from combining a content and adversarial loss. The content loss follows a classic approach in which the predicted saliency map is pixel-wise compared with the corresponding one from ground truth. The adversarial loss depends of the real/synthetic prediction of the discriminator over the generated saliency map.">
  <outline text="Content loss" _note="The content loss is computed in a per-pixel basis, where each value of the predicted saliency map is compared with its corresponding peer from the ground truth map. Given an image of dimensions , we represent the saliency map as vector of probabilities, where is the probability of pixel being fixated. A content loss function is defined between the predicted saliency map and its corresponding ground truth . The first considered content loss is mean squared error (MSE) or Euclidean loss, defined as: In our work, MSE is used as a baseline reference, as it has been adopted directly or with some variations in other state of the art solutions for visual saliency prediction .  Solutions based on MSE aim at maximizing the peak signal-to-noise ratio (PSNR). These works tend to filter high spatial frequencies in the output, favoring this way blurred contours. MSE corresponds to computing the Euclidean distance between the predicted saliency and the ground truth.  Ground truth saliency maps are normalized so that each value is in the range . Saliency values can therefore be interpreted as estimates of the probability that a particular pixel is attended by an observer. It is tempting to therefore induce a multinomial distribution on the predictions using a softmax on the final layer. Clearly, however, more than a single pixel may be attended, making it more appropriate to treat each predicted value as independent of the others. We therefore propose to apply an element-wise sigmoid to each output in the final layer so that the pixel-wise predictions can be thought of as probabilities for independent binary random variables. An appropriate loss in such a setting is the binary cross entropy, which is the average of the individual binary cross entropies (BCE) across all pixels:">
  </outline>
  <outline text="Adversarial loss" _note="Generative adversarial networks (GANs)  are commonly used to generate images with realistic statistical properties. The idea is to simultaneously fit two parametric functions. The first of these functions, known as the generator, is trained to transform samples from a simple distribution (e.g. Gaussian) into samples from a more complicated distribution (e.g. natural images). The second function, the discriminator, is trained to distinguish between samples from the true distribution and generated samples. Training proceeds alternating between training the discriminator using generated and real samples, and training the generator, by keeping the discriminator weights constant and backpropagating the error through the discriminator to update the generator weights.  The saliency prediction problem has some important differences from the above scenario. First, the objective is to fit a deterministic function that predict realistic saliency values from images, rather than realistic images from random noise. As such, in our case the input to the generator (saliency prediction network) is not random noise but an image. Second, the input image that a saliency map corresponds to is essential, due the fact the goal is not only to have the two saliency maps becoming indistinguishable but with the condition that they both correspond the same input image. We therefore include both the image and saliency map as inputs to the discriminator network. Finally, when using generative adversarial networks to generate realistic images, there is generally no ground truth to compare against. In our case, however, the corresponding ground truth saliency map is available. When updating the parameters of the generator function, we found that using a loss function that is a combination of the error from the discriminator and the cross entropy with respect to the ground truth improved the stability and convergence rate of the adversarial training. The final loss function for the saliency prediction network during adversarial training can be formulated as: where is the binary cross entropy loss, and 1 is the target category of real samples and 0 for the category of fake (predicted) sample. Here, instead of minimizing , we optimize which provides stronger gradient, similar to . is the probability of fooling the discriminator network, so that the loss associated to the saliency prediction network will grow more when chances of fooling the discriminator are lower. During the training of the discriminator, no content loss is available and the loss function is: At train time, we first bootstrap the saliency prediction network function by training for 15 epochs using only BCE, which is computed with respect to the down-sampled output and ground truth saliency. After this, we add the discriminator and begin adversarial training. The input to the discriminator network is an RGBS image of size containing both the source image channels and (predicted or ground truth) saliency.  We train the networks on the 15,000 images from the SALICON training set using a batch size of 32. During the adversarial training, we alternate the training of the saliency prediction network and discriminator network after each iteration (batch). We used L2 weight regularization (i.e. weight decay) when training both the generator and discriminator (). We used AdaGrad for optimization, with an initial learning rate of .">
  </outline>
</outline>
<outline text="Experiments" _note="The presented SalGAN model for visual saliency prediction was assessed and compared from different perspectives. First, the impact of using BCE and the downsampled saliency maps are assessed. Second, the gain of the adversarial loss is measured and discussed, both from a quantitative and a qualitative point of view. Finally, the performance of SalGAN is compared to published works to compare its performance with the current state-of-the-art. The experiments aimed at finding the best configuration for SalGAN were run using the TRAIN and VALIDATION partitions of the SALICON dataset . This is a large dataset built by collecting mouse clicks on a total of 20,000 images from the Microsoft Common Objects in Context (MS-CoCo) dataset . We have adopted this dataset for our experiments because it is the largest one available for visual saliency prediction. In addition to SALICON,we also present results on MIT300, the benchmark with the largest amount of submissions.">
  <outline text="Non-adversarial training" _note="The two content losses presented in content loss section, MSE and BCE, were compared to define a baseline upon which we later assess the impact of the adversarial training. The two first rows of Table   shows how a simple change from MSE to BCE brings a consistent improvement in all metrics. This improvement suggests that treating saliency prediction as multiple binary classification problem is more appropriate than treating it as a standard regression problem, in spite of the fact that the target values are not binary. Minimizing cross entropy is equivalent to minimizing the KL divergence between the predicted and target distributions, which is a reasonable objective if both predictions an targets are interpreted as probabilities. Based on the superior BCE-based loss compared with MSE, we also explored the impact of computing the content loss over downsampled versions of the saliency map. This technique reduces the required computational resources at both training and test times and, as shown in Table  , not only does it not decrease performance, but it can actually improve it. Given this results, we chose to train SalGAN on saliency maps downsampled by a factor , which in our architecture corresponds to saliency maps of .">
  </outline>
  <outline text="Adversarial gain" _note="The adversarial loss was introduced after estimating the value of the hyperparameter in Equation   by maximizing the most general metric, Information Gain (IG). As shown in Figure  , the search was performed on logarithmic scale, and we achieved the best performance for .  The Information Gains (IG) of SalGAN for different values of the hyper parameter are compared in Figure  . The search for finding an optimal hyper parameter is performed on logarithmic scale, and we achieved the best performance for .  The gain achieved by introducing the adversarial loss into the perceptual loss was assessed by using BCE as a content loss and feature maps of . The first row of results in Table   refers to a baseline defined by training SalGAN with the BCE content loss for 15 epochs only. Later, two options are considered: 1) training based on BCE only (2nd row), or 2) introducing the adversarial loss (3rd and 4th row). Figure   compares validation set accuracy metrics for training with combined GAN and BCE loss versus a BCE alone as the number of epochs increases. In the case of the AUC metrics (Judd and Borji), increasing the number of epochs does not lead to significant improvements when using BCE alone. The combined BCE/GAN loss however, continues to improve performance with further training. After 100 and 120 epochs, the combined GAN/BCE loss shows substantial improvements over BCE for five of six metrics.  The single metric for which Adversarial training fails to improve performance is normalized scanpath saliency (NSS). The reason for this may be that GAN training tends to produce a smoother and more spread out estimate of saliency, which better matches the statistical properties of real saliency maps, but may increase the false positive rate. As noted in , NSS is very sensitive to such false positives. The impact of increased false positives depends on the final application. In applications where the saliency map is used as a multiplicative attention model (e.g. in retrieval applications, where spatial features are importance weighted), false positives are often less important than false negatives, since while the former includes more distractors, the latter removes potentially useful features. Note also that NSS is differentiable, so could potentially be optimized directly when important for a particular application.">
  </outline>
  <outline text="Comparison with the state-of-the-art" _note="SalGAN is compared in Table   to several other algorithms from the state-of-the-art. The comparison is based on the evaluations run by the organizers of the SALICON and MIT300 benchmarks on a test dataset whose ground truth is not public. The two benchmarks offer complementary features: while SALICON is a much larger dataset with 5,000 test images, MIT300 has attracted the participation of many more researchers. In both cases, SalGAN was trained using 15,000 images contained in the training (10,000) and validation (5,000) partitions of the SALICON dataset. Notice that while both datasets aim at capturing visual saliency, the acquisition of data differed, as SALICON ground truth was generated based on crowdsourced mouse clicks, while the MIT300 was built with eye trackers on a limited and controlled group of users. Table   compares SalGAN with other contemporary works that have used SALICON and MIT300 datasets. SalGAN presents very competitive results in both datasets, as it improves or equals the performance of all other models in at least one metric.">
  </outline>
  <outline text="Qualitative results" _note="The impact of adversarial training has also been explored from a qualitative perspective by observing the resulting saliency maps.  Figure   shows one example from the MIT300 dataset, highlighted in as being particular challenges for existing saliency algorithms. The areas highlighted in yellow in the images on the left are regions that are typically missed by algorithms. In the this example, we see that SalGAN successfully detects the often missed hand of the magician and face of the boy as being salient.  Figure   illustrates the effect of adversarial training on the statistical properties of the generated saliency maps. Shown are two close up sections of a saliency map from cross entropy training (left) and adversarial training (right). Training on BCE alone produces saliency maps that while they may be locally consistent with the ground truth, are often less smooth and have complex level sets. Adversarial training on the other hand produces much smoother and simpler level sets.  Finally, Figure   shows some qualitative results comparing the results from training with BCE and BCE/Adversarial against the ground truth for images from the SALICON validation set.">
  </outline>
</outline>
<outline text="Conclusions" _note="To the best of our knowledge, this is the first work that proposes an adversarial-based approach to saliency prediction and has shown how adversarial training over a deep convolutional neural network can achieve state-of-the-art performance with a simple encoder-decoder architecture. A BCE-based content loss was shown to be effective for both initializing the saliency prediction network, and as a regularization term for stabilizing adversarial training. Our experiments showed that adversarial training improved all bar one saliency metric when compared to further training on cross entropy alone.  It is worth pointing out that although we use a VGG-16 based encoder-decoder model as the saliency prediction network in this paper, the proposed adversarial training approach is generic and could be applied to improve the performance of other saliency models.">
</outline>
<outline text="Acknowledgments" _note="The Image Processing Group at UPC is supported by the project TEC2016-75976-R, funded by the Spanish Ministerio de Economia y Competitividad and the European Regional Development Fund (ERDF). This material is based upon works supported by Science Foundation Ireland under Grant No 15/SIRG/3283. We gratefully acknowledge the support of NVIDIA Corporation for the donation of GPUs used in this work.">
</outline>
  </body>
</opml>