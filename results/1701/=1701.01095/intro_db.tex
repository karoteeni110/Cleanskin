%!TEX root = /Users/audrey/Dropbox/PhD/MOMAB/ArXiv/Latex/paper.tex

\section{Introduction}
\label{sec:intro}

Multi-objective optimization (MOO)~\cite{Coello2007} is a topic of great importance for real-world applications. Indeed, optimization problems are characterized by a number of conflicting, even contradictory, performance measures relevant to the task at hand. For example, when deciding on the healthcare treatment to follow for a given sick patient, a trade-off must be made between the efficiency of the treatment to heal the sickness, the side effects of the treatment, and the treatment cost. MOO is often tackled by combining the objective into a single measure (a.k.a.~scalarization). Such approaches are said to be \emph{a priori}, as the preferences over the objectives is defined before carrying out the optimization itself. The challenge lies in the determination of the appropriate scalarization function to use and its parameterization. Another way to conduct MOO consists in learning the optimal trade-offs (the so-called Pareto-optimal set). Once the optimization is completed, techniques from the field of multi-criteria decision-making are applied to help the user to select the final solution from the Pareto-optimal set. These \emph{a posteriori} techniques may require a huge number of evaluations to have a reliable estimation of the objective values over all potential solutions. Indeed, the Pareto-optimal set can be quite large, encompassing a majority, if not all, of the potential solutions. In this work, we tackle the MOO problem where the scalarization function \emph{exists} a priori, but might be unknown, in which case a user can act as a black box for articulating preferences. Integrating the user to the learning loop, she can provide feedback by selecting her preferred choice given a set of options -- the scalarization function lying in her head.

More specifically, we consider problems where outcomes are stochastic and costly to evaluate (e.g., involving a human in the loop). The challenge is therefore to identify the best solutions given random observations sampled from different (unknown) density distributions. We formulate this problem as multi-objective bandits, where we aim at finding the solution that maximizes the preference function while maximizing the performance of the solutions evaluated during the optimization. The Thompson sampling (TS)~\cite{Thompson1933} technique is a typical approach for bandits problems, where potential solutions are tried based on a Bayesian posterior over their expected outcome. Here we consider TS from multivariate normal (MVN) priors for multi-objective bandits.
% Let the \emph{right choice} denote the option that maximize the preference function -- the option that the user would select given that she had knowledge of the Pareto-optimal set. A learning algorithm for the multi-objective bandits setting aims at learning good-enough estimations of the available options to allow the user to make the right choices and its performance depends on the robustness of the preference function to the quality of estimations. We therefore need a measure for characterizing the quality of estimations required in order for the option maximizing the preference function to remain unchanged. For that purpose, we introduce the concept of preference radius providing the tolerance range over objective value estimations, such that the user preference would remain the same as if the Pareto-optimal set was known. We use this concept for providing a theoretical analysis of TS from MVN priors.
We introduce the concept of preference radius providing the tolerance range over objective value estimations, such that the \emph{best option} given the preference function remains unchanged. We use this concept for providing a theoretical analysis of TS from MVN priors.
%
Finally, we perform some empirical experiments to support the theoretical results and also highlight the importance of tackling multi-objective bandits problems as such instead of scalarizing those under the traditional bandit setting. 

% The original contributions of the paper consist in:
% \begin{itemize}
%     \item providing a general formulation of the MOO under the a priori multi-objective bandits setting;
%     \item proposing the preference radius to characterize the robustness of the preference function to the estimations quality;
%     \item proposing a theoretical analysis of the TS algorithm from MVN priors;
%     \item showing with empirical experiments that multi-objective bandits cannot simply be brought back to single-objective bandits.
% \end{itemize}
