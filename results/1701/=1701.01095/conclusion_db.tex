%!TEX root = /Users/audrey/Dropbox/PhD/MOMAB/ArXiv/Latex/paper.tex

\section{Conclusion}
\label{sec:conclusion}

In this work, we have addressed the online multi-objective optimization problem under the multi-objective bandits setting. Unlike previous formulations, we work in the a priori setting, where there exists a preference function to be maximized. However, acting in the the proposed setting would not require the preference function to be \emph{known}. Indeed, it would be sufficient for an expert user to pick her preferred estimate among a set of options with no requirement of providing an actual, real valued, evaluation of each option. We have introduced the concept of preference radius to characterize the difficulty of a multi-objective setting through the robustness of the preference function to the quality of estimations available. We have shown how this measure relates to the gap between the optimal action and the recommended action by a learning algorithm. We have used this new concept to provide a theoretical analysis of the Thompson sampling algorithm from multivariate normal priors in the multi-objective setting. More specifically, we were able to provide regret bounds for three families of preference functions. Empirical experiments confirmed the expected behavior of the multi-objective Thompson sampling in terms of cumulative regret growth. Results also highlight the important fact that one cannot simply reduce a multi-objective setting to a traditional, single-objective, setting since this might cause a change in the optimal action. Future work includes the application of the proposed approach to a real world application.