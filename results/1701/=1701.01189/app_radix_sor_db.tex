\subsection{Building a radix sort}\label{subsec:multisplit_sort}
In Section~\ref{sec:perf_eval}, we concentrated on the performance evaluation of our multisplit methods with user-defined bucket identifiers, and in particular the delta-bucket example.
In this section, we focus on identity buckets and how we can modify them to implement our own version of radix sort.

\paragraph{Multisplit with identity buckets} Suppose we have identity buckets, meaning that each key is identical to its bucket ID (i.e., $\defn{f}(u_i) = u_i = j$, for $0\leq j < m$).
In this case, sorting keys (at least the first $\lceil \log m \rceil$ bits of keys) turns out to be equivalent to the stable multisplit problem.
In this case, there is no need for the extra overheads inherent in RB-sort; instead, a direct radix sort can be a competitive alternate solution.

\paragraph{Radix sort} In Section~\ref{sec:radix} we briefly discussed the way radix sort operates.
Each round of radix sort sorts a group of bits in the input keys until all bits have been consumed.
For CUB, for example, in the Kepler architecture (e.g., Tesla K40c) each group consists of 5 consecutive bits, while for the more recent Pascal architecture (e.g., GeForce GTX 1080), each group is 7 bits.\footnote{These stats are for CUB 1.6.4.}

\paragraph{Multisplit-sort} Each round of radix sort is essentially bucketing its output based on the set of bits it considers in that round. If we define our buckets appropriately, multisplit can do the same.
Suppose we have $\defn{f}_k(u) = (u \gg kr) \& (2^{r}-1)$, where $\gg$ denotes bitwise shift to the right and $\&$ is a bitwise AND operator. $r$ denotes our radix size (i.e., the size of the group of bits to be considered in each iteration).
Then, with $0 \leq k < \lceil 32/r\rceil$ iterations of multisplit with $\defn{f}_k$ as each iteration's bucket identifier, we have built our own radix sort.

\paragraph{High level similarities and differences with CUB radix sort}
At a high level, multisplit-sort is similar to CUB's radix sort.
In CUB, contiguous chunks of bits from input keys are sorted iteratively. Each iteration includes an up-sweep where bin counts are computed, a scan operation to compute offsets, and a down-sweep to actually perform the stable sorting on a selected chunk of bits (similar roles to our pre-scan, scan and post-scan stages in our multisplit).
The most important differences are 1)~shared memory privatization (CUB's thread-level versus our warp-level), which decreases our shared memory usage compared to CUB, and 2)~our extensive usage of warp-wide intrinsics versus CUB's more traditional register-level computations.

\subsubsection{Performance Evaluation}
Our multisplit-based radix sort has competitive performance to CUB's radix sort. Our experiments show that, for example on the GeForce GTX 1080, our key-only sort can be as good as 0.9x of CUB's performance, while our key-value sort can be as good as 1.1x faster than CUB's.

\paragraph{Multisplit with identity buckets}
We first compare our multisplit methods (WMS and BMS) with identity buckets to  CUB's radix sort over $\lceil \log m \rceil$ bits.
Multisplit and CUB's performance are roughly similar.
Our multisplit methods are usually better for a small number of buckets, while CUB's performance is optimized around the number of bits that it uses for its internal iterations (5 bits for Kepler, 7 bits for Pascal).
Table~\ref{table:sort_bits_rate} shows our achieved throughput (billion elements sorted per second) as a function of the number of bits per key.
\input{sort_bits}

There are several important remarks to make:
\begin{itemize}
\item Our multisplit methods outperform CUB for up to 4 bits on the Tesla K40c and up to 6 bits on the GeForce GTX 1080. We note CUB is highly optimized for specific bit counts: 5-bit radixes on Kepler (Tesla K40c) and 7-bit radixes on Pascal (GeForce GTX 1080).
\item By comparing our achieved throughputs with those of delta-buckets in Table~\ref{table:ms_rate}, it becomes clear that the choice of bucket identifier can have an important role in the efficiency of our multisplit methods.
  In our delta-bucket computation, we used integer divisions, which are expensive computations.
  For example, in our BMS, the integer division costs 0.72x, 0.70x, and 0.90x geometric mean decrease in our key-only throughput for Tesla K40c (ECC on), Tesla K40c (ECC off) and GeForce GTX 1080 respectively. The throughput decrease for key-value scenarios is 0.87x, 0.82x and 0.98x respectively. The GeForce GTX 1080 is less sensitive to such computational load variations. Key-value scenarios also require more expensive data movement so that the cost of the bucket identifier is relatively less important.
\item On the GeForce GTX 1080, our BMS method is always superior to WMS. This GPU appears to be better at hiding the latency of BMS's extra synchronizations, allowing the marginally better locality from larger subproblems to become the primary factor in differentiating performance.
\end{itemize}

\paragraph{Multisplit-sort}
Now we turn to characterizing the performance of sort using multisplit with identity buckets.
It is not immediately clear what the best radix size ($r$) is for achieving the best sorting performance. As a result, we ran all choices of $4\leq r \leq 8$.
Because our bucket identifiers are also relatively simple (requiring one shift and one AND), our multisplit performance should be close to that of identity buckets.

Since BMS is almost always superior to WMS for $r\geq 4$ (Table~\ref{table:sort_bits_rate}), we have only used BMS in our implementations.
For sorting 32-bit elements, we have used $\lfloor 32/r \rfloor$ iterations of r-bit BMS followed by one last BMS for the remaining bits. For example, for $r = 7$, we run 4 iterations of 7-bit BMS then one iteration of 4-bit BMS\@.
Table~\ref{table:sort} summarizes our sort results.

\input{sort}

By looking at our achieved throughputs (sorting rates), we see that our performance increases up to a certain radix size, then decreases for any larger $r$. This optimal radix size is different for each different GPU and depends on numerous factors, for example available bandwidth, the efficiency of warp-wide ballots and shuffles, the occupancy of the device, etc.
For the Tesla K40c, this crossover point is earlier than the GeForce GTX 1080 (5 bits compared to 7 bits).
Ideally, we would like to process more bits (larger radix sizes) to have fewer total iterations. But, larger radix sizes mean a larger number of buckets ($m$) in each iteration, requiring more resources (shared memory storage, more register usage, and more shuffle usage), yielding an overall worse performance per iteration.

\paragraph{Comparison with CUB}
CUB is a carefully engineered and highly optimized library. For its radix sort, it uses a persistent thread style of programming~\cite{Gupta:2012:ASO}, where a fixed number of thread-blocks (around 750) are launched, each with only 64 threads (thus allowing many registers per thread).
Fine-tuned optimizations over different GPU architectures enables CUB's implementation to efficiently occupy all available resources in the hardware, tuned for various GPU architectures.

The major difference between our approach with CUB has been our choice of privatization. CUB uses thread-level privatization, where each thread keeps track of its local processed information (e.g., computed histograms) in an exclusively assigned portion of shared memory. Each CUB thread processes its portion of data free of contention, and later, combines its results with those from other threads.
However, as CUB considers larger radix sizes, it sees increasing pressure on each block's shared memory usage. The pressure on shared memory becomes worse when dealing with key-value sorts as it now has to store more elements into shared memory than before.

In contrast to CUB's thread privatization, our multisplit-based implementations instead target warp-wide privatization. An immediate advantage of this approach is that we require smaller privatized exclusive portions of shared memory because we only require a privatized portion per warp rather than per thread.
The price we pay is the additional cost of warp-wide communications (shuffles and ballots) between threads, compared to CUB's register-level communication within a thread.

The reduced shared memory usage of our warp privatization becomes particularly valuable when sorting key-value pairs.
Our key-value sort on GeForce GTX 1080 shows this advantage: when both approaches use 7-bit radixes, our multisplit-sort achieves up to a 1.10x higher throughput than CUB\@.
On the other hand, CUB demonstrates its largest performance advantage over our implementation (ours has 0.78x the throughput of CUB's) for key-only sorts on Tesla K40c (ECC off). In this comparison, our achieved shared memory benefits do not balance out our more costly shuffles.

% On Tesla K40c, CUB is not as effective when ECC is enabled (the default). Our 5-bit key-only sort and 6-bit key-value sort are 1.05x and 1.26x faster than CUB, respectively, on this configuration.
% \john{Here's what I currently have after an edit (the edit is not done): ``We conjecture that CUB's fine-tuned resource allocation, based on the available nominal bandwidth of the device,
% Having less available bandwidth means there are less data to be processed by all those aforementioned carefully allocated local registers in each thread.''
% I just don't see an actual point to be made here, so I think we should delete it all. End the paragraph after ``configuration''.}

Our multisplit-based radix sort proves to be competitive to CUB's radix sort, especially in key-value sorting. 
For key-only sort, our best achieved throughputs are 1.05x, 0.78x, and 0.88x times the throughput that CUB provides for Tesla K40c (ECC on), Tesla K40c (ECC off), and GeForce GTX 1080, respectively.
For key-value sorting and with the same order of GPU devices, our multisplit-based sort provides 1.26x, 0.83x, and 1.10x times more throughput than CUB, respectively. 
Our highest achieved throughput is 3.0 Gkeys/s (and 2.1 Gpairs/s) on a GeFroce GTX 1080, compared to CUB's 3.4 Gkeys/s (and 1.9 Gpairs/s) on the same device. 

\paragraph{Future of warp privatized methods}
We believe the exploration of the difference between thread-level and warp-level approaches has implications beyond just multisplit and its extension to sorting.
In general, any future hardware improvement in warp-wide intrinsics will reduce the cost we pay for warp privatization, making the reduction in shared memory size the dominant factor. We advocate further hardware support for warp-wide voting with a generalized ballot that returns multiple 32-bit registers, one for each bit of the predicate.
Another useful addition that would have helped our implementation is the possibility of shuffling a dynamically addressed register from the source thread.
This would enable the user to share lookup tables among all threads within a warp, only requesting the exact data needed at runtime rather than delivering every possible entry so that the receiver can choose.
