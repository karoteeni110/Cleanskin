\section{Introduction}\label{sec:intro}

This paper studies the multisplit primitive for GPUs.
\footnote{This paper is an extended version of initial results published at PPoPP 2016~\cite{Ashkiani:2016:GM}. The source code is available at \url{https://github.com/owensgroup/GpuMultisplit}.} 
Multisplit divides a set of items (keys or key-value pairs) into contiguous buckets, where each bucket contains items whose keys satisfy a programmer-specified criterion (such as falling into a particular range). Multisplit is broadly useful in a wide range of applications, some of which we will cite later in this introduction. But we begin our story by focusing on one particular example, the delta-stepping formulation of single-source shortest path (SSSP)\@.

The traditional (and work-efficient) serial approach to SSSP is Dijkstra's algorithm~\shortcite{Dijkstra:1959:ANO}, which considers one vertex per iteration---the vertex with the lowest weight. The traditional parallel approach (Bellman-Ford-Moore~\cite{Bang-Jensen:2009:DTA}) considers all vertices on each iteration, but as a result incurs more work than the serial approach. On the GPU, the recent SSSP work of Davidson et al.~\shortcite{Davidson:2014:WPG:nourl} instead built upon the delta-stepping work of Meyer and Sanders~\shortcite{Meyer:2003:DAP}, which on each iteration classifies candidate vertices into \emph{buckets} or \emph{bins} by their weights and then processes the bucket that contains the vertices with the lowest weights. Items within a bucket are unordered and can be processed in any order.

Delta-stepping is a good fit for GPUs. It avoids the inherent serialization of Dijkstra's approach and the extra work of the fully parallel Bellman-Ford-Moore approach. At a high level, delta-stepping divides up a large amount of work into multiple buckets and then processes all items within one bucket in parallel at the same time. How many buckets? Meyer and Sanders describe how to choose a bucket size that is ``large enough to allow for sufficient parallelism and small enough to keep the algorithm work-efficient''~\shortcite{Meyer:2003:DAP}. Davidson et al.\ found that 10 buckets was an appropriate bucket count across their range of datasets. More broadly, for modern parallel architectures, this design pattern is a powerful one: expose just enough parallelism to fill the machine with work, then choose the most efficient algorithm to process that work. (For instance, Hou et al.\ use this strategy in efficient GPU-based tree traversal~\shortcite{Hou:2011:MGS}.)

Once we've decided the bucket count, how do we efficiently classify vertices into buckets? Davidson et al.\ called the necessary primitive \emph{multisplit}. Beyond SSSP, multisplit has significant utility across a range of GPU applications.
Bucketing is a key primitive in one implementation of radix sort on GPUs~\cite{Merrill:2010:RSF}, where elements are reordered iteratively based on a group of their bits in their binary representation%
% Bucketing is a key primitive in reorganizing rays into 8 direction-based buckets for better coherence in a GPU-based ray tracer~\cite{Yang:2013:EDM};
; as the first step in building a GPU hash table~\cite{Alcantara:2009:RPH:nourl}; in hash-join for relational databases to group low-bit keys~\cite{Diamos:2012:ERA}; in string sort for singleton compaction and elimination~\cite{Deshpande:2013:CGS}; in suffix array construction to organize the lexicographical rank of characters~\cite{Deo:2013:PSA}; in a graphics voxelization pipeline for splitting tiles based on their descriptor (dominant axis)~\cite{Pantaleoni:2011:VAP}; in the shallow stages of $k$-d tree construction~\cite{Wu:2011:SKC}; in Ashari et al.'s sparse-matrix dense-vector multiplication work, which bins rows by length~\cite{Ashari:2014:FSM}; and in probabilistic top-$k$ selection, whose core multisplit operation is three bins around two pivots~\cite{Monroe:2011:RSO}. And while multisplit is a crucial part of each of these and many other GPU applications, it has received little attention to date in the literature. The work we present here addresses this topic with a comprehensive look at efficiently implementing multisplit as a general-purpose parallel primitive.

The approach of Davidson et al.\ to implementing multisplit reveals the need for this focus. If the number of buckets is 2, then a scan-based ``split'' primitive~\cite{Harris:2007:PPS:nourl} is highly efficient on GPUs. Davidson et al.\ built both a 2-bucket (``Near-Far'') and 10-bucket implementation. Because they lacked an efficient multisplit, they were forced to recommend their theoretically-less-efficient 2-bucket implementation:

\begin{quote}\vspace{-2pt}
  The missing primitive on GPUs is a high-performance \emph{multisplit} that separates primitives based on key value (bucket id); in our implementation, we instead use a sort; in the absence of a more efficient multisplit, we recommend utilizing our Near-Far work-saving strategy for most graphs.~\cite[Section~7]{Davidson:2014:WPG:nourl}\vspace{-2pt}
\end{quote}

Like Davidson et al., we could implement multisplit on GPUs with a sort. Recent GPU sorting implementations~\cite{Merrill:2010:RSF} deliver high throughput, but are overkill for the multisplit problem: unlike sort, multisplit has no need to order items within a bucket. In short, sort does more work than necessary. For Davidson et al., reorganizing items into buckets after each iteration with a sort is too expensive: ``the overhead of this reorganization is significant: on average, with our bucketing implementation, the reorganizational overhead takes 82\% of the runtime.''~~\cite[Section~7]{Davidson:2014:WPG:nourl}

In this paper we design, implement, and analyze numerous approaches to multisplit, and make the following contributions:
\begin{itemize}
\item On modern GPUs, ``global'' operations (that require global communication across the whole GPU) are more expensive than ``local'' operations that can exploit faster, local GPU communication mechanisms. Straightforward implementations of multisplit primarily use global operations. Instead, we propose a parallel model under which the multisplit problem can be factored into a sequence of local, global, and local operations better suited for the GPU's memory and computational hierarchies.
\item We show that reducing the cost of global operations, even by significantly increasing the cost of local operations, is critical for achieving the best performance.
  We base our model on a hierarchical divide and conquer, where at the highest level each subproblem is small enough to be easily solved locally in parallel, and at the lowest level we have only a small number of operations to be performed globally.
\item We locally reorder input elements before global operations, trading more work (the reordering) for better memory performance (greater coalescing) for an overall improvement in performance.
\item We promote the warp-level privatization of local resources as opposed to the more traditional thread-level privatization. This decision can contribute to an efficient implementation of our local computations by using warp-synchronous schemes to avoid branch divergence, reduce shared memory usage, leverage warp-wide instructions, and minimize intra-warp communication.
\item We design a novel voting scheme using only binary ballots. We use this scheme to efficiently implement our warp-wide local computations (e.g., histogram computations).
\item We use these contributions to implement a high-performance multisplit targeted to modern GPUs. We then use our multisplit as an effective building block to achieve the following:
  \begin{itemize}
  \item We build an alternate radix sort competitive with CUB (the current fastest GPU sort library). Our implementation is particularly effective with key-value sorts (Section~\ref{subsec:multisplit_sort}).
  \item We demonstrate a significant performance improvement in the delta-stepping formulation of the SSSP algorithm (Section~\ref{sec:app_sssp}).
  \item We build an alternate device-wide histogram procedure competitive with CUB\@. Our implementation is particularly suitable for a small number of bins (Section~\ref{subsec:multisplit_histogram}).
  \end{itemize}
\end{itemize}
%-----------------------
% In fact, an efficient multisplit enables a significant performance improvement for Davidson et al. (Section~\ref{sec:app_sssp}), as well as the other applications cited above.
% In this paper we design, implement, and analyze numerous approaches to multisplit, and make the following contributions:

% \begin{itemize}
% \item On modern GPUs, ``global'' operations (that require global communication across the whole GPU) are more expensive than ``local'' operations that can exploit faster, local GPU communication mechanisms. Straightforward implementations of multisplit primarily use global operations. Instead, we propose a parallel model under which the multisplit problem can be factored into a sequence of local, global, and local operations better suited for the GPU's memory and computational hierarchies.
% % \item We implement local operations efficiently by using warp-synchronous schemes to avoid branch divergence, reduce shared memory usage, leverage warp-wide instructions, and minimize intra-warp communication.
% \item We model our localization into a hierarchical structure with an arbitrary degree of freedom. This later let us tailor our algorithms systematically so that it matches the hardware characteristics that we seek.
% \item We show that reducing the cost of global operations, even by significantly increasing the cost of local operations, is critical for achieving the best performance.
% \item We locally reorder input elements before global operations, trading more work (the reordering) for better memory performance (greater coalescing) for an overall improvement in performance.
% \item We implement a novel and efficient warp-wide voting scheme using binary ballots. We use this scheme to perform our warp-wide local computations (including histograms).
% \item We promote the warp-level privatization of local resources as opposed to the more traditional thread-level privatization. This decision can lead to an efficient implementation of our local computations by using warp-synchronous schemes to avoid branch divergence, reduce shared memory usage, leverage warp-wide instructions, and minimize intra-warp communication.
% \item We use our multisplit implementations to build our own GPU radix-sort particularly effective for key-value sorts, GPU histogram suitable for small number of buckets, and a faster implementation of the delta-stepping SSSP formulations on the GPU.
% \end{itemize}
