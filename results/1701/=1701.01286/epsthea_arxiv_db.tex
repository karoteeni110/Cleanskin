\documentclass[10pt,a4paper]{article}
\usepackage[top=3cm, bottom=3cm, inner=3cm, outer=3cm]{geometry}
\pdfoutput=1
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage[authoryear]{natbib}
\usepackage{mathtools}
\usepackage{extarrows}
\usepackage{bm}
\def\bbeta{\bm{\beta}}
\def\bbetag{\bm{\beta}_{\text{g}}}
\def\betag{\beta_{\text{g}}}
\def\betagk{\beta_{\text{g}k}}
\def\bbetae{\bm{\beta}_{\text{e}}}
\def\bbetaeg{\bm{\beta}_{\text{eg}}}
\def\bx{\mathbf{x}}
\def\bX{\mathbf{X}}
\def\bxoi{\mathbf{x}_{0i}}
\def\bxni{\mathbf{x}_{1i}}
\def\bxgi{\mathbf{x}_{\text{g}i}}
\def\bXgi{\mathbf{X}_{\text{g}i}}
\def\bxei{\mathbf{x}_{\text{e}i}}
\def\bXei{\mathbf{X}_{\text{e}i}}
\def\bxgj{\mathbf{x}_{\text{g}j}}
\def\bxgk{\mathbf{x}_{\text{g}k}}
\def\bxgkk{\mathbf{x}_{\text{g}k'}}
\def\bxgK{\mathbf{x}_{\text{g}K}}
\def\bXgj{\mathbf{X}_{\text{g}j}}
\def\bxej{\mathbf{x}_{\text{e}j}}
\def\bXej{\mathbf{X}_{\text{e}j}}
\def\bxg{\mathbf{x}_{\text{g}}}
\def\bxe{\mathbf{x}_{\text{e}}}
\def\bXg{\mathbf{X}_{\text{g}}}
\def\bXe{\mathbf{X}_{\text{e}}}

\def\bxgb{\mathbf{x}_{\text{g}b}}
\def\bxgbb{\mathbf{x}_{\text{g}b'}}
\def\bxgna{\mathbf{x}_{\text{g}n_a}}
\def\bxea{\mathbf{x}_{\text{e}a}}
\def\bxea{\mathbf{x}_{\text{e}a}}

\def\E{\text{E}}


\begin{document}

\title{Improving power of genetic association studies by extreme phenotype sampling: a review and some new results}
\date{}
\author{}
\maketitle

\begin{center}
THEA BJØRNLAND$^{1 \ast}$, ANJA BYE$^2$, EINAR RYENG$^3$, \\ ULRIK WISLØFF$^2$, METTE LANGAAS$^1$

\vspace{3mm}
\textit{$^1$ Department of Mathematic Sciences,
Norwegian University of Science and Technology, Trondheim,
Norway \\
$^2$ Department of Circulation and Medical Imaging, Norwegian University of Science and Technology, Trondheim, Norway \\
$^3$ Department of Cancer Research and Molecular Medicine, Norwegian University of Science and Technology, Trondheim, Norway}
\end{center}

\begin{abstract}
Extreme phenotype sampling is a selective genotyping design for genetic association studies where only individuals with extreme values of a continuous trait are genotyped for a set of genetic variants. Under financial or other limitations, this design is assumed to improve the power to detect associations between genetic variants and the trait, compared to randomly selecting the same number of individuals for genotyping. Here we present extensions of likelihood models that can be used for inference when the data are sampled according to the extreme phenotype sampling design. Computational methods for parameter estimation and hypothesis testing are provided. We consider methods for common variant genetic effects and gene-environment interaction effects in linear regression models with a normally distributed trait. We use simulated and real data to show that extreme phenotype sampling can be powerful compared to random sampling, but that this does not hold for all extreme sampling methods and situations.

\textit{Key words:} {GWAS, gene-environment interactions, extreme phenotype sampling, outcome-dependent sampling, selective genotyping, the HUNT study}
\end{abstract}

\section{Introduction}
Extreme phenotype sampling (EPS) is an outcome-dependent sampling design for genetic association studies. For this design, individuals with high or low values of a particular continuously measurable phenotype (trait) are genotyped. When the number of individuals that can be genotyped is limited, such samples are assumed to give good power to detect associations between genetic variants and the trait. Random sampling is the most relevant competing design. Extreme samples must be analyzed with statistical methods that properly account for the sampling bias, and these methods are not trivial. Random samples can be analyzed with readily available standard methods and are therefore preferable when it comes to data analysis. However, low statical power is an important issue in genetic association studies (\cite{sham2014statistical}, \cite{hirschhorn2002comprehensive}). Here we attempt to answer whether, when and to what degree extreme sampling is more powerful than random sampling in genetic association studies. For this purpose we have extended relevant likelihood methods for parameter estimation and hypothesis testing. 

We consider genetic association studies where the aim is to detect common genetic variants that are associated with some trait, and also to quantify associations. We consider biallelic single-nucleotide polymorphism (SNP) data. For a particular SNP, the observed genotype for an individual is either $aa$, $aA$ or $AA$, where $A$ represents the minor-allele in the population. We consider additive genetic models so that the genotype is coded as 0, 1 or 2 according to the number of copies of the minor-allele. In a \textit{genome-wide association study} (GWAS) the observed genotypes of selected SNPs along the genome, so-called genetic markers, are tested for association with a phenotype in a sample of individuals. The number of SNPs is large ($\sim 10^6$). The purpose of such studies is to detect regions in the genome that are associated with the phenotype. In what we will refer to as a \textit{candidate SNP study}, a small collection SNPs are analyzed. These SNPs can for example be chosen based on results from studies in other populations, or from studies of related traits and diseases. Then the focus is on replication and effect size estimation rather than detection.

Due to high genotyping costs and low statistical power to detect significant associations between genetic variants and complex traits, selective genotyping has been proposed as a strategy for achieving good statistical power under sample size limitations. Genotyping only the phenotypically extreme individuals was proposed for mapping quantitative trait loci (QTL) in experimental organisms (\cite{lebowitz1987trait}, \cite{lander1989mapping}). The methodology was further developed for linkage disequilibrium mapping of QTLs by \cite{darvasi1992selective}, \cite{slatkin1999disequilibrium}, \cite{chen2005linkage}, \cite{wallace2006improved}, among others. Power studies have been performed by \cite{van2000power} and \cite{xing2009power}. Most methods were not fully efficient because they involved discretizing the continous trait, discarding individuals who were homozygous for the minor-allele or not accounting for the biased sampling. To that effect \cite{huang2007eps} proposed likelihood methods that made full use of the data and accounted for the selective genotyping design. Recently, likelihood methods for multivariate trait-dependent sampling has also been considered (\cite{lin2013quantitative}, \cite{tao2015epsmultivar}). Selective genotyping has also recently been proposed in studies of rare genetic variants (\cite{li2011using}, \cite{guey2011power}, \cite{barnett2013detecting}). The extreme phenotype sampling design can be considered a special case of the outcome-dependent sampling design described by \cite{zhou2002semiparametric} and \cite{weaver2005ods}. 

The power, limitations and practical utility of extreme phenotype sampling as compared to random sampling in modern GWAS and candidate SNP studies has in our opinion not been sufficiently characterized, and this might explain why the design has not been much used. \cite{huang2007eps} considered regression models for continuous phenotypes and developed likelihood models for extreme sampling data for the special case where only one covariate (the genetic variant) was included in the regression model. The power of two different extreme sampling designs was estimated, but not compared to random sampling. Using the asymptotic distribution of the score test statistic under the alternative hypothesis \cite{tang2010equivalence} showed that the methods by \cite{huang2007eps} theoretically can give better power than a random sampling design in the special case with no non-genetic covariates. \cite{zhou2002semiparametric} showed that their likelihood method for the outcome-dependent sampling design yielded more efficient parameter estimates than would be obtained using a simple random sample of the same size. We extend the likelihood methods for the EPS-design to include non-genetic (environmental) explanatory variables as well as gene-environment interaction terms. As experienced by us in a study of gene-environment interactions and obesity using an extreme sampling design this additional model complexity is necessary for application purposes \citep{WHRbjornland}. We assess the statistical power and other properties of our methods using both simulated and real data. The data set is from the HUNT study (Helseundersøkelsen i Nord-Trøndelag) which comprises health information on the population of Nord-Trøndelag county, Norway \citep{HUNT}. We use data from a GWAS on the trait \textit{maximum oxygen uptake} based on the HUNT Fitness study \citep{aspenes2011physical}. All our computational methods for parameter estimation and hypothesis testing under the EPS-design are available in our R-package.

\section{Models and methods}
We consider complex traits that are continuously measurable and can be assumed to be normally distributed in the population. Let the $i$th individual in a population (or large random sample) of size $N$ have observed trait value $y_i$, environmental variables $\bxei^{\text{T}} = (x_{\text{e}i1},\ldots,x_{\text{e}id})$, and SNP genotypes $\bxgi^{\text{T}} = (x_{\text{g}i1},\ldots,x_{\text{g}im})$. Assume that the continuous trait $Y_i$ can be modeled by the linear regression model
\begin{align}
	Y_i = \alpha + \bxei^{\text{T}} \bbetae + \bxgi^{\text{T}} \bbetag + (\bxei \bxgi)^{\text{T}} \bbetaeg + \varepsilon_i, \text{ } \varepsilon_i \text{ i.i.d } \mathcal{N}(0,\sigma^2), i = 1, \ldots, N,
	\label{eq:lm}
\end{align}
where $\bxei \bxgi$ represents a vector of interactions between some environmental and genetic covariates, e.g. $(\bxei \bxgi)^{\text{T}} = (x_{\text{e}ij} x_{\text{g}ik}, x_{\text{e}ij} x_{\text{g}il})$ for some $j \in \{1,\ldots d\}$ and $k,l \in \{1,\ldots,m\}$, $k \neq l$.
%Three cases of this linear model are relevant; case 1: $\bbetae = \bbetaeg = 0$, case 2: $\bbetaeg = 0$, and case 3: all parameters are non-zero. The main hypothesis of interest for case 1 and 2 is $H_0: \bbetag = 0$, and for case 3 $H_0: \bbetaeg = 0$.

We consider GWAS and candidate-SNP studies. In genome-wide studies we analyze a large number of SNPs, and each SNP is tested separately for association with the phenotype ($H_0: \betagk = 0$ against $H_1: \betagk \neq 0$, $k = 1,\ldots,m$) without any interaction effects. $P$-values are compared to a significance threshold that is determined by the multiple testing burden. In candidate-SNP studies, a few selected SNPs are studied, and the aim is to test for associations between the phenotype and all or some of the genetic variables ($H_0: \bbetag = 0$ against $H_1: \bbetag \neq 0$, or $H_0: \betagk = 0$ against $H_1: \betagk \neq 0$, $k = 1,\ldots,m$), and to obtain parameter estimates and confidence intervals for the genetic effects. Furthermore, it is also of interest to study gene-environment interaction effects ($H_0: \bbetaeg = 0$ vs $H_1: \bbetaeg \neq 0$). % We assume additive genetic effects such the genetic variable $x_{\text{g}ik}$ takes the values 0, 1 or 2 according to the number of copies of the minor allele of the $k$th SNP that individual $i$ carries.

Assume that due to financial or other limitations, only $n < N$ individuals can be genotyped. We define extreme phenotypes as observations by $y_i < c_l$ and $y_i > c_u$, for some cut-off values $c_l$ and $c_u$ such that $c_l < c_u$. In the sample of size $N$, the cut-offs can be chosen such that we sample the $n$ most extreme individuals (e.g. $n/2$ from each tail), or we could choose less extreme cut-offs and thereafter draw $n$ individuals from the extremes. The methods presented here apply to both situations. Our main analysis concerns two different extreme sampling designs which we refer to as the EPS-only and the EPS-full sampling designs. These are extensions of the designs discussed by \cite{huang2007eps}. In the EPS-only design, observations of any variable ($y_i$, $\bxei$ or $\bxgi$) are available \textit{only} for extreme phenotype individuals. In the EPS-full design, observations of the phenotype ($y_i$) and environmental covariates ($\bxei$) are available for the \textit{full} population, while genetic variants ($\bxgi$) are only observed for the extremes. Let $\mathcal{C}$ denote the set of indexes of the $n$ extreme phenotype individuals.

\subsection{EPS-only}
For the EPS-only sampling design the observations $(y_i,\bxei,\bxgi)$ are available for all individuals $i \in \mathcal{C}$, i.e. all extreme-phenotype individuals. We consider two different statistical methods for this design; the EPS-only binary method and the EPS-only (continuous) method.

 The first method treats the lower and upper extremes as binary responses and we refer to this method as the EPS-only binary method. The second method takes the continuity of the trait into account, and we refer to this as the EPS-only (continuous) method.

\subsubsection{EPS-only binary}
Were we treat the lower and upper phenotypic extremes as a binary response. One method for analysis of EPS-only data is to test whether allele frequencies are significantly different between the two extreme tails, for example by using contingency tables. We use a logistic regression model in order to include environmental covariates and gene-environment interaction terms. Define the variable $Y_{di}$ such that $Y_{di} = 0$ if $Y_i < c_l$, $Y_{di} = 1$ if $Y_i > c_u$. Let $\pi_i$ denote the probability $P(Y_{di} = 1; \bxei,\bxgi|Y_i < c_l \cup Y_i > c_u)$, such that $1-\pi_i = P(Y_{di} = 0; \bxei,\bxgi|Y_i < c_l \cup Y_i > c_u)$. A logistic regression model
\begin{align}
\text{logit}(\pi_i) = a + \bxei^{\text{T}} \mathbf{b}_{\text{e}} + \bxgi^{\text{T}} \mathbf{b}_{\text{g}} + (\bxei \bxgi)^{\text{T}} \mathbf{b}_{\text{eg}} ,
\label{eq:binepsonly}
\end{align}
can be fitted to the dichotomized extreme sample data. Under the two-sided hypothesis $H_0: \bbetag = 0$ in the linear regression model \eqref{eq:lm}, there is no difference between allele frequencies in the lower and upper extremes, which in the dichotomized sample can be tested by the two-sided hypothesis $H_0: \mathbf{b}_{\text{g}} = 0$, and similarly for gene-environment interactions. Hypothesis tests for this model can be done using standard methods for logistic regression. Note that the parameters of the logistic regression model \eqref{eq:binepsonly} are directly dependent upon the choice of $c_l$ and $c_u$ and comparison of results between studies must be done cautiously. 

\subsubsection{EPS-only (continuous)}
A likelihood model for EPS-only samples with a continuous response was proposed by \cite{huang2007eps}, then called the conditional likelihood. Here, we extend this likelihood to include environmental covariates and gene-environment interactions and develop the score test. Let $Y_{ci}$ denote a random variable from the extremes of the distribution of $Y_i$. Then $F_{Y_{ci}}(y) = P(Y_i \leq y | Y_i < c_l \cup Y_i > c_u)$ and the probability density can be derived accordingly (see Appendix A). The likelihood for the EPS-only sample is
\begin{align}
L = \prod_{i \in \mathcal{C}} \frac{ \frac{1}{\sigma} \phi \left( \frac{y_i - \mu(\bxei, \bxgi; \alpha, \bbetae, \bbetag, \bbetaeg)}{\sigma} \right)}{1 - \Phi \left( \frac{c_u - \mu(\bxei, \bxgi; \alpha, \bbetae, \bbetag, \bbetaeg)}{\sigma} \right) + \Phi \left( \frac{c_l - \mu(\bxei, \bxgi; \alpha, \bbetae, \bbetag, \bbetaeg)}{\sigma} \right)},
\label{eq:cclik}
\end{align}
where $\Phi()$ is the cumulative probability distribution and $\phi()$ is the density function of the standard normal distribution, and $\mu(\bxei, \bxgi; \alpha, \bbetae, \bbetag, \bbetaeg) = $  $\alpha + \bxei^{\text{T}}\bbetae + \bxgi^{\text{T}} \bbetag + (\bxei \bxgi)^{\text{T}} \bbetaeg$.

We have implemented a quasi-Newton numerical optimization method to obtain likelihood estimates. We obtain approximate $(1-\alpha)100\%$ confidence intervals for some parameter $\beta_{j}$ by $ \left[ \hat{\beta}_{j} - z_{\alpha/2}\sqrt{(I_o^{-1})_{\beta_{j}, \beta_{j}}}, \text{ } \hat{\beta}_{j} + z_{\alpha/2} \sqrt{(I_o^{-1})_{\beta_{j}, \beta_{j}}} \right] $, where $\hat{\beta}_j$ is the maximum likelihood estimate and $z_{\alpha/2}$ is such that $\Phi(z_{\alpha/2}) = 1 - \alpha/2$. The observed information matrix $I_o$ is estimated in the optimization.

We are interested in the two-sided hypothesis tests $H_0: \bbetag = 0$, $H_0: \betagk = 0$ for $k = 1,\ldots,m$, and $H_0: \bbetaeg = 0$. In the GWAS setting we strongly prefer a computationally efficient test due to the large number of tests. Using the score test, the null model can be fitted once and for GWAS in particular, the test is then computationally fast compared to the likelihood ratio test which requires model fitting under all $m$ alternative hypotheses. For the EPS-only likelihood \eqref{eq:cclik}, we have derived a closed form expression for the score test statistic (see Appendix A.1). The expression is mathematically complex and requires some tedious algebra, but the reward is significant computational efficiency. \cite{tang2010equivalence} showed that in the most simple model ($y = \alpha + x_{\text{g}} \beta_{\text{g}} + \varepsilon$) the score test statistic for $H_0: \beta_g = 0$ derived from the continuous EPS-only likelihood is equivalent to the the score test statistic derived from the likelihood for a normal linear regression model. In Appendix A.1, we show that this also holds for testing the two-sided hypothesis $H_0: \bbetag = 0$ in the model $y = \alpha + \bxg^{\text{T}} \bbetag + \varepsilon$, but not when other covariates ($\bxe$) are included in the null model.

\subsection{EPS-full}
The EPS-full sample consists of observations $(y_i, \bxei,\bxgi)$ for all $ i \in \mathcal{C}$, and observations $(y_i,\bxei)$ for all $i \not \in \mathcal{C}$. In other words, $\bxgi$ is missing for all individuals $i \not \in \mathcal{C}$. The missing observations are missing at random (MAR) because the observations are not missing due to the unobserved $\bxgi$, but rather due to the observed value of the phenotype $y_i$. We consider both a likelihood based method and multiple imputation for this sample.

\subsubsection{EPS-full likelihood}
Under MAR the likelihood that ignores the missing-mechanism is valid for likelihood inference from the frequentist perspective \citep[page 120]{littlerubin2002}. It is necessary to specify or estimate the distribution of the variables that are missing. We assume in the most general case that $\bXg$ is dependent upon some (or all) of the covariates in $\bXe$, denoted by $\bX$. We assume that the sample space of $\bX$ is discrete with elements $\bx_j$, $j = 1,\ldots,J$. We let $\bxgk$, $k=1,\ldots,K$ denote elements of the sample space of $\bXg$ and we assume that $\bXg|\bX = \bx_j$ can take any value in this sample space for all $j$, albeit with different probabilities. The likelihood for the EPS-full sample is an extension of the so-called full likelihood by \cite{huang2007eps} and is derived in Appendix B. The EPS-full likelihood is
\begin{align}
L = & \prod_{i \in \mathcal{C}} \frac{1}{\sigma} \phi \left( \frac{y_i - \mu(\bxei, \bxgi; \alpha, \bbetae, \bbetag, \bbetaeg)}{\sigma} \right) \sum_{j=1}^J f_{\bXg|\bX = \bx_j}(\bxgi) I(\bx_i = \bx_j) \cdot \nonumber \\ & \prod_{i \not \in \mathcal{C}} \sum_{k =1}^K \frac{1}{\sigma} \phi \left( \frac{y_i - \mu(\bxei, \bxgk; \alpha, \bbetae, \bbetag, \bbetaeg)}{\sigma} \right)\sum_{j=1}^J f_{\bXg|\bX = \bx_j}(\bxgk) I(\bx_i = \bx_j),
\label{eq:crlik}
\end{align}
where $\phi()$ is the density function of the standard normal distribution and $f_{\bXg|\bX = \bx}(\bxg)$ is the probability mass function of the SNPs. Note that $\bxei$ to all intents and purposes is a constant vector in the EPS-full model, while $Y_i$ and $\bXgi$ are random. The missing-structure is determined by the choice of $c_u$ and $c_l$. These parameters may be chosen such that the distinctness condition for the ignorability principle \citep[page 119]{littlerubin2002} does not hold (e.g. choosing as cut-offs the quantiles of the empirical distribution of $Y$). Then the likelihood that ignores the missing mechanism \eqref{eq:crlik} is still valid, but not fully efficient (e.g. there is some information about the distribution of $Y$ in the empirical quantiles). We prefer to ignore the missing-mechanism so that the likelihood can be used for various trait-dependent sampling designs.

In some models, the distribution of the genotypes of the SNPs does not depend on the value of the non-genetic covariates and $f_{\bXg|\bX = \bx_j}(\bxg) = f_{\bXg}(\bxg)$. However, in the presence of confounding effects (e.g. population stratification), the distribution of the genotypes will differ in subsets of the sample, and the assumed genotype distribution must account for this. The sample space for each SNP is $\{0,1,2\}$, and we assume that $P(X_{\text{g}k} = 0|\bX = \bx_j) = p_{0kj}$, $P(X_{\text{g}k}=1|\bX = \bx_j) = p_{1kj}$ and $P(X_{\text{g}k}=2|\bX = \bx_j) = 1 - p_{0kj} - p_{1kj}$. A more general distribution of the missing covariate in a similar likelihood model has been considered in the literature (\cite{lawless1999semiparametric}, \cite{ibrahim2005missing}) but we have used this simple SNP property for computational purposes. If Hardy-Weinberg equilibrium is assumed \citep[page 39]{ziegler2010statistical}, then we have $P(X_{\text{g}k} = 0|\bX = \bx_j) = (1-q_{kj})^2$, $P(X_{\text{g}k}=1|\bX = \bx_j) = 2q_{kj}(1-q_{kj})$ and $P(X_{\text{g}k}=2|\bX = \bx_j) = q_{kj}^2$, where $q_{kj}$ is the minor allele frequency. In GWA studies where SNPs are tested one at a time a joint distribution of the genetic variants is not relevant. In candidate-SNP studies, the SNPs that are considered are typically such that the genotype distributions can be assumed to be statistically independent (e.g. SNPs from different genes or chromosomes), and one can define the joint distribution as the product of the marginal distributions. Otherwise, a multivariate multinomial distribution can be used.

As for the EPS-only likelihood, we have implemented a quasi-Newton numerical optimization method to obtain maximum likelihood estimates and confidence intervals. \cite{kenward1998likelihood} showed that the observed information matrix can be used as an estimate of the true information matrix for likelihoods that ignore the missing-mechanism.

The EPS-full likelihood \eqref{eq:crlik} is valid for direct-likelihood inference \citep{rubin1976inference}, which ensures validity of the likelihood ratio test. For the score test, the asymptotic variance of the score vector depends on the missing-mechanism. We therefore derived the score test by using the observed information matrix under the null to approximate the asymptotic variance of the score vector, which is a valid estimate of the true information matrix \citep{kenward1998likelihood}. See Appendix B.1 for the derivation of the EPS-full score test for $H_0: \bbetag = 0$. Again, the derivation is complex, but the test is computationally efficient. We used the result of \cite{derkach2015score} to obtain the simplest closed form expression for the score test statistic. If covariates with a missing-structure are present in the null model, for example when testing $H_0: \bbetaeg = 0$, a similar (relatively) simple closed form expression for the score test statistic cannot be attained, and we have implemented the likelihood ratio test for this purpose.

\subsubsection{EPS-full with multiple imputation}
Multiple imputation (MI) is a tool that can be used for parameter estimation and hypothesis testing for the linear model \eqref{eq:lm} when covariates are MAR. We use the method of multivariate imputation by chained equations (MICE), also known as fully conditional specification \citep{buuren2007multiple}. Broadly speaking, the method imputes the missing genotypes by sampling from an empirical conditional distribution of the genetic variant for individual $i$, given all other observations in the sample. This is repeated to create $m_{\text{MI}}$ different data sets in which model inference is performed. Lastly, parameter estimates or test statistics are pooled into one estimate. The MICE method is readily available in many statistical softwares, and we have used the R-package \texttt{mice} \citep{buuren2011mice}. Our aim is not to develop a specific multiple imputation method for extreme sampling in genetic association studies, but rather to compare our proposed EPS-full likelihood method to an existing inference method for missing data problems. We refer to this method as the EPS-full MI method.

\section{Simulation study}
Using simulated data, we compare the performance of the EPS methods (EPS-only binary, EPS-only, EPS-full, EPS-full MI), with results from the full sample and a random sample. We generated a full data set of size $N$, and selected $n<N$ individuals for genotyping under extreme and random sampling. We set $c_l$ and $c_u$ such that the set $\mathcal{C}$ consisted of the $n/2$ lowest and $n/2$ highest extremes of the empirical phenotype distribution. For the EPS-only design we then discarded all information on non-extremes ($i \not\in \mathcal{C}$). For the EPS-full design, we discarded the genotype information for all individuals $i \not \in \mathcal{C}$. We considered the following simulations models;
\begin{align}
Y & = \alpha + \beta_{\text{e}1} x_{\text{e}1} + \beta_{\text{e}2} x_{\text{e}2} + \beta_{\text{g}} x_{\text{g}} + \varepsilon, \label{eq:sim1} \\
Y & = \alpha + \beta_{\text{e}1} x_{\text{e}1} + \beta_{\text{e}2} x_{\text{e}2} + \beta_{\text{g}} x_{\text{g}}  + \beta_{\text{e}1\text{g}} x_{\text{e}1}x_{\text{g}} + \varepsilon, \label{eq:sim2} \\
Y & = \alpha + \beta_{\text{e}1} x_{\text{e}1} + \beta_{\text{e}2} x_{\text{e}2} + \beta_{\text{g}} x_{\text{g}}  + \beta_{\text{e}2\text{g}} x_{\text{e}2}x_{\text{g}} + \varepsilon \label{eq:sim3}. 
\end{align}
The non-genetic covariate $x_{\text{e}1}$ is a $\text{Bernoulli(0.4)}$ random variable, $x_{\text{e}2}$ is a $N(2,1)$ random variable and the genetic marker $x_\text{g}$ is a multinomially distributed random variable taking values $(0,1,2)$ with probabilities $(0.49,0.42,0.09)$. These probabilities were generated by assuming Hardy-Weinberg equilibrium and a minor allele frequency $q=0.3$. For multiple imputation we set $m_{\text{MI}}=10$. The parameter values that we used are given in Table \ref{tab:param}. With these parameter choices, the environmental covariates $x_{e1}$ and $x_{e2}$ were much more important than the genetic covariate for describing the response ($R^2$ from fitting the regression model with and without the genetic covariates varied minimally). This choice was motivated by the assumption that environmental variables are more important for predicting a complex trait, compared to the genotype of a common genetic variant (\cite{darvasi1992selective}, \cite{manolio2009finding}).

\begin{table}[h]
\centering
\begin{tabular}{cc}
\hline
\rule{0pt}{2.5ex}
	Parameter & Value \\
	\hline
	\rule{0pt}{2.5ex}
	$N$ & 5000  \\
	$n$ & $N/2$ \\
	$\alpha$ & $50$  \\
	$\beta_{\text{e}1}$ & 10 \\
	$\beta_{\text{e}2}$ & 5 \\
	$\beta_{\text{g}}$ & 0.5  \\
	$\beta_{\text{e}1\text{g}}$ & 1  \\
	$\beta_{\text{e}2\text{g}}$ & 0.5 \\
	$\sigma$ & 6  \\
	$q$ & 0.3  \\
	\hline
\end{tabular}
\caption{Specification of parameters used in simulated data sets for simulation models \eqref{eq:sim1}, \eqref{eq:sim2} and \eqref{eq:sim3}.}
\label{tab:param}
\end{table}

\subsection{Main effects model}
\subsubsection{Parameter estimation}
We generated $R = $ 10 000 data sets using simulation model \eqref{eq:sim1} and obtained the maximum likelihood estimate $\hat{\beta}_{\text{g}}$ for the different designs and methods. We estimated the mean squared error (MSE) by $\frac{\sum_{r=1}^{R} (\hat{\beta}_{\text{g}r} - \beta_{\text{g}})^{2}}{R}$. For the full and random sample we used the function \texttt{lm()} in R to obtain parameter estimates. For the EPS-only and EPS-full likelihood methods we used functions provided in our R-package, and for the EPS-full MI method we used the R-package \texttt{mice} \citep{buuren2011mice}. Results for different sample sizes $N$ and genotyped sample size $n = N/2$ are presented in Table \ref{tab:sim1mse}. Compared to results from the full sample, the EPS-full likelihood method gave the lowest MSE, while the random sample, the EPS-only sample and the EPS-full MI method had similar and slightly higher MSE. This relationship was the same for increasing values of the full sample size $N$, and the MSE decreased as $N$ increased for all methods.

\begin{table}[h]
\centering
\begin{tabular}{cccccc}
\hline
\rule{0pt}{2.5ex}
  $N$& Full & Random & EPS-only & EPS-full & EPS-full \\ 
   & & & continuous & likelihood & MI \\
  \hline
  \rule{0pt}{2.5ex}
  1000 & 0.085 & 0.170 & 0.163 & 0.128 & 0.162 \\ 
  3000 & 0.028 & 0.056 & 0.055 & 0.043 & 0.057 \\ 
  5000 & 0.017 & 0.034 & 0.034 & 0.027 & 0.034 \\ 
  7000 & 0.012 & 0.024 & 0.023 & 0.018 & 0.022 \\ 
   \hline
\end{tabular}
	\caption{Estimated mean squared error for the coefficient of the genetic variant ($\beta_{\text{g}}$) in simulation model \eqref{eq:sim1} for different sample sizes $N$ with all other parameters held fixed at the values described in Table \ref{tab:param}.}
	\label{tab:sim1mse}
\end{table}

\subsubsection{Power}
We estimated the power to detect a non-zero genetic effect ($\beta_{\text{g}} \neq 0$) in simulation model \eqref{eq:sim1}. We generated $R = $ 10 000 data sets under the alternative hypothesis, and tested $H_0: \beta_{\text{g}} = 0$ against $H_1: \beta_{\text{g}} \neq 0$ for all designs and methods. For the full, random and EPS-only binary models we used the score test provided in the R-package \texttt{statmod} \citep{statmod}. Power was estimated at a $5\%$ significance level by $ \frac{\sum_{r=1}^R I_{(0, 0.05)}(p_r)}{R}$, where $I_{(0, 0.05)}(p_r)$ is the indicator function and $p_r$ is the $p$-value in the $r$th simulated data set.

Power estimates for different values of $\beta_{\text{g}}$,  $\sigma$, the minor allele frequency $q$ and the full sample size $N$ are presented in Table \ref{tab:sim1}. Expectedly, all methods had highest power when the variance $\sigma^2$ was low, the parameter $\beta_{\text{g}}$ was large, the minor allele frequency $q$ was high and the sample size $N$ was large. For all values of $\beta_{\text{g}}$, the EPS-only binary model had lowest power. The EPS-full likelihood method had higher estimated power than all the alternatives. The EPS-only continuous model was the second most powerful but only slightly better than random sampling. The EPS-full MI method was slightly less powerful than random sampling. When the variance was high all EPS-models performed better than random sampling. For low variance, the random sample was more powerful than the EPS-only binary and EPS-full MI methods. For all $q$ the EPS-only binary model performed worst, the random sample, EPS-full MI and EPS-only method were similar, while the EPS-full likelihood method was most powerful. We observe similar results for different values of $N$. Overall, the EPS-full likelihood model performed notably better than all alternatives. This is in contrast to the results by \cite{huang2007eps} who found that in models with no environmental covariates the EPS-full and EPS-only likelihood methods performed similarly.

We also considered power as a function of $n$ - the number of genotyped individuals. We set the parameters of the simulation model \eqref{eq:sim1} such that the power to detect non-zero $\beta_{\text{g}}$ was approximately $80\%$ in the full sample ($N=5000$). All parameters were as in Table \ref{tab:param} except that we set $\sigma = 8$. In $R=$ 10 000 simulated data sets we considered $n$ ranging from 1000 to 5000 in increments of 500. Power simulation results are presented in Figure \ref{fig:samplesize1}. If we for example wanted to design a study with $70\%$ power, we see that we would have to genotype approximately 3000 individuals for the EPS-full likelihood method, 3500 for EPS-only likelihood method and 4000 for a random sample and the EPS-full MI method. The EPS-only binary model never achieves $70\%$ power in this scenario. In EPS-only binary we test whether the genotype frequencies are significantly different between the upper and lower extreme groups. As $n$ increases, these two groups become more similar. Therefore, the power of the EPS-only binary model does not converge towards the power of the full sample as $n$ increases. 
%We see that random sampling from the full population can be more powerful than an extreme binary method when the random sampling method accounts for the continuity of the phenotype. 
%\cite{van2000power} argued that a method similar to EPS-only binary would be more powerful than sampling from the total distribution, then comparing genotype frequencies of the above and below-average individuals. We see that random sampling from the full population can be more powerful than an extreme binary method when the random sampling method accounts for the continuity of the phenotype.


\begin{table}[p]
\centering
	\begin{tabular}{cccccccc}
	\hline
	\rule{0pt}{2.5ex}
	 & & Full & Random & EPS-only & EPS-only & EPS-full & EPS-full \\
	 & & & & binary & continuous & likelihood & MI \\
	\hline
	\rule{0pt}{2.5ex}
	$\beta_{\text{g}}$ & 0.3 & 62.47 & 36.86 & 24.37 & 37.67 & 46.09 & 30.25 \\ 
  & 0.5 & 96.97 & 76.76 & 57.16 & 78.36 & 87.36 & 71.72 \\ 
  & 0.7 & 99.95 & 96.39 & 84.86 & 96.95 & 99.03 & 95.26 \\ 
	\rule{0pt}{3ex}
 $\sigma$ &4 & 99.99 & 98.25 & 55.17 & 94.72 & 98.99 & 94.48 \\ 
 & 6 & 96.97 & 77.42 & 57.94 & 79.77 & 87.81 & 73.53 \\ 
 & 8 & 81.30 & 52.89 & 47.29 & 60.85 & 68.34 & 51.61 \\ 
 & 10 & 63.20 & 36.60 & 38.42 & 47.60 & 52.83 & 38.97 \\ 
	\rule{0pt}{3ex}
 $q$&0.1 & 70.76 & 42.85 & 30.13 & 44.89 & 54.51 & 37.63 \\ 
  &0.2 & 91.80 & 65.61 & 47.27 & 68.42 & 77.85 & 60.92 \\ 
  &0.3 & 96.47 & 76.60 & 56.91 & 78.69 & 87.42 & 72.83 \\ 
 	\rule{0pt}{3ex}
 $N$&1000 & 39.48 & 22.09 & 17.05 & 24.18 & 28.06 & 20.12 \\ 
  &3000 & 83.98 & 55.68 & 39.14 & 57.37 & 67.49 & 51.28 \\ 
  &5000 & 96.79 & 77.09 & 58.30 & 79.17 & 87.66 & 73.62 \\ 
  &7000 & 99.53 & 88.96 & 72.26 & 90.32 & 96.03 & 86.41 \\ 
	\hline
	\end{tabular}
	\caption{Estimated power to detect a non-null genetic effect ($H_0: \beta_{\text{g}} = 0$) in simulation model \eqref{eq:sim1} for different values of $\beta_g$, $\sigma$, minor allele frequency $q$ and full sample size $N$. For each parameter that varied, all other parameters were held fixed at the values described in Table \ref{tab:param}.}
	\label{tab:sim1}
\end{table}

\begin{figure}[p]
	\centering
	\includegraphics[width = 0.6\textwidth]{samplesize12.png}
	\caption{Estimated power to detect a non-null genetic effect ($H_0: \beta_{\text{g}} = 0$) in simulation model \eqref{eq:sim1} for increasing number of genotyped individuals ($n$), compared to the power for the full sample where $n=N=5000$.}
	\label{fig:samplesize1}
\end{figure}

\subsubsection{Computational efficiency}
The computational time for testing $H_0: \beta_{\text{g}} = 0$ in 100 simulated data sets was 1.7 seconds for full samples, 1.1 seconds for random samples, 2.1 seconds for the EPS-only sample with the binary method, 18.37 seconds for the EPS-only continuous method, 1.7 seconds for the EPS-full likelihood method and 14 minutes for the EPS-full MI method. All computations were performed using R version 3.3.1 \citep{Rsoftware} on a personal computer (MacBook Air (13", Early 2014) with 1.7 GHz Intel Core i7-4650U with 4 MB cache). The score test for the EPS-full likelihood method is computationally efficient because under the null hypothesis we fit a linear model to the full sample (no missing variables). The score test for EPS-only is slower because model fitting under the null requires numerical optimization of the EPS-only likelihood. We observe that the EPS-full MI method that we have used is computationally slow.

\subsection{Gene-environment interaction models}
For simulation models \eqref{eq:sim2} and \eqref{eq:sim3} we estimated power for the two-sided tests of $H_0: \beta_{\text{e}1\text{g}} = 0$ and $H_0: \beta_{\text{e}2\text{g}} = 0$ at a $5\%$ significance level, for different values of $\beta_{\text{e}1\text{g}}$ and $\beta_{\text{e}2\text{g}}$. Model parameters were as in Table \ref{tab:param}. For the EPS-full MI method with interactions, we imputed using the passive method described in \cite{buuren2011mice} (Section 3.4). In simulation model \eqref{eq:sim2} there is an interaction between a genetic variant and a \textit{binary} environmental variable. From the power estimates for increasing values of $\beta_{\text{e}1\text{g}}$ (Table \ref{tab:sim2}) we see that the EPS-only binary method and EPS-full MI method performed poorly in this setting, and furthermore that the EPS-only continuous method was similar to a random sample. The EPS-full likelihood method had the highest estimated power. In simulation model \eqref{eq:sim3} there is an interaction between a genetic variant and \textit{continuous} environmental variable. Also here, the EPS-full method performed the best in our simulations, while the EPS-only method was slightly better than a random sample. The EPS-only binary and EPS-full MI methods again had the lowest estimated power.

\begin{table}[h]
\centering
	\begin{tabular}{cccccccc}
	\hline
	\rule{0pt}{2.5ex}
	& & Full & Random & EPS-only  & EPS-only & EPS-full & EPS-full \\
	&&&& continuous & binary & likelihood & MI \\
	\hline
	\rule{0pt}{2.5ex}
$\beta_{\text{e}1\text{g}}$ &0.8 & 84.13 & 56.45 & 55.95 & 33.21 & 62.25 & 18.51 \\ 
  &1.0 & 96.21 & 75.45 & 75.07 & 47.32 & 81.21 & 33.00 \\ 
  &1.2 & 99.34 & 89.45 & 88.13 & 62.44 & 93.00 & 52.73 \\   \\
	\rule{0pt}{3ex}
$\beta_{\text{e}2\text{g}}$ & 0.4 & 85.54 & 63.57 & 57.72 & 27.95 & 69.46 & 29.97 \\ 
  &0.6 & 99.51 & 92.96 & 89.73 & 50.66 & 96.14 & 73.08 \\ 
  &0.8 & 100.0 & 99.74 & 99.16 & 77.04 & 99.94 & 97.57 \\ 
	\hline
	\end{tabular}
	\caption{Estimated power to detect a non-null gene-environment interaction effect in simulation model \eqref{eq:sim2} ($H_0: \beta_{\text{e}1\text{g}} = 0$) and in simulation model \eqref{eq:sim3} ($H_0: \beta_{\text{e}2\text{g}} = 0$). All parameters other than $\beta_{\text{e}1\text{g}}$ and $\beta_{\text{e}2\text{g}}$ were held fixed at the values described in Table \ref{tab:param}.}
	\label{tab:sim2}
\end{table}


\section{Application to data from the HUNT study}
We assessed the extreme sampling methods by application in a relevant data set. Our data set comes from the HUNT Fitness study \citep{aspenes2011physical}. A genome-wide association study for maximum volume uptake of oxygen ($\text{VO}_2$) has been performed by co-authors Anja Bye, Einar Ryeng and Ulrik Wisløff. The participants in this study represent a full sample (all participants were genotyped) and the trait $\text{VO}_2$ can be assumed normally distributed in the population. In the original study, the regression model
\begin{align*}
\text{VO}_2 = \alpha + \beta_{\text{e}2} x_{\text{age}} + \beta_{\text{e}1}x_{\text{sex}} + \beta_{\text{e}3} x_{\text{PA}} + \betagk x_{\text{g}k} + \varepsilon
\end{align*}
was used to test $H_0: \betagk = 0$ against $H_1: \betagk \neq 0$ for $k = 1,\ldots,m$. The non-genetic covariates were age, sex and physical activity (PA). In the original study, the main associations between genetic variants and oxygen uptake was found in chromosome 1. We therefore used this chromosome to illustrate our methods and the number of SNPs to test was then $m = 11098$. We here excluded all participants with missing non-genetic covariates (age, sex, physical activity). The full sample size was then $N=2802$.

We considered the fictitious situation where we could only afford to genotype half of the full sample. We performed tests for association across chromosome 1 using the full data set, a random sample of size $N/2$ and extreme samples (lower and upper quartiles). The full data set also had some missing genotype observations, as is common in genetic association studies. For the full, random and EPS-only samples, we imputed the mean genotype. For the EPS-full model we assume that the sample space of each genetic variant is $\{0,1,2\}$ and mean imputation would be at odds with this assumption. However, when genotypes are missing at random (MAR) or missing completely at random (MCAR), the EPS-full likelihood \eqref{eq:crlik} is valid. We assumed MCAR for the initially missing genotypes, and MAR for the genotypes that were missing due to extreme sampling, and then no imputation was necessary. For the EPS-full MI method we considered one SNP at a time and imputation was then based only on $\text{VO}_2$, sex, age and physical activity. %If the missing-structure was different between individuals, imputation could be improved by using observations of other genetic variants as well.

The Manhattan plot for each sampling method is shown in Figure \ref{fig:manhattan1}. A Manhattan plot is a plot of $-\log_{10}(p)$, $p$ being the $p$-value from the two-sided test of $H_0: \beta_{\text{g}} = 0$, against the position of the SNP on the genome. Such plots are typical for GWAS, and regions on the genome where there is a peak in $-\log_{10}(p)$-values are considered to be of further interest. In the Manhattan plot of the full sample we see that such a peak appears in chromosome 1. Furthermore, we see a very similar result for the EPS-full sample for both the likelihood and MI methods. The peak can also be distinguished in the EPS-only model, but not when using the EPS-only binary method, nor in the random sample. Genotyping only $n = N/2$ extreme phenotype individuals in this study could have been sufficient to detect the same region that was found when genotyping all $N$ individuals. We note that the performance of EPS-full MI method was much better here than using simulated data. We also note that the extreme sample as analyzed by the EPS-only (continuous) method is clearly better than the "unlucky" random sample drawn here.

For the top finding in the full sample we also estimated $\beta_{\text{g}}$ in the full, random sample and extreme samples. Parameter estimates were $2.21$ ($95\% \text{ CI} = (1.24,3.17)$, $p = 7.9 \cdot 10^{-6}$) in the full sample, $1.23$ ($95\% \text{ CI} = (-0.16,2.98)$, $p = 0.08$) in the random sample, $2.85$ ($95\% \text{ CI} = (1.51,4.19)$, $p = 3.4 \cdot 10^{-5}$) in the EPS-only sample, $2.74$ ($95\% \text{ CI} = (1.55,3.92)$, $p = 3.0 \cdot 10^{-6}$) in the EPS-full sample with the maximum likelihood method, and $3.02$ ($95\% \text{ CI} = (1.57,4.47)$, $p = 4.0 \cdot 10^{-5}$) in the EPS-full sample using multiple imputation. The results from the EPS-full likelihood method was closest to the results of the full sample. All EPS-methods were slightly biased upwards.

\begin{figure}[h]
	\centering
	\includegraphics[width = 0.8\textwidth]{manhattan1mi.png}
	\caption{Manhattan-plot for testing each SNP in chromosome 1 against $\text{VO}_2$ for the full model (all $N$ study participants analyzed), a random sample ($n = N/2$ randomly drawn participants analyzed), an EPS-only sample ($n=N/2$ most extreme participants analyzed) using the EPS-only binary method and the EPS-only continous method, and an EPS-full sample ($N$ participants analyzed of which only $n=N/2$ most extreme participants had observed genotypes) using the EPS-full likelihood and EPS-full MI method.}
	\label{fig:manhattan1}
\end{figure}

\section{Other extreme sampling methods and designs}

\subsection{Gene-environment interactions by an extreme exposure sampling design}
With the purpose to study gene-environment interactions, \cite{boks2007ees} proposed an extreme-exposure sampling (EES) design. In this design, individuals with extreme values of the relevant environmental covariate are sampled for genotyping. We refer to the EES-only design as a sample where we only observe individuals with extreme exposure values, as considered by \cite{boks2007ees}. Furthermore, we considered an EES-full design where extreme exposure individuals are genotyped but where non-genetic information is also available for all other individuals. EES-only data can be analyzed using standard linear regression methods, while EES-full data can be analyzed with the EPS-full likelihood \eqref{eq:crlik}. The missing-mechanism is MCAR since $\bxei$ is a not a random variable in these models. We set up a simulation study to compare the performance of the EES designs to the EPS designs for gene-environment interaction effects. We consider the simulation model \eqref{eq:sim3}, which has an interaction between a continuous environmental covariate and a genetic covariate. Parameter values were as in Table \ref{tab:param} and we simulated $R = 10 \text{ } 000$ data sets. For different values of $\beta_{\text{e}2\text{g}}$, the estimated power of the different models are presented in Table \ref{tab:sim3ees}. We see that the EES-only and EES-full samples had almost identical estimated power in our simulations, and both performed better than the EPS-only and EPS-full samples. A discussion on why EES-only and EES-full have almost identical power under the MCAR criterion is given in Appendix D. For the specific purpose of studying an interaction between a continuously measurable environmental exposure and genetic variants, the extreme exposure design seems powerful and the data set is simple to analyze.

\begin{table}[h]
\centering
	\begin{tabular}{cccccc}
	\hline
	\rule{0pt}{2.5ex}
	$\beta_{\text{e}2\text{g}}$ & Full & EPS-only & EPS-full  & EES-only & EES-full \\
	\hline
	\rule{0pt}{2.5ex}
 0.3 & 51.78& 32.41 &35.98 &48.95& 49.12 \\
 0.4 & 76.70 &52.12 &57.55 &73.91& 74.04 \\
 0.5 & 91.94 &71.95& 78.06 &89.67& 89.93 \\
	\hline
	\end{tabular}
	\caption{Estimated power to detect a non-null gene-environment interaction effect in simulation model \eqref{eq:sim3} ($H_0: \beta_{\text{e}2\text{g}} = 0$) for extreme phenotype sampling (EPS) and extreme exposure sampling (EES). All parameters other than $\beta_{\text{e}2\text{g}}$ were held fixed at the values described in Table \ref{tab:param}.}
	\label{tab:sim3ees}
\end{table}

\subsection{Combining extreme and random sampling}
The outcome-dependent sampling design (\cite{zhou2002semiparametric}, \cite{weaver2005ods}) is a generalization of extreme phenotype sampling. The range of the trait $Y$ is divided into segments, and individuals from each segment are sampled with different probabilities. These probabilities can be set so that we sample for example only from the extremes. Additionally, \cite{zhou2002semiparametric} proposed to include a random sample of size $n_0$. Motivated by this design, we consider a design where we first select a random sample of size $n_0$ to be genotyped, and thereafter sample $n_e$ extreme-phenotype individuals. We assume an EPS-full sampling design so that non-genetic information is available in the full sample. We consider a full data set of size $N$, and a genotyped sample of size $n_0 + n_e = n$. We consider different sample sizes for the genotyped sample; $n$ ranging from 1000 to 5000, and for each $n$ we consider different sizes of the random and the extreme sample ($n_0$ and $n_e$). The parameter values were set as in Table \ref{tab:param} but with $\sigma = 8$ so that the power in the full sample was approximately $80\%$ at the $5\%$ significance level. We simulated $R = 10 \text{ } 000$ data sets. We used the EPS-full likelihood method to test $H_0: \beta_{\text{g}} = 0$ against $H_1: \beta_{\text{g}} \neq 0$ in each sample. The results of the simulation study is presented in Figure \ref{fig:samplesize2}. We observe that the design with $n_0 = n$ and $n_e = 0$ (a random sample of size $n$) performs poorly, while all sampling methods that combine extreme and random samples have almost the same power as the EPS-full sample ($n_0 = 0$ and $n_e = n$).

\begin{figure}[h]
	\centering
	\includegraphics[width = 0.6\textwidth]{samplesize2.png}
	\caption{Estimated power to detect a non-null genetic effect ($H_0: \beta_{\text{g}} = 0$) for outcome-dependent sampling in simulation model \eqref{eq:sim1} for increasing number of genotyped individuals ($n = n_0 + n_e$, where $n_0$ is the size of the random sample and $n_e$ is the size of the extreme sample), compared to the power for the full sample where $n=N=5000$.}
	\label{fig:samplesize2}
\end{figure}

\section{Discussion}
We have here considered a sampling design for genetic association studies that has been proposed to increase power of genetic association studies with limited sample sizes. Under the EPS design, individuals with an extreme phenotype are selected for genotyping. If a true association exists between the trait and a genetic variant, the extreme sample will be enriched with homozygous individuals, i.e. individuals with none or two copies of the minor-allele (assuming an additive effect). We have presented relevant statistical methods for this design; some methods are currently used (EPS-only binary) and some methods have to a lesser extent been used in practice (EPS-only continuous and EPS-full). The EPS-only binary method is a valid choice for extreme phenotype samples, and it is also simple to use. However, we have shown that the dichotomization of the continuous trait eliminates any potential gain in power due to extreme sampling, as compared to random sampling. The EPS-only continuous method is a likelihood method that takes into account the continuous probability distribution of the extremes. For this method, we have shown how to obtain parameter estimates and perform hypothesis tests for parameters of a linear regression model that is assumed to hold in the full population. Using the EPS-only likelihood method, we have seen that extreme samples can be more powerful than random samples. In some studies non-genetic variables will be known for the full population (or large sample) while only the extreme phenotype individuals are genotyped. We then have a missing covariate sample where the missing-mechanism is MAR. The EPS-full likelihood was derived based on this principle. We have shown through simulations and real data applications that the power of the EPS-full design can be similar to analyzing the full population. The EPS-full sample can also be analyzed using a multiple imputation approach. In our simulated data the EPS-full MI method had low power, while the then correctly specified EPS-full likelihood model performed significantly better. In the application to real data the multiple imputation method performed similarly to the likelihood method. Multiple imputation could be a useful method for analysis of extreme sampling data, but methodological improvements towards computational efficiency should be considered.

We have extended current statistical methods for EPS-data to include both genetic and non-genetic (environmental) covariates in order to comply with practical situations. Derivation of the score test for the EPS-only and EPS-full design is algebraically tedious and details are therefore only given in the appendix. We consider the computational efficiency of the score test to be important for genome-wide analysis where thousands of tests are performed. We also derived likelihood methods for estimating and testing gene-environment interaction effects. This was based on relevance for genetic association studies, as previously experienced by us \citep{WHRbjornland}. We showed that extreme phenotype sampling can be more powerful than random sampling when testing for gene-environment interaction effects. For specific gene-environment interaction situations we also showed that the extreme exposure design can be more powerful than both random and extreme phenotype sampling.

Some limitations of extreme sampling for genetic association studies also need to be discussed. First of all, extreme phenotype sampling is a complicated procedure compared to random sampling and requires sophisticated statistical methods. Second, we base our methods on the assumption of normality. This assumption can be difficult to check when one only has extreme data. Thirdly, the data could be difficult to include in larger studies (consortia, meta-analysis) due to the non-standard design. Furthermore, the issue of confounding (for example due to population stratification) is of relevance and the EPS-full method is particularly sensitive to unobserved confounding effects (see Appendix C for an illustration of this issue in the HUNT data). Lastly, for genetic association studies and GWAS in particular there are several quality control procedures that are done prior to data analysis. In an extreme sample it is not clear that all QC methods apply directly. Therefore, we advocate to supplement the extreme sample with a random sample so that the random sample can be used for checking model assumptions, confounding, etc. We showed in the last Section of this paper that a mixture of a random sample and an EPS-full sample can be almost as powerful as an EPS-full sample.

In conclusion, we have presented methods for extreme phenotype sampling and shown that the design \textit{can} give better power than random sampling in genetic association studies where the sample size for genotyping is limited. However, only one method (EPS-full) was here found to be clearly better, while other methods were slightly better (EPS-only continuous) or often worse (EPS-only binary) than random sampling. We have extended methods towards testing and modeling of gene-environment interaction effects and shown that also for this purpose, extreme phenotype sampling can be useful. If the appropriate statistical methods are used, it is possible to achieve significantly improved power in genetic association studies with continuous traits by genotyping extreme-phenotype individuals instead of a random sample. 

\section*{Software}
Software is available as an R-package at https://github.com/theabjorn/extremesampling.

\section*{Acknowledgments}
The Nord-Trøndelag Health Study (the HUNT study) is collaboration between the HUNT Research Centre (Faculty of Medicine, Norwegian University of Science and Technology), the Nord-Trøndelag County Council, the Central Norway Health Authority, and the Norwegian Institute of Public Health.

\section*{Conflict of interest}
None declared.

\newpage

\appendix
\allowdisplaybreaks

\section{The EPS-only likelihood and tests}
 \label{app:epsonly}
The linear regression model we have studied is given by
\begin{align}
	Y_i = \alpha + \bxei^{\text{T}} \bbetae + \bxgi^{\text{T}} \bbetag + (\bxei \bxgi)^{\text{T}} \bbetaeg + \varepsilon_i, \text{ } \varepsilon_i \text{ i.i.d } \mathcal{N}(0,\sigma^2).
	\label{eq:lma}
\end{align}
Here, $Y_i$ is the phenotype of individual $i$, $\bxei$ is a vector of environmental (non-genetic) covariates, while $\bxgi$ is a vector of genetic covariates (SNP genotypes for a set of SNPs). The term $\bxei \bxgi$ is a vector of interactions between relevant environmental and genetic covariates, e.g. $\bxei \bxgi = (x_{\text{e}ij} x_{\text{g}ik}, x_{\text{e}ij} x_{\text{g}il})$ for some $j \in \{1,\ldots d\}$ and $k,l \in \{1,\ldots,m\}$, $k \neq l$. For the EPS-only design, the observations $(y_i, \bxei, \bxgi)$ are available for all individuals $i \in \mathcal{C}$ for which $y_i < c_l$ or $y_i > c_u$. Let $Y_{ci}$ denote a random variable from the extremes of the distribution of $Y_i$. We derive the probability distribution of $Y_{ci}$ for known $\bxei$ and $\bxgi$. For simplicity, we denote $F_{Y_c}(y; \bxe, \bxg , \alpha, \bbetae, \bbetag, \bbetaeg, \sigma, c_l, c_u)$ by $F_{Y_c}(y)$, etc. The distribution of $Y_c$ is then;
\begin{align*}
F_{Y_c}(y)
		& =  P(Y_c \leq y) \\
		& = P(Y \leq y | (Y<c_l) \cup (Y > c_u)) \\
		& = \frac{P(Y \leq y \cap ((Y<c_l) \cup (Y>c_u)))}{P( (Y<c_l) \cup (Y > c_u))} \\
		& = \frac{P(((Y \leq y) \cap (Y<c_l)) \cup ((Y \leq y) \cap (Y>c_u)))}{P( (Y<c_l) \cup (Y > c_u))} \\
		& = \frac{P((Y \leq y) \cap (Y<c_l)) + P((Y \leq y) \cap (Y>c_u)) - P((Y \leq y) \cap(Y < c_l) \cap (Y>c_u))}{P( (Y<c_l) \cup (Y > c_u))} \\
		&= \frac{P((Y \leq y) \cap (Y<c_l)) + P((Y \leq y) \cap (Y>c_u)) }{P( (Y<c_l) \cup (Y > c_u))} \\
		&= \begin{dcases}
		\frac{P(Y \leq y)}{P( (Y<c_l) \cup (Y > c_u))} & \text{ if } y < c_l \\
		0 & \text{ if } c_l < y < c_u \\
		\frac{P(Y<c_l) + P(Y \leq y) - P(Y \leq c_u) }{P( (Y<c_l) \cup (Y > c_u))} & \text{ if } y > c_u \\
		\end{dcases}
\end{align*}
We have defined $Y$ to be normally distributed with mean $\mu(\bxe, \bxg; \alpha, \bbetae, \bbetag, \bbetaeg, \sigma) = \alpha + \bxe^{\text{T}} \bbetae + \bxg^{\text{T}} \bbetag + (\bxe \bxg)^{\text{T}} \bbetaeg$ and variance $\sigma^2$. We write
\begin{align*}
P(Y \leq y) &= \Phi \left( \frac{y - \mu(\bxe, \bxg; \alpha, \bbetae, \bbetag, \bbetaeg)}{\sigma} \right)
\end{align*}
where $\Phi()$ represents the cumulative probability distribution of the standard normal distribution. Let $\phi()$ denote the density function of the standard normal distribution. The probability density function for $Y_c$ is given by
\begin{align*}
f_{Y_c}(y) 
& = \frac{\partial }{\partial y}F_{Y_c}(y) \\
& = \begin{dcases}
\frac{ \frac{1}{\sigma} \phi \left( \frac{y - \mu(\bxe, \bxg; \alpha, \bbetae, \bbetag, \bbetaeg)}{\sigma} \right)}{1 - \Phi \left( \frac{c_u - \mu(\bxe, \bxg; \alpha, \bbetae, \bbetag, \bbetaeg)}{\sigma} \right) + \Phi \left( \frac{c_l - \mu(\bxe, \bxg; \alpha, \bbetae, \bbetag, \bbetaeg)}{\sigma} \right)} & \text{ if } y < c_l \text{ or } y > c_u, \\
0 & \text{ otherwise. }
\end{dcases}
\end{align*}
The likelihood for the continuous EPS-only design is then
\begin{align*}
L = \prod_{i \in \mathcal{C}} \frac{ \frac{1}{\sigma} \phi \left( \frac{y_i - \mu(\bxei, \bxgi; \alpha, \bbetae, \bbetag, \bbetaeg)}{\sigma} \right)}{1 - \Phi \left( \frac{c_u - \mu(\bxei, \bxgi; \alpha, \bbetae, \bbetag, \bbetaeg)}{\sigma} \right) + \Phi \left( \frac{c_l - \mu(\bxei, \bxgi; \alpha, \bbetae, \bbetag, \bbetaeg)}{\sigma} \right)},
\end{align*}
and the log-likelihood is
\begin{align*}
l = &  \sum_{i \in \mathcal{C}} \log\left(\frac{1}{\sigma} \phi \left( \frac{y_i - \mu(\mathbf{x}_{ei}, \mathbf{x}_{gi}; \alpha, \bbetae, \bbetag, \bbetaeg)}{\sigma} \right) \right)
\\
& - \sum_{i \in \mathcal{C}} \log\left( 1 - \Phi \left( \frac{c_u - \mu(\bxei, \bxgi; \alpha, \bbetae, \bbetag, \bbetaeg)}{\sigma}\right) + \Phi \left( \frac{c_l - \mu(\bxei, \bxgi; \alpha, \bbetae, \bbetag, \bbetaeg)}{\sigma} \right) \right)
\end{align*}
For simpler notation, define $\Phi_{u,i} = \Phi \left( \frac{c_u - \mu(\bxei, \bxgi; \alpha, \bbetae, \bbetag, \bbetaeg)}{\sigma}\right)$, and similarly for $\Phi_{l,i}$. Then
\begin{align*}
l \propto -n \log(\sigma) - \frac{1}{2 \sigma^2} \sum_{i \in \mathcal{C}} (y_i - \alpha - \bxei^{\text{T}} \bbetae - \bxgi^{\text{T}} \bbetag - (\bxei \bxgi)^{\text{T}} \bbetaeg )^2 
 - \sum_{i \in \mathcal{C}} \log\left( 1 - \Phi_{u,i} + \Phi_{l,i} \right)
\end{align*}
For hypothesis testing it is not necessary to distinguish between non-genetic covariates, genetic covariates and second-order interactions and we rewrite the linear model as 
\begin{align*}
Y_i = \alpha + \bxoi^{\text{T}} \bbeta_0 + \bxni^{\text{T}} \bbeta_1 + \varepsilon_i,
\end{align*}
where under the null hypothesis $\bbeta_0 = 0$, while $\bbeta_1$, $\alpha$ and $\sigma$ are nuisance parameters. Thus $\mathbf{x}_0$ is a vector of covariates that we want to test for association with $Y$ (e.g. $\bxg$), while $\mathbf{x}_{1}$ is a vector of all other covariates (e.g. $\bxe$). The log-likelihood can then be written as
\begin{align}
l \propto -n \log(\sigma) - \frac{1}{2 \sigma^2} \sum_{i \in \mathcal{C}} (y_i - \alpha - \bxoi^{\text{T}} \bbeta_0 - \bxni^{\text{T}} \bbeta_1)^2 
 - \sum_{i \in \mathcal{C}} \log\left( 1 - \Phi_{u,i} + \Phi_{l,i} \right).
\label{eq:ccloglik2}
\end{align}

\subsection{The score test}
We derive the score test statistic for the two-sided null hypothesis $H_0: \bbeta_0 = 0$ vs $H_1: \bbeta_0 \neq 0$. Let $\boldsymbol\theta_0$ denote all the parameters in the null model, $\alpha$, $\bbeta_1$ and $\sigma$, and let $\hat{\boldsymbol\theta}_0$ denote maximum likelihood estimators under the null hypothesis. The score vector is given by the first derivative of the log-likelihood with respect to $\bbeta_0$, evaluated in $\bbeta_0 = 0$ and $\hat{\boldsymbol\theta}_0$;
\begin{align*}
S = \frac{\partial l}{\partial \bbeta_1} (\hat{\boldsymbol\theta}_0, \bbeta_0 = 0).
\end{align*}
The variance of the score vector is given by
\begin{align*}
\Sigma = I_{\bbeta_0, \bbeta_0}(\hat{\boldsymbol\theta}_0, \bbeta_0 = 0) - I_{\bbeta_0, \theta_0}(\hat{\boldsymbol\theta}_0, \bbeta_0 = 0) I_{\theta_0, \theta_0}(\hat{\boldsymbol\theta}_0, \bbeta_0 = 0)^{-1} I_{\theta_0,\bbeta_0}(\hat{\boldsymbol\theta}_0, \bbeta_0 = 0),
\end{align*}
where $I_{\bbeta_0, \bbeta_0}$ is the element of the information matrix corresponding to $\bbeta_0$, etc. The score test statistic,
\begin{align*}
T = S^{\text{T}} \Sigma^{-1} S
\end{align*}
is asymptotically $\chi^2$-distributed under the null hypothesis, with degrees of freedom equal to the number of parameters that we test (i.e. the length of $\bbeta_0$).

In order to derive the score test statistic for the EPS-only likelihood we need the first and second derivatives of the log-likelihood \eqref{eq:ccloglik2}. For simpler notation, we write $f_i = y_i - \alpha - \bxoi^{\text{T}} \bbeta_0 - \bxni^{\text{T}} \bbeta_1$ and we define the following functions (extensions of similar functions defined by \cite{tang2010equivalence}):
\begin{align*}
h_{ij} = \frac{-\phi_{u,i}\cdot\left( \frac{c_u - \mu(\bxoi, \bxni; \alpha, \bbeta_0, \bbeta_1)}{\sigma} \right)^j +\phi_{l,i}\cdot\left( \frac{c_l - \mu(\bxoi, \bxni; \alpha, \bbeta_0, \bbeta_1)}{\sigma} \right)^j}{1-\Phi_{u,i}+\Phi_{l,i}},
\label{def:h}
\end{align*}
for $j = 0,1,2,3$, and furthermore
\begin{align*}
a_i & = 1 - h_{i1} - h_{i0}^2 \\
b_i & = h_{i0} - h_{i2} - h_{i0}h_{i1} \\
c_i & = - 1 + 2h_{i1} - h_{i3} - h_{i1}^2 \\
d_i & = 2 + 2h_{i1} - h_{i3} - h_{i1}^2.
\end{align*}
The first derivatives are
\begin{align*}
\frac{\partial l}{\partial \alpha} 
&= \frac{1}{\sigma^2}\sum_{i=1}^n f_i + \frac{1}{\sigma}\sum_{i=1}^n h_{i0}, \\
\frac{\partial l}{\partial \bbeta_0} 
&= \frac{1}{\sigma^2}\sum_{i=1}^n f_i \bxoi^{\text{T}} + \frac{1}{\sigma}\sum_{i=1}^n h_{i0}\bxoi^{\text{T}}, \\
\frac{\partial l}{\partial \bbeta_1} 
&= \frac{1}{\sigma^2}\sum_{i=1}^n f_i \bxni^{\text{T}} + \frac{1}{\sigma}\sum_{i=1}^n h_{i0}\bxni^{\text{T}}, \\
\frac{\partial l}{\partial \sigma} 
&= -\frac{n}{\sigma} + \frac{1}{\sigma^3}\sum_{i=1}^nf_i^2 + \frac{1}{\sigma}\sum_{i=1}^n h_{i1},
\end{align*}
and score vector can now be written as
\begin{align*}
S = \frac{1}{\sigma^2}\sum_{i=1}^n f_i (\hat{\boldsymbol\theta}_0, \bbeta_0 = 0) \bxoi^{\text{T}} + \frac{1}{\sigma}\sum_{i=1}^n h_{i0} (\hat{\boldsymbol\theta}_0, \bbeta_0 = 0)\bxoi^{\text{T}}.
\end{align*}

The second derivatives of the log likelihood are 
\begin{align*}
\frac{\partial^2 l}{\partial \alpha^2} 
&= \frac{1}{\sigma^2}\sum_{i=1}^n (-1 + h_{i1}+h_{i0}^2) = -\frac{1}{\sigma^2}\sum_{i=1}^n a_i,  \\
\frac{\partial^2 l}{\partial \alpha \partial \bbeta_0} 
&= \frac{1}{\sigma^2}\sum_{i=1}^n (-1 + h_{i1}+h_{i0}^2)\bxoi^{\text{T}} = -\frac{1}{\sigma^2}\sum_{i=1}^n a_i \bxoi^{\text{T}}, \\ 
\frac{\partial^2 l}{\partial \alpha \partial \bbeta_1} 
&= \frac{1}{\sigma^2}\sum_{i=1}^n (-1 + h_{i1}+h_{i0}^2)\bxni^{\text{T}}= -\frac{1}{\sigma^2}\sum_{i=1}^n a_i \bxni^{\text{T}}, \\ 
\frac{\partial^2 l}{\partial \alpha \partial \sigma} 
&= -\frac{2}{\sigma^3}\sum_{i=1}^n f_i + \frac{1}{\sigma^2}\sum_{i=1}^n (-h_{i0} + h_{2i} + h_{i0}h_{i1}) = -\frac{2}{\sigma^3}\sum_{i=1}^n f_i - \frac{1}{\sigma^2}\sum_{i=1}^n b_i, \\
\frac{\partial^2 l}{\partial \bbeta_0^{\text{T}} \bbeta_0}
&= \frac{1}{\sigma^2}\sum_{i=1}^n (-1 + h_{i1}+h_{i0}^2)\bxoi \bxoi^{\text{T}} = -\frac{1}{\sigma^2}\sum_{i=1}^n a_i \bxoi \bxoi^{\text{T}} , \\
\frac{\partial^2 l}{\partial \bbeta_0^{\text{T}} \partial \bbeta_1} 
&= \frac{1}{\sigma^2}\sum_{i=1}^n (-1 + h_{i1}+h_{i0}^2)\bxoi \bxni^{\text{T}} = -\frac{1}{\sigma^2}\sum_{i=1}^n a_i \bxoi \bxni^{\text{T}} , \\
\frac{\partial^2 l}{\partial \bbeta_1^{\text{T}} \bbeta_1} 
&= \frac{1}{\sigma^2}\sum_{i=1}^n (-1 + h_{i1}+h_{i0}^2)\bxni \bxni^{\text{T}} = -\frac{1}{\sigma^2}\sum_{i=1}^n a_i \bxni \bxni^{\text{T}} , \\
\frac{\partial^2 l}{\partial \bbeta_0 \partial \sigma} 
&= - \frac{2}{\sigma^3}\sum_{i=1}^n f_i \bxoi^{\text{T}} + \frac{1}{\sigma^2}\sum_{i=1}^n (-h_{i0} + h_{2i} + h_{i0}h_{i1})\bxoi^{\text{T}} = - \frac{2}{\sigma^3}\sum_{i=1}^n f_i \bxoi^{\text{T}} - \frac{1}{\sigma^2}\sum_{i=1}^n b_i\bxoi^{\text{T}}, \\ 
\frac{\partial^2 l}{\partial \bbeta_1 \partial \sigma} 
&= - \frac{2}{\sigma^3}\sum_{i=1}^n f_i \bxni^{\text{T}} + \frac{1}{\sigma^2}\sum_{i=1}^n (-h_{i0} + h_{2i} + h_{i0}h_{i1})\bxni^{\text{T}} = - \frac{2}{\sigma^3}\sum_{i=1}^n f_i \bxni^{\text{T}} - \frac{1}{\sigma^2}\sum_{i=1}^n b_i\bxni^{\text{T}}, \\ 
\frac{\partial^2 l}{\partial \sigma^2} 
&= \frac{n}{\sigma^2} - \frac{3}{\sigma^4}\sum_{i=1}^n f_i^2 + \frac{1}{\sigma^2}\sum_{i=1}^n (-2h_{i1} - h_{i2} + h_{i1}^2) = - \frac{3}{\sigma^4}\sum_{i=1}^n f_i^2 - \frac{1}{\sigma^2}\sum_{i=1}^n c_i .
\end{align*}

For simplicity, let $a_i(\boldsymbol\theta_0, \bbeta_0 = 0)$ be denoted by $a_i^0$, etc. Then,
\begin{align*}
I_{\bbeta_0, \bbeta_0}(\boldsymbol\theta_0, \bbeta_0 = 0) = - \E\left( \frac{\partial^2 l}{\partial \bbeta_0^{\text{T}} \bbeta_0} \right)\bigg|_{(\boldsymbol\theta_0, \bbeta_0 = 0)} = \frac{1}{\sigma^2}\sum_{i=1}^n a_i^0 \bxoi \bxoi^{\text{T}},
\end{align*}
\begin{align*}
I_{\boldsymbol\theta_0, \bbeta_0}(\boldsymbol\theta_0, \bbeta_0 = 0) &= - \begin{bmatrix}
\E\left( \frac{\partial^2 l}{\partial \alpha \bbeta_0}\right) \\ \E\left( \frac{\partial^2 l}{\partial \bbeta_1^{\text{T}} \bbeta_0} \right) &\\ \E\left( \frac{\partial^2 l}{\partial \sigma \bbeta_0}\right)
\end{bmatrix}_{(\boldsymbol\theta_0, \bbeta_0 = 0)}
= \frac{1}{\sigma^2} \begin{bmatrix}
\sum_{i=1}^n a_i^0 \bxoi^{\text{T}} \\ \sum_{i=1}^n a_i^0 \bxni \bxoi^{\text{T}} \\ \sum_{i=1}^n b_i^0\bxoi^{\text{T}}
\end{bmatrix},
\end{align*}
$I_{\bbeta_0, \boldsymbol\theta_0}(\boldsymbol\theta_0, \bbeta_0 = 0) = I_{\boldsymbol\theta_0,\bbeta_0}(\boldsymbol\theta_0, \bbeta_0 = 0)^{\text{T}}$, and
\begin{align*}
I_{\boldsymbol\theta_0, \boldsymbol\theta_0}(\boldsymbol\theta_0, \bbeta_0 = 0) &= -
\begin{bmatrix}
\E\left(\frac{\partial^2 l}{\partial \alpha^2}\right)  & \E\left(\frac{\partial^2 l}{\partial \alpha \partial \bbeta_1}\right) & \E\left(\frac{\partial^2 l}{\partial \alpha \partial \sigma}\right) \\
\E\left(\frac{\partial^2 l}{\partial \alpha \partial \bbeta_1}\right)^{\text{T}} & \E\left(\frac{\partial^2 l}{\partial \bbeta_1^{\text{T}} \bbeta_1}\right) & \E\left(\frac{\partial^2 l}{\partial \bbeta_1 \partial \sigma}\right)^{\text{T}} \\
\E\left(\frac{\partial^2 l}{\partial \alpha \partial \sigma}\right) & \E\left(\frac{\partial^2 l}{\partial \bbeta_1 \partial \sigma}\right) & \E\left(\frac{\partial^2 l}{\partial \sigma^2}\right)
\end{bmatrix}_{(\boldsymbol\theta_0, \bbeta_0 = 0)} \\
&= \frac{1}{\sigma^2} \begin{bmatrix}
\sum_{i=1}^n a_i^0 & \sum_{i=1}^n a_i^0 \bxni^{\text{T}} & \sum_{i=1}^n b_i^0 \\
\sum_{i=1}^n a_i^0 \bxni & \sum_{i=1}^n a_i^0 \bxni \bxni^{\text{T}} & \sum_{i=1}^n b_i^0\bxni \\
\sum_{i=1}^n b_i^0 & \sum_{i=1}^n b_i^0 \bxni^{\text{T}} & \sum_{i=1}^n d_i^0
\end{bmatrix}.
\end{align*}
In the last expression we have used the fact that $\E(f_i) = 0$ and $\E(f_i^{2}) = \sigma^{2}$, for example to derive $- \E\left(\frac{\partial^2 l}{\partial \sigma^2}\right) = -\E(- \frac{3}{\sigma^4}\sum_{i=1}^n f_i^2 - \frac{1}{\sigma^2}\sum_{i=1}^n c_i^{0} ) = \frac{1}{\sigma^2}\sum_{i=1}^n 3 + c_i^{0} = \frac{1}{\sigma^2}\sum_{i=1}^n d_i^{0}$. The variance $\Sigma$ of the score vector can  be obtained by evaluating these expression in $\hat{\boldsymbol\theta}_0$, the maximum likelihood estimates under the null.

For the special case when no other covariates than $\bx_0$ are included in the model, then $h_{ij}$, $a_i$, $b_i$, $c_i$ and $d_i$ evaluated in $\bxg = 0$ does not depend on the index $i$. Furthermore, the maximum likelihood estimators $\hat{\alpha}$ and $\hat{\sigma}^2$ under the null hypothesis can be found by solving
\begin{align*}
0 & = \frac{1}{\sigma^2}\sum_{i=1}^n (y_i - \alpha) + \frac{1}{\sigma}nh_0 \\
0 & = -\frac{n}{\sigma} + \frac{1}{\sigma^3}\sum_{i=1}^n(y_i - \alpha)^2 + \frac{1}{\sigma}nh_1,
\end{align*}
which yields 
\begin{align*}
\hat{\sigma}^2 &= \frac{\frac{1}{n}\sum_{i=1}^n (y_i - \bar{y})^2}{a}, \\
\hat{\alpha} &= \bar{y} + \hat{\sigma}h_0.
\end{align*}
The score vector is given by
\begin{align*}
S = \frac{1}{\hat{\sigma}^2}\sum_{i=1}^n (y_i - \hat{\alpha}) \bxoi^{\text{T}} + \frac{1}{\hat{\sigma}}h_0\sum_{i=1}^n  \bxoi^{\text{T}} = a \frac{\sum_{i=1}^n (y_i - \bar{y})\bxoi^{\text{T}}}{\frac{1}{n}\sum_{i=1}^n (y_i - \bar{y})^2} = aS^*,
\end{align*}
where $S^*$ is the score vector derived from a linear regression model for a random sample. Furthermore, the variance of the score vector becomes
\begin{align*}
\Sigma &= \frac{a}{\hat{\sigma}^2}\sum_{i=1}^n \bxoi \bxoi^{\text{T}} - \frac{1}{\hat{\sigma}^2} \sum_{i=1}^n \bxoi \begin{bmatrix}
a & a 
\end{bmatrix} \begin{bmatrix}
n a & n b \\ n b & n d 
\end{bmatrix}^{-1} \sum_{i=1}^n \bxoi^{\text{T}} \begin{bmatrix}
a \\ a 
\end{bmatrix} \\ &= \frac{a^2}{\frac{1}{n}\sum_{i=1}^n (y_i - \bar{y})^2} \sum_{i=1}^n (\bxoi-\bar{\bx_0})(\bxoi-\bar{\bx_0})^{\text{T}} = a^2 \Sigma^*,
\end{align*}
where $\Sigma^*$ is the variance of the score vector derived from a linear regression model for a random sample. Then $T = S^{\text{T}} \Sigma^{-1} S = (aS^*)^{\text{T}} (a^2\Sigma^*)^{-1} aS^* = (S^*)^{\text{T}} (\Sigma^*)^{-1} S^* = T^*$, i.e. equivalent to the score test statistic derived under the assumption that the we have a random sample and not an EPS-only sample. This confirms the finding of \cite{tang2010equivalence}.

\section{The EPS-full likelihood and tests}
We consider a regression model as in \eqref{eq:lma}. In the EPS-full design we have observed the variables $(Y_i,\bXei,\bXgi)$ for all individuals $i \in \mathcal{C}$ and $(Y_i,\bXei)$ for all individuals $i \not \in \mathcal{C}$. The genetic covariate $\bXg$ is missing at random (MAR) and we therefore derive the likelihood that ignores the missing-mechanism \citep{littlerubin2002}. The joint density of $Y_i$, $\bXei$ and $\bXgi$ is $f_{Y_i,\bXei, \bXgi}(y_i,\bxei,\bxgi)$. The marginal distribution of the observed data is defined by integrating out missing data. In our case, we have a discrete missing covariate $\bXgi$. For $i \not \in \mathcal{C}$ the variable $\bXgi$ is missing such that the marginal distribution of the observed variables is $f_{Y_i,\bXei}(y_i,\bxei;\theta) = \sum_{\bxg} f_{Y_i,\bXei,\bXgi}(y_i,\bxei,\bxg)$. The likelihood that \textit{ignores} the missing-mechanism is then
\begin{align*}
L_{ign} = & \prod_{i \in \mathcal{C}} f_{Y_i,\bXei,\bXgi}(y_i,\bxei,\bxgi) \prod_{i \not \in \mathcal{C}} \sum_{\bxg} f_{Y_i,\bXei,\bXgi}(y_i,\bxei,\bxg) \\
= & \prod_{i \in \mathcal{C}} f_{Y_i|\bXei = \bxei, \bXgi = \bxgi}(y_i) f_{\bXgi|\bXei = \bxei}(\bxgi)f_{\bXei}(\bxei)  \\ & \prod_{i \not \in \mathcal{C}}\sum_{\bxg} f_{Y_i|\bXei = \bxei, \bXgi = \bxg}(y_i) f_{\bXgi|\bXei = \bxei}(\bxg)f_{\bXei}(\bxei).
\end{align*}
We write $f_{Y_i|\bXei = \bxei, \bXgi = \bxgi}(y_i) = \frac{1}{\sigma}\phi_i$, where $\phi_i = \phi\left(\frac{y_i - \mu(\bxei, \bxgi; \alpha, \bbetae, \bbetag, \bbetaeg)}{\sigma}\right)$ and $\phi()$ is the standard normal density function. For individuals $i \not \in \mathcal{C}$, we write  $\phi_i(\bxg)$ to denote \\ $\phi\left(\frac{y_i - \mu(\bxei, \bxg; \alpha, \bbetae, \bbetag, \bbetaeg)}{\sigma}\right)$. We assume that the distribution of $\bXg$ depends on a subset of $\bXe$, denoted $\bX$, and that the sample space of $\bX$ is discrete and of size $J$. Furthermore, we assume that $f_{\bXgi|\bX_i = \bx_j}(\bxg) = f_{\bXg|\bX = \bx_j}(\bxg)$, i.e. that the distribution of genotypes is the same for all individuals with equal value of $\bX$. We assume that the sample space of $\bXg$ is discrete and of size $K$. The likelihood is then
\begin{align*}
L \propto & \prod_{i \in \mathcal{C}} \frac{1}{\sigma} \phi_i \sum_{j=1}^J f_{\bXg|\bX = \bx_j}(\bxgi)I(\bx_i=\bx_j) \prod_{i \not \in \mathcal{C}} \sum_{k=1}^K \frac{1}{\sigma} \phi_i(\bxgk) \sum_{j=1}^J f_{\bXg|\bX = \bx_j}(\bxgk)I(\bx_i=\bx_j)
\end{align*}
The log-likelihood for the EPS-full model can then be written as
\begin{align*}
l \propto & - N\log(\sigma) + \sum_{i \in \mathcal{C}} \left( \log\left( \phi_i \right) + \sum_{j=1}^J \log\left( f_{\bXg|\bX = \bx_j}(\bxgi)\right) I(\bx_i = \bx_j) \right) \\ &+ \sum_{i \not \in \mathcal{C}} \log\left( \sum_{j=1}^J \sum_{k=1}^{K} \phi_i(\bxgk) f_{\bXg|\bX = \bx_j}(\bxgk)  I(\bx_i = \bx_j) \right)
\end{align*}

\subsection{The score test}
We derive the score test for the two-sided null hypothesis $H_0: \bbetag = 0$ in the model $y = \alpha + \bxe^{\text{T}} \bbetae + \bxg^{\text{T}} \bbetag + \varepsilon $. In this case, the model under the null is simply a linear regression model that can be fitted with standard methods due to the complete sampling of $(y_i, \bxei)$. In our work with common genetic variants, we consider genetic variants that can take three possible values (0, 1 or 2). To obtain a more general result we consider a discrete sample space of $\bXg$ of size $K$. We then have $P(\bXg = \bxgk|\bX = \bx_j) = p_{jk}$ for $k = 1,\ldots,K-1$ and $P(\bXg = \bxgK|\bX = \bx_j) = 1-\sum_{k=1}^{K-1}p_{jk}$, for $j = 1, \ldots, J$. Let $\boldsymbol\theta_0$ denote all parameters in the null model ($\alpha$, $\bbetae$, $\sigma$, and $p_{jk}$, $j=1,\ldots,J$, $k=1,\ldots,K-1$). For simplified notation, we write $f_i = y_i - \alpha - \bxei^{\text{T}} \bbetae - \bxgi^{\text{T}} \bbetag$ and $f_i(\bxg) = y_i - \alpha - \bxei^{\text{T}} \bbetae - \bxg^{\text{T}} \bbetag$. We write $\phi_i(\bxg)$ to denote $\phi(f_i(\bxg)/\sigma)$. We define
\begin{align*}
h_{iab} = \frac{\sum_j \sum_{k} f_i(\bxgk)^a \phi_i(\bxgk) f_{\bXg|\bX = \bx_j}(\bxgk) \bxgk^b I(\bx_i = \bx_j)}{\sum_j \sum_{k}\phi_i(\bxgk) f_{\bXg|\bX = \bx_j}(\bxgk) I(\bx_i = \bx_j)},
\end{align*}
where $\bxgk^0 = 1$, $\bxgk^1 = \bxgk^{\text{T}}$ and $\bxgk^2 = \bxgk \bxgk^{\text{T}}$. We also define
\begin{align*}
h_{iab}^{(j'k')} = \frac{ \left(f_i(\bxgkk)^a \phi_i(\bxgkk) \bxgkk^b - f_i(\bxgK)^a \phi_i(\bxgK) \bxgK^b \right)I(\bx_i = \bx_{j'})}{\sum_j \sum_{k}\phi_i(\bxgk) f_{\bXg|\bX = \bx_j}(\bxgk) I(\bx_i = \bx_j)}.
\end{align*}
Note that $h_{i00} = 1$. Furthermore, we will use that
\begin{align*}
h_{iab}(\bbetag = 0) &= \frac{\sum_j \sum_{k} f_i(\bbetag = 0)^a \phi_i(\bbetag = 0) f_{\bXg|\bX = \bx_j}(\bxgk) \bxgk^b I(\bx_i = \bx_j)}{\sum_j \sum_{k}\phi_i(\bbetag = 0) f_{\bXg|\bX = \bx_j}(\bxgk) I(\bx_i = \bx_j)} \\
& = \sum_j f_i(\bbetag = 0)^a \left( \sum_{k} f_{\bXg|\bX = \bx_j}(\bxgk) \bxgk^b \right) I(\bx_i = \bx_j) \\
& = \sum_j f_i(\bbetag = 0)^a \E(\bXg^b|\bX = \bx_j) I(\bx_i = \bx_j)
\end{align*}
Let $n_{jk}$ denote the number of individuals $i \in \mathcal{C}$ for which $\bx_i = \bx_{j}$ and $\bxgi = \bxgk$. 

The first derivatives of the log-likelihood are
\begin{align*}
\frac{\partial l}{\partial \alpha} 
&= \frac{1}{\sigma^2}\sum_{i \in \mathcal{C}} f_i + \frac{1}{\sigma^2} \sum_{i \not \in \mathcal{C}} h_{i10}, \\
\frac{\partial l}{\partial \bbetae} 
&= \frac{1}{\sigma^2}\sum_{i \in \mathcal{C}} f_i \bxei^{\text{T}} + \frac{1}{\sigma^2} \sum_{i \not \in \mathcal{C}} h_{i10} \bxei^{\text{T}}, \\
\frac{\partial l}{\partial \bbetag} 
&= \frac{1}{\sigma^2}\sum_{i \in \mathcal{C}} f_i \bxgi^{\text{T}} + \frac{1}{\sigma^2} \sum_{i \not \in \mathcal{C}} h_{i11}, \\
\frac{\partial l}{\partial \sigma} 
&= -\frac{N}{\sigma} + \frac{1}{\sigma^3}\sum_{i \in \mathcal{C}}f_i^2  + \frac{1}{\sigma^3}\sum_{i \not \in \mathcal{C}} h_{i20}, \\
\frac{\partial l}{\partial p_{j'k'}} 
&=  \frac{n_{j'k'}}{p_{j'k'}} - \frac{n_{j' K}}{1- \sum_{k=1}^{K-1}p_{j'k}}   + \sum_{i \not \in \mathcal{C}} h_{i00}^{j'k'} \text{, for } j'=1,\ldots,J \text{, } k' = 1,\ldots,K-1,
\end{align*}
and the score vector is then given by 
\begin{align*}
S & = \frac{\partial l}{\partial \bbetag} (\hat{\boldsymbol\theta}_0, \bbetag = 0) \\
&= \frac{1}{\sigma^2}\sum_{i \in \mathcal{C}} f_i(\hat{\boldsymbol\theta}_0, \bbetag = 0) \bxgi^{\text{T}} + \frac{1}{\sigma^2} \sum_{i \not \in \mathcal{C}} h_{i11}(\hat{\boldsymbol\theta}_0, \bbetag = 0) \\
&= \frac{1}{\sigma^2}\sum_{i \in \mathcal{C}} f_i(\hat{\boldsymbol\theta}_0, \bbetag = 0) \bxgi^{\text{T}} + \frac{1}{\sigma^2} \sum_{i \not \in \mathcal{C}} f_i(\hat{\boldsymbol\theta}_0, \bbetag = 0) \sum_{j=1}^J E(\bXg|\bX = \bx_j) I(\bx_i = \bx_j).
\end{align*}
Below we have calculated the second derivatives of the log-likelihood and evaluated these under the null hypothesis ($\bbetag=0$). The second derivatives are
\begin{align*}
\frac{\partial^2 l}{\partial \alpha^2} 
&= \frac{1}{\sigma^2}\sum_{i \in \mathcal{C}} (-1) + \frac{1}{\sigma^2} \sum_{i \not \in \mathcal{C}} \left( -h_{i00} + \frac{1}{\sigma^2} h_{i20} - \frac{1}{\sigma^2} h_{i10}^2 \right)\\
& =  -\frac{N}{\sigma^2} + \frac{1}{\sigma^4} \sum_{i \not \in \mathcal{C}} (h_{i20} - h_{i10}^2)  \xlongequal{\bbetag = 0} -\frac{N}{\sigma^2} \\
\frac{\partial^2 l}{\partial \alpha \partial \bbetae} 
&= \frac{1}{\sigma^2}\sum_{i \in \mathcal{C}} (-\bxei^{\text{T}}) + \frac{1}{\sigma^2} \sum_{i \not \in \mathcal{C}} \left( -h_{i00}\bxei^{\text{T}} + \frac{1}{\sigma^2} h_{i20}\bxei^{\text{T}} - \frac{1}{\sigma^2} h_{i10}^2 \bxei^{\text{T}} \right) \\
&=  -\frac{1}{\sigma^2} \sum_{i=1}^N \bxei^{\text{T}} + \frac{1}{\sigma^4} \sum_{i \not \in \mathcal{C}} (h_{i20} - h_{i10}^2)\bxei^{\text{T}} \xlongequal{\bbetag = 0} -\frac{1}{\sigma^2} \sum_{i=1}^N \bxei^{\text{T}} \\
\frac{\partial^2 l}{\partial \alpha \partial \bbetag} 
&= \frac{1}{\sigma^2}\sum_{i \in \mathcal{C}} (-\bxgi^{\text{T}}) + \frac{1}{\sigma^2} \sum_{i \not \in \mathcal{C}} \left( -h_{i01} + \frac{1}{\sigma^2} h_{i21} - \frac{1}{\sigma^2} h_{i10}h_{i11} \right) \\
&= - \frac{1}{\sigma^2}\sum_{i \in \mathcal{C}} \bxgi^{\text{T}} + \frac{1}{\sigma^2} \sum_{i \not \in \mathcal{C}}  -h_{i01}  +  \frac{1}{\sigma^4} \sum_{i \not \in \mathcal{C}} \left(h_{i21} - h_{i10}h_{i11} \right) \\ & \xlongequal{\bbetag = 0} - \frac{1}{\sigma^2}\sum_{i \in \mathcal{C}} \bxgi^{\text{T}} - \frac{1}{\sigma^2}\sum_{i \in \mathcal{C}}\sum_{j=1}^J E(\bXg|\bXe = \bxej)^{\text{T}} I(\bx_i = \bx_j)  \\
\frac{\partial^2 l}{\partial \alpha \partial \sigma} 
&= -\frac{2}{\sigma^3}\sum_{i \in \mathcal{C}} f_i -2 \frac{1}{\sigma^3} \sum_{i \not \in \mathcal{C}} h_{i10} +  \frac{1}{\sigma^5} \sum_{i \not \in \mathcal{C}} \left( \frac{1}{\sigma^3}h_{i30} - h_{i20}h_{i10} \right) \xlongequal{\bbetag = 0}  -\frac{2}{\sigma^3}\sum_{i =1}^N f_i(0) \\
\frac{\partial^2 l}{\partial \alpha \partial p_{j'k'}} 
&= \frac{1}{\sigma^2} \sum_{i \not \in \mathcal{C}} \left( h_{i10}^{(j'k')} - h_{i00}^{(j'k')}h_{i10} \right) \xlongequal{\bbetag = 0}  0 \\
\frac{\partial^2 l}{\partial \bbetae^{\text{T}} \bbetae} 
&= \frac{1}{\sigma^2}\sum_{i \in \mathcal{C}} (-\bxei \bxei^{\text{T}}) + \frac{1}{\sigma^2} \sum_{i \not \in \mathcal{C}} \bxei \left( -h_{i00} \bxei^{\text{T}} + \frac{1}{\sigma^2} h_{i20}\bxei^{\text{T}} - \frac{1}{\sigma^2} h_{i10}^2 \bxei^{\text{T}} \right)\\
&= -\frac{1}{\sigma^2}\sum_{i =1}^N \bxei \bxei^{\text{T}} + \frac{1}{\sigma^4} \sum_{i \not \in \mathcal{C}} \left( h_{i20} - h_{i10}^2 \right)\bxei \bxei^{\text{T}} \xlongequal{\bbetag = 0} -\frac{1}{\sigma^2}\sum_{i =1}^N \bxei \bxei^{\text{T}} \\
\frac{\partial^2 l}{\partial \bbetae^{\text{T}} \bbetag} 
&= \frac{1}{\sigma^2}\sum_{i \in \mathcal{C}} (-\bxei \bxgi^{\text{T}}) + \frac{1}{\sigma^2} \sum_{i \not \in \mathcal{C}} \bxei \left( -h_{i01}
 + \frac{1}{\sigma^2} h_{i21} - \frac{1}{\sigma^2} h_{i10}h_{i11} \right)\\
 &= -\frac{1}{\sigma^2}\sum_{i \in \mathcal{C}} \bxei \bxgi^{\text{T}} - \frac{1}{\sigma^2} \sum_{i \not \in \mathcal{C}} \bxei h_{i01} + \frac{1}{\sigma^4} \sum_{i \not \in \mathcal{C}} \bxei\left(h_{i21} - h_{i10}h_{i11} \right) \\ & \xlongequal{\bbetag = 0} -\frac{1}{\sigma^2}\sum_{i \in \mathcal{C}} \bxei \bxgi^{\text{T}} - \frac{1}{\sigma^2} \sum_{i \not \in \mathcal{C}} \bxei \sum_{j=1}^J E(\bXg|\bXe = \bxej)^{\text{T}} I(\bx_i = \bx_j)  \\ 
\frac{\partial^2 l}{\partial \bbetae^{\text{T}} \sigma} 
&= -\frac{2}{\sigma^3}\sum_{i \in \mathcal{C}} f_i \bxei^{\text{T}} - \frac{2}{\sigma^3} \sum_{i \not \in \mathcal{C}} h_{i10} \bxei^{\text{T}} +  \frac{1}{\sigma^5} \sum_{i \not \in \mathcal{C}} \left(h_{i30}\bxei^{\text{T}} - h_{i20}h_{i10}\bxei^{\text{T}}  \right) \\
& \xlongequal{\bbetag = 0} -\frac{2}{\sigma^3}\sum_{i=1}^N f_i(0) \bxei^{\text{T}} \\
\frac{\partial^2 l}{\partial \bbetae p_{j'k'}} 
&= \frac{1}{\sigma^2} \sum_{i \not \in \mathcal{C}} h_{i10}^{j'k'}\bxei^{\text{T}} - h_{i00}^{j'k'}h_{i10}\bxei^{\text{T}} \xlongequal{\bbetag = 0} \mathbf{0}_{1 \times |\bxe|} \\
\frac{\partial^2 l}{\partial \bbetag^{\text{T}} \bbetag} 
&= \frac{1}{\sigma^2}\sum_{i \in \mathcal{C}} (-\bxgi \bxgi^{\text{T}}) + \frac{1}{\sigma^2} \sum_{i \not \in \mathcal{C}} \left( -h_{i02} + \frac{1}{\sigma^2}h_{i22} - \frac{1}{\sigma^2} h_{i11}^{\text{T}} h_{i11} \right)\\
&= -\frac{1}{\sigma^2}\sum_{i \in \mathcal{C}} \bxgi \bxgi^{\text{T}} - \frac{1}{\sigma^2} \sum_{i \not \in \mathcal{C}} h_{i02}  + \frac{1}{\sigma^4} \sum_{i \not \in \mathcal{C}} \left( h_{i22} - h_{i11}^{\text{T}} h_{i11}  \right)\\
& \xlongequal{\bbetag = 0} -\frac{1}{\sigma^2}\sum_{i \in \mathcal{C}} \bxgi \bxgi^{\text{T}} - \frac{1}{\sigma^2} \sum_{i \not \in \mathcal{C}} \sum_{j=1}^J E(\bXg \bXg^{\text{T}}|\bX = \bx_j)I(\bx_i=\bx_j) 
\\ &+ \frac{1}{\sigma^4} \sum_{i \not \in \mathcal{C}} f_i(0)^2 \sum_{j=1}^J \left( E(\bXg \bXg^{\text{T}}|\bX = \bx_j) - E(\bXg|\bX=\bx_j)E(\bXg|\bX=\bx_j)^{\text{T}} \right)I(\bx_i=\bx_j)  \\
& = -\frac{1}{\sigma^2}\sum_{i \in \mathcal{C}} \bxgi \bxgi^{\text{T}} - \frac{1}{\sigma^2} \sum_{i \not \in \mathcal{C}} \sum_{j=1}^J E(\bXg \bXg^{\text{T}}|\bX = \bx_j)I(\bx_i=\bx_j) 
\\ & + \frac{1}{\sigma^4} \sum_{i \not \in \mathcal{C}} \sum_{j=1}^J  f_i(0)^2 \text{Var}(\bXg|\bX = \bx_j) I(\bx_i=\bx_j) 
\\
\frac{\partial^2 l}{\partial \bbetag \sigma}
&= -\frac{2}{\sigma^3}\sum_{i \in \mathcal{C}} f_i \bxgi^{\text{T}} - \frac{2}{\sigma^3} \sum_{i \not \in \mathcal{C}} h_{i11} +  \frac{1}{\sigma^5} \sum_{i \not \in \mathcal{C}} \left( h_{i31} - h_{i20} h_{i11} \right) \\
& \xlongequal{\bbetag = 0} -\frac{2}{\sigma^3}\sum_{i \in \mathcal{C}} f_i(0) \bxgi^{\text{T}} - \frac{2}{\sigma^3} \sum_{i \not \in \mathcal{C}} f_i(0) \sum_{j=1}^J E(\bXg|\bXe = \bxej)^{\text{T}} I(\bx_i = \bx_j) 
\\
\frac{\partial^2 l}{\partial \bbetag p_{j'k'}} 
&= \frac{1}{\sigma^2} \sum_{i \not \in \mathcal{C}} h_{i11}^{j'k'} - h_{i00}^{j'k'} h_{i11}  \xlongequal{\bbetag = 0} \frac{1}{\sigma^2} \sum_{i \not \in \mathcal{C}} f_i(0)(\bxgkk^{\text{T}} - \bxgK^{\text{T}})I(\bx_i = \bx_{j'})\\
\frac{\partial^2 l}{\partial \sigma^2}
&= \frac{N}{\sigma^2} - \frac{3}{\sigma^4} \sum_{i \in \mathcal{C}}f_i^2 - \frac{3}{\sigma^4} \sum_{i \not \in \mathcal{C}} h_{i20} + \frac{1}{\sigma^6}\sum_{i \not \in \mathcal{C}} \left( h_{i40} - h_{i20}^2  \right)  \xlongequal{\bbetag = 0} \frac{N}{\sigma^2} - \frac{3}{\sigma^4} \sum_{i =1}^N f_i(0)^2  
\\
\frac{\partial^2 l}{\partial \sigma p_{j'k'}} 
&= \frac{1}{\sigma^3} \sum_{i \not \in \mathcal{C}} h_{i20}^{j'k'} - h_{i00}^{j'k'} h_{i20}  \xlongequal{\bbetag = 0}  0 \\
\frac{\partial^2 l}{\partial p_{j'k'}^2} & = 
- \frac{n_{j'k'}}{p_{j'k'}^2} + \frac{n_{j' K}}{\left(1- \sum_{k=1}^{K-1}p_{j'k}\right)^2}  - \sum_{i \not \in \mathcal{C}} (h_{i00}^{j'k'})^2  \xlongequal{\bbetag = 0} - \frac{n_{j'k'}}{p_{j'k'}^2} + \frac{n_{j' K}}{\left(1- \sum_{k=1}^{K-1}p_{j'k}\right)^2} \\
\frac{\partial^2 l}{\partial p_{j'k''} p_{j'k'}} & = 
 \frac{n_{j' K}}{\left(1- \sum_{k=1}^{K-1}p_{j'k}\right)^2}  - \sum_{i \not \in \mathcal{C}} h_{i00}^{j'k'} h_{i00}^{j'k''}  \xlongequal{\bbetag = 0} \frac{n_{j' K}}{\left(1- \sum_{k=1}^{K-1}p_{j'k}\right)^2} \\
\frac{\partial^2 l}{\partial p_{j''k'} p_{j'k'}} & = \frac{\partial^2 l}{\partial p_{j''k''} p_{j'k'}} =  0.
\end{align*}

To simplify notation, let $\mathbf{p}_j = (p_{j1}, \ldots, p_{jK-1})$, $j=1,\ldots,J$, and let $\mathbf{p} = (\mathbf{p}_1, \ldots, \mathbf{p}_J)$ be a vector of length $J(K-1)$. We then write
\begin{align*}
\frac{\partial^2 l}{\partial \bbetag^{\text{T}} \mathbf{p}} = 
\begin{bmatrix}
\frac{\partial^2 l}{\partial \bbetag^{\text{T}} \mathbf{p}_1} & \ldots & \frac{\partial^2 l}{\partial \bbetag^{\text{T}} \mathbf{p}_J}
\end{bmatrix},
\end{align*}
and similarly for other derivatives with respect to $p_{jk}$. We let $\boldsymbol\theta_0'$ denote the parameters $(\alpha, \bbetae, \sigma)$. We use the observed information matrix (evaluated in $\hat{\boldsymbol\theta}_0$ and $\bbetag = 0$) as an estimate for the true information matrix under the null, i.e $\hat{I} = I_o(\hat{\boldsymbol\theta}_0, \bbetag= 0)$.  An estimate of the variance of $S$ is then given by
\begin{align*}
\hat{\Sigma} = \hat{I}_{\bbetag, \bbetag} - \hat{I}_{\bbetag, \theta_0}\hat{I}_{\theta_0, \theta_0}^{-1} \hat{I}_{\theta_0,\bbetag},
\end{align*}
where
\begin{align*}
\hat{I}_{\bbetag, \bbetag} & = -\frac{\partial^2 l}{\partial \bbetag^{\text{T}} \bbetag}\bigg|_{(\hat{\boldsymbol\theta}_0, \bbetag= 0)}
\end{align*}
\begin{align*}
\hat{I}_{\boldsymbol\theta_0, \bbetag} &= - \begin{bmatrix}
 \frac{\partial^2 l}{\partial \alpha \bbetag} \\
\frac{\partial^2 l}{\partial \bbetae^{\text{T}} \bbetag } \\
\frac{\partial^2 l}{\partial \sigma \bbetag} \\
\frac{\partial^2 l}{\partial \mathbf{p}^{\text{T}} \bbetag}
\end{bmatrix}_{(\hat{\boldsymbol\theta}_0, \bbetag= 0)} = 
\begin{bmatrix}
\hat{I}_{\boldsymbol\theta_0',\bbetag} \\
\hat{I}_{\mathbf{p},\bbetag}
\end{bmatrix},
\end{align*}

\begin{align*}
\hat{I}_{\boldsymbol\theta_0, \boldsymbol\theta_0} & = -
\begin{bmatrix}
\frac{\partial^2 l}{\partial \alpha^2}  & \frac{\partial^2 l}{\partial \alpha \partial \bbetae} & \frac{\partial^2 l}{\partial \alpha \partial \sigma} & \frac{\partial^2 l}{\partial \alpha \partial \mathbf{p}} \\
\frac{\partial^2 l}{\partial \bbetae^{\text{T}} \partial \alpha} & \frac{\partial^2 l}{\partial \bbetae^{\text{T}} \bbetae} & \frac{\partial^2 l}{\partial \bbetae^{\text{T}} \partial \sigma} & \frac{\partial^2 l}{\partial \bbetae^{\text{T}} \partial \mathbf{p}}\\
\frac{\partial^2 l}{\partial \alpha \partial \sigma} & \frac{\partial^2 l}{\partial \sigma \partial \bbetae} & \frac{\partial^2 l}{\partial \sigma^2}& \frac{\partial^2 l}{\partial \sigma \partial \mathbf{p}} \\
\frac{\partial^2 l}{\partial \mathbf{p}^{\text{T}} \partial \alpha } & \frac{\partial^2 l}{ \partial \mathbf{p}^{\text{T}} \partial \bbetae} & \frac{\partial^2 l}{ \partial \mathbf{p}^{\text{T}} \partial \sigma} & \frac{\partial^2 l}{\partial \mathbf{p}^{\text{T}} \mathbf{p}}
\end{bmatrix}_{(\hat{\boldsymbol\theta}_0, \bbetag= 0)}
= \begin{bmatrix}
\hat{I}_{\boldsymbol\theta_0', \boldsymbol\theta_0'} & \mathbf{0} \\
\mathbf{0} & \hat{I}_{\mathbf{p}, \mathbf{p}}
\end{bmatrix}.
\end{align*}
The variance estimate $\hat{\Sigma}$ can then be written as
\begin{align*}
\hat{\Sigma} = \hat{I}_{\bbetag, \bbetag} - \hat{I}_{\bbetag, \theta_0'} \hat{I}_{\theta_0', \theta_0'}^{-1} \hat{I}_{\theta_0',\bbetag} - \hat{I}_{\bbetag,\mathbf{p}} \hat{I}_{\mathbf{p}, \mathbf{p}}^{-1} \hat{I}_{\mathbf{p},\bbetag},
\end{align*}

Under the null, the estimates of $p_{jk}$ are simply $\hat{p}_{jk} = n_{jk}/N_j$ where $N_j$ is the number of individuals with $\bx_i = \bx_j$, and estimates of $\boldsymbol\theta_0'$ can be found by fitting a linear regression model to the completely observed data ($Y = \alpha + \bxe^{\text{T}} \bbetae + \varepsilon$) using standard methods. A recent paper by \cite{derkach2015score} presents a further simplified expression for $\hat{\Sigma}$. We present the calculations used to come to their expression here. We have
\begin{align*}
\hat{I}_{\mathbf{p}, \mathbf{p}} = 
\text{Diag}(A_1, \ldots, A_J),
\end{align*}
where each block matrix $A_{j}$, $j = 1, \ldots, J$ is a $(K-1) \times (K-1)$ matrix with diagonal elements
\begin{align*}
(A_j)_{kk} = \frac{n_{jk}}{\hat{p}_{jk}^2} - \frac{n_{jK}}{\left( 1- \sum_{k'=1}^{K-1} \hat{p}_{jk'} \right)^2}
\end{align*}
and off-diagonal elements
\begin{align*}
(A_j)_{kk'} = - \frac{n_{jK}}{\left( 1- \sum_{k=1}^{K-1} \hat{p}_{jk} \right)^2} \text{ , } k \neq k' \text{, and } k, k' = 1,\ldots, K-1.
\end{align*}
Note that $A_j$ equals the observed information matrix for the multinomially distributed variable $\bXg|\bX = \bx_j \sim \text(p_{j1},\ldots,p_{jK};N_j)$, where $p_{jK} = 1 - \sum_{k=1}^{K-1}p_{jk}$. The inverse of this matrix ($A_j^{-1}$) is then the estimated covariance matrix for the maximum likelihood estimators of the parameters $p_{j1}, \ldots, p_{j(K-1)}$. The MLE for $p_{jk}$ is $\hat{p}_{jk} = N_{jk}/N_j$, where $N_{jk}$ is the number of outcomes of type $k$ of $N_j$ trials. Then,
\begin{align*}
(A_j^{-1})_{kk} &= \text{Var}(\hat{p}_{jk}) = \frac{1}{N_j^2} \text{Var}(N_{jk}) = \frac{1}{N_j} p_{jk}(1-p_{jk}) \\
(A_j^{-1})_{kk'} & = \text{Covar}(\hat{p}_{jk},\hat{p}_{jk'}) = \frac{1}{N_j^2} \text{Covar}(N_{jk},N_{jk'}) = \frac{1}{N_j} p_{jk}p_{jk'}
\end{align*}
for $k\neq k'$ and $k,k' = 1,\ldots,K-1$. Then
\begin{align*}
\hat{I}_{\bbetag, \mathbf{p}} \hat{I}_{\mathbf{p}, \mathbf{p}}^{-1} \hat{I}_{\mathbf{p},\bbetag} & = 
\begin{bmatrix}
\frac{\partial^2 l}{\partial \bbetag^{\text{T}} \mathbf{p}_1 \bbetag} &
\cdots & 
\frac{\partial^2 l}{\partial \bbetag^{\text{T}} \mathbf{p}_J \bbetag}
\end{bmatrix}_{(\hat{\boldsymbol\theta}_0, \bbetag= 0)}
\text{Diag}(A_1^{-1}, \ldots, A_J^{-1}) 
\begin{bmatrix}
\frac{\partial^2 l}{\partial \mathbf{p}_1^{\text{T}} \bbetag} \\
\vdots \\
\frac{\partial^2 l}{\partial \mathbf{p}_J^{\text{T}} \bbetag}
\end{bmatrix}_{(\hat{\boldsymbol\theta}_0, \bbetag= 0)}  \\
& = \sum_{j = 1}^{J} \left(\sum_{i \not \in \mathcal{C}} f_i(\hat{\boldsymbol\theta}_0, \bbetag= 0)I(\bx_i = \bx_j)\right)^2 \boldsymbol\alpha_j^{\text{T}} A_j^{-1} \boldsymbol\alpha_j \\
& = \sum_{j = 1}^{J} \left(\sum_{i \not \in \mathcal{C}} f_i(\hat{\boldsymbol\theta}_0, \bbetag= 0)I(\bx_i = \bx_j)\right)^2 \text{Var}(\bXg|\bX = \bx_j)
\end{align*}
where $(\boldsymbol\alpha_j)_k = - (\bxgk^{\text{T}} - \bxgK^{\text{T}})$, $k=1,\ldots,K-1$, and we have used the fact that $\boldsymbol\alpha_j^{\text{T}} A_j^{-1} \boldsymbol\alpha_j = \text{Var}(\bXg|\bX = \bx_j)$. Under $H_0$, $\text{Var}(\bXg|\bX = \bx_j)$ can be estimated by the sample variance of complete observations $i \in \mathcal{C}$.

\subsection{The likelihood ratio test}
For the EPS-full design, we use the likelihood ratio test for testing for gene-environment interactions $H_0: \bbetaeg = 0$. Let $\hat{\boldsymbol\theta}$ denote the maximum likelihood estimator of the parameters $\alpha, \bbetae, \bbetag, \bbetaeg, \sigma$ under the alternative hypothesis, and let $\hat{\boldsymbol\theta}_0$ denote the corresponding maximum likelihood estimator under the null model ($\bbetaeg = 0$). Note that under both the null and the alternative, the MLEs must be found by optimizing the EPS-full log-likelihood. The likelihood ratio test statistic is then given by
\begin{align*}
\lambda = 2(l(\hat{\boldsymbol\theta}) - l(\hat{\boldsymbol\theta}_0)),
\end{align*}
and under the null hypothesis $\lambda$ is $\chi^2$-distributed.

\section{Confounding effects in the HUNT data}
We use the $\text{VO}_2$ data to illustrate the issue of confounding effects in the EPS-full likelihood specifically, and association studies in general. This also highlights the importance of developing models and tests for genetic effects where non-genetic environmental covariates $\bxe$ are included. In the $\text{VO}_2$ data, we observed confounding effects between genotypes of some SNPs, sex and $\text{VO}_2$. Some of the SNPs in the data have genotypes that are highly correlated with sex. Sex has a strong effect on $\text{VO}_2$, so that spurious results were found when sex was not included as a covariate in the regression model. The effect of sex on oxygen uptake was then mediated through the genetic variants. We have illustrated this in Figure \ref{fig:manhattan2}. The plot Full 1 shows the $-\log_{10}(p)$-values for the full sample when the covariate $x_{\text{sex}}$ was not included in the model, while the plot Full 2 shows the results when $x_{\text{sex}}$ was included. The 30 smallest $p$-values for the first model are marked red in both plots. We see that including sex as a covariate removed the false positives due to confounding. In Figure \ref{fig:manhattan2} we have also presented results for the EPS-full model when $x_{\text{sex}}$ was not included as a covariate and $X_g$ was not assumed dependent upon $x_{\text{sex}}$ (EPS-full 1), the EPS-full model when $x_{\text{sex}}$ was included as a covariate but $X_g$ was not assumed dependent upon $x_{\text{sex}}$ (EPS-full 2) and finally when confounding effects were accounted for by including $x_{\text{sex}}$ as a covariate and letting the distribution of $X_g$ be different between men and women (EPS-full 3). The top 30 findings in the first EPS-full model are marked blue in all three plots. We see that the EPS-full model is more sensitive to false positives due to confounding, compared to the full model. In EPS-full confounding must be accounted for both as a covariate in the linear model, as well as in the (unknown) distribution of the genetic variants.

\begin{figure}[h]
	\centering
	\includegraphics[width = 0.9\textwidth]{manhattan22.png}
	\caption{Manhattan-plots for testing each SNP in chromosome 1 against $\text{VO}_2$ for the full model (all $N$ study participants analyzed) without including sex as a covariate in the regression model (Full 1) and when including sex as a covariate in the regression model (Full 2). Red points illustrate top 30 findings in Full 1. Manhattan-plots for the same test in the EPS-full setting ($N$ participants analyzed of which only $n=N/2$ most extreme participants had observed genotypes) without including sex as a covariate in the regression model or accounting for sex in models of genotype distributions (EPS-full 1), including sex as a covariate in the regression model but not accounting for sex in models of genotype distributions (EPS-full 2), and including sex as a covariate in the regression model and accounting for sex in models of genotype distributions (EPS-full 3). Blue points illustrate top 30 findings in EPS-full 1.}
	\label{fig:manhattan2}
\end{figure}


\section{Power estimates under MCAR}
Under the assumption that $n$ out of $N$ individuals in a population can be genotyped, a reasonable alternative to extreme sampling is random sampling. Throughout this paper, we compared the power of different tests for both extreme and random samples with equally many genotyped individuals. We considered random samples of size $n$ where no information was known for the $N-n$ individuals that were not sampled for genotyping. However, as with extreme samples, random samples (RS) can also come in two types. We refer to these as RS-only and RS-full. The RS-only sample consists of observations $(y_i,\bxei,\bxgi)$ for the genotyped individuals that were randomly chosen, as used in our analysis. This sample can be analyzed with standard methods for linear regression due to the MCAR (missing completely at random) criterion. The RS-complete sample consists of observations $(y_i,\bxei,\bxgi)$ for the randomly chosen genotyped individuals, as well as observations $(y_i,\bxei)$ for the remaining individuals. For this sample the EPS-full likelihood can be used for model inference. 

In section 5.1 of the main paper we considered extreme exposure sampling. Then the interest was on the interaction term $x_{\text{e}2} x_{\text{g}}$ and $n$ individuals were genotyped due to extreme values of $x_{\text{e}2}$. Since $x_{\text{e}2}$ can be regarded as a constant for the models considered here, the missing-mechanism is also here MCAR. Therefore, the following discussion can explain why the EES-full design was found to be only slightly better than the EES-only design.

\citet{white2010bias} considered a regression model with several covariates, where one covariate ($X_1$) had a MCAR structure. They considered the difference between a complete case (CC) approach (only individuals with observations of $X_1$ were analyzed) and a maximum likelihood (ML) approach that took into account all available information. They showed that for estimating the coefficient $\beta_1$ of $X_1$, the variance in the estimate $\hat{\beta}_1$ was lower in the ML setting than in the CC setting, when the partial correlation of $Y$ and $X_1$ given all other covariates was nonzero, i.e. when the covariate $X_1$ was independently associated with $Y$. Then the power to detect a non-zero effect of some SNP should be greater in an RS-full sample than in an RS-only sample. We evaluated this in our main-effects simulation model \eqref{eq:sim1}. Using the same simulation set-ups as in the main paper, the power estimates for RS-only and RS-complete were almost identical, as shown in Table \ref{tab:simRS}. The RS-full method was only marginally more powerful than the RS-only method. Recall that the genetic effect size ($\beta_{\text{g}}$) was chosen to be very low in our simulations. For a particular data set simulated from model \eqref{eq:sim1}, the partial correlation of $y$ and $x_{\text{g}}$, given $x_{\text{e}1}$ and $x_{\text{e}2}$ was $0.05$, while the partial correlation of $y$ and $x_{\text{e}1}$, given $x_{\text{g}}$ and $x_{\text{e}2}$ was $0.64$. Because the partial correlation of the genetic variant and the response is close to zero, by the results of \citet{white2010bias} the difference between a method that only includes the complete cases (RS-only) and a method that includes all cases (RS-complete) is negligible. To check this results in real data, we test the $\text{VO}_2$-data using the RS-only and RS-full methods. The corresponding Manhattan plots are presented in Figure \ref{fig:manhattanRS}. We see that the results from RS-full method are very similar to the RS-only method.

\begin{table}
\centering
	\begin{tabular}{c|cc|c|cc|c|cc}
	\hline
$\beta_g$ & RS-only & RS-complete & $\sigma$ & RS-only & RS-complete & $q$ & RS-only & RS-complete \\
	\hline
 0.3 &  36.18 & 36.26  & 6&  76.43& 76.51 & 0.1&  42.16& 42.28 \\
 0.5 &  76.03 & 76.08  & 8&  51.82& 51.92 & 0.2&  65.61& 65.73 \\
 0.7 &  96.34 & 96.37  & 10&  36.16& 36.20& 0.3&  76.09& 76.14 \\
	\hline
	\end{tabular}
	\caption{Estimated power to detect a non-null genetic effect ($H_0: \beta_{\text{g}} = 0$) in simulation model \eqref{eq:sim1} for different values of $\beta_g$, $\sigma$ and minor allele frequency $q$.}
	\label{tab:simRS}
\end{table}

\begin{figure}
	\centering
	\includegraphics[width = 0.5\textwidth]{manhattan3.png}
	\caption{Manhattan-plot for testing each SNP in chromosome 1 against $\text{VO}_2$ in two random sampling models; RS-only ($n = N/2$ randomly drawn participants analyzed), and RS-full ($N$ participants analyzed of which only $n=N/2$ randomly drawn participants had observed genotypes).}
	\label{fig:manhattanRS}
\end{figure}

\bibliographystyle{plainnat}
\bibliography{epsbiblio}


\end{document}