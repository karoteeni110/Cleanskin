In this section, we show example kernel machines that motivated us to propose the ODC framework to improve their performance and scalability.   Specifically, we review GPR for single output regression, and TGP for structured output regression.\ignore{ Specifically, we review two powerful machines for single output (Gaussian Process Regression(GPR)) and structured output regression (Twin Gaussian Processes(TGP)).Then, we present an importance weighting algorithm to resolve data bias under Twin-Gaussian Processes (IWTGP)} We selected GPR and TGP kernel machines for their increasing interest and impact. However, our framework is not  restricted to them\ignore{ as it depends on the overlapping domain cover notion that could be generally adopted}. \ignore{We conclude this section by showing practical limitation of these machines that inspires our method. }

%\subsection{Gaussian Process Regression (GPR)}
%\label{sec:relgpr}



\textbf{GPR}~\cite{Rasmussen:2005}  assumes a linear model in the kernel space with Gaussian noise in a single-valued output, i.e., $y = f(\textbf{x})  + \mathcal{N} (0, \sigma_{n}^2)$, where $\textbf{x} \in \mathbb{R}^{d_X}$ and ${y} \in \mathbb{R}$. Given a training set $\lbrace \textbf{x}_i, {y}_i , i =1:N \rbrace$, the posterior distribution of $y$ given a test point $\textbf{x}_{*}$ is:   \ignore{dimension $i$ of the classifier to be predicted ($\textbf{y}_* \in R^{d_Y}$)}
\begin{comment}
\begin{equation}
y_i = f_i(t) +e_i ,  \,\,\,\,     e_i âˆ¼ \mathcal{N} (0, \sigma_{ni}^2),        
\end{equation}
\end{comment}
\begin{equation}
\small
\begin{split}
p(y|\textbf{x}_*)=   \mathcal{N} (&\boldsymbol{\mu}_{y} =  \textbf{k}{(\textbf{x}_*)}^\top (\textbf{K} + \sigma^2_{n} \textbf{I})^{-1} \mathbf{f},   \\  &  \sigma_{y}^2 = k({\textbf{x}_*, \textbf{x}_*}) -\textbf{k}{(\textbf{x}_*)}^\top (\textbf{K} + \sigma^2_{n} \textbf{I})^{-1} \textbf{k}{(\textbf{x}_*)})
\end{split}
\label{eq:gprpred}
\end{equation}
where $k(\textbf{x},\textbf{x}')$ is kernel defined in the input space, $\mathbf{K}$ is an $N \times N$ matrix, such that $\mathbf{K} (l,m) = k(\textbf{x}_l, \textbf{x}_m)$, $\textbf{k}{(\textbf{x}_*)}  = [k(\textbf{x}_*,\textbf{x}_1),, ..., {k}(\textbf{x}_*,\textbf{x}_{N})]^\top$, $\textbf{I}$ is an  identity matrix of size $N$, $\sigma_{n}$ is the variance of the measurement noise, $\mathbf{f} = [y_1,\cdots  $ $, y_N]^\top$. GPR could predict structured output $\textbf{y} \in \mathbb{R}^{d_Y}$ by training a GPR  model for each dimension\ignore{ $i = 1 : d_Y$}. However, this indicates that GPR does not capture dependency between output dimensions which limit its performance. 
%\subsection{Twin Gaussian Processes (TGP)}
%\label{sec:reltgp}

\textbf{TGP}~\cite{Bo:2010} encodes the relation between both inputs and outputs using GP priors. This was achieved by minimizing the Kullback-Leibler divergence between the marginal GP of outputs (e.g., poses) and observations (e.g., features)\ignore{; we refer the reader to~\cite{Bo:2010} for derivation}. Hence,  TGP prediction is given by:\ignore{ the estimated pose in TGP is given as the solution of the following optimization problem:}
\begin{equation}
\small
\begin{split}
\hat{\textbf{y}}(\textbf{x}_*) =  &\underset{\textbf{y}}{\operatorname{argmin       }}[  k_Y(\textbf{y},\textbf{y})  -2 \textbf{k}_Y(\textbf{y})^\top (\textbf{K}_X + \lambda_X  \textbf{I})^{-1} \textbf{k}_X(\textbf{x}_*) \\&- \eta  log (k_Y(\textbf{y},\textbf{y})  -\textbf{k}_Y(y)^\top ({\textbf{K}_Y}+ \lambda_Y \textbf{I})^{-1} {\textbf{k}_Y(\textbf{y})} ) ]
\end{split}
\label{eq:tgp}
\vspace{-5mm}
\end{equation}
where $\eta  = k_X(\textbf{x}_*,\textbf{x}_*) -\textbf{k}_X(\textbf{x}_*)^\top  (\textbf{K}_X + \lambda_X  \textbf{I})^{-1} \textbf{k}_X($ $\textbf{x}_*)$, $k_X(\textbf{x},\textbf{x}') = exp(\frac{- \|\textbf{x}-\textbf{x}' \|}{2 \rho_x^2})$ and $k_Y(\textbf{y},\textbf{y}')$ $= exp(\frac{- \|\textbf{y}-\textbf{y}' \|}{2 \rho_y^2})$ are Gaussian kernel functions for input feature $\textbf{x}$ and output vector $\textbf{y}$, $\rho_x$ and $\rho_y$ are the kernel bandwidths for the input and the output  . $\textbf{k}_Y(\textbf{y}) = [k_Y(\textbf{y},\textbf{y}_1), ..., k_Y(\textbf{y},\textbf{y}_{N})]^\top$, where $N$ is the number of the training examples. $\textbf{k}_X(\textbf{x}_*) = [k_X(\textbf{x}_*,\textbf{x}_1), ...,$ ${k}_X(\textbf{x}_*,\textbf{x}_{N})]^\top$, and $\lambda_X$ and $\lambda_Y$ are regularization parameters to avoid overfitting. This optimization problem can be solved using a quasi-Newton optimizer with cubic polynomial line search \ignore{for optimal step size selection}~\cite{Bo:2010}; we denote the number of steps to convergence as $l_2$.




\begin{table*}[t!]
\centering
 %\vspace{-5mm}
\caption{Comparison of computational Complexity of training and testing for each of Full, NN (Nearest Neighbor), FITC, Local-RPC, and our ODC. Training is the time include all computations that does not depend on test data, which includes clustering in some of these methods. Testing includes computations only needed for prediction\ignore{performed at test time, i.e., Mean $\textbf{Y}$ and variance for GPR kernel machine and the prediction $\textbf{Y}$ for TGP kernel machine}}
%\vspace{2mm}
 \label{tab:thcomp}
  \scalebox{0.68}{
    \begin{tabular}{|l|ccc|cc|c|}
    %\toprule
    \hline
          &  \multicolumn{3}{c|}{\textbf{Training for GPR and TGP}}    & \multicolumn{3}{|c|}{\textbf{Testing for each point} }  \\ \hline
    %\midrule
          & \textbf{Ekmeans Clustering} & \textbf{RPC Clustering}   &    \textbf{Model training}   & \textbf{GPR-Y} & \textbf{GPR-Var} & \textbf{TGP-Y} \\ \hline 
    \textbf{Full} & -     & -     & $O(N^3 + N^2 d_X)$    & $O(N \cdot (d_X +d_Y)$ & $O(N^2 \cdot d_Y)$ & $O(l_2 \cdot N^2 \cdot d_Y)$ \\
    \textbf{NN {\cite{Bo:2010}}} & -     & -     & -     & $O(M^3 \cdot d_Y)$ & $O(M^3 \cdot d_Y)$ & $O(M^3 + l_2 \cdot M^2 \cdot d_Y)$ \\
    \textbf{FIC (GPR only, $d_Y=1$ {~\cite{fic06}})} & - & - & $O(M^2 \cdot ( {N} + d_X))$  & $O(M \cdot d_X)$  & $O(M^2)$ & -  \\
    \textbf{Local-RPC (only GPR, $d_Y=1${ \cite{Chalupka:2013}})} & - & $N \cdot log(\frac{N}{M})$ & $O(M^2 \cdot ( {N} + d_X) )$  & $O(M \cdot d_X)$ & $O(M^2)$ & -  \\
     %\textbf{SoD} & - & - & O($M^3 \cdot d_X$)  & O($M \cdot d_X$) & O($M^2 $) & -  \\
     \textbf{ODC (our framework)} & $O(N \cdot \frac{N}{(1-p) M} \cdot d_X \cdot l_1)$ & $O(N \cdot log(\frac{N}{(1-p) M}) \cdot d_X)$ & O($M^2 \cdot(\frac{N}{1-p} + d_X)$)  & $O(K' \cdot M \cdot( d_X + d_Y))$ & $O(K' \cdot M^2 \cdot d_Y)$ & $O(l_2 \cdot K' \cdot M^2 \cdot d_Y)$ \\ \hline 
    %\bottomrule
    \end{tabular}}%
      \vspace{-2mm}
\end{table*}%


\section{Importance Weighted Twin Gaussian Processes (IWTGP)}
\label{ss:cstgp}

Yamada et al~\cite{Yamada:2012} proposed the importance-weighted variant of twin Gaussian processes \cite{Bo:2010} called IWTGP.  The weights are calculated using RuLSIF \cite{YamadaSKHS11} (relative unconstrained least-squares importance fitting). The weights were modeled as $w_{\alpha}(\textbf{x},\boldsymbol{\theta}) = \sum_{l=1}^{n_{te}} \theta_l k(\textbf{x}, \textbf{x}_l)$ to minimize $E_{p_{te}(x)} [\,(w_{\alpha}(x,\mathbf{\theta})-w_{\alpha}(\textbf{x}))^2\,]$. where $k(\textbf{x},\textbf{x}_l) = exp(-\frac{\|\textbf{x}-\textbf{x}_l\|}{2 \tau^2})\,\,\,$, $w_{\alpha}(\textbf{x}) =\,\,\,$ $\frac{p_{te}(\textbf{x})}{(1-\alpha) p_{te}(\textbf{x}) +\alpha p_{tr}(\textbf{x})}$, $0\leq\alpha \leq 1$. To cope with this instability issue, setting $\alpha$ to $0 \le\alpha \le 1$ is practically useful for stabilizing the covariate shift adaptation, even though it cannot give an unbiased model under covariate shift \cite{YamadaSKHS11}. According  \cite{Yamada:2012} the optimal $\boldsymbol{\hat{\theta}}$ vector is computed in a closed form solution as follows.to
\begin{equation}
\boldsymbol{\hat{\theta}}= ({\hat{\textbf{H}}} + \nu \textbf{I} )^{-1} {\hat{\textbf{h}}}
\end{equation} 
where  $\hat{\textbf{H}}_{l,l'} = \frac{1-\alpha}{n_{te}}  \sum_{i=1}^{n_{te}} k(\textbf{x}_i^{te}, \textbf{x}_l^{te} k(\textbf{x}_i^{te},\textbf{x}_{l'}^{te}) + $ $\frac{\alpha}{n_{tr}} \sum_{j=1}^{n_{tr}}  $ $ k(\textbf{x}_j^{tr}, \textbf{x}_l^{te} k(\textbf{x}_j^{tr},\textbf{x}_{l'}^{te})$, $\hat{\textbf{h}}$ is an $n_{te}$- dimensional vector with the $l^{th}$ element $\hat{\textbf{h}}_l = \frac{1}{n_{te}} \sum_{i=1}^{n_{te}} k(\textbf{x}_i^{te}, \textbf{x}_l^{te})$, $\textbf{I}$ is an $n_{te}\times n_{te}$-dimensional identity matrix. where $n_{te}$ and $n_{tr}$ and the number of testing and training points respectively. Model selection of RuLSIF is based on cross-validation with respect to the squared-error criterion $J$ in \cite{YamadaSKHS11}. Having computed $\boldsymbol{\hat{\theta}}$, each input and output examples are simply re-weighted by $w_{\alpha}^{\frac{1}{2}}$ \cite{Yamada:2012}. Therefore, the output of the importance
weighted TGP (IWTGP) is given by
\begin{equation}
\begin{split}
\hat{y} =  \underset{y}{\operatorname{argmin       }}[ & K_Y(\textbf{y},\textbf{y}) -2 k_y(\textbf{y})^T \textbf{u}_w - \eta_w  log (K_Y(\textbf{y},\textbf{y}) -\\& k_y(\textbf{y})^T \textbf{W}^\frac{1}{2} (\textbf{W}^\frac{1}{2} \textbf{K}_Y \textbf{W}^\frac{1}{2} +  \lambda_y I)^{-1} \textbf{W}^\frac{1}{2} k_y(\textbf{y}) ) ]
\end{split}
\label{eq:IWTGP}
\end{equation}
where $\textbf{u}_w = \textbf{W}^\frac{1}{2}  (\textbf{W}^\frac{1}{2} \textbf{K}_X \textbf{W}^\frac{1}{2} + \lambda_x I)^{-1} \textbf{W}^\frac{1}{2} k_x(\textbf{x})$, $\eta_w = k_X(\textbf{x},\textbf{x}) - k_x(\textbf{x})^T \textbf{u}_w$. Similar to TGP, IWTGP can also be solved using a second order, BFGS quasi-Newton optimizer with cubic polynomial line search for optimal step size selection.





 Table~\ref{tab:thcomp} shows the training an testing complexity of full GPR and TGP models, where $d_Y$ is the dimensionality of the output. Table~\ref{tab:thcomp} also summarizes the computational complexity of the related approximation methods, discussed in the following section, and our method. N\ignore{The hyper-parameters of both GPR and TGP were trained by cross validation, similar to ~\cite{Bo:2010}; the SM include the learnt hyper-parameters.}\ignore{Yamada et al \cite{Yamada:2012} proposed the importance-weighted variant of twin Gaussian processes \cite{Bo:2010} called IWTGP.  The weights are calculated using RuLSIF \cite{YamadaSKHS11}. There are more details about using this approach in our framework in the attached SM. }.  %It is important to note that, there exist several existing kernel methods such as KTA~\cite{}, HSIC~\cite{}, W-KNN~\cite{}.  We focus on studing our ODC notion on GPR and TGPs (TGP and IWTGP) since they are among the most popular and best perfoming kernel machines. However,  we think the notion is applicable to other methods.  
 
 %Note that both TGP and IWTGP has same complexity under N
 
 
\begin{comment}
\subsection{Importance Weighted Twin Gaussian Processes (IWTGP)}
\label{ss:cstgp}
Yamada et al \cite{Yamada:2012} proposed the importance-weighted variant of twin Gaussian processes \cite{Bo:2010} called IWTGP.  The weights are calculated using RuLSIF \cite{YamadaSKHS11} (relative unconstrained least-squares importance fitting). The weights were modeled as $w_{\alpha}(\textbf{x},\boldsymbol{\theta}) = \sum_{l=1}^{n_{te}} \theta_l k(\textbf{x}, \textbf{x}_l)$ to minimize $E_{p_{te}(x)} [(w_{\alpha}(x,\mathbf{\theta})-w_{\alpha}(\textbf{x}))^2]$. where $k(\textbf{x},\textbf{x}_l) = exp(-\frac{\|\textbf{x}-\textbf{x}_l\|}{2 \tau^2})$, $w_{\alpha}(\textbf{x}) =$ $\frac{p_{te}(\textbf{x})}{(1-\alpha) p_{te}(\textbf{x}) +\alpha p_{tr}(\textbf{x})}$, $0\leq\alpha \leq 1$. To cope with this instability issue, setting $\alpha$ to $0 \le\alpha \le 1$ is practically useful for stabilizing the covariate shift adaptation, even though it cannot give an unbiased model under covariate shift \cite{YamadaSKHS11}. According  \cite{Yamada:2012} the optimal $\boldsymbol{\hat{\theta}}$ vector is computed in a closed form solution as follows.to


\begin{equation}
\boldsymbol{\hat{\theta}}= ({\hat{\textbf{H}}} + \nu \textbf{I} )^{-1} {\hat{\textbf{h}}}
\end{equation} 

where  $\hat{\textbf{H}}_{l,l'} = \frac{1-\alpha}{n_{te}}  \sum_{i=1}^{n_{te}} k(\textbf{x}_i^{te}, \textbf{x}_l^{te} k(\textbf{x}_i^{te},\textbf{x}_{l'}^{te}) + \frac{\alpha}{n_{tr}} \sum_{j=1}^{n_{tr}}  k(\textbf{x}_j^{tr}, \textbf{x}_l^{te} k(\textbf{x}_j^{tr},\textbf{x}_{l'}^{te})$, $\hat{\textbf{h}}$ is $n_{te}$  dimensional vector with the $l^{th}$ element $\hat{\textbf{h}}_l = \frac{1}{n_{te}} \sum_{i=1}^{n_{te}} k(\textbf{x}_i^{te}, \textbf{x}_l^{te})$, $\textbf{I}$ is $n_{te}\times n_{te}$-dimensional identity matrix. where $n_{te}$ and $n_{tr}$ and the number of testing and training points respectively. Model selection of RuLSIF is based on cross-validation with respect to the squared-error criterion $J$ in \cite{YamadaSKHS11}. Having computed $\boldsymbol{\hat{\theta}}$, each input and output examples are simply re-weighted by $w_{\alpha}^{\frac{1}{2}}$ \cite{Yamada:2012}. Therefore, the output of the importance
weighted TGP (IWTGP) is given by
\begin{equation}
\begin{split}
\hat{y} =  \underset{y}{\operatorname{argmin       }}[ & K_Y(\textbf{y},\textbf{y}) -2 k_y(\textbf{y})^\top \textbf{u}_w - \eta_w  log (K_Y(\textbf{y},\textbf{y}) -\\& k_y(\textbf{y})^\top \textbf{W}^\frac{1}{2} (\textbf{W}^\frac{1}{2} \textbf{K}_Y \textbf{W}^\frac{1}{2} +  \lambda_Y I)^{-1} \textbf{W}^\frac{1}{2} k_y(\textbf{y}) ) ]
\end{split}
\label{eq:IWTGP}
\end{equation}
where $\textbf{u}_w = \textbf{W}^\frac{1}{2}  (\textbf{W}^\frac{1}{2} \textbf{K}_X \textbf{W}^\frac{1}{2} + \lambda_X I)^{-1} \textbf{W}^\frac{1}{2} k_x(\textbf{x})$, $\eta_w = k_X(\textbf{x},\textbf{x}) - k_x(\textbf{x})^\top \textbf{u}_w$. IWTGP can also be solved using a second order, BFGS quasi-Newton optimizer with cubic polynomial line search for optimal step size selection.

\end{comment}

%\noindent\textbf{Current Practical problems:}
%\subsection*{Practical Problems}

\begin{comment} 
  Finally, These practical problems motivated us to introduce our our new local prediction framework.  
\end{comment} 
 
\begin{comment}
\begin{enumerate}
\item{A TGP model is computed for each query point $x$ which result in a scalability  problem in prediction (i.e. Matrix inversions (i.e $K_X^{-1}, K_Y^{-1}$) have to be computed for each query point). }	
\item{Number of neighbors might not be large enough to create an accurate prediction model.}
\item{Consistency between models are not handled which affects the prediction error.}
\end{enumerate}
\end{comment}