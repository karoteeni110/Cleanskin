\section{Related Work and Background}\label{sec:related}
\subsection{The Graphics Processing Unit (GPU)}
The GPU of today is a highly parallel, throughput-focused programmable processor. GPU programs (``kernels'') launch over a \emph{grid} of numerous \emph{blocks}; the GPU hardware maps blocks to available parallel cores. Each block typically consists of dozens to thousands of individual \emph{threads}, which are arranged into 32-wide \emph{warps}. Warps run under SIMD control on the GPU hardware. While blocks cannot directly communicate with each other within a kernel, threads within a block can, via a user-programmable 48~kB \emph{shared-memory}, and threads within a warp additionally have access to numerous warp-wide instructions. The GPU's global memory (DRAM), accessible to all blocks during a computation, achieves its maximum bandwidth only when neighboring threads access neighboring locations in the memory; such accesses are termed \emph{coalesced}.
In this work, when we use the term ``\emph{global}'', we mean an operation of device-wide scope. Our term ``\emph{local}'' refers to an operation limited to smaller scope (e.g., within a thread, a warp, a block, etc.), which we will specify accordingly. The major difference between the two is the cost of communication: global operations must communicate through global DRAM, whereas local operations can communicate through lower-latency, higher-bandwidth mechanisms like shared memory or warp-wide intrinsics.
Lindholm et al.~\shortcite{Lindholm:2008:NTA} and Nickolls et al.~\shortcite{Nickolls:2008:SPP} provide more details on GPU hardware and the GPU programming model, respectively.

We use NVIDIA's CUDA as our programming language in this work~\cite{NVIDIA:2016:CUDA}. CUDA provides several warp-wide voting and shuffling instructions for intra-warp communication of threads. All threads within a warp can see the result of a user-specified predicate in a bitmap variable returned by \texttt{\_\_ballot(predicate)}~\cite[Ch.~B13]{NVIDIA:2016:CUDA}. Any set bit in this bitmap denotes the predicate  being non-zero for the corresponding thread. Each thread can also access registers from other threads in the same warp with \texttt{\_\_shfl(register\_name, source\_thread)}~\cite[Ch.~B14]{NVIDIA:2016:CUDA}. Other shuffling functions such as \texttt{\_\_shfl\_up()} or \texttt{\_\_shfl\_xor()} use relative addresses to specify the source thread.
In CUDA, threads also have access to some efficient integer intrinsics, e.g., \texttt{\texttt{\_\_popc()}} for counting the number of set bits in a register.

\subsection{Parallel primitive background}
In this paper we leverage numerous standard parallel primitives, which we briefly describe here. A \emph{reduction} inputs a vector of elements and applies a binary associative operator (such as addition) to reduce them to a single element; for instance, sum-reduction simply adds up its input vector.
The \emph{scan} operator takes a vector of input elements and an associative binary operator, and returns an output vector of the same size as the input vector.
In exclusive (resp., inclusive) scan, output location $i$ contains the reduction of input elements 0 to $i-1$ (resp., 0 to $i$).
Scan operations with binary addition as their operator are also known as \emph{prefix-sum}~\cite{Harris:2007:PPS:nourl}.
Any reference to a multi- operator (multi-reduction, multi-scan) refers to running multiple instances of that operator in parallel on separate inputs. \emph{Compaction} is an operation that filters a subset of its input elements into a smaller output array while preserving the order.

\subsection{Multisplit and Histograms}
Many multisplit implementations, including ours, depend heavily on knowledge of the total number of elements within each bucket (bin), i.e., histogram computation.
Previous competitive GPU histogram implementations share a common philosophy: divide the problem into several smaller sized subproblems and assign each subproblem to a thread, where each thread sequentially processes its subproblem and keeps track of its own \emph{privatized} local histogram.
Later, the local histograms are aggregated to produce a globally correct histogram.
There are two common approaches to this aggregation: 1) using atomic operations to correctly add bin counts together (e.g., Shams and Kennedy~\shortcite{Shams:2007:EHA}), 2)~storing per-thread sequential histogram computations and combining them via a global reduction (e.g., Nugteren et al.~\shortcite{Nugteren:2011:HPP}).
The former is suitable when the number of buckets is large; otherwise atomic contention is the bottleneck.
The latter avoids such conflicts by using more memory (assigning exclusive memory units per-bucket and per-thread), then performing device-wide reductions to compute the global histogram.

The hierarchical memory structure of NVIDIA GPUs, as well as NVIDIA's more recent addition of faster but local shared memory atomics (among all threads within a thread block), provides more design options to the programmer.
With these features, the aggregation stage could be performed in multiple rounds from thread-level to block-level and then to device-level (global) results.
Brown et al.~\shortcite{Brown:2012:MFH} implemented both Shams's and Nugteren's 
aforementioned methods, as well as a variation of their own, focusing only on 8-bit data, considering careful optimizations that make the best use of the GPU, including loop unrolling, thread coarsening, and subword parallelism, as well as others.
Recently, NVIDIA's CUDA Unbound (CUB)~\cite{Merrill:2015:CUB} library has included an efficient and consistent histogram implementation that carefully uses a minimum number of shared-memory atomics to combine per-thread privatized histograms per thread-block, followed by aggregation via global atomics. CUB's histogram supports any data type (including multi-channel 8-bit inputs) with any number of bins.

Only a handful of papers have explored multisplit as a standalone primitive. He et al.~\cite{He:2008:RJG} implemented multisplit by reading multiple elements with each thread, sequentially computing their histogram and local offsets (their order among all elements within the same bucket and processed by the same thread), then storing all results (histograms and local offsets) into memory. Next, they performed a device-wide scan operation over these histogram results and scattered each item into its final position. Their main bottlenecks were the limited size of shared memory, an expensive global scan operation, and random non-coalesced memory accesses.%
\footnote{On an NVIDIA 8800 GTX GPU, for 64 buckets, He et al.\ reported 134~Mkeys/sec. As a very rough comparison, our GeForce GTX 1080 GPU has 3.7x the memory bandwidth, and our best 64-bucket implementation runs 126 times faster.}

Patidar~\cite{Patidar:2009:SPD} proposed two methods with a particular focus on a large number of buckets (more than 4k): one based on heavy usage of shared-memory atomic operations (to compute block level histogram and intra-bucket orders), and the other by iterative usage of basic binary split for each bucket (or groups of buckets). Patidar used a combination of these methods in a hierarchical way to get his best results.%
\footnote{On an NVIDIA GTX280 GPU, for 32 buckets, Patidar reported 762~Mkeys/sec. As a very rough comparison, our GeForce GTX 1080 GPU has 2.25x the memory bandwidth, and our best 32-bucket implementation runs 23.5 times faster.}
Both of these multisplit papers focus only on key-only scenarios, while data movements and privatization of local memory become more challenging with key-value pairs.
