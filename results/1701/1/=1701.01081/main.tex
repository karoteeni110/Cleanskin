%% This is file `ycviu-template.tex',
%% 
%% Copyright 2013 Elsevier Ltd
%% 
%% This file is part of the 'Elsarticle Bundle'.
%% ---------------------------------------------
%% 
%% It may be distributed under the conditions of the LaTeX Project Public
%% License, either version 1.2 of this license or (at your option) any
%% later version.  The latest version of this license is in
%%    http://www.latex-project.org/lppl.txt
%% and version 1.2 or later is part of all distributions of LaTeX
%% version 1999/12/01 or later.
%% 
%% The list of all files belonging to the 'Elsarticle Bundle' is
%% given in the file `manifest.txt'.
%% 
%% Template article for Elsevier's document class `elsarticle'
%% with harvard style bibliographic references
%%
%% $Id: ycviu-template-with-authorship.tex 69 2016-07-29 10:15:25Z aptara $
%%
%% This template has no review option
%% 
%% Use the options `twocolumn,final' to obtain the final layout
\documentclass[times,twocolumn,final,authoryear]{elsarticle}

%% Stylefile to load YCVIU template
\usepackage{ycviu}
\usepackage{framed,multirow}

%% The amssymb package provides various useful mathematical symbols
\usepackage{amssymb}
\usepackage{latexsym}

% Following three lines are needed for this document.
% If you are not loading colors or url, then these are
% not required.
\usepackage{url}
\usepackage{xcolor}
\definecolor{newcolor}{rgb}{.8,.349,.1}


\journal{Computer Vision and Image Understanding}

\begin{document}

\ifpreprint
  \setcounter{page}{1}
\else
  \setcounter{page}{1}
\fi

\begin{frontmatter}

\title{SalGAN: visual saliency prediction with adversarial networks}

\author[1]{Junting \snm{Pan}}
\author[2]{Cristian \snm{Canton-Ferrer}}
\author[3]{Kevin \snm{McGuinness}}
\author[3]{Noel E. \snm{O'Connor}}
\author[4]{Jordi \snm{Torres}}
\author[1]{Elisa \snm{Sayrol}}
\author[1]{Xavier \snm{Giro-i-Nieto}\corref{cor1}} 
\cortext[cor1]{Corresponding author: 
  Tel.: +34-934-015-769;}
\ead{xavier.giro@upc.edu}

\address[1]{Universitat Politecnica de Catalunya, Barcelona 08034, Catalonia/Spain}
\address[2]{Facebook AML, Seattle, WA, United States of America}
\address[3]{Insight Center for Data Analytics, Dublin City University, Dublin 9, Ireland}
\address[4]{Barcelona Supercomputing Center, Barcelona 08034, Catalonia/Spain}

% \received{1 May 2013}
% \finalform{10 May 2013}
% \accepted{13 May 2013}
% \availableonline{15 May 2013}
% \communicated{S. Sarkar}

\begin{abstract}
Recent approaches for saliency prediction are generally trained with a loss function based on a single saliency metric.  This could lead to low  performance when evaluating with other saliency metrics.  
In this paper, we propose a novel data-driven metric based saliency prediction method, named SalGAN (Saliency GAN), trained with adversarial loss function. SalGAN consists of two networks: one predicts saliency maps from raw pixels of an input image; the other one takes the output of the first one to discriminate whether a saliency map is a predicted one or ground truth.
By trying to make the predicted saliency map indistinguishable with the ground truth, SalGAN is expected to generate saliency maps that resembles the ground truth. Our experiments show that the adversarial training allows our model to obtain state-of-the-art performances across various saliency metrics.
\end{abstract}

\begin{keyword}
\MSC 41A05\sep 41A10\sep 65D05\sep 65D17
\KWD Keyword1\sep Keyword2\sep Keyword3

%% MSC codes here, in the form: \MSC code \sep code
%% or \MSC[2008] code \sep code (2000 is the default)
\end{keyword}

\end{frontmatter}

%\linenumbers

\section{Introduction}
\label{sec:Motivation}

% Visual saliency prediction, saliency maps and applications
Visual saliency describes the spatial locations in an image that attract the human attention.  It is understood as a result of a bottom-up process where a human observer explores the image for a few seconds with no particular task in mind. Therefore, saliency prediction is indispensable for various machine vision tasks such as object recognition~\citep{walther2002attentional}.

Visual saliency data are traditionally collected by eye-trackers~\citep{judd2009learning}, and more recently with mouse clicks \citep{jiang2015salicon} or webcams \citep{cvpr2016_Khosla}. The salient points of the image are aggregated and convolved with a Gaussian kernel to obtain a saliency map. As a result, a gray-scale image or a heat map is generated to represent the probability of each corresponding pixel in the image to capture the human attention.

% \begin{figure}
% \centering
% \begin{tabular}{ccc}
% Ground Truth & BCE & SalGAN\\
% \includegraphics[width=0.28\columnwidth]{./fig1/Img1G_GT.pdf} &     \includegraphics[width=0.28\columnwidth]{./fig1/Img1G_BCE.pdf} &
% \includegraphics[width=0.28\columnwidth]{./fig1/Img1G_SalGAN.pdf}\\
% \includegraphics[width=0.28\columnwidth]{./fig1/Img2_GT.pdf} &     \includegraphics[width=0.28\columnwidth]{./fig1/Img2_BCE.pdf} &
% \includegraphics[width=0.28\columnwidth]{./fig1/Img2_SalGAN.pdf}\\
% \end{tabular}
% \caption{Example of saliency map generation where the proposed system (SalGAN) outperforms a standard binary cross entropy (BCE) prediction model.}
% %\xuworries{It seems that the improvement is not very obvious. As the title image, you need to find some images that really outstanding. Otherwise, just remove this figure. The original image should also be added. Don't use "where", it's too weak.}
% \label{fig:catchyFigure}
% \end{figure}

\begin{figure}
\centering
\begin{tabular}{cc}
Image & Ground Truth \\
\includegraphics[width=0.4\columnwidth]{./fig1_img.pdf} &     \includegraphics[width=0.4\columnwidth]{./fig1_gt.pdf} \\
BCE & SalGAN  \\
\includegraphics[width=0.4\columnwidth]{./fig1_bce.pdf} &     \includegraphics[width=0.4\columnwidth]{./fig1_gan.pdf} \\
\end{tabular}
\caption{Example of saliency map generation where the proposed system (SalGAN) outperforms a standard binary cross entropy (BCE) prediction model.}
%\xuworries{It seems that the improvement is not very obvious. As the title image, you need to find some images that really outstanding. Otherwise, just remove this figure. The original image should also be added. Don't use "where", it's too weak.}
\label{fig:catchyFigure}
\end{figure}
% Multiple metrics
% \par The family of computational metrics for visual saliency prediction is diverse. \xuworries{How to choose or design a best training loss is still an open problem.} %This is weird to me. It seems that you are talking about evaluation. However, the paper is talking about training losses. Needs to make this more clear. 
%, and what is the best choice is still an open problem~\cite{Bylinskii2016metrics}. 
% For example, the widely used MIT300 benchmark~\cite{mit-saliency-benchmark} evaluates its results on 8 different metrics, and the more recent SALICON data set considers four of them; Information Gain (IG)~\cite{kummerer2054IG} has been presented as a powerful relative metric in the field. However, these hand-crafted metrics easily lead to performance bottleneck using deep learning models trained on large data sets~\cite{jiang2015salicon}. Thus, 
A lot of research effort has been made in designing an optimal loss function for saliency prediction. State-of-the-art methods \citep{huang2015salicon} adopt saliency based metrics while others \citep{Pan_2016_CVPR,mlnet2016,jetley2016end,sun2017integrated} use distance in saliency map space. How to choose or design a best training loss is still an open problem. In addition, different saliency metrics diverge at defining the meaning of saliency maps, and there exist inconsistency with the model comparison. For instance, it has been pointed that the optimal metric for model optimization may depend on the final application  \citep{Bylinskii2016metrics}.
% Coming up with
% loss functions that force the CNN to do what we really want
% – e.g., output sharp, realistic images – is an open problem
% and generally requires expert knowledge.
% Models based on back-propagation training have adopted loss functions capable to capture the different features assessed by the multiple metrics. 
% Our approach, though, differs from other state-of-the-art solutions as we focus on exploring the benefits of introducing the agnostic loss function proposed in  adversarial training. 

% \par \xuworries{(Adversarial loss is still a manually designed loss. Is it?) 

To this end, instead of designing  a tailored loss function, we introduce adversarial training for visual saliency prediction inspired by generative adversarial networks (GANs)\citep{goodfellow2014generative}. We dub the proposed method as SalGAN. We focus on exploring the benefits of using such an adversarial loss to make the output saliency map not able to be distinguished from the real saliency maps. 
In GANs, training is driven by two competing agents: first, the \textit{generator} synthesizing samples that match with the training data; second, the \textit{discriminator} distinguishing between a real sample drawn directly from the training data and a fake one synthesized by the generator. In our case, this data distribution corresponds to pairs of real images and their corresponding visual saliency maps. 

Specifically, SalGAN estimates the saliency map of an input image using a deep convolutional neural network (DCNN). As shown in the Figure~\ref{fig:architecture} this network is initially trained with a binary cross entropy (BCE) loss over down-sampled versions of the saliency maps. The model is then refined with a discriminator network trained to solve a binary classification task between the saliency maps generated by SalGAN and the real ones used as ground truth. Our experiments show how adversarial training allows reaching state-of-the-art performance across different metrics when combined with a BCE content loss in a single-tower and single-task model. 
% \xuworries{Remove this sentence. A simple repeat of previous one.} For example, our models outperform the current state-of-the-art model in all metrics in the SALICON dataset.

% Contributions
% \par To summarize, we investigate adversarial training for visual saliency prediction, showing how the simple binary classification between real and synthetic samples significantly benefits a wide range of visual saliency metrics, \xuworries{BCE loss is still a loss function?} without the need to specify a loss function. Our results achieve state-of-the-art performance with a simple DCNN where the parameters are trained with a discriminator. 

To summarize, we investigate the introduction of the adversarial loss to the visual saliency learning. By introducing adversarial loss to the BCE saliency prediction model, we achieve the state-of-the-art performance in MIT300 and SALICON dataset for almost all the evaluation metrics
% As a secondary contribution, we show the benefits of using the binary cross entropy (BCE) loss function and downsampled saliency maps when training this DCNN.

% Organization of the paper
The remaining of the text is organized as follows. 
Section \ref{sec:RelatedWork} reviews the state-of-the-art models for visual saliency prediction, discussing the loss functions they are based upon, their relations with the different metrics as well as their complexity in terms of architecture and training. 
Section \ref{sec:Architecture} presents SalGAN, our deep convolutional neural network based on a convolutional encoder-decoder architecture, as well as the discriminator network used during its adversarial training.
Section \ref{sec:Training} describes the training process of SalGAN and the loss functions used.
Section \ref{sec:Experiments} includes the experiments and results of the presented techniques. Finally, Section \ref{sec:Conclusions} closes the paper by drawing the main conclusions.

Our results can be reproduced with the source code
and trained models available at \url{https://imatge-upc.github.io/saliency-salgan-2017/}.


\section{Related work}
\label{sec:RelatedWork}

% Architectures
Saliency prediction has received interest by the research community for many years. Thus seminal works \citep{Itti1998PAMI} proposed to predict saliency maps considering low-level features at multiple scales and combining them to form a saliency map. \citep{harel2006nips}, also starting from low-level feature maps, introduced a graph-based saliency model that defines Markov chains over various image maps, and treat the equilibrium distribution over map locations as activation and saliency values. \citep{judd2009iccv} presented a bottom-up, top-down model of saliency based not only on low but mid and high-level image features. \citep{borji2012cvpr} combined low-level features saliency maps of previous best bottom-up models with top-down cognitive visual features and learned a direct mapping from those features to eye fixations.

As in many other fields in computer vision, a number of deep learning solutions have very recently been proposed that significantly improve the performance.
%\xuworries{for visual saliency prediction}. 
For example, the Ensemble of Deep Networks (eDN) \citep{vig2014large} represented an early architecture that automatically learns the representations for saliency prediction, blending feature maps from different layers. 
%Their network might be consider a shallow network given the number of layers. 
In \citep{Pan_2016_CVPR} two convolutional neural networks trained ebd-to-end for saliency prediction are compared, a lighter one designed and trained from scratch, and a second and deeper one pre-trained for image classification. DCNN have shown better results even when pre-trained with datasets build for other purposes. DeepGaze \cite{kummerer2015deep} provided a deeper network using the well-know AlexNet \citep{krizhevsky2012imagenet}, with pre-trained weights on Imagenet \citep{deng2009imagenet} and with a readout network on top whose inputs consisted of some layer outputs of AlexNet. The output of the network is blurred, center biased and converted to a probability distribution using a softmax. Huang et al.~\citep{huang2015salicon}, in the so call SALICON net, obtained better results by using VGG rather than AlexNet or GoogleNet \citep{Szegedy2015}. In their proposal they considered two networks with fine and coarse inputs, whose feature maps outputs are concatenated.

\begin{figure*}[!ht]
\includegraphics[width=\textwidth]{./Full_Architecture.pdf}
 \caption{Overall architecture of the proposed saliency system. The input for the saliency prediction network predicts an output saliency map given a natural image as input. Then, the pair of saliency and image is feed into the discriminator network. The output of the discriminator is a score that tells about whether the input saliency map is real or fake.} %and image pair
\label{fig:architecture}
\end{figure*}

Li et al. \citep{Li_2015_CVPR} proposed a multi resolution convolutional neural network that is trained from image regions centered on fixation and non-fixation locations over multiple  resolutions. Diverse top-down visual features can be learned in higher layers and  bottom-up visual saliency can also be inferred by combining information over multiple resolutions. 
These ideas are further developed in they recent work called DSCLRCN \cite{liu2018deep}, where the proposed model learns saliency related local features on each image location in parallel and then learns to simultaneously incorporate global context and scene context to infer saliency. They incorporate a model to effectively learn long-term spatial interactions and scene contextual modulation to infer image saliency. 
Deep Gaze II \citep{kummerer2017understanding} sets the state of the art in the MIT300 dataset by combining features trained for image recognition with four layer of 1x1 convolutions. 
Both DSCLRCN and Deep Gaze II obtain excellent results in the benchmarks when combined with a center bias, which is not considered in SalGAN as the results are purely the results at inference time.
MLNET \citep{mlnet2016} proposes an architecture that combines features extracted at different levels of a DCNN. They introduce a loss function inspired by three objectives: to measure similarity with the ground truth, to keep invariance of predictive maps to their maximum and to give importance to pixels with high ground truth fixation probability. In fact choosing an appropriate loss function has become an issue that can lead to improved results. Thus, another interesting contribution of ~\citep{huang2015salicon} lies on minimizing loss functions based on metrics that are differentiable, such as NSS, CC, SIM and KL divergence to train the network (see \cite{riche2013iccv} and  \cite{kummerer2054IG} for the definition of these metrics. A thorough comparison of metrics can be found in \citep{Bylinskii2016metrics}). In \citep{huang2015salicon} KL divergence gave the best results. \citep{jetley2016end} also tested loss functions based on probability distances, such as {X2} divergence, total variation distance, KL divergence and Bhattacharyya distance by considering saliency map models as generalized Bernoulli distributions. The Bhattacharyya distance was found to give the best results. 
% Finally, the work by Johnson et al. \cite{Johnson2016eccv} defines a perceptual loss combining a per-pixel loss together with another loss term based on the semantics of the image. It is applied to image style transfer but it may be used in models for saliency prediction.

In our work we present a network architecture that takes a different approach. By incorporating the high-level adversarial loss into the conventional saliency prediction training approach, the proposed method achieves the state-of-the-art performance in both MIT300 and SALICON datasets by a clear margin.
%, we adopted an hight-level adversarial loss combined with a traditional loss, such as binary cross entropy.

\section{Architecture}
\label{sec:Architecture}

The training of SalGAN is the result of two competing convolutional neural networks: a generator of saliency maps, which is SalGAN itself, and a discriminator network, which aims at distinguishing between the real saliency maps and those generated by SalGAN. This section provides details on the structure of both modules, the considered loss functions, and the initialization before beginning adversarial training.
Figure~~\ref{fig:architecture} shows the architecture of the system.

\subsection{Generator}
\label{ssec:Saliency Prediction Network}

% Layers
The generator network, SalGAN, adopts a convolutional encoder-decoder architecture, where the encoder part includes max pooling layers that decrease the size of the feature maps, while the decoder part uses upsampling layers followed by convolutional filters to construct an output that is the same resolution as the input. 

% Encoder
The encoder part of the network is identical in architecture to VGG-16 \citep{simonyan2014very}, omitting the final pooling and fully connected layers. The network is initialized with the weights of a VGG-16 model trained on the ImageNet data set for object classification \citep{deng2009imagenet}. Only the last two groups of convolutional layers in VGG-16 are modified during the training for saliency prediction, while the earlier layers remain fixed from the original VGG-16 model. We fix weights to save computational resources during training, even at the possible expense of some loss in performance.

% Decoder
The decoder architecture is structured in the same way as the encoder, but with the ordering of layers reversed, and with pooling layers being replaced by upsampling layers. Again, ReLU non-linearities are used in all convolution layers, and a final $1\times 1$ convolution layer with sigmoid non-linearity is added to produce the saliency map. The weights for the decoder are randomly initialized. The final output of the network is a saliency map in the same size to input image.

The implementation details of SalGAN are presented in Table \ref{tab:generator}.

\begin{table}
\centering
\footnotesize
\begin{tabular}{lrrrrr}
%\toprule
\hline
layer & depth & kernel & stride & pad & activation \\
%\midrule
\hline
conv1\_1 & 64& $ 1 \times 1 $ & 1 & 1 & ReLU \\
conv1\_2 & 64 & $ 3 \times 3 $ & 1 & 1 & ReLU \\
\hline
pool1 &  & $ 2 \times 2 $ & 2 & 0 & - \\
\hline
conv2\_1 & 128 & $ 3 \times 3 $ & 1 & 1 & ReLU \\
conv2\_2 & 128& $ 3 \times 3 $ & 1 & 1 & ReLU \\
\hline
pool2 &  & $ 2 \times 2 $ & 2 & 0 & - \\
\hline
conv3\_1 & 256 & $ 3 \times 3 $ & 1 & 1 & ReLU \\
conv3\_2 & 256 & $ 3 \times 3 $ & 1 & 1 & ReLU \\
conv3\_3 & 256 & $ 3 \times 3 $ & 1 & 1 & ReLU \\
\hline
pool3 &  & $ 2 \times 2 $ & 2 & 0 & - \\
\hline
conv4\_1 & 512 & $ 3 \times 3 $ & 1 & 1 & ReLU \\
conv4\_2 & 512 & $ 3 \times 3 $ & 1 & 1 & ReLU \\
conv4\_3 & 512 & $ 3 \times 3 $ & 1 & 1 & ReLU \\
\hline
pool4 &  & $ 2 \times 2 $ & 2 & 0 & - \\
\hline
conv5\_1 & 512 & $ 3 \times 3 $ & 1 & 1 & ReLU \\
conv5\_2 & 512 & $ 3 \times 3 $ & 1 & 1 & ReLU \\
conv5\_3 & 512 & $ 3 \times 3 $ & 1 & 1 & ReLU \\
\hline
conv6\_1 & 512 & $ 3 \times 3 $ & 1 & 1 & ReLU \\
conv6\_2 & 512 & $ 3 \times 3 $ & 1 & 1 & ReLU \\
conv6\_3 & 512 & $ 3 \times 3 $ & 1 & 1 & ReLU \\
\hline
upsample6 &  & $ 2 \times 2 $ & 2 & 0 & - \\
\hline
conv7\_1 & 512 & $ 3 \times 3 $ & 1 & 1 & ReLU \\
conv7\_2 & 512 & $ 3 \times 3 $ & 1 & 1 & ReLU \\
conv7\_3 & 512 & $ 3 \times 3 $ & 1 & 1 & ReLU \\
\hline
upsample7 &  & $ 2 \times 2 $ & 2 & 0 & - \\
\hline
conv8\_1 & 256 & $ 3 \times 3 $ & 1 & 1 & ReLU \\
conv8\_2 & 256 & $ 3 \times 3 $ & 1 & 1 & ReLU \\
conv8\_3 & 256 & $ 3 \times 3 $ & 1 & 1 & ReLU \\
\hline
upsample8 &  & $ 2 \times 2 $ & 2 & 0 & - \\
\hline
conv9\_1 & 128 & $ 3 \times 3 $ & 1 & 1 & ReLU \\
conv9\_2 & 128 & $ 3 \times 3 $ & 1 & 1 & ReLU \\
\hline
upsample9 &  & $ 2 \times 2 $ & 2 & 0 & - \\
\hline
conv10\_1 & 64 & $ 3 \times 3 $ & 1 & 1 & ReLU \\
conv10\_2 & 64 & $ 3 \times 3 $ & 1 & 1 & ReLU \\
\hline
output&1&1x1&1&0&Sigmoid\\
%\bottomrule
\hline
\end{tabular}
\caption{Architecture of the generator network.}
\label{tab:generator}
\end{table}


\subsection{Discriminator}
\label{ssec:Discriminator}

%Both the generator and discriminator function in SalGAN are deep convolutional neural networks. 
\par Table~\ref{tab:discriminator} gives the architecture and layer configuration for the discriminator. In short, the network is composed of six 3x3 kernel convolutions interspersed with three pooling layers ($\downarrow$2), and followed by three fully connected layers. The convolution layers all use ReLU activations while the fully connected layers employ $\tanh$ activations, with the exception of the final layer, which uses a sigmoid activation. 

\begin{table}
\begin{tabular}{lrrrrr}
%\toprule
\hline
layer & depth & kernel & stride & pad & activation \\
%\midrule
\hline
conv1\_1 & 3 & $ 1 \times 1 $ & 1 & 1 & ReLU \\
conv1\_2 & 32 & $ 3 \times 3 $ & 1 & 1 & ReLU \\
%\hline
pool1 &  & $ 2 \times 2 $ & 2 & 0 & - \\
\hline
%\midrule
conv2\_1 & 64 & $ 3 \times 3 $ & 1 & 1 & ReLU \\
conv2\_2 & 64 & $ 3 \times 3 $ & 1 & 1 & ReLU \\
%\hline
pool2 &  & $ 2 \times 2 $ & 2 & 0 & - \\
%\midrule
\hline
conv3\_1 & 64 & $ 3 \times 3 $ & 1 & 1 & ReLU \\
conv3\_2 & 64 & $ 3 \times 3 $ & 1 & 1 & ReLU \\
%\hline
pool3 &  & $ 2 \times 2 $ & 2 & 0 & - \\
%\midrule
\hline
fc4 & 100 & - & - & - & tanh \\
fc5 & 2 & - & - & - & tanh \\
fc6 & 1 & - & - & - & sigmoid \\
%\bottomrule
\hline
\end{tabular}
\caption{Architecture of the discriminator network.}
\label{tab:discriminator}
\end{table}



\section{Training}
\label{sec:Training}

The filter weights in SalGAN have been trained over a perceptual loss \citep{Johnson2016eccv} resulting from combining a content and adversarial loss. 
The content loss follows a classic approach in which the predicted saliency map is pixel-wise compared with the corresponding one from ground truth.
The adversarial loss depends of the real/synthetic prediction of the discriminator over the generated saliency map.

\subsection{Content loss}
\label{ssec:Content}

The content loss is computed in a per-pixel basis, where each value of the predicted saliency map is compared with its corresponding peer from the ground truth map.
Given an image $I$ of dimensions $N = W \times H$, we represent the saliency map $S$ as vector of probabilities, where $S_{j}$ is the probability of pixel $I_j$ being fixated. A content loss function $\mathcal{L}(S,\hat{S})$ is defined between the predicted saliency map $\hat{S}$ and its corresponding ground truth $S$.
%Given an image $I$ of dimensions $n = W \times H$, we represent a saliency map $S$ as a probability distribution over a set of pixels forming an image, where $S_{x,y}$ is the probability of pixel $(x,y)$ being fixated. \textcolor{red}{FIXME: the saliency map is NOT a probability distribution over the pixels in an image as it doesn't sum to 1. In fact, in a few paragraphs I argue that it shouldn't be one, since multiple parts may be attended.}
%A loss function $\mathcal{L}(S,\hat{S})$ is defined between the predicted saliency map $\hat{S}$ and its corresponding ground truth $S$. 

% MSE
The first considered content loss is mean squared error (MSE) or Euclidean loss, defined as:
%
\begin{equation}
%\begin{split} commented for IMAVIS
\mathcal{L}_{MSE} = 
\frac{1}{N}
\sum_{j=1}^{N} (S_j - \hat{S}_j)^{2}.
%\end{split} commented for IMAVIS
%\sum_{(x,y)=(1,1)}^{(W,H)} (S_{x,y} - \hat{S}_{x,y})^{2}
\end{equation}
In our work, MSE is used as a baseline reference, as it has been adopted directly or with some variations in other state of the art solutions for visual saliency prediction \citep{Pan_2016_CVPR,mlnet2016}.

Solutions based on MSE aim at maximizing the peak signal-to-noise ratio (PSNR).
These works tend to filter high spatial frequencies in the output, favoring this way blurred contours.
MSE corresponds to computing the Euclidean distance between the predicted saliency and the ground truth.

% BCE
Ground truth saliency maps are normalized so that each value is in the range $[0, 1]$. Saliency values can therefore be interpreted as estimates of the probability that a particular pixel is attended by an observer. It is tempting to therefore induce a multinomial distribution on the predictions using a softmax on the final layer. Clearly, however, more than a single pixel may be attended, making it more appropriate to treat each predicted value as independent of the others. We therefore propose to apply an element-wise sigmoid to each output in the final layer so that the pixel-wise predictions  can be thought of as probabilities for independent binary random variables. An appropriate loss in such a setting is the
binary cross entropy, which is the average of the individual binary cross entropies (BCE) across all pixels:
%As an alternative to the limitations of MSE as a content loss, we propose Binary Cross Entropy (BCE), defined as:
\begin{equation}
%\begin{split} commented for IMAVIS
\mathcal{L}_{BCE} = 
-\frac{1}{N}
\sum_{j=1}^{N} 
%\sum_{(x,y)=(1,1)}^{(W,H)}
(S_j \log(\hat{S}_j) + (1 - S_j)\log(1 - \hat{S}_j)).  
%\end{split} commented for IMAVIS
\end{equation}



\subsection{Adversarial loss}
\label{ssec:Adversarial}


Generative adversarial networks (GANs)~\citep{goodfellow2014generative} are commonly used to generate images with realistic statistical properties. The idea is to simultaneously fit two parametric functions. The first of these functions, known as the generator, is trained to transform samples from a simple distribution (e.g. Gaussian) into samples from a more complicated distribution (e.g. natural images). The second function, the discriminator, is trained to distinguish between samples from the true distribution and generated samples. Training proceeds alternating between training the discriminator using generated and real samples, and training the generator, by keeping the discriminator weights constant and backpropagating the error through the discriminator to update the generator weights.

% Differences of saliency prediction with respect to classic GAN
The saliency prediction problem has some important differences from the above scenario. First, the objective is to fit a deterministic function that predict realistic saliency values from images, rather than realistic images from random noise. As such, in our case the input to the generator (saliency prediction network) is not random noise but an image. Second, the input image that a saliency map corresponds to is essential, due the fact the goal is not only to have the two saliency maps becoming indistinguishable but with the condition that they both correspond the same input image. We therefore include both the image and saliency map as inputs to the discriminator network. Finally, when using generative adversarial networks to generate realistic images, there is generally no ground truth to compare against. In our case, however, the corresponding ground truth saliency map is available. When updating the parameters of the generator function, we found that using a loss function that is a combination of the error from the discriminator and the cross entropy with respect to the ground truth improved the stability and convergence rate of the adversarial training.
The final loss function for the saliency prediction network during adversarial training can be formulated as:
% \begin{equation}
% \begin{split}
% \mathcal{L} =
% \alpha \cdotp \mathcal{L}_{BCE}
% %\sum_{n=1}^{ N} 
% -\log D(I,\hat{S}), 
% \end{split}
% \end{equation}
\begin{equation}
%\begin{split} commented for IMAVIS
\mathcal{L} =
\alpha \cdotp \mathcal{L}_{BCE} +
% \sum_{i=1}^{ N} 
L( D(I,\hat{S}), 1).
%\end{split}
\label{eq:alpha}
\end{equation}
%
where $L$ is the binary cross entropy loss, and 1 is the target category of real samples and 0 for the category of fake (predicted) sample.  Here,  instead of minimizing $-L( D(I,S), 0)$, we optimize  $L( D(I,S), 1)$ which provides stronger gradient, similar to \citep{goodfellow2014generative} .
%$n$ refers to a sample in a training dataset of $N$ images and 
$D(I,\hat{S})$ is the probability of fooling the discriminator network, so that
%estimated probability that
%the generated saliency map $\hat{S}$ is a natural saliency map from the ground truth, in both cases related to sample $n$ from a training dataset of $N$ images.
the loss associated to the saliency prediction network will grow more when chances of fooling the discriminator are lower.
%In our experiments, we used an hyperparameter of $\alpha=0.05$.
During the training of the discriminator, no content loss is available and  the loss function is:
\begin{equation}
% \begin{split} commented for IMAVIS
\mathcal{L_D} =
% \sum_{i=1}^{ N} 
L( D(I,S), 1) +
L( D(I,\hat{S}), 0).
% \end{split} commented for IMAVIS
\end{equation}
%\textcolor{red}{This formulation must be validated by Junting and Kevin.}
%become larger if the probability
At train time, we first bootstrap the saliency prediction network  function by training for 15 epochs using only BCE, which is computed with respect to the down-sampled output and ground truth saliency. After this, we add the discriminator and begin adversarial training. The input to the discriminator network is an RGBS image of size $256\times 192\times 4$ containing both the source image channels and (predicted or ground truth) saliency. 

% The loss function is a weighted combination of the GAN loss and BCE with respect to the ground truth. (\textcolor{red}{what are the weights??}\textcolor{blue}{@Kevin: The weight for the BCE is 0,005 since I divided the BCE by 20 before I added it to the Adversarial Cost for the learning of the Generator})

We train the networks on the 15,000 images from the SALICON training set using a batch size of 32. 
%This was the largest batch size we could use given the memory constraints of our hardware. 
During the adversarial training, we alternate the training of the saliency prediction network and discriminator network after each iteration (batch). We used L2 weight regularization (i.e. weight decay) when training both the generator and discriminator ($\lambda = 1\times 10^{-4}$). We used AdaGrad for optimization, with an initial learning rate of $3\times 10^{-4}$.



% \textcolor{red}{This section must be reviewed, as I do not really know what is the loss function you are computing. I was getting inspiration from this paper, in case you want some reference: \cite{ledig2016photo}}

%In addition to a content loss, the perceptual loss used to train SalGAN includes an additional term.
%The adversarial loss is computed based on the probability of the discriminator of being fooled with a synthetic image generated by SalGAN.
%This teaches SalGAN to generate more natural images which we assume that will have a positive impact in the metrics for saliency prediction.

%The adversarial loss for SalGAN is compute considering all the $N$ samples in the training dataset. 
%In the case of the generator, which corresponds to SalGAN, this loss can be defined as

% \begin{equation}
% \mathcal{L}_{Adv}(\hat{S}) 
% = 
% -\sum_{n=1}^{N}
% \log D(\hat{S})  
% \end{equation}

% where $D(\hat{S})$ is the probability estimated by the discriminator that the synthetic saliency map $\hat{S}$ is actually natural, that is, the probability that the generator will fool the discriminator.

% % Loss function for the discriminator
% On the other hand, the filter weights for the discriminator network presents an opposite formulation. As presented...

% \textcolor{red}{The loss function and whatever relevant particularity for the discriminator should be explained here.}

\section{Experiments}
\label{sec:Experiments}

% Overview of the experiments
\par The presented SalGAN model for visual saliency prediction was assessed and compared from different perspectives. First, the impact of using BCE and the downsampled saliency maps are assessed. Second, the gain of the adversarial loss is measured and discussed, both from a quantitative and a qualitative point of view. Finally, the performance of SalGAN is compared to published works to compare its performance with the current state-of-the-art.
% Datasets
The experiments aimed at finding the best configuration for SalGAN were run using the \textit{train} and \textit{validation} partitions of the SALICON dataset~\citep{jiang2015salicon}.
This is a large dataset built by collecting mouse clicks on a total of 20,000 images from the Microsoft Common Objects in Context (MS-CoCo) dataset~\citep{lin2014microsoft}.
We have adopted this dataset for our experiments because it is the largest one available for visual saliency prediction.
In addition to SALICON,we also present results on MIT300, the benchmark with the largest amount of submissions.

\begin{table}
\begin{center}
\begin{tabular}{lccccc}
% \toprule commented for IMAVIS
\hline
 			& sAUC $\uparrow$ 			& AUC-B $\uparrow$		& NSS $\uparrow$ 	& CC $\uparrow$ 		& IG $\uparrow$\\
%\midrule commented for IMAVIS
\hline 
BCE 		& 0.752					& 0.825						& 2.473				& 0.761  				& 0.712 \\
BCE/2 		& 0.750					& 0.820						& \textbf{2.527}	& \textbf{0.764} 		& 0.592 \\
BCE/4 		& \textbf{0.755}		& \textbf{0.831}			& 2.511				& 0.763 				& 0.825 \\
BCE/8 		& 0.754					& 0.827						& 2.503				& 0.762 				& \textbf{0.831}\\
%\bottomrule commented for IMAVIS
\hline
\end{tabular}
\end{center}
\caption{Impact of downsampled saliency maps at (15 epochs) evaluated over SALICON validation. BCE/$x$ refers to a downsample factor of $1/x$ over a saliency map of $256 \times 192$.}
\label{tab:Downsample15}
\end{table}

% Metrics
%The reported metrics for SALICON are the ones considered in the public challenge hosted at Codalab.
%Analogously, results on MIT300 also adjust to the metrics adopted by the organizers.

\subsection{Non-adversarial training}
\label{ssec:ContentLossExp}

%\textcolor{red}{The tone of this section must be rewritten to just state that downsample maps were used to speed up teh process without decraseing the performance. Also show how the simple change from MSE to BCE increases performance.}
% Intro to experiments on the content loss
The two content losses presented in content loss section, MSE and BCE, were compared to define a baseline upon which we later assess the impact of the adversarial training.
%two content loss functions were considered as  content loss used in SalGAN has been studied from two perspectives: by comparing MSE and BCE as loss functions, and by computing them over downsampled versions of the saliency maps.
% MSE vs BCE
The two first rows of Table~\ref{tab:Adversarial} shows how a simple change from MSE to BCE brings a consistent improvement in all metrics. This improvement suggests that treating saliency prediction as multiple binary classification problem is more appropriate than treating it as a standard regression problem, in spite of the fact that the target values are not binary. Minimizing cross entropy is equivalent to minimizing the KL divergence between the predicted and target distributions, which is a reasonable objective if both predictions an targets are interpreted as probabilities.
%\textcolor{red}{We could add some qualitative results comparing MSE vs BCE if necessary}
% The Figure \ref{fig:qualitative} shows two examples of saliency maps computed with the two metrics, where the blurring effect is clearly noticeable. 
% \textcolor{red}{Junting: DO we have images to show qualitatively that the saliency maps trained with BCE are not as blurry as those trained with MSE ?}.\textcolor{blue}{@Xavi: We do have results of both loss function, however I think is not very clear what you  mentioned}

% \begin{table}
% \begin{center}
% \begin{tabular}{lcccc}
% \toprule
% 												&	sAUC $\uparrow$ 	& AUC-B $\uparrow$		& NSS $\uparrow$ 	& CC $\uparrow$ \\
% \midrule
% MSE												& 0.7282				& 0.8198				& 1.6797				& 0.7075 \\
% BCE												& 0.7547				& 0.8311				& 2.4729			& 0.7613 \\
% \bottomrule
% \end{tabular}
% \end{center}
% \caption{MSE vs BCE as a content loss function. }
% \label{tab:MSEvsBCE}
% \end{table}

% Downsample
Based on the superior BCE-based loss compared with MSE, we also explored the impact of computing the content loss over downsampled versions of the saliency map.
This technique reduces the required computational resources at both training and test times and, as shown in Table~\ref{tab:Downsample15}, not only does it not decrease performance, but it can actually improve it.
Given this results, we chose to train SalGAN on saliency maps downsampled by a factor $1/4$, which in our architecture corresponds to saliency maps of $64 \times 48$.

%required to train and feed-forward the 
%addressed at minizing the blurring effect of training models with MSE.
%\textcolor{red}{XAVI: Must be better explained. More info \url{https://swarbrickjones.wordpress.com/2016/01/24/generative-adversarial-autoencoders-in-theano/}}.


%The impact of computing the content loss over a downsampled saliency map is presented in Table \ref{tab:Downsample45}.
%Results indicate that working with a downsampled version of the saliency maps does not decrease performance and that it could actually even increase it.
%As processing lower definition maps reduces the computational requirements of the system, we chose to train SalGAN one saliency maps downsampled by a factor $1/4$, which correspondons to saliency maps of $64 \times 48$.

%\textcolor{red}{Junting: DO we have images to show qualitatively that the saliency maps trained with BCE are not as blurry as those trained with MSE ?}
%\textcolor{red}{All: Any hypothesis about why we obtain this gain with this low definition training ?}



% \begin{table}
% \begin{center}
% \begin{tabular}{lccccc}
% \toprule
%  			& sAUC $\uparrow$ 			& AUC-B $\uparrow$		& NSS $\uparrow$ 	& CC $\uparrow$ 		& IG $\uparrow$\\
% \midrule
% BCE 		& 0.7519					& 0.8250				& 2.4729			& 0.7613  				& 0.7115 \\
% BCE/2 		& 0.7499					& 0.8198				& \textbf{2.5274}	& \textbf{0.7642} 		& 0.5920 \\
% BCE/4 		& \textbf{0.7547}			& \textbf{0.8311}		& 2.5111			& 0.7631 				& 0.8246 \\
% BCE/8 		& 0.7536					& 0.8273				& 2.5026			& 0.7620 				& \textbf{0.8314}\\
% \bottomrule
% \end{tabular}
% \end{center}
% \caption{Impact of downsampled saliency maps at (15 epochs) evaluated over SALICON validation. BCE/$x$ refers to a downsample factor of $1/x$ over a saliency map of $256 \times 192$.}
% \label{tab:Downsample15}
% \end{table}

% \begin{table}
% \begin{center}
% \begin{tabular}{lcccc}
% \toprule
%  Map size			& sAUC $\uparrow$ 		& AUC-B $\uparrow$		& NSS $\uparrow$ 	& CC $\uparrow$ \\
% \midrule
% $256 \times 192$	& 0.7527						& 0.8243					& 2,5622				& 0,7717  \\
% $128 \times 96$		& 0.7465				& 0.8146				& \textbf{2.6133}	& \textbf{0.7723} \\
% $64 \times 48$		& 0.7473				& 0.8161				& 2.6045			& 0.7721 \\
% $32 \times 24$		& \textbf{0.7537}		& \textbf{0.8280}		& 2.5312			& 0.7688 \\
% \bottomrule
% \end{tabular}
% \end{center}
% \caption{Impact of downsampled saliency maps. (45 epochs)}
% \label{tab:Downsample45}
% \end{table}

\subsection{Adversarial gain}

The adversarial loss was introduced after estimating the value of the hyperparameter $\alpha$ in Equation \ref{eq:alpha} by maximizing the most general metric, Information Gain (IG).  As shown in Figure \ref{fig:alpha}, the search was performed on logarithmic scale, and we achieved the best performance for $\alpha=0.005$. %Figure \ref{fig:alpha} plots the evolution of IG for different values of 

\begin{figure}[ht]
\centering
\includegraphics[width=1.0\columnwidth]{IG.pdf}
\caption{SALICON validation set Information Gain for different hyper parameter $\alpha$ on varying numbers of epochs.}
\label{fig:alpha}
\end{figure}


The Information Gains (IG) of SalGAN  for different values of the hyper parameter $\alpha$ are compared in Figure~\ref{fig:alpha}. The search for finding an optimal hyper parameter $\alpha$ is performed on logarithmic scale, and we achieved the best performance for $\alpha=0.005$.



\begin{figure}
\centering
\includegraphics[width=1.0\columnwidth]{acc-epochs3.pdf}
\caption{SALICON validation set accuracy metrics for Adversarial+BCE vs BCE on varying numbers of epochs. AUC shuffled is omitted as the trend is identical to that of AUC Judd.\vspace*{-4mm}}
\label{fig:ganvsbce}
\end{figure}

The gain achieved by introducing the adversarial loss into the perceptual loss was assessed by using BCE as a content loss and feature maps of $68 \times 48$.
The first row of results in Table~\ref{tab:Adversarial} refers to a baseline defined by training SalGAN with the BCE content loss for 15 epochs only. 
Later, two options are considered: 1) training based on BCE only (2nd row), or 2) introducing the adversarial loss (3rd and 4th row). %\textcolor{red}{TODO: Figure 3 is never referred to in the text.}

Figure~\ref{fig:ganvsbce} compares validation set accuracy metrics for training with combined GAN and BCE loss versus a BCE alone as the number of epochs increases. In the case of the AUC metrics (Judd and Borji), increasing the number of epochs does not lead to significant improvements when using BCE alone. The combined BCE/GAN loss however, continues to improve performance with further training. After 100 and 120 epochs, the combined GAN/BCE loss shows substantial improvements over BCE for five of six metrics.

The single metric for which Adversarial training fails to improve performance is normalized scanpath saliency (NSS). The reason for this may be that GAN training tends to produce a smoother and more spread out estimate of saliency, which better matches the statistical properties of real saliency maps, but may increase the false positive rate. As noted in \citep{Bylinskii2016metrics}, NSS is very sensitive to such false positives. The impact of increased false positives depends on the final application. In applications where the saliency map is used as a multiplicative attention model (e.g. in retrieval applications, where spatial features are importance weighted), false positives are often less important than false negatives, since while the former includes more distractors, the latter removes potentially useful features. Note also that NSS is differentiable, so could potentially be optimized directly when important for a particular application.


%\textcolor{red}{TODO: Including iteration 0 (i.e. the starting model) may make the improvements clearer.}

\begin{table}
\begin{center}
\begin{tabular}{lcccccc}
%\toprule commented for IMAVIS
\hline
			& sAUC $\uparrow$ 	& AUC-B $\uparrow$		& NSS $\uparrow$ 	& CC $\uparrow$ 	& IG \\
%\midrule commented for IMAVIS
\hline
MSE			& 0.728			& 0.820				& 1.680			& 0.708 			& 0.628 \\
BCE			& 0.753			& 0.825				& 2.562			& 0.772 			& 0.824 \\
%\midrule commented for IMAVIS
\hline
BCE/4		& 0.757			& 0.833				& \textbf{2.580}	& 0.772 			& 1.067\\
GAN/4		& \textbf{0.773}	& \textbf{0.859}		& 2.560			& \textbf{0.786} 	& \textbf{1.243}\\

%\bottomrule commented for IMAVIS
\hline
\end{tabular}
\end{center}
\caption{Best results through epochs obtained with non-adversarial (MSE and BCE) and adversarial training. BCE/4 and GAN/4 refer to downsampled saliency maps.  Saliency maps assessed on SALICON validation. }
\label{tab:Adversarial}
\end{table}


% \begin{table}
% \begin{center}
% \begin{tabular}{lcccccc}
% \toprule
% 			& sAUC $\uparrow$ 	& AUC-B $\uparrow$		& NSS $\uparrow$ 	& CC $\uparrow$ 	& IG \\
% \midrule
% MSE			& 0.7282			& 0.8198				& 1.6797			& 0.7075 			& 0.6282 \\
% BCE			& 0.7527			& 0.8250				& 2.5622			& 0.7717 			& 0.8243 \\
% \midrule
% BCE/4		& 0.7574			& 0.8331				& \textbf{2.5803}	& 0.7723 			& 1.0669\\
% GAN/4		& \textbf{0.7731}	& \textbf{0.8588}		& 2.5892			& \textbf{0.7858} 	& \textbf{1.2425}\\

% \bottomrule
% \end{tabular}
% \end{center}
% \caption{Best results through epochs obtained with non-adversarial (MSE and BCE) and adversarial (GAN) training. BCE/4 and GAN/4 refer to downsampled saliency maps.  Saliency maps assessed on SALICON validation. }
% \label{tab:Adversarial}
% \end{table}

\subsection{Comparison with the state-of-the-art}
\label{ssec:SoAExp}

% \begin{figure}[t]
% \includegraphics[width=\linewidth]{fig/MIT_Examples.pdf}
% \caption{Example images from MIT300 containing salient region (marked in yellow) that is often missed by computational models, and saliency map estimated by SalGAN.}
% \label{fig:MITExamples}
% \end{figure}

SalGAN is compared in Table~\ref{tab:soa} to several other algorithms from the state-of-the-art.
The comparison is based on the evaluations run by the organizers of the SALICON and MIT300 benchmarks on a test dataset whose ground truth is not public.
The two benchmarks offer complementary features: while SALICON is a much larger dataset with 5,000 test images, MIT300 has attracted the participation of many more researchers.
In both cases, SalGAN was trained using 15,000 images contained in the training (10,000) and validation (5,000) partitions of the SALICON dataset.
Notice that while both datasets aim at capturing visual saliency, the acquisition of data differed, as SALICON ground truth was generated based on crowdsourced mouse clicks, while the MIT300 was built with eye trackers on a limited and controlled group of users.
Table \ref{tab:soa} compares SalGAN with other contemporary works that have used SALICON and MIT300 datasets.
%not only with published models, but also with works that have not been peer reviewed.
SalGAN presents very competitive results in both datasets, as it improves or equals the performance of all other models in at least one metric. 

%reported scores include in our tables algorithms presented in publications with their corresponding citations, but also some other models which are described only in preprints that have not been reviewed by peers.

%The presented figures have been obtained from the leaderboards of the referred public challenges.

% SALICON
%The results presented in the previous section have all been obtained by learning the parameters of SalGAN from the training partition of the SALICON dataset, and assessed the results on the validation partition. 

%However, SALICON results in Table \ref{tab:soa} refer to the test partition, whose ground truth is not publicly available.
%the comparison with other solutions in the state-of-the-art shown in Table \ref{tab:SALICON} were computed by the online Codalab platform for benchmarking on a different \textit{test} partition.
%In this case, SalGAN was trained with data from both the train and validation partitions in SALICON.

% MIT300
% SalGAN was compared with a larger amount of solutions at the MIT300 benchmark.
% This is a much smaller dataset as it only considers 300 images, but has a large history and a much higher amount of works have used it.
% In this case, fixation points were captured with eye trackers indoor and outdoor natural scenes from the Flickr Creative Commons and LabelMe \cite{russell2008labelme} datasets.
%Table \ref{tab:mit300} compares SalGAN with the top performing solutions at the time of writing this paper.


%\textcolor{red}{It would be desirable to study in which metrics does SalGAN work better and why. The arXiv from Zoya about the different metrics should help in this task.}

%\textcolor{red}{Kevin: Need a discussion section. Probably worth noting that DSCLRCN is based on ResNet 50, for which the authors observed a sizable improvement over VGG 16. We could expect a similar improvement. Also worth noting that our model is not an ensemble, and that performance improvements could be achieved by an ensemble. We didn't tune hyperparameters, which would probably also give gains.}



% \begin{table}
% \begin{center}
% \begin{tabular}{lcccc}
% \toprule
% 												&	sAUC $\uparrow$ 	& AUC-B $\uparrow$		& NSS $\uparrow$ 	& CC $\uparrow$ \\
% \midrule

% DSCLRCN \cite{liu2016deep}(*)						& 0.776				& 0.884					& 3.157				& 0.831 \\
% %SAM											& 0.778					& 0.887					& 3.058				& 0.851 \\
% \textbf{SalGAN}									& \textbf{0.772}		& \textbf{0.884}		& \textbf{2.459}	& \textbf{0.781}\\
% DeepGaze II	\cite{kummerer2016deepgaze}*		& 0.787					& (0.867)				& (1.271)			& (0.479) \\
% ML-NET \cite{mlnet2016}							& (0.768)				& (0.866)		   		& 2.789				& (0.743) \\
% SalNet \cite{Pan_2016_CVPR}						& (0.724)				& (0.858)				& (1.859)			& (0.609) \\
% \bottomrule
% \end{tabular}
% \end{center}
% \caption{Results for the SALICON test set, according to the scores provided by the Codalab on November 10, 2016. [*] Not reviewed by peers.}
% \label{tab:SALICON}
% \end{table}

% \begin{table}
% \begin{center}
% \begin{tabular}{lcccc}
% \toprule
% 												&	sAUC $\uparrow$ 	& AUC-B $\uparrow$		& NSS $\uparrow$ 	& CC $\uparrow$ \\
% \midrule

% DSCLRCN \cite{liu2016deep}[*]					& 0.776					& 0.884					& 3.157				& 0.831 \\
% %SAM											& 0.778					& 0.887					& 3.058				& 0.851 \\
% \textbf{SalGAN}									& \textbf{0.772}		& \textbf{0.884}		& \textbf{2.459}	& \textbf{0.781}\\
% DeepGaze II	\cite{kummerer2016deepgaze}[*]		& 0.787					& (0.867)				& (1.271)			& (0.479) \\
% ML-NET \cite{mlnet2016}							& (0.768)				& (0.866)		   		& 2.789				& (0.743) \\
% SalNet \cite{Pan_2016_CVPR}						& (0.724)				& (0.858)				& (1.859)			& (0.609) \\
% \bottomrule
% \end{tabular}
% \end{center}
% \caption{Results for the SALICON test set, according to the scores provided by the Codalab on November 10, 2016. [*] Not reviewed by peers.}
% \label{tab:SALICON}
% \end{table}

\begin{table*}
\small
\begin{center}
\begin{tabular}{lcccccccc}
%\toprule commented for IMAVIS
\hline
SALICON (test)				&	AUC-J $\uparrow$	& Sim $\uparrow$ & EMD  $\downarrow$ & AUC-B $\uparrow$ & sAUC $\uparrow$ 	& CC $\uparrow$ & NSS $\uparrow$ & KL $\downarrow$ \\		
%\midrule
\hline
DSCLRCN \citep{liu2018deep} 			& -					&	-			&	-				&	0.884			&	0.776			&	0.831		& 3.157			&	-				\\
%SAM							& -						&	-			&	-				&   0.887			& 0.778			& 0.851 		& 3.058				\\

\textbf{SalGAN}				& -						&	-			&	-				&	\textbf{0.884}	&	\textbf{0.772}	&	\textbf{0.781}	& \textbf{2.459} &	-				\\
ML-NET \citep{mlnet2016}		& -						&	-			&	-				&	(0.866)			&	(0.768)			&	(0.743)		& 2.789			&	-				\\
SalNet \citep{Pan_2016_CVPR}	& -						&	-			&	-				&	(0.858)			&	(0.724)			&	(0.609)		& (1.859)		&	-				\\
%\bottomrule commented for IMAVIS
\hline
MIT300						&	AUC-J $\uparrow$	& Sim $\uparrow$ & EMD  $\downarrow$ & AUC-B $\uparrow$ & sAUC $\uparrow$ 	& CC $\uparrow$ & NSS $\uparrow$ & KL $\downarrow$ \\
%\midrule commented for IMAVIS
\hline
Humans 						& 0.92	 	& 1.00		& 0.00 		& 0.88		& 0.81 		& 1.0 		& 3.29 		& 0.00	\\
Deep Gaze II \cite{kummerer2017understanding}			& (0.84)		& (0.43)	& (4.52)	& (0.83)		& 0.77		& (0.45)	& (1.16) 	& (1.04) \\
DSCLRCN \citep{liu2018deep}	& 0.87		& 0.68		& 2.17		& (0.79)	& 0.72		& 0.80		& 2.35		& 0.95\\	
% DeepFix \cite{kruthiventi2015deepfix}(*) 				& 0.87		& 0.67		& 2.04		& (0.80)	& (0.71)	& 0.78		& 2.26		& 0.63 \\
SALICON	\citep{huang2015salicon}		& 0.87		& (0.60)	& (2.62)	& 0.85		& 0.74		& 0.74		& 2.12		& 0.54 \\
\textbf{SalGAN}				& \textbf{0.86}	& \textbf{0.63}		& \textbf{2.29}		& \textbf{0.81}		& \textbf{0.72}		& \textbf{0.73}	& \textbf{2.04}	& \textbf{1.07} \\
PDP	\citep{jetley2016end}	& (0.85)	& (0.60)	& (2.58)	& (0.80)	& 0.73		& (0.70)	& 2.05		& 0.92 \\
ML-NET \citep{mlnet2016}		& (0.85)	& (0.59)	& (2.63)	& (0.75)	& (0.70)	& (0.67)	& 2.05		& (1.10) \\
Deep Gaze I \citep{kummerer2015deep} & (0.84)& (0.39) & (4.97)	& 0.83		& (0.66)	& (0.48)	& (1.22)	& (1.23) \\
% iSEEL \cite{tavakoli2016exploiting}(*)					& (0.84)	& (0.57)	& (2.72)	& 0.81		& (0.68)	& (0.65)	& (1.78)	& 0.65 \\
SalNet	\citep{Pan_2016_CVPR}& (0.83)		& (0.52) & (3.31)	& 0.82		& (0.69)	& (0.58)	& (1.51)	& 0.81 \\
BMS \citep{Zhang2013iccv}	& (0.83)	& (0.51)	& (3.35)	& 0.82		& (0.65)	& (0.55)	& (1.41)	& 0.81 \\ 			
%\bottomrule
\hline
\end{tabular}
\end{center}
\caption{Comparison of SalGAN with other state-of-the-art solutions on the SALICON (test) and MIT300 benchmarks. Values in brackets correspond to performances worse than SalGAN.}
\label{tab:soa}
\end{table*}

\subsection{Qualitative results}

The impact of adversarial training has also been explored from a qualitative perspective by observing the resulting saliency maps.
\begin{figure}[t]
\includegraphics[width=\linewidth]{mit_example_new.pdf}
\caption{Example images from MIT300 containing salient region (marked in yellow) that is often missed by computational models, and saliency map estimated by SalGAN.}
\label{fig:MITExamples}
\end{figure}
Figure~\ref{fig:MITExamples} shows one example from the MIT300 dataset, highlighted in \citep{Bylinsk2016eccv} as being particular challenges for existing saliency algorithms. The areas highlighted in yellow in the images on the left are regions that are typically missed by algorithms. In the this  example, we see that SalGAN successfully detects the often missed hand of the magician and face of the boy as being salient. 
% The second example illustrates a failure case where SalGAN, like other algorithms, also fails to place sufficient saliency on the area around the white ball (though this region does have more saliency than most of the rest of the image). The final example illustrates what we believe to be one of the limitations of this dataset. The ground truth places most of the saliency on the smaller text at the bottom of the sign. This is because observers tend spend more \textit{time} attending this area (reading the text), and not because it is the \textit{first} area that observers tend to attend. Existing metrics, however, tend to be agnostic to the order in which areas are attended, a limitation we hope to look into in the future.

\begin{figure}
\centering
\includegraphics[width=1.0\columnwidth]{bcegan_artifacts.pdf}
\caption{Close-up comparison of output from training on BCE loss vs combined BCE/Adversarial loss. Left: saliency map from network trained with BCE loss. Right: saliency map from proposed adversarial training.}
\label{fig:bcsgan_artifacts}
\end{figure}

\par Figure~\ref{fig:bcsgan_artifacts} illustrates the effect of adversarial training on the statistical properties of the generated saliency maps. Shown are two close up sections of a saliency map from cross entropy training (left) and adversarial training (right). Training on BCE alone produces saliency maps that while they may be locally consistent with the ground truth, are often less smooth and have complex level sets. Adversarial training on the other hand produces much smoother and simpler level sets.
\begin{figure*}
\centering
\includegraphics[width=1\linewidth]{new_qualitative_vertical.pdf}
\caption{Qualitative results of SalGAN on the SALICON validation set. SalGAN predicts well those high salient regions which are missed by BCE model. Saliency maps of BCE model are very localized in a few salient regions, they tend to fail when the number of salient regions increases. }
\label{fig:qualitativeResults}
\end{figure*}
Finally, Figure~\ref{fig:qualitativeResults} shows some qualitative results comparing the results from training with BCE and BCE/Adversarial against the ground truth for images from the SALICON validation set. %\textcolor{red}{TODO: comment on results when figure is replaced!}


% % Low-level artifacts
% The examples in Figure~\ref{fig:qualitativeResults} show how some perceptual artifacts are solved with the adversarial training.
% The images show how the saliency maps trained with GAN are smoother and cover more areas.
% This can be explained because during GAN training the generator tries to fool the discriminator by mimicking the statistical properties of the ground truth data.
% On the the other hand, the training based on BCE only focuses in the individual and local values of each pixel in the saliency map.

%\textcolor{red}{These qualitative results might be further detailed based on the actual Figure 4, as we wrote them without actually looking at them :)}

% \begin{figure*}[!htb]
% \minipage{0.32\textwidth}
%   \includegraphics[width=\linewidth]{fig/cake-gt.png}
% %  \caption{(a)}\label{fig:cake-gt}
% \endminipage\hfill
% \minipage{0.32\textwidth}
%   \includegraphics[width=\linewidth]{fig/cake-bce.png}
% %  \caption{BCE}\label{fig:cake-bce}
% \endminipage\hfill
% \minipage{0.32\textwidth}%
%   \includegraphics[width=\linewidth]{fig/cake-gan.png}
% %  \caption{GAN}\label{fig:cake-gan}
% \endminipage
% \vspace{10px}
% \label{fig:qualitative}
% \caption{Saliency map overlaid its source image: Ground truth (left), BCE (center) and GAN (right)}\label{fig:cake-gt}
% \end{figure*}



% Any more semantics detected ?
% The cases in Figure X correspond to situations where the adversarial has increased the saliency of semantically relevant parts.
% This way, the adversarial training is focusing in areas of the image containing objects or faces, despite not being trained with any type of semantic supervision.

%% Comment on MIT images:
% By visualizing the predictions of images in the Figure 5. It shows that SalGAN preforms well on some of high-density region of human fixation such as object of gaze and people. However the prediction continues missing some semantic parts like actions and informativeness of text.




\section{Conclusions}
\label{sec:Conclusions}

To the best of our knowledge, this is the first work that proposes an adversarial-based approach to saliency prediction and has shown how adversarial training over a deep convolutional neural network can achieve state-of-the-art performance with a simple encoder-decoder architecture.
A BCE-based content loss was shown to be effective for both initializing the saliency prediction network, and as a regularization term for stabilizing adversarial training. Our experiments showed that adversarial training improved all bar one saliency metric when compared to further training on cross entropy alone.

% Future work
It is worth pointing out that although we use a VGG-16 based encoder-decoder model as the saliency prediction network in this paper, the proposed adversarial training approach is generic and could be applied to improve the performance of other saliency models. 
% Further improvements could also likely be achieved by carefully tuning hyperparameters, and in particular the trade-off between BCE and adversarial loss , which we did not attempt to optimize for this paper. %Finally, an ensemble of models is also likely to improve performance, at the cost of additional computation at predict time.


%can improve performance in most visual saliency prediction metrics. \textcolor{red}{??}
%Our model and software will be publicly on GitHub and GitXiv with MIT license once the anonymously requirement for double blind peer reviewed is waived.



% \include{01-Introduction}
% \input{02-RelatedWork}
% \input{03-Architecture}
% \input{04-Training}
% \input{05-Experiments}
% \input{99-Conclusions}

\section*{Acknowledgments}
The Image Processing Group at UPC is supported by the project TEC2016-75976-R, funded by the Spanish Ministerio de Economia y Competitividad and the European Regional Development Fund (ERDF). 
This material is based upon works supported by Science Foundation Ireland under Grant No 15/SIRG/3283.
We gratefully acknowledge the support of NVIDIA Corporation for the donation of GPUs used in this work.

% \section*{References}

% Please ensure that every reference cited in the text is also present in
% the reference list (and vice versa).

% \section*{\itshape Reference style}

% Text: All citations in the text should refer to:
% \begin{enumerate}
% \item Single author: the author's name (without initials, unless there
% is ambiguity) and the year of publication;
% \item Two authors: both authors' names and the year of publication;
% \item Three or more authors: first author's name followed by `et al.'
% and the year of publication.
% \end{enumerate}
% Citations may be made directly (or parenthetically). Groups of
% references should be listed first alphabetically, then chronologically.

\bibliographystyle{model2-names}
\bibliography{refs}

% \section*{Supplementary Material}

% Supplementary material that may be helpful in the review process should
% be prepared and provided as a separate electronic file. That file can
% then be transformed into PDF format and submitted along with the
% manuscript and graphic files to the appropriate editorial office.

\end{document}

%%
