<?xml version="1.0" encoding="UTF-8"?>
<opml version="2.0">
  <head>
    <title></title>
    <abstract> </abstract>
  </head>
  <body>
<outline text="Matrix Factorization Model" _note="This section will present the matrix factorization model which is used&#10;for outlier detection. Before discussing the model in detail, we present&#10;the notations and definitions. We represent the corpus of text documents&#10;as a bag of words matrix. A lowercase or uppercase letter such as or ,&#10;is used to denote a scalar. A boldface lowercase letter, such as , is&#10;used to denote a vector, and a boldface uppercase letter, such as , is&#10;used to denote a matrix. This is consistent with what is commonly used&#10;in much of the data mining literature. Indices typically start from ,&#10;unless otherwise mentioned. For a , denotes its column, denotes its row&#10;and or or denote its element.&#10;&#10;For greater expressibility, we have also borrowed certain notations from&#10;matrix manipulation scripts such as Matlab and Octave. For example, the&#10;notation returns the maximal element and returns a vector of maximal&#10;elements from each column . Similarly, denotes the -th row of the matrix&#10;and for -th column. For the readerâ€™s convenience, the notations used in&#10;the paper are summarized in Table \[table:notations\].&#10;&#10;Let be the matrix representing the underlying data. In the context of a&#10;text collection, this corresponds to a term-document matrix, where terms&#10;correspond to rows and documents correspond to columns. In other words,&#10;denotes the number of times the term appears in document . Generally, we&#10;can write as follows:&#10;&#10;\[0.29\][]{}&#10;&#10;Here, is a low rank matrix and represents the matrix of outlier entries.&#10;Typically, the matrix represents the documents created by a lower rank&#10;generative process (such as that modeled by pLSI), and the parts of the&#10;documents that do not correspond to the generative process are&#10;represented as part of the matrix . In real world scenarios, the outlier&#10;matrix contains entries which are very close to zero, and only a small&#10;number of entries have [SIGNIFICANTLY]{} non-zero values. These&#10;significantly nonzero entries are often present in only a small fraction&#10;of the columns. Columns which are fully representable in terms of&#10;factors are consistent with the low rank behavior of the data, and&#10;therefore [NOT]{} outliers. The rank of is not known in advance, and it&#10;can be expressed in terms of its underlying factors. Here, the two&#10;matrices have dimensions , , and . The matrices and are non-negative,&#10;and this provides interpretability in terms of being able to express a&#10;document as a non-negative linear combination of the relevant basis&#10;vectors, each of which in itself can be considered a frequency-annotated&#10;bag of words (topics) because of its non-negativity. Specifically,&#10;corresponds to the coefficients for the basis matrix . Intuitively, this&#10;corresponds to the case that every document , is represented as the&#10;linear combination of the topics. In cases, where this is [NOT]{} true,&#10;the document is an outlier, and those unrepresentable sections of the&#10;matrix are captured by the non-zero entries in the matrix. In real&#10;scenarios, the entries in this matrix are often extremely skewed, and&#10;the small number of non-zero entries very obviously expose the outliers.&#10;The decomposition of the matrix into different component is pictorially&#10;illustrated in Figure \[fig:outlieroverview\].&#10;&#10;In order to determine the best low rank factorization, one must try to&#10;optimize the aggregate values of the residuals in the matrix. This can&#10;of course be done in a variety of ways, depending upon the goals of the&#10;underlying factorization process. We model the determination of the&#10;matrices ,, and , as the following optimization problem:&#10;&#10;The specific location of outliers in each column does not have a closed&#10;form solution, since the -norm penalty is applied to . The logic for&#10;applying the -norm in the context of the outlier detection problem is as&#10;follows. Each entry in the corresponds to a term in a document, whereas&#10;we are interested in the outlier behavior of entire document. This&#10;aggregate outlier behavior of the document can be modeled with the norm&#10;score of a particular column . In a real scenario, if a large segment of&#10;a document is not representable as the linear combination of the topics&#10;through , the corresponding column in the matrix will be compensated by&#10;having more entries in its column. In other words, we will have a higher&#10;value for the corresponding column , and this corresponds to a higher&#10;outlier score. Furthermore, the -norm penalty on defines the sum of the&#10;norm outlier scores over all the documents. Therefore, the optimization&#10;problem essentially tries to find the best model, an important component&#10;of which is to minimize the sum of the outlier scores over all&#10;documents. While a variety of different (and more commonly used)&#10;penalties such as the Frobenius norm are available for matrix&#10;factorization models, we have chosen the -norm penalty because of its&#10;intuitive significance in the context of the outlier detection problem,&#10;and its tendency to create skewed outlier scores across the columns of&#10;the matrix. As we will see in the next section, this comes at the&#10;expense of a formulation which is more difficult to solve&#10;algorithmically.&#10;&#10;For high dimensional data, sparse coefficients are desirable for&#10;obtaining an interpretable low rank matrix . For this purpose, we add&#10;the -penalty on : The constant defines the weight for the outlier matrix&#10;over the recovery of the low rank space and the sparsity term. In the&#10;case of outlier detection in text documents, we give more weight for the&#10;outlier matrix over the low rank representation . This problem does not&#10;have a closed form solution, and therefore we cannot directly recover&#10;the low rank matrix in closed form. However, we can recover the column&#10;space. Without non-negativity constraints, this property is also known&#10;as the rotational invariant property . This particular formulation of&#10;the matrix factorization model is a bit different from the commonly used&#10;formulations, and off-the-shelf solutions do not directly exist for this&#10;scenario. Therefore, in a later section, we will carefully design an&#10;algorithm with the use of block coordinate descent for this problem.&#10;&#10;In order to understand the modeling of the outliers better, we present&#10;the readers with a toy example from a real world data set, to show how&#10;skewed the typical values of the corresponding column may be in real&#10;scenarios. In this case, we used the [BBC]{} dataset[^1]. . We computed&#10;the matrix and generated the scores of the columns of outlier matrix .&#10;Figure \[fig:outlierz\] shows the outlier() scores of the documents. The&#10;-axis illustrates the index of the document, and the -axis illustrates&#10;the outlier score. It is evident that the scores for some columns are so&#10;close to zero, that they cannot even be seen on the diagram drawn to&#10;scale. These columns also happened to be the non-outlier/regular&#10;documents of the collection. Such documents correspond to the low rank&#10;space, and are approximately representable as a product of the basis&#10;matrix with the corresponding column vector of coefficients drawn from .&#10;However, the documents that are not representable in such a low rank&#10;space have a large outlier score. From the distribution of the outlier&#10;score, we can also observe that the scores of outlier documents against&#10;non-outliers are clearly separable, by using a simple statistical mean&#10;and standard deviation analysis. Therefore, while we use the scores to&#10;rank the documents in terms of their outlier behavior, the skew in the&#10;entries ensures that it is often easy to choose a cut-off in order to&#10;distinguish the outliers from the non-outliers.&#10;&#10;In the following sections, we will analyze the property and performance&#10;of this model for outlier detection problems.&#10;&#10;[^1]: &lt;http://mlg.ucd.ie/datasets/bbc.html&gt;">
</outline>
  </body>
</opml>