<?xml version="1.0" encoding="UTF-8"?>
<opml version="2.0">
  <head>
    <title></title>
    <abstract> </abstract>
  </head>
  <body>
<outline text="Algorithmic Solution" _note="As discussed earlier our technique is based on NMF, and this particular&#10;formulation , which is suited to outlier analysis, is relatively&#10;uncommon, and does not have a closed form solution. In order to address&#10;this issue we use a Block Coordinate Descent (BCD) framework and its&#10;application to solve the optimization problem . The BCD framework is a&#10;popular choice not only because of the ease in implementation, but also&#10;because it is scalable. First, we will lay the foundation for the basic&#10;BCD technique, as it generally applies to non-linear optimization&#10;problems. We will then relate it to our non-negative matrix&#10;factorization problem, and explain our algorithm () in detail.">
  <outline text="Block coordinate Descent" _note="In this section, we will see relevant foundation for using this&#10;framework. Consider a constrained non-linear optimization problem as&#10;follows: Here, is a closed convex subset of . An important assumption to&#10;be exploited in the BCD method is that the set is represented by a&#10;Cartesian product: where , , is a closed convex subset of , satisfying .&#10;Accordingly, the vector is partitioned as so that for . The BCD method&#10;solves for by fixing all other subvectors of in a cyclic manner. That&#10;is, if is given as the current iterate at the step, the algorithm&#10;generates the next iterate block by block, according to the solution of&#10;the following subproblem: Also known as a NON-LINEAR GAUSS-SEIDEL&#10;method , this algorithm updates one block each time, always using the&#10;most recently updated values of other blocks . This is important since&#10;it ensures that after each update, the objective function value does not&#10;increase. For a sequence where each is generated by the BCD method, the&#10;following property holds.&#10;&#10;\[thm:bcd\] Suppose is continuously differentiable in , where , , are&#10;closed convex sets. Furthermore, suppose that for all and , the minimum&#10;of is uniquely attained. Let be the sequence generated by the block&#10;coordinate descent method as in Eq. . Then, every limit point of is a&#10;stationary point. The uniqueness of the minimum is not required for the&#10;case when .&#10;&#10;The proof of this theorem for an arbitrary number of blocks is shown in&#10;Bertsekas . For a non-convex optimization problem, most algorithms only&#10;guarantee the stationarity of a limit point .&#10;&#10;When applying the BCD method to a constrained non-linear programming&#10;problem, it is critical to wisely choose a partition of , whose&#10;Cartesian product constitutes . An important criterion is whether the&#10;sub-problems in Eq.  are efficiently solvable. For example, if the&#10;solutions of sub-problems appear in a closed form, each update can be&#10;computed fast. In addition, it is worth checking how the solutions of&#10;sub-problems depend on each other. The BCD method requires that the most&#10;recent values be used for each sub-problem in Eq. . When the solutions&#10;of sub-problems depend on each other, they have to be computed&#10;sequentially to make use of the most recent values. If solutions for&#10;some blocks are independent of each other, they can be computed&#10;simultaneously. We discuss how different choices of partitions lead to&#10;different NMF algorithms. The partitioning can be achieved in several&#10;ways, by using either matrix blocks, vector blocks or scalar blocks.">
    <outline text="BCD with Two Matrix Blocks - ANLS Method" _note="The most natural partitioning of the variables is to have two big&#10;blocks, and . In this case, following the BCD method in Eq. , we take&#10;turns solving the following:&#10;&#10;Since the sub-problems are non-negativity constrained least squares&#10;(NLS) problems, the two-block BCD method has been called the alternating&#10;non-negative least square (ANLS) framework .">
    </outline>
    <outline text="BCD with 2k Vector Blocks - HALS/RRI Method" _note="We partition the unknowns into 2k blocks in which each block is a&#10;column/row of or . In this case, it is easier to consider the objective&#10;function in the following form: where and . The form in Eq.  represents&#10;the fact that can be approximated by the sum of rank-one matrices.&#10;&#10;Following the BCD scheme, we can minimize by iteratively solving the&#10;following: for , and for .&#10;&#10;The 2K-block BCD algorithm has been studied as Hierarchical Alternating&#10;Least Squares (HALS) proposed by Cichocki et al. and independently by Ho&#10;et al. as rank-one residue iteration (RRI).">
    </outline>
    <outline text="BCD with k(n + m) Scalar Blocks" _note="We can also partition the variables with the smallest element blocks of&#10;scalars, where every element of and is considered as a block in the&#10;context of \[thm:bcd\]. To this end, it helps to write the objective&#10;function as a quadratic function of scalar or assuming all other&#10;elements in and are fixed:&#10;&#10;\[eq:nmf\_element\_objective\]&#10;&#10;where and denote the row and the column of , respectively.&#10;&#10;In this paper for solving the optimization problem , we partition the&#10;matrices into vector blocks such as . The reasoning behind this&#10;partitioning is explained in the next section.">
    </outline>
  </outline>
  <outline text="()" _note="In this section, we propose an efficient algorithm for the outlier&#10;detection model .&#10;&#10;To determine the for the aforementioned optimization problem , we use&#10;the block coordinate descent method. In other words, by fixing , we&#10;determine the optimal as vector blocks and vice versa. Due to -norm,&#10;this optimization corresponds to the two block non-smooth BCD framework.&#10;&#10;Regarding , the minimization problem in has a separable structure: where&#10;. Therefore, we only need to define a solution with respect to one&#10;variable . Thus, we partition the matrix into vector blocks and&#10;construct as a set of vectors . Also, the blocks is independent of .&#10;That is, the closed form solution of is dependent only on . When all&#10;other blocks of , are fixed, every vector , can be solved to optimal in&#10;parallel. Thus, we adhere to BCD framework of solving the vector blocks&#10;of , to optimal, when all the other blocks are fixed.&#10;&#10;\[thm:updatez\] The solution of the following minimization problem is&#10;the generalized shrinkage operator: where generalized shrinkage operator&#10;is defined as:&#10;&#10; When , Therefore we have:&#10;&#10;When , let then where Therefore, we get Now, utilizing the generalized&#10;shrinkage operator as defined in , where .&#10;&#10;Now, we need to solve the following NMF model with sparsity constraints&#10;on : where . Let where and . For any , (\[hals\]) can be rewritten as&#10;The following is the framework of the block coordinate descent method&#10;with a separable regularizer such as the Frobenius norm. We iteratively&#10;minimize with respect to each column of and : where and&#10;&#10;According to \[thm:updatez\], the solution of is independent of , and it&#10;enables us to solve the solution in parallel. This is very useful when&#10;computing for very large input matrices. Similarly, the vector blocks of&#10;can also be updated in parallel. Now, we have all the building blocks&#10;for the algorithm. We will be using \[thm:updatez\] and the update for&#10;from . The Algorithm \[alg:tomf\], gives the outline of the and its&#10;complete implementation can be obtained from&#10;&lt;https://github.com/ramkikannan/outliernmf&gt; to try with any real world&#10;text dataset.&#10;&#10;INITIALIZE **W**, **H**, **Z** AS A NONNEGATIVE RANDOM MATRIX">
  </outline>
</outline>
  </body>
</opml>