<?xml version="1.0" encoding="UTF-8"?>
<opml version="2.0">
  <head>
    <title></title>
    <abstract> </abstract>
  </head>
  <body>
<outline text="Formalization" _note="We assume both the premise tree and the hypothesis tree are binarized.&#10;&#10;We use the premise tree and hypothesis tree in Figure \[fig:egtree\] to&#10;demonstrate the process of our approach. The premise sentence is “two&#10;women are hugging one another”, and the hypothesis sentence is “the&#10;women are sleeping”.&#10;&#10;Following the traditional approaches , we first find the alignments from&#10;hypothesis tree nodes to premise tree nodes (i.e., the dashed or dotted&#10;curves in Figure \[fig:egtree\]). Then we explore inducing the&#10;sentence-level entailment relations by 1) first computing the entailment&#10;relation at each node of the hypothesis tree based on the alignments,&#10;and then 2) composing the entailment relations at the internal&#10;hypothesis nodes from bottom up to the root in a recursive way. Our&#10;model resembles the work of Natural Logic in the spirit that the&#10;entailment relation is inferred [MODULARLY]{}, and composed&#10;[RECURSIVELY]{}.&#10;&#10;We formalize this entailment task as a structured prediction problem&#10;similar to , , and . The inputs are two trees: premise tree , and&#10;hypothesis tree . The goal is to predict a label . Note that although&#10;the output label is not structured, we can still consider the problem as&#10;a structured prediction problem, because: 1) the input is a pair of&#10;trees; and 2) the internal alignments are structured.&#10;&#10;More formally, we aim to minimize the negative log likelihood of the&#10;gold label given the two trees. The objective can be written in the&#10;online fashion as: \&#10;where the structured latent variable represents an alignment. is the&#10;number of nodes in the tree. if and only if node in is aligned to node&#10;in , otherwise .&#10;&#10;However, enumerating over all possible alignments takes exponential&#10;time, we need to efficiently approximate the above log expectation.&#10;&#10;Fortunately, as point out, as long as the calculation only consists of&#10;linear calculation, simple nonlinearities like , and softmax, we can&#10;have following simplification via first-order Taylor approximation: \&#10;which means instead of enumerating over all alignments and calculating&#10;the label probability for each alignment, we can use the label&#10;probability for the expected alignment as an approximation:[^1] \&#10;Figure \[fig:exptalign\] shows an example of expected alignment&#10;calculation. The objective is simplified to \&#10;&#10;[0.48]{}&#10;&#10;With this observation, we split our calculation into two steps as the&#10;top two modules in Figure \[fig:arch\]. First in the Alignment module,&#10;we calculate the expected alignments using Equation \[eq:expatt\]&#10;(Section \[sec:att\]). Then we calculate the node-wise entailment&#10;relation, propagate and compose the relation from bottom up to find out&#10;the final entailment relation (Equation \[eq:newobj\]) in the Entailment&#10;Composition module (Section \[sec:ent\]). Both of these two modules rely&#10;on the composition of tree node meaning representations&#10;(Section \[sec:treelstm\]).&#10;&#10;[^1]: We use bold letter, , for binary alignments, and tilde version, ,&#10;    for the expected alignments in the real number space.">
</outline>
<outline text="Attention over Tree Nodes" _note="First we calculate the expected alignments between the hypothesis and&#10;the premise (Equation \[eq:expatt\]):&#10;&#10;To simplify the calculation, we further approximate the global (binary)&#10;alignment to be consisted of the alignment of each tree node&#10;independently. is the th row of : is the probability of the node being&#10;aligned to node , which is defined as: are vectors representing the&#10;semantic meanings of node , , respectively, whose calculation will be&#10;described in Section \[sec:treelstm\]. is an affine transformation from&#10;to . This formulation essentially is equivalent to the widely used&#10;attention calculation in neural networks , i.e., for each node , we find&#10;the relevant nodes and use the softmax of the relevances as a&#10;probability distribution. In the rest of the paper, we use “expected&#10;alignment” and “attention” interchangeably.&#10;&#10;The expected alignment of node being aligned to node , by definition,&#10;is:">
</outline>
<outline text="Entailment Composition" _note="Now we can calculate the entailment relation at each tree node and&#10;propagate the entailment relation following the hypothesis tree from&#10;bottom up, assuming the expected alignment is given&#10;(Equation \[eq:newobj\]):&#10;&#10;Let vector denote the entailment relation in a latent relation space at&#10;hypothesis tree node . At the root of the hypothesis tree. We can induce&#10;the final entailment relation from entailment relation vector . We use a&#10;simple layer to project the entailment relation to the 3 relations&#10;defined in the task, and use a softmax layer to calculate the&#10;probability for each relation:&#10;&#10;At each hypothesis node , is calculated recursively given the meaning&#10;representation at this tree node , the meaning representation of every&#10;node in the premise tree , and the entailment from ’s children, :&#10;&#10;Note the resemblance between the above function and the definition of&#10;Binary-Tree LSTM transition function (Equation \[eq:lstmdef\]), this&#10;suggests that we can directly use a Binary-Tree LSTM layer to calculate&#10;the entailment in a way similar to . That is, using as the input, and ,&#10;as the hidden states passed from children.[^1] Figure \[fig:entnode\]&#10;illustrates the calculation of the entailment composition. We will&#10;discuss in Section \[sec:treelstm\].&#10;&#10;[0.49]{}&#10;&#10;[0.49]{}&#10;&#10;[^1]: We also need to add two vectors representing the memories from the&#10;    children.">
</outline>
<outline text="Dual-attention Over Tree Nodes" _note="We can further improve our alignment approximation in&#10;Section \[sec:att\], which does not consider any structural information&#10;of current tree, nor any alignment information from the premise tree.&#10;&#10;We can take a closer look at our conceptual example in&#10;Figure \[fig:egtree\]. Note that the alignments have, to some extent, a&#10;symmetric property: if a premise node is most relevant to a hypothesis&#10;node , then the hypothesis node should also be most relevant to premise&#10;node . For example, in Figure \[fig:egtree\], the premise phrase&#10;“hugging one another” contradicts the hypothesis word “sleeping”. In the&#10;perspective of the premise tree, the hypothesis word “sleeping”&#10;contradicts by the known claim “hugging one another”. This suggests us&#10;to calculate the alignments from both side, and eliminate the unlikely&#10;alignment if it only exists in one side. This technique is similar to&#10;the widely used forward and reversed alignment technique in the machine&#10;translation area.&#10;&#10;In detail, we calculate the expected alignments from hypothesis to&#10;premise, and also the expected alignments from premise to hypothesis,&#10;and use their element-wise product as the attention to feed into the&#10;Entailment Composition module.[^1] This element-wise product is a mimic&#10;of the intersection of two alignments in machine translation.&#10;Figure \[fig:dualatt\] shows an example. In addition to our&#10;dual-attention, also explore to use the structural information to&#10;improve the alignment. However, their approach requires introducing some&#10;extra terms in the objective function, and is not straightforward to&#10;integrate into our model. We leave adding more structural constraints to&#10;further improve the attention as an open problem to explore in the&#10;future.&#10;&#10;[^1]: We need to normalize at each row to make each row a probability&#10;    distribution.">
</outline>
  </body>
</opml>