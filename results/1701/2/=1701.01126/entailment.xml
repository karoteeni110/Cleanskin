<?xml version="1.0" encoding="UTF-8"?>
<opml version="2.0">
  <head>
    <title></title>
    <abstract> </abstract>
  </head>
  <body>
<outline text="Entailment Relation Composition" _note="Composing entailment relations induced from fractals of the sentence to&#10;form the sentence-level entailment relation is highly non-trivial. For&#10;the Natural Logic framework provided a matrix to handle all the possible&#10;combinations for the entailment relation composition. However the&#10;composition result still can not be deterministically decided. Recent&#10;research of propose to learn the entailment relation composition matrix&#10;with neural networks, but it still seems difficult.&#10;&#10;Here we propose a simple model for entailment representation. In our&#10;model, each entailment relation is represented as a vector , where is&#10;the length of the entailment relation representation. The composition of&#10;two entailment relations and is modeled by the outer product between the&#10;two vectors: , which gives a result matrix of . We then treat this&#10;matrix as a vector of and use this vector as an input for a non-linear&#10;layer and map it to a vector of as the composition result.&#10;&#10;A formal summarization of the above procedure is where is the&#10;composition result, and is the matrix of that maps the tensor dot&#10;product result to a vector of length .[^1]&#10;&#10;The intuition behind our design is that, we consider each dimension of&#10;the entailment vector as a feature value, and the outer product and&#10;conversion from matrix to vector is just to collect feature bigrams of&#10;the composition. At last we use a non-linear layer to learn a simple&#10;classifier to get the correct entailment from these bigram features.&#10;&#10;As a special case of our design when and there is no non-linear&#10;projection instead of a linear projection, we can consider each element&#10;of the entailment vector as an indicator of how likely the entailment&#10;belongs to one of the three entailment relation candidates (entailment,&#10;neutral, and contradiction). Our composition operation is just a linear&#10;classifier based on the bigrams of the indicators of the entailment&#10;relations being composed.&#10;&#10;[^1]: We abuse the mathematical notation and assume an automatic&#10;    conversion from a matrix of to a vector of . \[footnote:abuse2\]">
</outline>
<outline text="Structured Entailment Model" _note="Now we combine the entailment composition with the attention model in&#10;Section \[sec:attention\].&#10;&#10;The combined model is similar to the attention model&#10;(Equations \[eq:attention-sim\]-\[eq:attention-repr\]) that it&#10;calculates the entailments following the hypothesis tree in a bottom-up&#10;order.&#10;&#10;The transitions at each tree node is summarized by&#10;Equations \[eq:ent-sim\] and \[eq:ent-ent\].&#10;&#10;Equation \[eq:ent-sim\] is exactly the same as&#10;Equation \[eq:attention-sim\] in the attention model, which calculates&#10;the relatedness between the given hypothesis node and every premise&#10;node.&#10;&#10;Equation \[eq:ent-ent\] calculates the entailment relation with&#10;attention for the given hypothesis node . In matrix each row vector&#10;corresponds to a node in the premise tree, and the value of the elements&#10;in the row vector indicates the strongness of the alignment and the&#10;entailment relation.&#10;&#10; is calculated using two terms. The first term considers the pairwise&#10;relatedness between and every premise node. The second term considers&#10;the attention and entailment relation propagated from ’s children in one&#10;step.&#10;&#10;This matrix-tensor-matrix dot product might seem difficult to interpret.&#10;Here we can consider a simplified case where is a 3-d identity tensor,&#10;i.e., if.f. . The result is a tensor of .&#10;&#10;If we take a look at th slice of , which essentially means .&#10;&#10;This explains why this tensor product performs the operation of&#10;entailment relation composition. After this the result is mapped to a&#10;matrix of by a dot product with .[^1]&#10;&#10;Finally at the root of the hypothesis tree, we induce the entailment&#10;relation by summing up the entailment matrix alone the dimension of the&#10;nodes, linearly project the result to a vector of length of the real&#10;number of relations in the dataset, and pass through a softmax layer to&#10;make it a probability distribution.&#10;&#10;Again we use cross-entropy as the optimization objective in the&#10;training.&#10;&#10;[^1]: Again we abuse the mathematical notations here and assume an&#10;    automatic conversion similar to Footnote \[footnote:abuse2\] on the&#10;    last two dimensions of the dot product result. Then the converted&#10;    result is mapped by a linear projection.">
</outline>
  </body>
</opml>