<?xml version="1.0" encoding="UTF-8"?>
<opml version="2.0">
  <head>
    <title></title>
    <abstract> </abstract>
  </head>
  <body>
<outline text="Evaluation" _note="The evaluation procedure focuses on identifying a person from the&#10;footprints of individual steps. As shown in Fig. \[fig:step\_frames\],&#10;the steps are present in sequences of the pressure mapping imageries of&#10;individual steps, which we use as the original dataset. The dataset&#10;includes overall 529 sequences from 13 participants.&#10;&#10;In our previous work , a fast wavelet transform is applied to these&#10;sequences of steps to generate a single 336 dimension wavelet descriptor&#10;for each step. These features are then classified using a support vector&#10;machine classifier with a quadratic kernel . This approach results in a&#10;classification accuracy of 76.9%. Our approach diverges after the steps&#10;are segmented and the flowchart for our proposed approach can be seen in&#10;Fig. \[fig:flowchart\]. We generate the dataset of images which are&#10;maximum frame (sum of all pixels per frame), average frame (average of&#10;all the frames present in a sequence) and the set of all the frames&#10;forming a step; after that this set of images is passed through the&#10;pre-trained Inception model, for feature extraction, which upon&#10;classification gives the recognition results for person identification.">
  <outline text="Maximum Frames" _note="As described in Section \[sec:mod\], the maximum frame corresponds to&#10;the point at which the foot exerts maximum pressure on the ground. The&#10;pressure mat scans at the rate of 25 fps, thus a single frame&#10;corresponds to the information captured in 40 milliseconds. Our&#10;assumption is that the maximum frame of each step corresponds to the&#10;situation when a personâ€™s entire foot is on the ground, and hence,&#10;contains enough spatial information to be discriminative.&#10;&#10;We evaluate our system by using these images for all the steps present&#10;in our dataset. These images are passed through the pre-trained&#10;Inception model and the activations of the fully-connected layer are&#10;used as image descriptors. To complete the classification task, the&#10;image descriptors are fed to a fully-connected layer, which computes the&#10;probability distribution over the different classes using SOFTMAX&#10;activation function. The schematic diagram of the architecture followed&#10;for this approach is shown in Fig. \[fig:ann\_pipeline\]. This entire&#10;process is carried out with a 10-fold cross validation and repeated for&#10;10 iterations. We calculate the average of the result obtained after&#10;each repetition. The final recognition rate after this method comes out&#10;to be 71.99% as shown in Table \[tab:results\].">
  </outline>
  <outline text="Average Frames" _note="The walking pattern of a person has a temporal component within it. This&#10;time dimension includes the way a person starts with engaging his/her&#10;foot on the floor, which begins with the heel strike and then carries on&#10;until the toe off. Within these stages, the way an individual exerts&#10;pressure on the floor varies from person to person. In order to&#10;accommodate this temporal information, we average over all the frames in&#10;the sequence of a single step and compute images corresponding to all&#10;the steps. The evaluation is carried out in the similar manner as with&#10;the maximum frames (as seen in Fig. \[fig:ann\_pipeline\]) with a&#10;cross-validation and the average recognition rate is 78.41% as seen in&#10;Table \[tab:results\].">
  </outline>
  <outline text="Image Sequences with Recurrent Neural Network" _note="In our experiments with the maximum and average frames, we observe that&#10;the average frames show an improvement over the maximum frames. Even&#10;though the classification accuracy is improved by the information&#10;encoded in the average frames, a certain amount of information is lost&#10;by the averaging procedure. Hence, we experiment with using a RNN to&#10;classify the temporal sequence of each step. As shown in Fig.&#10;\[fig:rnn\_pipeline\], all of frames associated with a step are&#10;processed through the Inception-v3 model to extract a single descriptor&#10;for each frame. These descriptors are then fed one after another into a&#10;layer of Gated Recurrent Units (GRU) , which generates a classification&#10;upon completing the sequence. We follow the same evaluation procedure&#10;outlined in the previous experiments and we obtain a classification&#10;accuracy of 87.66% (Table \[tab:results\]).&#10;&#10;">
  </outline>
</outline>
  </body>
</opml>