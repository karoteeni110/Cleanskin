<?xml version="1.0" encoding="UTF-8"?>
<opml version="2.0">
  <head>
    <title></title>
    <abstract> </abstract>
  </head>
  <body>
<outline text="Modality Transformation" _note="Modality transformation refers to the steps taken to convert the data&#10;from a source mode to a target mode. The purpose of such a&#10;transformation is to exploit the knowledge present in pre-trained models&#10;in the target mode to allow for easy classification. In this section, we&#10;describe the process of modality transformation on the example of&#10;pressure sensor data.&#10;&#10;In our specific case, the modality transformation of the raw data from&#10;sensors constitute the steps to convert the sensor mode into the visual&#10;mode in the form of images. The raw data from the sensor is a temporal&#10;sequence of 2-dimensional pressure mappings. For the force-sensitive&#10;resistor fabric sensor, every sensing pointâ€™s value is essentially the&#10;voltage potential measurement that is related to the pressure. We&#10;transfer these values linearly into a gray-scale color map, in which&#10;each pixel represents a sensing point, and brighter color corresponds to&#10;higher pressure. A complete step is a sequence of such pressure mapping&#10;frames; every frame corresponds to the moment of a step as shown in Fig.&#10;\[fig:step\_frames\](a). The number of frames which comprise the entire&#10;step varies among people. Thus, it is important to segment each step&#10;along temporal dimension and find the individual moments for each step.&#10;It is noteworthy that, the idea of modality transformation is not&#10;limited to pressure data. For other matrix-sensors, a similar modality&#10;transformation strategy can be applied. The main idea of this paper is&#10;that after such a modality transformation, it is possible to apply&#10;transfer learning on the transformed data.&#10;&#10;The modality transformation starts with the pre-processing of the raw&#10;data. Here, we first separate the steps from the background noise by&#10;converting each frame into a binary frame and applying an adaptive&#10;threshold. For the threshold, we sort the pixel values of the frame into&#10;a 10-bin histogram, and the threshold is decided as the center value of&#10;the next bin of the highest count bin. However, this binarization&#10;process can be omitted for other pressure sensor data.&#10;&#10;Next, we find the largest bounding box of all frames which encloses each&#10;individual step. It is, therefore, ensured that all the moments&#10;belonging to a same step will fit into that enclosing bounding box.&#10;Since, the bounding box is dynamically calculated for a step, the size&#10;of the enclosing box is different for different steps. However, within&#10;one step, all the moments are captured and extracted using the same&#10;sized bounding box. For the general modality transfer we suggest a&#10;position normalization in a similar manner, i.e., either cutting&#10;irrelevant parts with a bounding box or setting the center of the images&#10;to the mean position over all frames. There can be many ways to convert&#10;the data from the source mode to a visual imagery mode. This depends on&#10;a number of factors; the dimensionality, range, heterogeneity or&#10;homogeneity, volume, noisiness. Ideally the best way is the one which&#10;transforms the source mode data into a form as close as possible to the&#10;target mode on which the CNN has been trained. In the following, we&#10;describe three ways in which we extract the images after fitting the&#10;bounding box.&#10;&#10;The transformation on the sensor data can be done with three strategies:&#10;max-frame, averaging, sequential analysis, giving us three possible&#10;images per data sequence (per step).&#10;&#10;For the first strategy, we capture the maximum frame out of the frame&#10;sequence of each sample, which corresponds to the frame with highest&#10;value of pixel sum, then convert it into the respective image and label&#10;it with the class ID. As shown in Fig. \[fig:step\_frames\] (the frame&#10;with red bounding box), in our dataset, we obtain one such image for&#10;every step, hence, in total we have 529 such images.&#10;&#10;For the second strategy, we average over all the frames in the sequence&#10;of a single sample and generate the corresponding image with averaged&#10;pixel values. This is visualized for footstep data in Fig.&#10;\[fig:step\_frames\](b). This averaged frame carries the temporal&#10;information from all the moments of the step and should be more&#10;effective than the maximum frames for classification.&#10;&#10;For the third strategy, we use all the frames which form a temporal&#10;sequence of a step within each sample and transform them into the&#10;images. The sequence of frames capturing the moments of a single step&#10;are shown in Fig. \[fig:step\_frames\]. This carries the original raw&#10;values at each frame and provides more granularity than previous&#10;approaches for the feature set calculation.">
</outline>
  </body>
</opml>