<?xml version="1.0" encoding="UTF-8"?>
<opml version="2.0">
  <head>
    <title></title>
    <abstract> </abstract>
  </head>
  <body>
<outline text="Introduction" _note="The presence of sensors in the ubiquitous environment has led to the&#10;production of an enormous amount of data. These sensors belong to&#10;diverse categories including planar pressure, thermal, optical,&#10;acoustic, and proximity modalities. They provide information for&#10;activity recognition and context-aware models that could be used for a&#10;wide range of applications such as automatic monitoring in the smart&#10;environment and wellness scene, computer-human-interaction, user&#10;experience, etc. There has been extensive research on wearable and&#10;pervasive sensors, which record and sense data in the context of human&#10;activities; ranging from physical activities (running, sleeping,&#10;walking, etc.), such as monitoring gym exercises , analyzing gait&#10;patterns , to biological activities (breathing, eating, etc.), such as&#10;breathing detection , eating and drinking arm gesture detection . The&#10;task of extracting useful information from the raw sensor data has been&#10;performed using various machine learning and data mining techniques .&#10;One of the many examples is the use of such methods in human activity&#10;recognition . Usually, when the raw sensor data is concerned, the&#10;feature extraction is done using numerical statistical features. These&#10;features have proven to be quite reliable in tasks related to&#10;classification, recognition and segmentation .&#10;&#10;Deep learning has been recently proven to be extremely successful in&#10;various domains. Convolutional neural networks (CNNs) , have already&#10;been applied to practical tasks by Le Cun et al. , they have recently&#10;risen in popularity after achieving superhuman accuracy on image&#10;classification tasks . Recurrent neural networks (RNN) especially with&#10;Long Short-Term Memory cells (LSTM) have been used to classify sequences&#10;and to recognize activities with varying degrees of success. Both, CNN&#10;and RNN have been used in combination to create systems which are&#10;capable of understanding images, and to provide temporal context to&#10;these individual images.&#10;&#10;A limitation of these techniques, however, is the requirement of large&#10;amounts of labeled data to facilitate the training of these very deep&#10;networks. While the computer vision community has facilitated this&#10;requirement with large labeled datasets, such as, the ImageNet and&#10;MS-COCO datasets for object recognition, classification, detection and&#10;captioning; for various other tasks not many labeled datasets exist&#10;because the scope can be very specific when compared to general images.">
  <outline text="Transfer Learning" _note="For many Computer Vision problems, the above-mentioned limitation can be&#10;bypassed by performing transfer learning, i.e., using labeled data from&#10;one domain and transferring the learned knowledge to a target domain.&#10;Transfer learning involves using the knowledge acquired on a specific&#10;task, and adapting this knowledge to a different, but related task.&#10;Caruana first introduced the concept of multi-task learning, targeting&#10;the improvement in generalization by using the domain information of&#10;related tasks. A common scenario for transfer learning involves using a&#10;convolutional neural network trained on a very large dataset, and then&#10;further fine-tuning it on the target dataset which is relatively small&#10;in size. A pre-trained CNN is used for transfer learning by removing the&#10;last fully-connected layer and using the activations of the last hidden&#10;layer as the feature descriptors of the input dataset. The resulting&#10;feature descriptors are then used to train a classification model.&#10;Recently, transfer learning has been done on semantic segmentation of&#10;images . The learned representations of fully convolutional networks&#10;like AlexNet , VGGnet are transferred by fine-tuning the semantic&#10;segmentation task. Similarly, Li et al. explored the concept of transfer&#10;learning on images with limited semantic meanings which do not perform&#10;well for high level visual tasks. The use of large number of pre-trained&#10;generic object detectors improved performances on recognition tasks with&#10;simple classifiers like linear SVM . The key advantage of transfer&#10;learning is that it removes the need to create a large dataset required&#10;to train the CNNs. Also, the time and computational resources needed to&#10;perform training on such a large scale is considerably high and thus,&#10;transfer learning benefits us by saving this additional cost.&#10;&#10;Conventionally, transfer learning has been performed on domains that are&#10;easily visually interpretable. We define a domain as being easily&#10;visually interpretable as follows:\&#10;\&#10;A DOMAIN IS SAID TO BE EASILY VISUALLY INTERPRETABLE IF BY LOOKING AT&#10;ITS VISUAL REPRESENTATION, A HUMAN CAN EXTRACT RELEVANT INFORMATION AND&#10;A SENSE OF THE MEANING IT CONVEYS.\&#10;\&#10;Additionally, if the data is conventionally visually interpreted, it&#10;belongs to the category of easily visually interpretable data. For&#10;example, images of everyday objects such as automobiles, animals,&#10;landscapes; documents, X-ray, and MRI scan data all belong to the&#10;category of easily visually interpretable images. In the context of&#10;these examples, an average human can easily learn to distinguish between&#10;different classes, e.g., several sub-species of animals (Fig.&#10;\[fig:imagenet\](c)(d)) or several models of automobiles . Also the&#10;classification of documents into categories such as legal, scientific,&#10;or historical is a trivial task for most people. Doctors or radiologists&#10;analyze and interpret MRI or X-ray data (See Fig.&#10;\[fig:imagenet\](a)(b)) to detect irregularities in healthy organs. The&#10;application of transfer learning on such images is feasible for mainly&#10;two reasons. Firstly, it is possible due to their nature of being&#10;visually meaningful and secondly, there are large datasets from the same&#10;domain available within the community to carry out the transfer learning&#10;tasks.&#10;&#10;However, there exist types of data, such as sensor data, which are not&#10;easily visually interpretable, and it is unclear if it would be possible&#10;to visually interpret them. Not visually interpretable data can be, for&#10;example, position updates of moving objects in location-based services,&#10;fluctuations in the stock market, medical experimental observations, or&#10;streaming sensor data. An example is illustrated in Fig.&#10;\[fig:similar\_steps\] which shows the pressure mappings for a&#10;particular moment of a foot step. We can see that Fig.&#10;\[fig:similar\_steps\] (a) and (c) look similar but they belong to&#10;different persons. Similarly, Fig. \[fig:similar\_steps\] (a) and (b)&#10;look different but they belong to the same person. Hence, such sensor&#10;data is clearly not easily visually interpretable.&#10;&#10;">
  </outline>
  <outline text="Paper Contribution" _note="In this paper, we introduce the idea of using the concept of transfer&#10;learning on a domain which is not easily visually interpretable. We&#10;carry out a shift procedure, which involves the shift from a domain&#10;which is not suitable for transfer learning to a domain on which the&#10;CNNs have been trained. This shift facilitates the use of transfer&#10;learning even for data which normally is not an ideal candidate for&#10;transfer learning. In order to show that such transformation is useful,&#10;we extend the transfer learning methods on pressure sensor data. The raw&#10;sensor data is first transformed into a set of visual images and then&#10;used as an input dataset for a pre-trained convolutional network model.&#10;Thus, the core contributions of this paper are as follows:&#10;&#10;MODALITY TRANSFORMATION: We transform non-interpretable data to the&#10;image domain and explore the effectiveness of deep neural networks. We&#10;observe that models which are pre-trained on the aesthetic and visually&#10;interpretable datasets like ImageNet, are powerful and accurate enough&#10;in terms of feature calculation that the artificially generated images&#10;are also recognized with high accuracy rates.&#10;&#10;UNIFIED FEATURE EXTRACTION PROCESS: Typically, the feature extraction&#10;process using conventional methods is customized for each unique&#10;application. In the case of sensor data, even with the same kind of&#10;sensors used for collecting the information, the feature extraction and&#10;data mining techniques vary depending on the target application of the&#10;data. The same pressure sensor has been used to cater to different&#10;applications but uses different feature extraction techniques. We&#10;provide a unified feature extraction process, which can be applied to&#10;the sensor data after conversion into the visual domain independent from&#10;its application.&#10;&#10;EVALUATION ON PRESSURE SENSOR DATA: We evaluate our approach of modality&#10;transformation with pressure values of single steps as each person walks&#10;on a Smart-Mat , a fabric based real-time pressure force mapping system.&#10;The domain shift is carried out by transforming the pixel data values&#10;corresponding to the pressure exerted on the floor while walking (See&#10;Section \[pressuredata\]), to the respective images. This information&#10;consisting of images serve as the input to pre-trained CNNs. With the&#10;application of our approach of modality transformation, we achieve a&#10;person identification accuracy of 87.66% which significantly outperforms&#10;the state of the art (76.9 %) (See Section \[sec:eval\])">
  </outline>
</outline>
  </body>
</opml>