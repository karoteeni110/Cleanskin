<?xml version="1.0" encoding="UTF-8"?>
<opml version="2.0">
  <head>
    <title></title>
    <abstract> </abstract>
  </head>
  <body>
<outline text="Architecture" _note="For transfer learning, our strategy follows the idea of transferring&#10;from the image classification task, i.e., using a pre-trained model from&#10;ImageNet or Coco-DB. Either the classification layer is removed or used&#10;as feature descriptor and a new classification layer is added. Thus the&#10;CNN is used as a fixed feature extractor.&#10;&#10;The pre-trained CNN we use in our experiments is the Inception-v3 model&#10;from . It is a CNN variant that focuses on improving computational&#10;efficiency along with performance. We choose this model for two reasons;&#10;with a Top-5 error of 3.58%, it clearly performs extremely well on the&#10;ILSVRC-2012 classification benchmark and it requires relatively less&#10;computational resources to process the input. This ensures the&#10;possibility of applying such CNN models to real-time processing of high&#10;velocity sensor data.&#10;&#10;The Inception-v3 architecture consists of 3 convolutional layers&#10;followed by a pooling layer, 3 convolutional layers, 10 Inception blocks&#10;and a final fully connected layer. This results in 17 layers which can&#10;be learned by training the network on the data. We resize the images to&#10;the dimensions 229 x 229 as required by Inception-v3. We extract the&#10;activations from the fully-connected layer as shown in Fig.&#10;\[fig:inception\]. This results in a 2048 dimensional output for each&#10;input. Each output can be interpreted as the descriptor for each frame&#10;in the sequence.&#10;&#10;The CNN will then be provided with the transformed input image (see&#10;Section \[sec:mod\]) and resized to fit the CNN input size. The&#10;activations for the entire network are computed by forward propagating&#10;the input through the network. As an example, Fig.&#10;\[fig:max\_conv\_vis\] shows feature visualizations of all the filters&#10;present in the first and second convolutional layers of AlexNet . While&#10;it is possible to visualize the activations of deeper layers, typically&#10;they are harder to interpret.&#10;&#10;">
</outline>
  </body>
</opml>