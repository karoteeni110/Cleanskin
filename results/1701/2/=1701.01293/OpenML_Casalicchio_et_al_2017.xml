<?xml version="1.0" encoding="UTF-8"?>
<opml version="2.0">
  <head>
    <title>[[[`OpenML`]{}: An R Package to Connect to the Machine Learning Platform
OpenML]{}]{}</title>
    <abstract> </abstract>
  </head>
  <body>
<outline text="Introduction" _note="OpenML is an online machine learning platform for sharing and organizing&#10;data, machine learning algorithms and experiments . It is designed to&#10;create a frictionless, networked ecosystem , allowing people all over&#10;the world to collaborate and build directly on each other’s latest&#10;ideas, data and results. Key elements of OpenML are data sets, tasks,&#10;flows and runs:&#10;&#10;**Data sets** can be shared (under a licence) by uploading them or&#10;simply linking to existing data repositories (e.g.,&#10;[mldata.org](http://mldata.org), [figshare.com](https://figshare.com)).&#10;For known data formats (e.g., ARFF for tabular data), OpenML will&#10;automatically analyze and annotate the data sets with measurable&#10;characteristics to support detailed search and further analysis. Data&#10;sets can be repeatedly updated or changed and are then automatically&#10;versioned.&#10;&#10;**Tasks** can be viewed as containers including a data set and&#10;additional information defining what is to be learned. They define which&#10;input data are given and which output data should be obtained. For&#10;instance, classification tasks will provide the target feature, the&#10;evaluation measure (e.g., the area under the curve) and the estimation&#10;procedure (e.g., cross-validation splits) as inputs. As output they&#10;expect a description of the machine learning algorithm or workflow that&#10;was used and, if available, its predictions.&#10;&#10;**Flows** are implementations of single machine learning algorithms or&#10;whole workflows that solve a specific task, e.g., a random forest&#10;implementation is a flow that can be used to solve a classification or&#10;regression task. Ideally, flows are already implemented (or custom)&#10;algorithms in existing software that take OpenML tasks as inputs and can&#10;automatically read and solve them. They also contain a list (and&#10;description) of possible hyperparameters that are available for the&#10;algorithm.&#10;&#10;**Runs** are the result of executing flows, optionally with preset&#10;hyperparameter values, on tasks and contain all expected outputs and&#10;evaluations of these outputs (e.g., the accuracy of predictions). Runs&#10;are fully reproducible because they are automatically linked to specific&#10;data sets, tasks, flows and hyperparameter settings. They also include&#10;the authors of the run and any additional information provided by them,&#10;such as runtimes. Similar to data mining challenge platforms , OpenML&#10;evaluates all submitted results (using a range of evaluation measures)&#10;and compares them online. The difference, however, is that OpenML is&#10;designed for collaboration rather than competition: anyone can browse,&#10;immediately build on and extend all shared results.&#10;&#10;As an open science platform, OpenML provides important benefits for the&#10;science community and beyond.">
  <outline text="Benefits for Science:" _note="Many sciences have made significant breakthroughs by adopting online&#10;tools that help organizing, structuring and analyzing scientific data&#10;online . Indeed, any shared idea, question, observation or tool may be&#10;noticed by someone who has just the right expertise to spark new ideas,&#10;answer open questions, reinterpret observations or reuse data and tools&#10;in unexpected new ways. Therefore, sharing research results and&#10;collaborating online as a (possibly cross-disciplinary) team enables&#10;scientists to quickly build on and extend the results of others,&#10;fostering new discoveries. Moreover, ever larger studies become feasible&#10;as a lot of data are already available. Questions such as “Which&#10;hyperparameter is important to tune?”, “Which is the best known workflow&#10;for analyzing this data set?” or “Which data sets are similar in&#10;structure to my own?” can be answered in minutes by reusing prior&#10;experiments, instead of spending days setting up and running new&#10;experiments .">
  </outline>
  <outline text="Benefits for Scientists:" _note="Scientists can also benefit personally from using OpenML. For example,&#10;they can SAVE TIME, because OpenML assists in many routine and tedious&#10;duties: finding data sets, tasks, flows and prior results, setting up&#10;experiments and organizing all experiments for further analysis.&#10;Moreover, new experiments are immediately compared to the state of the&#10;art without always having to rerun other people’s experiments.&#10;&#10;Another benefit is that linking one’s results to those of others has a&#10;large potential for NEW DISCOVERIES [[]{}]{}, leading to more&#10;publications and more collaboration with other scientists all over the&#10;world. Finally, OpenML can help scientists to REINFORCE THEIR REPUTATION&#10;by making their work (published or not) visible to a wide group of&#10;people and by showing how often one’s data, code and experiments are&#10;downloaded or reused in the experiments of others.">
  </outline>
  <outline text="Benefits for Society:" _note="OpenML also provides a useful learning and working environment for&#10;students, citizen scientists and practitioners. Students and citizen&#10;scientist can easily explore the state of the art and work together with&#10;top minds by contributing their own algorithms and experiments. Teachers&#10;can challenge their students by letting them compete on OpenML tasks or&#10;by reusing OpenML data in assignments. Finally, machine learning&#10;practitioners can explore and reuse the best solutions for specific&#10;analysis problems, interact with the scientific community or efficiently&#10;try out many possible approaches.\&#10;The remainder of this paper is structured as follows. First, we discuss&#10;the web services offered by the OpenML server and the website on&#10;[OpenML.org](http://openml.org) that allows web access to all shared&#10;data and several tools for data organization and sharing. Second, we&#10;briefly introduce the [`mlr`]{} package , which is a machine learning&#10;toolbox for [`R`]{} and offers a unified interface to many machine&#10;learning algorithms. Third, we discuss and illustrate some important&#10;functions of the [`OpenML R`]{} package. After that, we illustrate its&#10;usage in combination with the [`mlr R`]{} package by conducting a short&#10;case study. Finally, we conclude with a discussion and an outlook to&#10;future developments.">
  </outline>
</outline>
<outline text="The OpenML Platform" _note="The OpenML platform consists of several layers of software:">
  <outline text="Web API:" _note="Any application (or web application), can communicate with the OpenML&#10;server through the extensive Web API, [[ an application programming&#10;interface (API) that offers a set of calls (e.g., to download/upload&#10;data) using representational state transfer (REST) which is a simple,&#10;lightweight communication mechanism based on standard HTTP requests.&#10;]{}]{} Data sets, tasks, flows and runs can be created, read, updated,&#10;deleted, searched and tagged through simple HTTP calls. An overview of&#10;calls is available on &lt;http://www.openml.org/api_docs&gt;.">
  </outline>
  <outline text="Website:" _note="[OpenML.org](http://openml.org) is a website offering easy browsing,&#10;organization and sharing of all data, code and experiments. It allows&#10;users to easily search and browse all shared data sets, tasks, flows and&#10;runs, as well as to compare and visualize all combined results. It&#10;provides an easy way to check and manage your experiments anywhere,&#10;anytime and discuss them with others online. See Figure \[screenshot\]&#10;for a few screenshots of the OpenML website.&#10;&#10;">
  </outline>
  <outline text="&lt;span&gt;&lt;span&gt; Programming Interfaces: &lt;/span&gt;&lt;/span&gt;" _note="OpenML also offers interfaces in multiple programming languages, such as&#10;the [`R`]{} interface presented here, which hides the API calls and&#10;allow scientists to interact with the server using language-specific&#10;functions. As we demonstrate below, the [`OpenML R`]{} package allows&#10;[`R`]{} users to search and download data sets and upload the results of&#10;machine learning experiments in just a few lines of code. Other&#10;interfaces exist for [`Python`]{}, [`Java`]{} and [`C# (.NET)`]{}. For&#10;tools that usually operate through a graphical interface, such as WEKA ,&#10;MOA  and RapidMiner , plug-ins exist that integrate OpenML sharing&#10;facilities.\&#10;OpenML is organized as an open source project, hosted on GitHub [[&#10;(&lt;https://github.com/openml&gt;) ]{}]{} and is free to use under the CC-BY&#10;licence. When uploading new data sets and code, users can select under&#10;which licence they wish to share the data, OpenML will then state&#10;licences and citation requests online and in descriptions downloaded&#10;from the Web API.&#10;&#10;OpenML has an active developer community and everyone is welcome to help&#10;extend it or post new suggestions through the website or through GitHub.&#10;Currently, there are close to runs on about data sets and unique flows&#10;available on the OpenML platform. While still in beta development, it&#10;has over  registered users, over frequent visitors and the website is&#10;visited by around 200 unique visitors every day, from all over the&#10;world. It currently has server-side support for classification,&#10;regression, clustering, data stream classification, learning curve&#10;analysis, survival analysis and machine learning challenges for&#10;classroom use.">
  </outline>
</outline>
<outline text="The mlr R Package" _note="The [`mlr`]{} package offers a clean, easy-to-use and flexible&#10;domain-specific language for machine learning experiments in [`R`]{}. An&#10;object-[[oriented]{}]{} interface is adopted to unify the definition of&#10;machine learning tasks, setup of learning algorithms, training of&#10;models, predicting and evaluating the algorithm’s performance. This&#10;unified interface hides the actual implementations of the underlying&#10;learning algorithms. Replacing one learning algorithm with another&#10;becomes as easy as changing a string. Currently, [`mlr`]{} has built-in&#10;support for classification, regression, multilabel classification,&#10;clustering and survival analysis and includes in total 160 modelling&#10;techniques. [[ A complete list of the integrated learners and how to&#10;integrate own learners, as well as further information on the [`mlr`]{}&#10;package can be found in the corresponding tutorial&#10;(&lt;http://mlr-org.github.io/mlr-tutorial/&gt;). ]{}]{} A plethora of further&#10;functionality is implemented in [`mlr`]{}, e.g., assessment of&#10;generalization performance, comparison of different algorithms in a&#10;scientifically rigorous way, feature selection and algorithms for&#10;hyperparameter tuning, [[including Iterated F-Racing and Bayesian&#10;optimization with the package [`mlrMBO`]{} ]{}]{}. On top of that,&#10;[`mlr`]{} offers a wrapper mechanism, which allows to extend learners&#10;through pre-train, post-train, pre-predict and post-predict hooks. A&#10;wrapper extends the current learner with added functionality and expands&#10;the hyperparameter set of the learner with additional hyperparameters&#10;provided by the wrapper. Currently, many wrappers are available, e.g.,&#10;missing value imputation, class imbalance correction, feature selection,&#10;tuning, bagging and stacking, as well as a wrapper for user-defined data&#10;pre-processing. Wrappers can be nested in other wrappers, which can be&#10;used to create even more complex workflows. The package also supports&#10;parallelization on different levels [[based on different parallelization&#10;backends (local multicore, socket, MPI) with the package&#10;[`parallelMap`]{}  or on managed high-performance systems via the&#10;package [`batchtools`]{} . Furthermore, visualization methods for&#10;research and teaching are also supplied.]{}]{}&#10;&#10;The [`OpenML`]{} package makes use of [`mlr`]{} as a supporting package.&#10;It offers methods to automatically run [`mlr`]{} learners (flows) on&#10;OpenML tasks while hiding all of the necessary structural&#10;transformations (see Section \[sec:runs\]).">
</outline>
<outline text="The OpenML R Package" _note="The [`OpenML`]{} [`R`]{} package is an interface to interact with the&#10;OpenML server directly from within [`R`]{}. Users can retrieve data&#10;sets, tasks, flows and runs from the server and also create and upload&#10;their own. This section details how to install and configure the package&#10;and demonstrates its most important functionalities.">
  <outline text="Installation and Configuration" _note="To interact with the OpenML server, users need to authenticate using an&#10;API KEY, a secret string of characters that uniquely identifies the&#10;user. It is generated and shown on users’ profile page after they&#10;register on the website [`http://www.openml.org`]{}. For demonstration&#10;purposes, we will use a public read-only API key that only allows to&#10;retrieve information from the server and should be replaced with the&#10;user’s personal API key to be able to use all features. The [`R`]{}&#10;package can be easily installed and configured as follows:&#10;&#10;    install.packages(&quot;OpenML&quot;)&#10;    library(&quot;OpenML&quot;)&#10;    saveOMLConfig(apikey = &quot;c1994bdb7ecb3c6f3c8f3b35f4b47f1f&quot;)&#10;&#10;The [`saveOMLConfig`]{} function creates a [`config`]{} file, which is&#10;always located in a folder called [`.openml`]{} within the user’s home&#10;directory. This file stores the user’s API key and other configuration&#10;settings, which can always be changed manually or through the&#10;[`saveOMLConfig`]{} function. Alternatively, the [`setOMLConfig`]{}&#10;function allows to set the API key and the other entries TEMPORARILY,&#10;i.e., only for the current [`R`]{} session.">
  </outline>
  <outline text="Listing Information" _note="In this section, we show how to list the available OpenML data sets,&#10;tasks, flows and runs using listing functions that always return a&#10;[`data.frame`]{} containing the queried information. Each data set,&#10;task, flow and run has a unique ID, which can be used to access it&#10;directly.">
    <outline text="Listing Data Sets and Tasks:" _note="A list of all data sets and tasks that are available on the OpenML&#10;server can be obtained using the [`listOMLDataSets`]{} and&#10;[`listOMLTasks`]{} function, respectively. Each entry provides&#10;information such as the ID, the name and basic characteristics&#10;(e.g., number of features, number of observations, classes, missing&#10;values) of the corresponding data set. In addition, the list of tasks&#10;contains information about the task type (e.g.,&#10;[`Supervised Classification`]{}), the evaluation measure (e.g.,&#10;[`Predictive Accuracy`]{}) and the estimation procedure (e.g.,&#10;[`10-fold Crossvalidation`]{}) used to estimate model performance. Note&#10;that multiple tasks can be defined for a specific data set, for example,&#10;the same data set can be used for multiple task types (e.g.&#10;classification and regression tasks) as well as for tasks differing in&#10;their estimation procedure, evaluation measure or target value.&#10;&#10;To find data sets or tasks that meet specific requirements, one can&#10;supply arguments to the listing functions. In the example below, we list&#10;all supervised classification tasks based on data sets having two&#10;classes for the target feature, between 500 and 999 instances, at most&#10;100 features and no missing values:&#10;&#10;    tasks = listOMLTasks(task.type = &quot;Supervised Classification&quot;,&#10;      number.of.classes = 2, number.of.instances = c(500, 999),&#10;      number.of.features = c(1, 100), number.of.missing.values = 0)&#10;    tasks[1:2, c(&quot;task.id&quot;, &quot;name&quot;, &quot;number.of.instances&quot;, &quot;number.of.features&quot;)]&#10;    ##   task.id        name number.of.instances number.of.features&#10;    ## 1      37    diabetes                 768                  9&#10;    ## 2      49 tic-tac-toe                 958                 10">
    </outline>
    <outline text="Listing Flows and Runs:" _note="When using the [`mlr`]{} package, flows are basically learners from&#10;[`mlr`]{}, which, as stated previously, can also be a more complex&#10;workflow when different [`mlr`]{} wrappers are nested. Custom flows can&#10;be created by integrating custom machine learning algorithms and&#10;wrappers into [`mlr`]{}. The list of all available flows on OpenML can&#10;be downloaded using the [`listOMLFlows`]{} function. Each entry contains&#10;information such as its ID, its name, its version and the user who first&#10;uploaded the flow to the server. Note that the list of flows will not&#10;only contain flows created with [`R`]{}, but also flows from other&#10;machine learning toolkits, such as WEKA , MOA  and scikit-learn , which&#10;can be recognized by the name of the flow. When a flow, along with a&#10;specific setup (e.g., specific hyperparameter values), is applied to a&#10;task, it creates a run. The [`listOMLRuns`]{} function lists all runs&#10;that, for example, refer to a specific [`task.id`]{} or [`flow.id`]{}.&#10;To list these evaluations as well, the [`listOMLRunEvaluations`]{}&#10;function can be used. In Figure \[fig:evalplot\], we used [`ggplot2`]{}&#10;to visualize the predictive accuracy of runs, for which only flows&#10;created with [`mlr`]{} were applied to the task with ID [`37`]{}:&#10;&#10;    res = listOMLRunEvaluations(task.id = 37, tag = &quot;openml_r_paper&quot;)&#10;    res$flow.name = reorder(res$flow.name, res$predictive.accuracy)&#10;&#10;    library(&quot;ggplot2&quot;)&#10;    ggplot(res, aes(x = predictive.accuracy, y = flow.name)) + &#10;      geom_point() + xlab(&quot;Predictive Accuracy&quot;) + ylab(&quot;Flow Name&quot;)&#10;&#10;">
    </outline>
  </outline>
  <outline text="Downloading OpenML Objects" _note="Most of the listing functions described in the previous section will&#10;list entities by their OpenML IDs, e.g., the [`task.id`]{} for tasks,&#10;the [`flow.id`]{} for flows and the [`run.id`]{} for runs. In this&#10;section, we show how these IDs can be used to download a certain data&#10;set, task, flow or run from the OpenML server. All downloaded data sets,&#10;tasks, flows and runs will be stored in the [`cachedir`]{} directory,&#10;which will be in the [`.openml`]{} folder by default but can also be&#10;specified in the configuration file (see Section \[sec:config\]). Before&#10;downloading an OpenML object, the cache directory will be checked if&#10;that object is already available in the cache. If so, no internet&#10;connection is necessary and the requested object is retrieved from the&#10;cache.">
    <outline text="Downloading Data Sets and Tasks:" _note="The [`getOMLDataSet`]{} function returns an [`S3`]{}-object of class&#10;[`OMLDataSet`]{} that contains the data set as a [`data.frame`]{} in a&#10;[`$data`]{} slot, in addition to some pieces of meta-information:&#10;&#10;    ds = getOMLDataSet(data.id = 15)&#10;    ds&#10;    ## &#10;    ## Data Set &quot;breast-w&quot; :: (Version = 1, OpenML ID = 15)&#10;    ##   Default Target Attribute: Class&#10;&#10;To retrieve tasks, the [`getOMLTask`]{} function can be used with their&#10;corresponding task ID. Note that the ID of a downloaded task is not&#10;equal to the ID of the data set. Each task is returned as an&#10;[`S3`]{}-object of class [`OMLTask`]{} and contains the [`OMLDataSet`]{}&#10;object as well as the predefined estimation procedure, evaluation&#10;measure and the target feature in an additional [`$input`]{} slot.&#10;Further technical information can be found in the package’s help page.">
    </outline>
    <outline text="Downloading Flows and Runs:" _note="The [`getOMLFlow`]{} function downloads all information of the flow,&#10;such as the name, all necessary dependencies and all available&#10;hyperparameters that can be set. If the flow was created in [`R`]{}, it&#10;can be converted into an [`mlr`]{} learner using the&#10;[`convertOMLFlowToMlr`]{} function:&#10;&#10;    mlr.lrn = convertOMLFlowToMlr(getOMLFlow(4782))&#10;    mlr.lrn&#10;    ## Learner classif.randomForest from package randomForest&#10;    ## Type: classif&#10;    ## Name: Random Forest; Short name: rf&#10;    ## Class: classif.randomForest&#10;    ## Properties: twoclass,multiclass,numerics,factors,ordered,prob,class.weights&#10;    ## Predict-Type: response&#10;    ## Hyperparameters:&#10;&#10;This allows users to apply the downloaded learner to other tasks or to&#10;modify the learner using functions from [`mlr`]{} and produce new runs.&#10;&#10;The [`getOMLRun`]{} function downloads a single run and returns an&#10;[`OMLRun`]{} object containing all information that are connected to&#10;this run, such as the ID of the task and the ID of the flow:&#10;&#10;    run = getOMLRun(run.id = 1816245)&#10;    run&#10;    ## &#10;    ## OpenML Run 1816245 :: (Task ID = 42, Flow ID = 4782)&#10;    ##  User ID  : 348&#10;    ##  Tags     : study_30&#10;    ##  Learner  : mlr.classif.randomForest(17)&#10;    ##  Task type: Supervised Classification&#10;&#10;The most important information for reproducibility, next to the exact&#10;data set and flow version, are the hyperparameter and seed settings that&#10;were used to create this run. This information is contained in the&#10;[`OMLRun`]{} object and can be extracted via [`getOMLRunParList(run)`]{}&#10;and [`getOMLSeedParList(run)`]{}, respectively.&#10;&#10;If the run solves a supervised regression or classification task, the&#10;corresponding predictions can be accessed via [`run$predictions`]{} and&#10;the evaluation measures computed by the server via&#10;[`run$output.data$evaluations`]{}.">
    </outline>
  </outline>
  <outline text="Creating Runs" _note="The easiest way to create a run is to define a learner, optionally with&#10;a preset hyperparameter value, using the [`mlr`]{} package. Each&#10;[`mlr`]{} learner can then be applied to a specific [`OMLTask`]{} object&#10;using the function [`runTaskMlr`]{}. This will create an [`OMLMlrRun`]{}&#10;object, for which the results can be uploaded to the OpenML server as&#10;described in the next section. For example, a random forest from the&#10;[`randomForest`]{} [`R`]{} package  can be instantiated using the&#10;[`makeLearner`]{} function from [`mlr`]{} and can be applied to a&#10;classification task via:&#10;&#10;    lrn = makeLearner(&quot;classif.randomForest&quot;, mtry = 2)&#10;    task = getOMLTask(task.id = 37)&#10;    run.mlr = runTaskMlr(task, lrn)&#10;&#10;To run a previously downloaded OpenML flow, one can use the&#10;[`runTaskFlow`]{} function, optionally with a list of hyperparameters:&#10;&#10;    flow = getOMLFlow(4782)&#10;    run.flow = runTaskFlow(task, flow, par.list = list(mtry = 2))&#10;&#10;To display benchmarking results, one can use the&#10;[`convertOMLMlrRunToBMR`]{} function to convert one or more&#10;[`OMLMlrRun`]{} objects to a single [`BenchmarkResult`]{} object from&#10;the [`mlr`]{} package so that several powerful plotting functions [[&#10;(see for examples) ]{}]{} from [`mlr`]{} can be applied to that object&#10;(see, e.g., Figure \[fig:bmrplot\]).">
  </outline>
  <outline text="Uploading and Tagging">
    <outline text="Uploading OpenML Objects:" _note="It is also possible to upload data sets, flows and runs to the OpenML&#10;server to share and organize experiments and results online. Data sets,&#10;for example, are uploaded with the [`uploadOMLDataSet`]{} function.&#10;OpenML will ACTIVATE the data set if it passes all checks, meaning that&#10;it will be returned in listing calls. Creating tasks from data sets is&#10;currently only possible through the website, see&#10;&lt;http://www.openml.org/new/task&gt;.&#10;&#10;[`OMLFlow`]{} objects can be uploaded to the server with the&#10;[`uploadOMLFlow`]{} function and are automatically versioned by the&#10;server: when a learner is uploaded carrying a different [`R`]{} or&#10;package version, a new version number and [`flow.id`]{} is assigned. If&#10;the same flow has already been uploaded to the server, a message that&#10;the flow already exists is displayed and the associated [`flow.id`]{} is&#10;returned. Otherwise, the flow is uploaded and a new [`flow.id`]{} is&#10;assigned to it:&#10;&#10;    lrn = makeLearner(&quot;classif.randomForest&quot;)&#10;    flow.id = uploadOMLFlow(lrn)&#10;&#10;A run created with the [`runTaskMlr`]{} or the [`runTaskFlow`]{}&#10;function can be uploaded to the OpenML server using the&#10;[`uploadOMLRun`]{} function. The server will then automatically compute&#10;several evaluation measures for this run, which can be retrieved using&#10;the [`listOMLRunEvaluations`]{} function as described previously.">
    </outline>
    <outline text="Tagging and Untagging OpenML Objects:" _note="The [`tagOMLObject`]{} function is able to tag data sets, tasks, flows&#10;and runs with a user-defined string, so that finding OpenML objects with&#10;a specific tag becomes easier. For example, the task with ID 1 can be&#10;tagged as follows:&#10;&#10;    tagOMLObject(id = 1, object = &quot;task&quot;, tags = &quot;test-tagging&quot;)&#10;&#10;To retrieve a list of objects with a given tag, the [`tag`]{} argument&#10;of the listing functions can be used (e.g.,&#10;[`listOMLTasks(tag = test-tagging)`]{}). The listing functions for data&#10;sets, tasks, flows and runs also show the tags that were already&#10;assigned, for example, we already tagged data sets from UCI  with the&#10;string [`uci`]{} so that they can be queried using&#10;[`listOMLDataSets(tag = uci)`]{}. In order to remove one or more tags&#10;from an [`OpenML`]{} object, the [`untagOMLObject`]{} function can be&#10;used, however, only self-created tags can be removed, e.g.:&#10;&#10;    untagOMLObject(id = 1, object = &quot;task&quot;, tags = &quot;test-tagging&quot;)">
    </outline>
  </outline>
  <outline text="Further Features" _note="Besides the aforementioned functionalities, the [`OpenML`]{} package&#10;allows to fill up the cache directory by downloading multiple objects at&#10;once (using the [`populateOMLCache`]{} function), to remove all files&#10;from the cache directory (using [`clearOMLCache`]{}), to get the current&#10;status of cached data sets (using [`getCachedOMLDataSetStatus`]{}), to&#10;delete OpenML objects created by the uploader (using&#10;[`deleteOMLObject`]{}), to list all estimation procedures (using&#10;[`listOMLEstimationProcedures`]{}) as well as all available evaluation&#10;measures (using [`listOMLEvaluationMeasures`]{}) and to get more&#10;detailed information on data sets (using [`getOMLDataSetQualities`]{}).">
  </outline>
</outline>
<outline text="Case Study" _note="In this section, we illustrate the usage of OpenML by performing a small&#10;comparison study between a random forest, bagged trees and single&#10;classification trees. We first create the respective binary&#10;classification learners using [`mlr`]{}, then query OpenML for suitable&#10;tasks, apply the learners to the tasks and finally evaluate the results.">
  <outline text="Creating Learners" _note="We choose three implementations of different tree algorithms, namely the&#10;CART algorithm implemented in the [`rpart`]{} package , the C5.0&#10;algorithm from the package [`C50`]{}  and the CONDITIONAL INFERENCE&#10;TREES implemented in the [`ctree`]{} function from the package&#10;[`party`]{} . For the RANDOM FOREST, we use the implementation from the&#10;package [`randomForest`]{} . The bagged trees can conveniently be&#10;created using [`mlr`]{}’s bagging wrapper. [[Note that we do not use&#10;bagging for the [`ctree`]{} algorithm due to large memory&#10;requirements.]{}]{} For the random forest and all bagged tree learners,&#10;the number of trees is set to 50. We create a list that contains the&#10;random forest, the two bagged trees and the three tree algorithms:&#10;&#10;    lrn.list = list(&#10;      makeLearner(&quot;classif.randomForest&quot;, ntree = 50),&#10;      makeBaggingWrapper(makeLearner(&quot;classif.rpart&quot;), bw.iters = 50),&#10;      makeBaggingWrapper(makeLearner(&quot;classif.C50&quot;), bw.iters = 50),&#10;      makeLearner(&quot;classif.rpart&quot;),&#10;      makeLearner(&quot;classif.C50&quot;),&#10;      makeLearner(&quot;classif.ctree&quot;)&#10;    )">
  </outline>
  <outline text="Querying OpenML" _note="For this study, we consider only binary classification tasks that use&#10;smaller data sets from UCI , e.g., between 100 and 999 observations,&#10;have no missing values and use 10-fold cross-validation for validation:&#10;&#10;    tasks = listOMLTasks(data.tag = &quot;uci&quot;,&#10;      task.type = &quot;Supervised Classification&quot;, number.of.classes = 2,&#10;      number.of.missing.values = 0, number.of.instances = c(100, 999),&#10;      estimation.procedure = &quot;10-fold Crossvalidation&quot;)&#10;&#10;Table \[tab:tasks\] shows the resulting tasks of the query, which will&#10;be used for the further analysis.">
  </outline>
  <outline text="Evaluating Results" _note="We now apply all learners from [`lrn.list`]{} to the selected tasks&#10;using the [`runTaskMlr`]{} function and use the&#10;[`convertOMLMlrRunToBMR`]{} function to create a single&#10;[`BenchmarkResult`]{} object containing the results of all experiments.&#10;This allows using, for example, the [`plotBMRBoxplots`]{} function from&#10;[`mlr`]{} to visualize the experiment results (see&#10;Figure \[fig:bmrplot\]):&#10;&#10;    grid = expand.grid(task.id = tasks$task.id, lrn.ind = seq_along(lrn.list))&#10;    runs = lapply(seq_row(grid), function(i) {&#10;      task = getOMLTask(grid$task.id[i])&#10;      ind = grid$lrn.ind[i]&#10;      runTaskMlr(task, lrn.list[[ind]])&#10;    })&#10;    bmr = do.call(convertOMLMlrRunToBMR, runs)&#10;    plotBMRBoxplots(bmr, pretty.names = FALSE)&#10;&#10;We can upload and tag the runs, e.g., with the string [`study_30`]{} to&#10;facilitate finding and listing the results of the runs using this tag:&#10;&#10;    lapply(runs, uploadOMLRun, tags = &quot;study_30&quot;)&#10;&#10;The server will then compute all possible measures, which takes some&#10;time depending on the number of runs. The results can then be listed&#10;using the [`listOMLRunEvaluations`]{} function and can be visualized&#10;using the [`ggplot2`]{} package:&#10;&#10;    evals = listOMLRunEvaluations(tag = &quot;study_30&quot;)&#10;    evals$learner.name = as.factor(evals$learner.name)&#10;    evals$task.id = as.factor(evals$task.id)&#10;&#10;    library(&quot;ggplot2&quot;)&#10;    ggplot(evals, aes(x = data.name, y = predictive.accuracy, colour = learner.name, &#10;      group = learner.name, linetype = learner.name, shape = learner.name)) +&#10;      geom_point() + geom_line() + ylab(&quot;Predictive Accuracy&quot;) + xlab(&quot;Data Set&quot;) +&#10;      theme(axis.text.x = element_text(angle = -45, hjust = 0))&#10;&#10;Figure \[fig:res\] shows the cross-validated predictive accuracies of&#10;our six learners on the considered tasks. Here, the random forest&#10;produced the best predictions, except on [[the tic-tac-toe data&#10;set]{}]{}, where the bagged C50 trees achieved a slightly better result.&#10;In general, the two bagged trees performed marginally worse than the&#10;random forest and better than the single tree learners.">
  </outline>
</outline>
<outline text="Conclusion and Outlook" _note="OpenML is an online platform for open machine learning that is aimed at&#10;connecting researchers who deal with any part of the machine learning&#10;workflow. The OpenML platform automates the sharing of machine learning&#10;tasks and experiments through the tools that scientists are already&#10;using, such as [`R`]{}. The [`OpenML`]{} package introduced in this&#10;paper makes it easy to share and reuse data sets, tasks, flows and runs&#10;directly from the current [`R`]{} session without the need of using&#10;other programming environments or the web interface. Current work is&#10;being done on implementing the possibility to connect to OpenML via&#10;browser notebooks [[(&lt;https://github.com/everware&gt;)]{}]{} and running&#10;analysis directly on online servers without the need of having [`R`]{}&#10;or any other software installed locally. In the future, it will also be&#10;possible that users can specify with whom they want to share, e.g., data&#10;sets.">
</outline>
  </body>
</opml>