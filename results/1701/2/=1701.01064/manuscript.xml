<?xml version="1.0" encoding="UTF-8"?>
<opml version="2.0">
  <head>
    <title>Optimal Low-Rank Dynamic Mode Decomposition</title>
    <abstract>Dynamic Mode Decomposition (DMD) has emerged as a powerful tool for
analyzing the dynamics of non-linear systems from experimental datasets.
Recently, several attempts have extended DMD to the context of low-rank
approximations. This extension is of particular interest for
reduced-order modeling in various applicative domains, [E.G., ]{}for
climate prediction, to study molecular dynamics or
micro-electromechanical devices. This low-rank extension takes the form
of a non-convex optimization problem. To the best of our knowledge, only
sub-optimal algorithms have been proposed in the literature to compute
the solution of this problem. In this paper, we prove that there exists
a closed-form optimal solution to this problem and design an effective
algorithm to compute it based on Singular Value Decomposition (SVD). A
toy-example illustrates the gain in performance of the proposed
algorithm compared to state-of-the-art techniques. </abstract>
  </head>
  <body>
<outline text="Introduction" _note="In many fields of Sciences, one is interested in studying the&#10;[spatio-temporal]{} evolution of a state variable characterized by a&#10;partial differential equation. Numerical discretization in space and&#10;time leads to a high dimensional system of equations [of the form:]{}&#10;[where]{} each element of the sequence of state variables belongs to ,&#10;with the initial condition . [Because may correspond to a very&#10;high-dimensional system in some applications, computing a trajectory&#10;given an initial condition ]{} may lead to a heavy computational load,&#10;which may prohibit the direct use of the original high-dimensional&#10;system.&#10;&#10;The context of uncertainty quantification provides an appealing example.&#10;Assume we are interested in characterizing the distribution of random&#10;trajectories generated by with respect to the distribution of the&#10;initial condition. A straightforward approach would be to sample the&#10;initial condition and run the high-dimensional system. However, in many&#10;applicative contexts, it is impossible to generate enough trajectories&#10;to make accurate approximations with Monte-Carlo techniques.&#10;&#10;As a response to this computational bottleneck, reduced-order models aim&#10;to approximate the trajectories of the system for a range of regimes&#10;determined by a set of initial conditions . A common approach is to&#10;assume that the trajectories of interest are well approximated in a&#10;sub-space of . In this spirit, many tractable low-rank approximations of&#10;high-dimensional systems have been proposed in the literature, the most&#10;familiar being proper orthogonal decomposition (POD) , balanced&#10;truncation , Taylor expansions  or reduced-basis techniques . Other&#10;popular sub-space methods, such as linear inverse modeling (LIM) ,&#10;principal oscillating patterns (POP) , or more recently, dynamic mode&#10;decomposition (DMD) , are known as Koopman operator approximations.&#10;&#10;In this paper, we consider the setting where system is a black-box. In&#10;other words, we assume that we do not know the exact form of in and we&#10;only have access to a set of representative trajectories , , so-called&#10;SNAPSHOTS, obtained by running the high-dimensional system for different&#10;initial conditions. Moreover, we focus on the low-rank DMD approximation&#10;problem studied in . In a nutshell, these studies provide a procedure&#10;for determining a matrix of rank , which substitutes [for]{} function in&#10;and generates the approximations with a low computational effort.&#10;Alternatively, given , and its non-zero eigenvalues and associated&#10;eigenvectors , trajectories of can be computed by using the&#10;reduced-order model as long as matrix is symmetric. We will assume it is&#10;always the case for simplification issues. In what follows, we will&#10;refer to the parameters and as the -TH LOW-RANK DMD MODE AND AMPLITUDE&#10;at time .&#10;&#10;Matrix targets the solution of the following non-convex optimization&#10;problem, which we will refer to as the [LOW-RANK DMD APPROXIMATION&#10;PROBLEM]{} where refers to the norm. In order to compute [a]{} solution&#10;, the authors in propose to rely on the assumption of linear dependence&#10;of recent snapshots on previous ones. This assumption may not be&#10;reasonable, especially in the case of non-linear systems.&#10;&#10;Beyond the reduced modeling context discussed above, there has been a&#10;resurgence of interest for low-rank solutions of linear matrix equations&#10;. This class of problems is very large and includes in particular&#10;problem . Problems in this class are generally nonconvex and do not&#10;admit explicit solutions. Howewer, important results have arisen at the&#10;theoretical and algorithmic level, enabling the characterization of the&#10;solution for this class of problems by convex relaxation . Applications&#10;concern scenarios such as low-rank matrix completion, image compression&#10;or minimum order linear system realization, see . Nevertheless, there&#10;exists certain instances with a very special structure, which admit&#10;closed-form solutions . This occurs typically when the solution can be&#10;deduced from the well-known Eckart-Young theorem .&#10;&#10;The contribution of this paper is to show that the special structure of&#10;problem enables the characterization of an exact closed-form solution&#10;and an easily implementable solver based on singular value decomposition&#10;(SVD). In the case , the proposed algorithm computes the solution of .&#10;More interestingly, for , [I.E., ]{}in the constrained case, our&#10;approach enables to solve exactly the low-rank DMD approximation problem&#10;without 1) any assumption of linear dependence, 2) the use of an&#10;iterative solver, on the contrary to the approaches proposed in . The&#10;paper is organized as follows. In section \[sec:stateArt\], we provide a&#10;brief review of state-of-the-art techniques to compute low-rank DMD of&#10;experimental data. Section \[sec:contrib\] details our analytical&#10;solution and the algorithm solving . Given this optimal solution, it&#10;then presents the reduced-order model solving . Finally, a numerical&#10;evaluation of the method is presented in [Section]{} \[sec:numEval\] and&#10;concluding remarks are [given]{} in a last section.">
</outline>
<outline text="State-Of-The-Art Overview" _note="In what follows, we assume that we have at our disposal trajectories of&#10;snapshots. We will need in the following some matrix notations. The&#10;symbol and the upper script will respectively refer to the Frobenius&#10;norm and the transpose operator. will denote the -dimensional identity&#10;matrix. Let consecutive elements of the -th snapshot trajectory between&#10;time and be gathered in a matrix and let two large matrices with be&#10;defined as Without loss of generality, this work will assume that and&#10;that . We introduce the SVD decomposition of a matrix with : with , and&#10;so that and is diagonal. The Moore-Penrose pseudo-inverse of a matrix&#10;will be defined as . With these notations, problem can be rewritten as&#10;In what follows, we begin by presenting two state-of-the-art methods&#10;which enable to compute an approximation of the solution of problem .">
  <outline text="Projected DMD and Low-Rank Formulation" _note="As detailed herafter, the original DMD approach first proposed in ,&#10;so-called PROJECTED DMD in , assumes that columns of are in the span of&#10;. The assumption is written by the authors in as the existence of , the&#10;so-called COMPANION MATRIX of parametrized by coefficients, such that We&#10;remark that this assumption is in particular valid when the -th snapshot&#10;can be expressed as a linear combination of the columns of and when is&#10;linear. Using the SVD decomposition and noticing is full rank, we obtain&#10;from a projected representation of in the basis spanned by the columns&#10;of , where Therefore, the low-rank formulation in proposes to approach&#10;the solution of by determining the coefficients of matrix which minimize&#10;the Frobenius norm of the residual . This yields after some algebraic&#10;manipulations to solve the problem The Eckart-Young theorem then&#10;provides the optimal solution to this problem based on a rank- SVD&#10;approximation of matrix given by where is a diagonal matrix containing&#10;only the -largest singular values of and with zero entries&#10;[otherwise]{}. Exploiting the low-dimensional representation , a&#10;reduced-order model for trajectories can then be obtained by inserting&#10;in the low-rank approximation As an alternative, the authors propose a&#10;reduced-order model for trajectories relying on the so-called DMD modes&#10;and their amplitudes. These modes are related to the eigenvectors of the&#10;solution of . The amplitudes are given by solving a convex optimization&#10;problem with an iterative gradient-based method, see details in .">
  </outline>
  <outline text="Non-projected DMD" _note="If we remove the low-rank constraint, becomes a least-squares problem&#10;whose solution is Based on the approximation , DMD modes and amplitudes&#10;serve to design a model to reconstruct trajectories of . We note that&#10;the DMD modes are simply given by the eigendecomposition of , which can&#10;be [efficiently]{} [computed]{} using SVD, as proposed in . The&#10;[associated]{} DMD amplitudes can then easily be derived.&#10;&#10;It is important to remark that truncating to a rank- the solution of the&#10;above unconstrained minimization problem will not necessarily yield the&#10;solution of . This approach will generally be sub-optimal. However&#10;surprisingly, the solution to problem remain to our knowledge overlooked&#10;in the literature, and no algorithms enabling non-projected low-rank DMD&#10;approximations have yet been proposed.">
  </outline>
</outline>
<outline text="The Proposed Approach">
  <outline text="Closed-form Solution to" _note="Let the columns of matrix be the real orthonormal eigenvectors&#10;associated to the largest eigenvalues of matrix&#10;&#10;\[prop22\] A solution of is&#10;&#10;This theorem states that can be simply solved by computing the&#10;orthogonal projection of the unconstrained problem solution onto the&#10;subspace spanned by the first eigenvectors of . A detailed proof is&#10;provided in [the technical report associated to this paper .]{}">
  </outline>
  <outline text="&lt;span&gt;Efficient&lt;/span&gt; Solver" _note="The matrix is of size . [Since is typically very large, t]{}his&#10;prohibits the direct computation of an eigenvalue decomposition. The&#10;following well-know remark is useful to overcome this difficulty.\&#10;&#10;\[rem:1\] The eigenvectors associated to the non-zero eigenvalues of&#10;matrix with can be obtained [from]{} the eigenvectors and eigenvalues of&#10;the smaller matrix . Indeed, the SVD of a matrix of rank is where the&#10;columns of matrix are the eigenvectors of . Since is unitary, we obtain&#10;that the sought vectors are the [first ]{} columns of , [I.E., ]{}of&#10;&#10;In the light of this remark, it is straightforward to design Algorithm&#10;\[algo:1\], which will compute efficiently the solution of based on&#10;SVDs.&#10;&#10;**input**: -sample 1) Form matrix and as defined in . 2) Compute the SVD&#10;of . 3) Compute the columns of using Remark \[rem:1\].**output**: matrix&#10;&#10;**input**: matrices , with 1) Compute the SVD of matrix .2) Solve for&#10;the eigen equation where and denote eigenvectors and eigenvalues of&#10;&#10;**output**: DMD modes and amplitudes">
  </outline>
  <outline text="Reduced-Order Models" _note="We now discuss the resolution of the reduced-order model given the&#10;solution of . Trajectories of are fully determined by a -dimensional&#10;recursion involving the projected variable : Then, by multiplying both&#10;sides by matrix , we obtain the sought low-rank approximation .&#10;Alternatively, we can employ reduced-order model . The parameters of&#10;this model, [I.E., ]{}low-rank DMD modes and amplitudes, are efficiently&#10;computed without any minimization procedure, in contrast to what is&#10;proposed by the author in []{}. Indeed, we rely on the following remark&#10;stating that DMD modes and amplitudes can be obtained by means of SVDs&#10;using Algorithm \[algo:2\]. The remark is proved in the [technical&#10;report ]{}.&#10;&#10;\[rem:2\] Each pair generated by Algorithm \[algo:2\] is one of the&#10;eigenvector/eigenvalue pair of .">
  </outline>
</outline>
<outline text="Numerical Evaluation" _note="In what follows, we evaluate on a toy model the different approaches for&#10;solving the low-rank DMD approximation problem. We consider a&#10;high-dimensional space of dimensions, a low-dimensional subspace of&#10;dimensions and snapshots. Let be a matrix of rank generated randomly&#10;according to , where entries of ’s are independent samples of the&#10;standard normal distribution. Let the initial condition be randomly&#10;chosen according to the same distribution. The snapshots, gathered in&#10;matrices and , are generated using for three configurations of :&#10;&#10;, s.t. satisfying ,&#10;&#10;,&#10;&#10;.&#10;&#10;Setting corresponds to a linear system satisfying the assumption , as&#10;made in the projected DMD approaches . Setting and , do not make this&#10;assumption and simulate respectively linear and non-linear dynamical&#10;systems. We assess three different methods for computing :&#10;&#10;optimal rank- approximation given by Algorithm \[algo:1\],&#10;&#10;th-order SVD approximation of , [I.E., ]{}-th order approximation of the&#10;rank- non-projected DMD solution ,&#10;&#10;rank- approximation by , corresponding to the projected DMD approach (or&#10;for ).&#10;&#10;The performance is measured in terms of the error norm with respect to&#10;the rank . Results for the three settings are displayed in Figure&#10;\[fig:1\]. As a first remark, we notice that the solution provided by&#10;Algorithm \[algo:1\] (method ) yields the best results, in agreement&#10;with [T]{}heorem \[prop22\].&#10;&#10;Second, in setting , the experiments confirm that when the linearity&#10;assumption is valid, the low-rank projected DMD (method ) achieves the&#10;same performance as the optimal solution (method ). Moreover, truncating&#10;the rank- DMD solution (method ) induces as expected an increase of the&#10;error norm. This deterioration is however moderate in our experiments.&#10;&#10;Then, in settings and we remark that the behavior of the error norms are&#10;analogous (up to an order of magnitude). The [performance]{} of the&#10;projected approach (method ) differs notably from the optimal solution.&#10;A significant deterioration is visible for . This is the consequence of&#10;the non-validity of the assumption made in method . Nevertheless, we&#10;notice that method  accomplishes a slight gain in performance compared&#10;to method  up to a moderate rank (). Besides, we also notice that the&#10;error norm of method  in the case is not optimal.&#10;&#10;Finally, as expected, all methods succeed in properly characterizing the&#10;low-dimensional subspace as soon as .">
</outline>
<outline text="Conclusion" _note="Following recent attempts to characterize an optimal low-rank&#10;approximation based on DMD, this paper provides a closed-form solution&#10;to this non-convex optimization problem. To the best of our knowledge,&#10;state-of-the-art methods are all sub-optimal. The paper further proposes&#10;effective algorithms based on SVD to solve this problem and run&#10;reduced-order models. Our numerical experiments attest that the proposed&#10;algorithm is more accurate than state-of-the-art methods. In particular,&#10;we illustrate the fact that simply truncating the full-rank DMD&#10;solution, or exploiting too restrictive assumptions for the&#10;approximation subspace is insufficient.">
</outline>
<outline text="Acknowledgements" _note="This work was supported by the “Agence Nationale de la Recherche&quot;&#10;through the GERONIMO project (ANR-13-JS03-0002).">
</outline>
  </body>
</opml>