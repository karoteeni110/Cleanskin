<?xml version="1.0" encoding="UTF-8"?>
<opml version="2.0">
  <head>
    <title>More lessons from the six box toy experiment[^1]

[^1]: Addendum to [arXiv:1612.05292]{}.</title>
    <abstract>Following a paper in which the fundamental aspects of probabilistic
inference were introduced by means of a toy experiment, details of the
analysis of simulated long sequences of extractions are shown here. In
fact, the striking performance of probability-based inference and
forecasting, compared to those obtained by simple ‘rules’, might impress
those practitioners who are usually underwhelmed by the philosophical
foundation of the different methods. The analysis of the sequences also
shows how the smallness of the probability of what has actually been
observed, given the hypotheses of interest, is irrelevant for the
purpose of inference. </abstract>
  </head>
  <body>
<outline text="Introduction" _note="For years I have been using a toy experiment for introducing&#10;probabilistic reasoning. Irrespective of whether my audience has been of&#10;professional physicists and engineers,[^1] high school students,&#10;teachers and general public,[^2] or even managers and senior officers&#10;attending a decision-making school,[^3] this toy model has always been&#10;an “eye opener”. This is how it was defined by the editors of a special&#10;issue of the American Journal of Physics,[^4] in which this ‘experiment’&#10;was first published .&#10;&#10;A thorough description of the game, and what can be learned from it, is&#10;given in Ref. . In particular, in that paper I explain the reasons why I&#10;do not let anyone see the balls in the box. At most we can make&#10;simulated extractions, in which we can ‘almost’ see the game from the&#10;God’s perspective: we know the box composition with certainty, and give&#10;a superior smile at the algorithm that is trying to guess it. ‘Almost’,&#10;because we remain uncertain about the color of future extractions.&#10;&#10;Let us see then what we can learn from simulations. Firstly, in order to&#10;allow readers to reproduce the results, details of the simulation are&#10;given in Section 2. For this reason, as examples of how to generate and&#10;analyze the sequences, R commands are given. In Sections 3-5, some&#10;sequences are analyzed in detail. Whenever possible, the numbers&#10;obtained from the probability theory algorithm are compared with those&#10;resulting from ‘simple rules’. But, as made clear in Section 6, this is&#10;not always possible. Finally, in Section 7, I emphasize the fact that&#10;most real life cases – as random sequences of black and white balls&#10;extracted from a box are – might have ‘astronomically’ small&#10;probabilities of occurrence, given the hypotheses of interest. But,&#10;nevertheless, the smallness of each conditional probability is&#10;irrelevant for the inference. Instead, what matters are their ratios and&#10;the relative prior beliefs of the different hypotheses.&#10;&#10;[^1]: See e.g. &lt;http://indico.cern.ch/event/419045/&gt; and&#10;    &lt;http://www.roma1.infn.it/~dagos/prob+stat.html#cern05&gt;;&#10;    &lt;http://2015.imtc.ieee-ims.org/content/tutorials&gt; and&#10;    &lt;http://www.roma1.infn.it/~dagos/prob+stat.html#IEEEPisa&gt;.&#10;&#10;[^2]: See e.g. &lt;http://www.lnf.infn.it/edu/openlabs/2016/conference.php&gt;&#10;    and&#10;    &lt;http://orientamento.matfis.uniroma3.it/fisincittastorico.php#dagostini&gt;&#10;    (in Italian).&#10;&#10;[^3]: See e.g.&#10;    &lt;http://www.pangeaformazione.it/en/training/decision-making-school.html&gt;.&#10;&#10;[^4]: &lt;http://stp.clarku.edu/ajp_contributors.html&gt;.">
</outline>
<outline text="Simulated sequences" _note="Thousand extractions from each of the boxes , and (Fig. \[fig:sixbox\]),&#10;since we can form an idea of what happens from the others just by&#10;(anti-)symmetry. The R code to generate and analyze the sequences is&#10;based on that shown in Footnote 31 of , but we report here also the&#10;inferential story as the extractions go on. Moreover, for the benefit of&#10;the reader, who can then check the details of the results the ‘seed’ of&#10;the random generator is given, equal to 20160715, for the date of the&#10;talk upon which this paper is based (no special, fancy sequences have&#10;been cherry-picked).&#10;&#10;Here are the four lines of R code to make initializations and&#10;extractions (‘0’ for Black and ‘1’ for White):[^1]\&#10;`N = 5; i = 0:N; pii = i/N; n = 1000; set.seed(20160715)`\&#10;`seq.B0 = rbinom(n, 1, pii[1])`\&#10;`seq.B1 = rbinom(n, 1, pii[2])`\&#10;`seq.B2 = rbinom(n, 1, pii[3])`\&#10;Later, in order to have a feeling of the performances of the method,[^2]&#10;we can split the sequences in ‘runs’ of 100 extractions and analyze them&#10;independently.&#10;&#10;[^1]: The csv files of the sequences analyzed here can be downloaded&#10;    from &lt;http://www.roma1.infn.it/~dagos/prob+stat.html#addendum_ME&gt;,&#10;    and then loaded in a R session by e.g.&#10;    `seq.B0 = as.vector( read.csv(&quot;seq_B0.txt&quot;)$x )`.&#10;&#10;[^2]: I you wish and it helps you, you might think of the ‘propensity’&#10;    of the algorithm to produce some numbers rather then others.">
</outline>
<outline text="Box" _note="The runs from will obviously be all equal, since that composition can&#10;only produce 0’s (Black), and therefore only the first run is shown&#10;(Fig. \[fig:Simulazione\_B0\]).&#10;&#10;We see (upper plot) that as we continue to observe Black our confidence&#10;that we have ‘picked’ steady increases. is obviously ruled out from the&#10;very beginning, while the probability we [HAVE TO]{} assign the&#10;compositions having both colors decreases exponentially with the&#10;increasing number of extractions (note the log scale in the&#10;ordinates).[^1] After 100 extractions the probability of differ from&#10;unity by about , essentially the probability of , while all others are&#10;less probable by tens of orders of magnitude.[^2] So, we are&#10;[PRACTICALLY CERTAIN]{} about – but those who only like certainty have&#10;to remember that [OUR ONLY CERTAINTY IS THAT IS RULED OUT]{}.&#10;&#10;The lower plot of Fig. \[fig:Simulazione\_B0\] shows, instead, the&#10;probability of White in a next extraction, that is (blue circles). Its&#10;exponential decrease results from the exponential decrease of , for&#10;.[^3] Thus after 100 Black in a row we become ‘practically certain’ to&#10;observe Black in the 101-th extraction, being the probability of White&#10;only – but yet [NOT IMPOSSIBLE!]{}[^4] (If you think that very&#10;improbable events do not occur in real life, then wait for Appendix B.)&#10;&#10;For comparison we could show in the same plot also the relative&#10;frequency of White in the extractions, as we shall do with the&#10;simulations from the other boxes, but, since in this case it is always&#10;zero, it is of little interest, and anyway not representable in a log&#10;scale. It is, instead, more interesting, the probability evaluated&#10;([INCORRECTLY]{}!) applying the Laplace rule of succession (Equation 15&#10;of Ref. ), that in this case becomes , and, by complement for Black. As&#10;we can see, the performance is rather poor.&#10;&#10;However is not Laplace to be wrong[^5], but rather those who would use&#10;his formula a-critically, without understanding the assumptions behind&#10;it, which were discussed in detail in the text. In our specific case, as&#10;it might be in important cases of real life, the prior of the propensity&#10;of the box to give White was not uniform between between 0 and 1. We had&#10;instead only six possible values, and the full calculation takes into&#10;account of the real situation.[^6] For this reason the name of Laplace&#10;is in quote marks in the legends of the figures, to mean “misused&#10;Laplace rule.”&#10;&#10;[^1]: The reason is quite simple, with the approximation being valid for&#10;    ‘large’ values of (and for ): \[fn:exponential\_PBi\],&#10;&#10;[^2]: More precisely, executing the R command of Footnote 31 of Ref.&#10;    with `ri=0` we get\&#10;    `[1] 1.000000e+00 2.037036e-10 6.533186e-23 1.606938e-40 1.267651e-70`&#10;&#10;[^3]: In fact, using the result of Footnote \[fn:exponential\_PBi\],&#10;    With we obtain , or 1 in 25 billions (remember that ‘who’ analyses&#10;    the sequence does not know the real content of the box.)&#10;&#10;[^4]: You can evaluate this probability also using the of R code in&#10;    Footnote 31 of Ref. remember to change the first `sprintf()` format,&#10;    for example with `%.15f` or `%.5e`. The results are ‘numerically’&#10;    the same.&#10;&#10;[^5]: This is, for example, verbatim what he wrote concerning his too&#10;    much misunderstood probability of the sun rising tomorrow: “[ON&#10;    TROUVE AINSI QU’UN ÉVÉNEMENT ÉTANT ARRIV‘’E DE SUITE, UN NOMBRE&#10;    QUELCONQUE DE FOIS; LA PROBABILITÉ QU’IL ARRIVERA ENCORE LA FOIS&#10;    SUIVENTE, EST ÉGALE À CE NOMBRE AUGMENTÉ DE L’UNITÉ, DIVISÉ PAR LE&#10;    MÊME NOMBRE AUGMENTÉ DE DEUX UNITÉS.]{} \[\] [EN FAISANT, PAR&#10;    EXAMPLE, REMONTER LA PLUES ANCIENNE ÉPOQUE DE L’HISTOIRE, À CINQ&#10;    MILL ANS, OU À 1826213 JOURS, ET LE SOLEI S’ÉTANT LEVÉ CONSTENMMENT&#10;    DANS CET INTERVALL, À CHAQUE RÉVOLUTION DE VINGHT-QUATRE HEURES; IL&#10;    Y A 1826214 À PARIER CONTRE UN, QU’IL SE LEVERA ENCORE DEMAIN. [MAIS&#10;    CE NOMBRE EST INCOMPARABLEMENT PLUS FORT POUR CELUI QUI CONNAISSANT&#10;    PAR L’ENSEMBLE DES PHÉNOMÈNES, LE PRINCIPE RÉGULATEUR DES JOURS ET&#10;    DES SAISONS, VOIT QUE RIEN , NE PEUT EN ARRÊTER LE COURS.]{}]{}”&#10;    \[[“THUS WE FIND THAT AN EVENT HAVING OCCURRED SUCCESSIVELY ANY&#10;    NUMBER OF TIMES, THE PROBABILITY THAT IT WILL HAPPEN AGAIN THE NEXT&#10;    TIME IS EQUAL TO THIS NUMBER INCREASED BY UNITY DIVIDED BY THE SAME&#10;    NUMBER, INCREASED BY TWO UNITS. PLACING THE MOST ANCIENT EPOCH OF&#10;    HISTORY AT FIVE THOUSAND YEARS AGO, OR AT 182623 DAYS, AND THE SUN&#10;    HAVING RISEN CONSTANTLY IN THE INTERVAL OF EACH REVOLUTION OF&#10;    TWENTY-FOUR HOURS, IT IS A BET OF 1826214 TO ONE THAT IT WILL RISE&#10;    AGAIN TOMORROW.]{} [BUT THIS NUMBER IS INCOMPARABLY GREATER FOR HIM&#10;    WHO, RECOGNIZING IN THE TOTALITY OF PHENOMENA THE PRINCIPAL&#10;    REGULATOR OF DAYS AND SEASONS, SEES THAT NOTHING CAN ARREST THE&#10;    COURSE OF IT]{}.”\] (italics and underline mine) . Great Laplace!&#10;    (And please note once more the probability expressed in terms of a&#10;    virtual coherent bet.) \[fn:Laplace\_Sole\]&#10;&#10;[^6]: I have called the attention several times in the past (e.g. ) that&#10;    prior-less methods are not by default ‘objective’ because “they do&#10;    not use priors.” On the contrary, it is possible to show that there&#10;    are in most cases hidden (most times) uniform priors, like in the&#10;    result of the so called maximum-likelihood method of the&#10;    statisticians (see e.g. ).">
</outline>
<outline text="Box" _note="The analysis of the sequences from box (and, by symmetry, from ) is in&#10;general the most interesting and instructive, because the probabilities&#10;calculated using probability theory, taking into account all the&#10;available information properly, differ quite a lot from those obtained&#10;using intuitive heuristics, or from ‘prescriptions’ based on improper&#10;use of theoretical results not fully understood (see Footnote&#10;\[fn:Laplace\_Sole\]). Figures \[fig:Simulazione\_B1\_0\_100\],&#10;\[fig:Simulazione\_B1\_100\_100\] and \[fig:Simulazione\_B1\_200\_100\]&#10;show the results of the inferences and of the (probabilistic)&#10;predictions based on three sequential runs of 100 extractions each.&#10;&#10;Each story is peculiar, as real life situations are, and we see that –&#10;in the simulations we know the ‘truth’ – the method based on probability&#10;theory, and which take into account at best all available information,&#10;performs [MUCH BETTER]{} than the others.&#10;&#10;It is worth remembering that all real cases are [UNIQUE]{} and we can&#10;only rely on the quality of the methods, as well as of the data and all&#10;relevant information. As someone says (reference missing), in the&#10;Bayesian analysis “the result is the result.” For example, in the first&#10;part of sequence on which of Fig.\[fig:Simulazione\_B1\_0\_100\] is&#10;based, and seemed practically equally likely, and, as consequence, the&#10;probability of White in the next extraction was in between 1/5 and 2/5.&#10;That’s all. This the best we could say at that moment, but as soon as&#10;the overall relative frequencies of White approaches 20% (frequencies&#10;are reported as black triangles in the lower plot) there as a kind of&#10;‘attraction’ from : its probability suddenly rises, and the probability&#10;of White approaches rapidly 20%. Once balls of both colors are observed,&#10;if the relative frequency of observed White goes under 20% the effect of&#10;‘attraction’ gets more enhanced, because the next possibility, related&#10;to box , is too far.[^1]&#10;&#10;Also interesting is run 3 (Fig.\[fig:Simulazione\_B1\_200\_100\]),&#10;characterized by 16 black in a row.[^2] As a result, for a while we&#10;believed stronger and stronger we had picked up , and thus the&#10;probability of White in the next extraction was (exponentially)&#10;decreasing. resulting in small probability of White in the next&#10;extraction. Than, suddenly, we observe White, and the probability of&#10;instantly drops to zero,[^3] while the probability jumps practically at&#10;100% and that of a next White at 20%.[^4] It is nice, and instructive,&#10;to observe that from this extraction on, will always be [SLIGHTLY ABOVE&#10;20%]{}. Those who have a biased mind would speak about a ‘biased&#10;estimator’. In reality, it is a just logical consequence of the fact&#10;that, once we have ruled out , the probability of White in a future&#10;extraction, weighted average of all possible values of the propensity of&#10;the box to give White, [HAS TO BE]{} slightly above the minimum possible&#10;value of propensity, that is 1/5. The frequency based value, as well as&#10;that from the [ LAPLACE RULE]{}, remains quite for a while below 20%,&#10;and than oscillates around it, in contradiction with the fact that has&#10;been definitively ruled out.&#10;&#10;You might object that after a very long sequence also the other&#10;evaluations will eventually ’converge’ (see Footnote 28 of Ref. for&#10;remarks on the precise meaning of this term in probability theory), but,&#10;as it as been famously said, “[IN THE LONG RUN WE ARE ALL DEAD]{}.”[^5]&#10;Let us see what happens if we analyze the full sequence of 1000&#10;extractions (Fig. \[fig:Simulazione\_B1\_0\_1000\]).&#10;&#10;The frequency based evaluations of the next observation is still&#10;oscillating around 20%, while that obtained from probability theory&#10;approaches 1/5 (from above!) by \[rough estimate obtained extrapolating&#10;the probability of from the above plot\].&#10;&#10;[^1]: No esoteric meaning is attached to the term ‘attraction’. It is&#10;    just because the next possible value of propensity, 4/5 of box , is&#10;    “too far” – see Appendix B.&#10;&#10;[^2]: But if you check the file you will see that there were already 11&#10;    Black just before, summing thus to 27 Black in a run (and, after 2&#10;    White, other 6 Black follow). It simply happened, and for this&#10;    reason I would like to insist on the worries already expressed in&#10;    Footnote 28 of Ref. , i.e. interpreting probabilistic statements as&#10;    pedantic regularities.&#10;&#10;[^3]: I am sorry for those who dislike discontinuities, but a crucial&#10;    single (solid) experimental information can change dramatically our&#10;    vision of the world, as it happens to those who suddenly learn that&#10;    their quite and polite neighbor was indeed a serial killer keeping&#10;    rests of human bodies in his fridge.&#10;&#10;[^4]: More precisely,\&#10;    `&gt; N=5; i=0:N; pii=i/N; n=17; x=1`\&#10;    `&gt; ( PBi =  pii^x * (1-pii)^(n-x) / sum( pii^x * (1-pii)^(n-x) ) )`\&#10;    `[1] 0.000000e+00 9.803047e-01 1.965040e-02 4.487479e-05 9.129799e-10`\&#10;    `&gt; sum( pii * PBi )`\&#10;    `[1] 0.203948`&#10;&#10;[^5]: &lt;https://en.wikiquote.org/wiki/John_Maynard_Keynes#Quotes&gt;">
</outline>
<outline text="Box" _note="Let us conclude this round up by also showing the results of the&#10;analysis of three runs and of the complete sequence from the box&#10;(Figures \[fig:Simulazione\_B2\_0\_100\],&#10;\[fig:Simulazione\_B2\_100\_100\], \[fig:Simulazione\_B2\_200\_100\] and&#10;\[fig:Simulazione\_B2\_0\_100\])&#10;&#10;without further comments, besides that in probabilistic matter, as in&#10;real life, the German dictum “einmal ist keinmal” applies.">
</outline>
<outline text=": Bayesian vs frequentistic evaluations" _note="After having seen the comparisons of the evaluations of probabilities of&#10;White in a future extraction, someone would like to see something&#10;similar concerning the probability of the different box compositions in&#10;the light of the observed sequence. But he/she will be disappointed to&#10;learn that such a comparison is simply impossible because [FREQUENTISTS&#10;PROHIBIT THE VERY CONCEPT OF]{} . That’s all, Sorry! (And I am sorry for&#10;you, if you thought you were a frequentist, but, nevertheless, you also&#10;thought that such a probability had a meaning – see and references&#10;therein for details).\&#10;\&#10;">
</outline>
<outline text="Most of our observations &lt;span&gt;HAD&lt;/span&gt; very small chance to occur" _note="An important misconception about probability is to confuse the&#10;probability of the effect given a given hypothesis with the probability&#10;of that hypothesis given the observed effect. The name “prosecutor&#10;fallacy”, with which this logical error is often designed gives, by&#10;itself, an idea of its relevant importance in real life.[^1] Continuing&#10;with the style of this paper, I would like to touch this point using the&#10;third run of the sequence from box (Fig.&#10;\[fig:Simulazione\_B1\_200\_100\]), which I find particular instructive.&#10;We shall analyze what we have learned after the 16th, the 17th and the&#10;100th extraction, also giving the details of the calculations in R,&#10;which start with the usual initialization (`N=5; i=0:N; pii=i/N`):&#10;&#10;[ (run 3):]{} At the beginning we had 16 black in a row, resulting on&#10;the following probabilities:\&#10;[&#10;`&gt; n=16; x=0; ( PBi =  pii^x * (1-pii)^(n-x) / sum( pii^x * (1-pii)^(n-x) ) )`\&#10;`[1] 9.723559e-01 2.736939e-02 2.743123e-04 4.176237e-07 6.372432e-12`\&#10;`&gt; sum( pii * PBi )`\&#10;`[1] 0.005583852`\&#10;]{} We are 97% confident to have got , 2.7% to have got , and so on. On&#10;the other hand, the probabilities to get “0 white in 16 trials” – be&#10;careful, I am trying to fool you – under the different hypotheses are ,&#10;that is\&#10;[ `&gt; ((5-i)/5)^16`\&#10;`[1] 1.000000e+00 2.814750e-02 2.821110e-04 4.294967e-07 6.553600e-12 0.000000e+00`&#10;]{}\&#10;So it seems than that the small probability to is due to the small&#10;probability to get the ‘observation’ from ; and similarly with the other&#10;boxes which contain white balls.&#10;&#10;[ (run 3):]{} Here is what happened after the next extraction, in which&#10;we observe White:\&#10;[&#10;`&gt; n=17; x=1; ( PBi =  pii^x * (1-pii)^(n-x) / sum( pii^x * (1-pii)^(n-x) ) )`\&#10;`[1] 0.000000e+00 9.803047e-01 1.965040e-02 4.487479e-05 9.129799e-10  0.000000e+00`\&#10;`&gt; sum( pii * PBi )`\&#10;`[1] 0.203948` ]{}\&#10;What is, instead, the probability of the observation, subject to the&#10;different compositions? You might think at binomial distributions&#10;resulting in 1 success in 17 trials, with probabilities given by , that&#10;is\&#10;[ `&gt; dbinom(x, n, pii)`\&#10;`[1] 0.000000e+00 9.570149e-02 1.918355e-03 4.380867e-06 8.912896e-11 0.000000e+00`&#10;]{}\&#10;However, this is not we have really observed, but just [ITS SUMMARY]{}.&#10;The observation was indeed [THE]{} sequence! And the probability of the&#10;sequence is quite different:[^2]\&#10;[ `&gt; pii^x * (1-pii)^(n-x)`\&#10;`[1] 0.000000e+00 5.629500e-03 1.128444e-04 2.576980e-07 5.242880e-12 0.000000e+00`&#10;]{}&#10;&#10;[ (run 3):]{} As it easy to predict, the difference becomes ‘dramatic’&#10;for very large values of . Having observed 18 White in 100 extractions,&#10;these are the probabilities of the hypotheses:\&#10;[&#10;`&gt; n=100; x=18; ( PBi =  pii^x * (1-pii)^(n-x) / sum( pii^x * (1-pii)^(n-x) ) )`\&#10;`[1] 0.000000e+00 9.999851e-01 1.491273e-05 8.011548e-17 2.938692e-39 0.000000e+00`&#10;]{}\&#10;to be compared with the probability of 18 successes in 100 trials for&#10;the different boxes:\&#10;[ `&gt; dbinom(x, n, pii)`\&#10;`[1] 0.000000e+00 9.089812e-02 1.355559e-06 7.282455e-18 2.671256e-40`&#10;]{}\&#10;But the conditional probabilities of what we have really observed are&#10;now strikingly different:\&#10;[ `&gt; pii^x * (1-pii)^(n-x)`\&#10;`[1] 0.000000e+00 2.964277e-21 4.420612e-26 2.374881e-37 8.711229e-60  0.000000e+00`&#10;]{}\&#10;But indeed, the fact that this sequence [HAD]{} [CHANCE]{} (really in&#10;the sense of a propensity) to occur from [IS ABSOLUTELY IRRELEVANT]{}.&#10;What matters is only this probability with respect to those from the&#10;other boxes. Indeed the respective conditional probabilities provide the&#10;Bayes-Turing factors for every pair of hypotheses. And therefore, since&#10;[IN OUR TOY EXPERIMENT]{} the different compositions were initially&#10;[EQUALLY LIKELY]{}, we get odds of vs of ; vs and and , respectively.&#10;These are the numbers that matter.&#10;&#10;At this point I hope the lesson is clear, without you need to be further&#10;impressed with the numbers that we would get analyzing the full sequence&#10;of 1000 extractions:&#10;&#10;that fact that we can make our inference and prediction based on the&#10;number of trials and the number of successes it is because these&#10;[SUMMARIES]{} are [‘SUFFICIENT’]{} for the purpose of the inference (and&#10;forecasting); but the real [OBSERVATION]{} is the sequence;&#10;&#10;most of the fact of real life [HAD]{} very little chance to occur, if we&#10;analyze them with enough accuracy. But this implies little on the&#10;probabilities of the cause that might have produced them. What matters&#10;are the ratio of conditional probabilities: .&#10;&#10;[^1]: See e.g.&#10;    &lt;http://www.agenarisk.com/resources/probability_puzzles/prosecutor.shtml&gt;.&#10;    This fallacy is somehow similar to the misinterpretation of&#10;    ‘p-values’ as probability of hypotheses (see and references&#10;    therein), but, even if the numbers are less impressive, the logical&#10;    fallacy of misinterpreting p-values is even worst, because the&#10;    erroneous conclusion is not based solely on the data, but also on&#10;    data less probable than those actually observed.&#10;&#10;[^2]: The difference is due to the binomial coefficient, equal to 17 in&#10;    this case, for which we have to divide the previous numbers.">
</outline>
<outline text="Conclusions" _note="Having to evaluate probabilities of hypotheses and probabilities of&#10;occurrences of future events, unless you possess a crystal ball, it is&#10;hard to out-perform Bayesian reasoning, if it is used consistently, and&#10;all the available pieces of information are properly taken into account.&#10;But the lesson which comes from playing with the simulated sequences&#10;goes beyond the demonstration of the power of the so called Bayesian&#10;methods.&#10;&#10;For example I find it important that, in the training of [PROBABILISTIC&#10;THINKING]{}, people should be exposed to the rich variety of what can&#10;occur randomly. And, therefore, most events of real life [HAD]{} very&#10;little chance to occur. Think, for example, at a given configuration of&#10;light content in pixels, when you shoot a picture with a digital camera.&#10;More simply, and easier to calculate, consider a number to twelve&#10;decimal places that can come from a Gaussian random number generator,&#10;like that obtained with the following R commands:\&#10;[ `&gt; options(digits=14); set.seed(20160715); nd=12; dxm=10^(-nd)/2`\&#10;`&gt; (xr=round(rnorm(1), nd)); as.double(sprintf(&quot;%.2e&quot;, dnorm(xr,) * 2 * dxm))`\&#10;`[1] 1.479427401471`\&#10;`[1] 1.34e-13`\&#10;`&gt; (xr=round(rnorm(1), nd)); as.double(sprintf(&quot;%.2e&quot;, dnorm(xr,) * 2 * dxm))`\&#10;`[1] -0.762658301757`\&#10;`[1] 2.98e-13` ]{}\&#10;(Yes, every time you repeat this line of code you will observe, [WITH&#10;CERTAINTY]{}, numbers which [HAD]{} about 1-in-trillions chance to&#10;occur! And they all come with probability 1 from a Gaussian random&#10;generator with and and )&#10;&#10;The reason I insist on these apparently trivial considerations is that I&#10;have seen too often in the past, and even quite recently, attempts of&#10;indoctrinating people with ‘statistical regularities’. These attempts&#10;imply a misinterpretation of probability theorems and, at the same time,&#10;a refusal of the concept of a single event probability. Instead, not&#10;only degrees of beliefs apply to single events, but also propensities,&#10;if we reflect on the fact that a propensity might change with time .">
</outline>
<outline text="Acknowledgements" _note="This work was partially supported by a grant from Simons Foundation&#10;which allowed me a stimulating working environment during my visit at&#10;the Isaac Newton Institute of Cambridge, UK (EPSRC grant EP/K032208/1).&#10;It is a pleasure to thank Paolo Agnoli of Pangea Formazione, for&#10;discussions on probabilistic matter, always with some philosophical&#10;flavor, encouragements to go ahead with the six box toy experiment, and&#10;for comments on the manuscript, which has also benefitted of comments by&#10;Patricia Wiltshire.&#10;&#10;[ref99]{}&#10;&#10;G. D’Agostini, [“TEACHING STATISTICS IN THE PHYSICS CURRICULUM. UNIFYING&#10;AND CLARIFYING ROLE OF SUBJECTIVE PROBABILITY”]{}, [Am. J. Phys]{}.&#10;[**67**]{} (1999) 1260 \[[physics/9908014]{}\].&#10;&#10;G. D’Agostini, [PROBABILITY, PROPENSITY AND PROBABILITIES OF&#10;PROPENSITIES (AND OF PROBABILITIES)]{},&#10;&lt;https://arxiv.org/abs/1612.05292&gt;.&#10;&#10;P.S. Laplace, [ESSAI PHILOSOPHIQUE SUR LES PROBABILITÉS]{}, 1814,&#10;(English quotes from A.I. Dale’s translation ([A PHILOSOPHICAL ESSAY ON&#10;PROBABILITIES]{}), Dover Publ. 1995.&#10;&#10;G. D’Agostini, [BAYESIAN REASONING IN DATA ANALYSIS – A CRITICAL&#10;INTRODUCTION]{}, World Scientific 2003.&#10;&#10;G. D’Agostini, [PROBABLY A DISCOVERY: BAD MATHEMATICS MEANS ROUGH&#10;SCIENTIFIC COMMUNICATION]{}, &lt;https://arxiv.org/abs/1112.3620&gt;.&#10;&#10;G. D’Agostini, [THE WAVES AND THE SIGMAS (TO SAY NOTHING OF THE 750 GEV&#10;MIRAGE)]{}, &lt;https://arxiv.org/abs/1609.01668&gt;.">
</outline>
  </body>
</opml>