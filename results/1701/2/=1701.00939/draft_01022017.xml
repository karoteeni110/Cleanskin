<?xml version="1.0" encoding="UTF-8"?>
<opml version="2.0">
  <head>
    <title></title>
    <abstract> </abstract>
  </head>
  <body>
<outline text="Introduction" _note="Deep neural networks are highly sensitive to small well-designed&#10;perturbations of the inputs, known as adversarial perturbations, which&#10;lead to misclassifications of these perturbed inputs. Consider images as&#10;an example of the data. A raw image, which is correctly classified by&#10;the neural network, can be modified in a small (and often imperceptible&#10;to our vision) way, so that the resulting deformed image is classified&#10;as a different class . A related problem is that DNN classify certain&#10;images as belonging to some class, although they are unrecognizable to&#10;humans as exemplars of any class . These rubbish or fooling images&#10;correspond to patches of the image space that have a small value of the&#10;objective function used in training and are located far away from any of&#10;the training data. These observations has challenged the integrity of&#10;DNN classification and has led to an opinion that their predictions are&#10;untrustworthy (see discussion of this issue in numerous blogs), though&#10;similar problems are shared by many other machine learning techniques,&#10;such as logistic regression, support vector machines, K-nearest&#10;neighbors and others .&#10;&#10;Further investigation have shown that both adversarial images and&#10;rubbish images can be transferred between many different models having&#10;distinct architectures, different hyperparameters, and even trained on&#10;different training sets . Moreover, some of the adversarial and rubbish&#10;images can be transferred between a diverse set of models in machine&#10;learning . This opens up a possibility for a potential adversarial&#10;attack, when a hacker can train his own model and create a set of images&#10;that are misclassified by it, and then deploy this set of images against&#10;another victim model, which will also misclassify them. Realistic&#10;attacks were studied in . Often they do not require any internal&#10;knowledge of the victim model.&#10;&#10;In addition to being a security issue, transferability suggests that&#10;various computational models learn very similar representations of the&#10;data. It also suggests that in order to address these problems, one&#10;might have to design training algorithms that learn a very different&#10;representation of the data compared to the existing methods. An ideal&#10;solution to these issues should be an algorithm that assigns small&#10;values of the objective function only to those areas of the image space&#10;that are recognizable by humans as images of the corresponding class. It&#10;should also require a substantial and recognizable by humans deformation&#10;of the initial correctly classified image towards a different target&#10;class before the label changes. In spite of a substantial amount of work&#10;on these problems , no algorithm have been identified so far which&#10;satisfies these requirements and at the same time is competitive to&#10;state of the art algorithms in terms of classification accuracy.&#10;&#10;In a recent paper it was proposed that Dense Associative Memory (DAM)&#10;models with higher order interactions in the energy function learn&#10;representations of the data, which strongly depend on the power of the&#10;interaction vertex. The network extracts features form the data for&#10;small values of this power, but as the power of the interaction vertex&#10;is increased there is a gradual shift to a prototype-based&#10;representation, the two extreme regimes of pattern recognition known in&#10;cognitive psychology. Remarkably, there is a broad range of powers of&#10;the energy function, for which the representation of the data is already&#10;in the prototype regime, but the accuracy of classification is still&#10;competitive to the best available algorithms (based on DNN with ReLUs).&#10;This suggests that the DAM models might behave very differently compared&#10;to the standard methods used in deep learning with respect to&#10;adversarial deformations.&#10;&#10;In the present paper we report three main results. First, using a&#10;gradient decent in the pixel space, a set of “rubbish” images is&#10;constructed that correspond to the minima of the objective function used&#10;in training. This is done on the MNIST dataset of handwritten digits&#10;using different values of the power of the interaction vertex, which is&#10;denoted by . For small values of the power these images indeed look like&#10;speckled rubbish noise, and do not have any semantic content for human&#10;vision, a result consistent with . However, as the power of the&#10;interaction vertex is increased the images gradually become less&#10;speckled and more semantically meaningful. In the limit of very large ,&#10;these images are no longer rubbish at all. They represent plausible&#10;images of handwritten digits that could have possibly been produced by a&#10;human. Second, starting from clean images from the dataset a set of&#10;adversarial images is constructed in such a way that each image is&#10;placed exactly on the decision boundary between two label classes. For&#10;small powers these images look very similar to the initial clean image&#10;with a little bit of speckled noise added, but are misclassified by the&#10;neural network, a result consistent with . However, as the power of the&#10;interaction vertex is increased these adversarial images become less and&#10;less similar to the initial clean image. In the limit of very large&#10;powers these adversarial images look either like a morphed image of two&#10;digits (the initial clean image and another digit from the class that&#10;the deformation targets), or the initial digit superimposed on a “ghost”&#10;image from the target class. Either way the interpretation of the&#10;artificial patterns generated by the neural net on the decision boundary&#10;requires the presence of another digit from the target class in addition&#10;to the initial seed from the dataset, and cannot be explained by simply&#10;adding noise to the initial clean image. Third, adversarial and rubbish&#10;images generated by models with small can be transferred to and fool&#10;another model with small (but possibly different) value . However, they&#10;fail to transfer to models with large . Thus rubbish and adversarial&#10;images generated by models with small cannot fool models with large . In&#10;contrast, the “rubbish” images generated by models with large can be&#10;transferred to models with small , but this is not a problem since those&#10;“rubbish” images are actually not rubbish at all and look like credible&#10;handwritten digits. These results suggest that the DAMs with a large&#10;power of the interaction vertex in the energy function better mimic the&#10;psychology of human visual perception than DAMs with a small power. The&#10;latter are equivalent to DNNs with ReLUs .">
</outline>
<outline text="Data representation in  networks" _note="In order to illustrate these results in the simplest possible setting we&#10;trained a family of DAM networks for several values of the power of the&#10;energy function on the MNIST pixel permutation invariant task.&#10;&#10;The model is defined by a set of weights and and the feedforward update&#10;rule where summation over repeated index is assumed. The functions are&#10;rectified polynomials  As we explained in , the argument of the update&#10;rule (\[update\_rule\]) is the difference of two energies, corresponding&#10;to the initial and the final states of the neurons. For this reason the&#10;functions are called the energy functions in the rest of the paper, and&#10;the integer is called the power of the interaction vertex. The weights&#10;are learned using a backpropagation algorithm (see Appendix A of )&#10;starting from random initial seeds by minimizing the objective function&#10;where is the target output ( for the wrong classes and for the correct&#10;class). Hyperparameter regulates the shape of the objective function.&#10;&#10;In we investigated this model for varying powers of the energy functions&#10;and discovered that the network learns feature-based representations of&#10;the data for small and prototype-based representations for large . This&#10;is clear from the Fig.\[feature\_to\_prototype\]. For small each feature&#10;detector describes a feature which is useful for recognizing several&#10;different digit classes.&#10;&#10;As the power is increased, most of the feature detectors specialize and&#10;become responsible for recognizing one possible prototype, which is a&#10;constructed representation and is not simply a copy of one image from&#10;the training set. Importantly, throughout the range the classification&#10;accuracy remains approximately the same, . This is comparable to the&#10;best result , achieved by the standard DNNs trained with backpropagation&#10;alone .">
</outline>
<outline text="Rubbish examples in  networks" _note="Once the training is complete, the neural network can be used to inspect&#10;the minima of the objective function. In order to do that one can define&#10;a set of objective functions each penalizing the deviations from the&#10;corresponding class with the target output A random image generated from&#10;a gaussian distribution can then be deformed into 10 images&#10;(sufficiently close to the initial one in the pixel space) corresponding&#10;to the 10 label classes by following the (negative) gradient of the&#10;objective functions according to the iterative rule where is a unit&#10;vector, which points in the direction of the gradient of the objective&#10;function , and is the size of the update step. This dynamics flows&#10;towards the minimum of the objective function, thus for a good learning&#10;algorithm we expect to see a recognizable image of the corresponding&#10;digit once it reaches the fixed point. The actual results are shown in&#10;Fig.\[irreg\_images\].&#10;&#10;For small the dynamics converges to local minima which are not&#10;recognizable as digits by humans. These are the rubbish minima of . The&#10;training algorithm has learned them while trying to minimize the&#10;classification error on the training set. In contrast, for large the&#10;dynamics flows towards a minimum of the objective function that&#10;corresponds to a recognizable image of the corresponding digit. Thus&#10;this simple experiment demonstrates that for DAM with large the minima&#10;of the objective function have semantic meaning in the image space,&#10;while for models with small these minima are semantically meaningless.&#10;All this is achieved by training the network in a purely supervised way&#10;on the pixel permutation invariant task.">
</outline>
<outline text="Adversarial deformations" _note="A clean image, which is classified by a neural network as belonging to a&#10;certain class, can be modified by a small perturbation so that the&#10;deformed image is classified as a different class. What makes this&#10;statement a problem is that the perturbation, which is sufficient for&#10;changing the label, is small and typically is a speckled pattern that&#10;lacks semantic meaning. The most common technique for generating&#10;adversarial images is the sign of the gradient method of . For our&#10;purposes it is necessary to generate images that are placed exactly on&#10;the decision boundary between the classes and not just in its vicinity.&#10;For this reason we use a slightly different method.&#10;&#10;For each input image the vector of labels can be calculated by using&#10;(\[update\_rule\]). The elements of this vector can be sorted from&#10;largest to smallest value. The largest value of the output corresponds&#10;to the top classification choice of the network, the second largest&#10;output corresponds to the second choice, etc.&#10;&#10;The initial image can then be iteratively deformed along the (negative)&#10;gradient of the objective function corresponding to the second choice of&#10;the label made by the network. During this iterative process the first&#10;choice objective function will increase, and the second choice objective&#10;function will decrease (see top panel in Fig.\[adv\_deform\]). The image&#10;itself should gradually change from the initial clean image to an image&#10;that the network thinks should correspond to the second choice label. At&#10;some iteration the two objective functions become equal. This is the&#10;mathematical definition of the decision boundary. From the point of view&#10;of the neural network the image that corresponds to this point on the&#10;deformation trajectory is exactly in the middle between the two classes.&#10;The question is whether a human observer would agree with this&#10;interpretation, in other words whether or not this image looks ambiguous&#10;to humans.&#10;&#10;This method of generating the adversarial images has an advantage&#10;compared to since it guarantees that the image at the crossing point of&#10;the two objective functions is placed exactly on the decision boundary,&#10;and not just close to it. Also, in contrast to , it does not require a&#10;careful choice of the iteration step in (\[gradient rule\]), provided&#10;that it is sufficiently small. The procedure will simply need more steps&#10;in order to find the correct image corresponding to the crossing point,&#10;if the step is too small. The drawback of this method compared to is&#10;that it is slower.&#10;&#10;In Fig.\[adv\_deform\] one can find a set of triplets of images&#10;generated by models with different values of power . In each triplet,&#10;the first image is the initial clean image from the dataset, the second&#10;image is the one that the neural network has generated on the decision&#10;boundary, and the third one is the image that the network has generated&#10;when the iterative procedure (\[gradient rule\]) has converged to the&#10;second choice label of the initial clean image. Two observations can be&#10;made from this figure. First, as the power of the interaction vertex is&#10;increased, the third image in each triplet becomes more semantically&#10;meaningful. This is in accord with the results of Fig.\[irreg\_images\].&#10;Second, as the power of the interaction vertex is increased, the&#10;adversarial image (the middle image) on the decision boundary becomes&#10;more meaningful as well. For or , the middle image looks almost the same&#10;as the initial clean image with a little bit of added speckled noise.&#10;The noise does not share much similarity with the second choice label.&#10;In contrast, for , the deformation does not look like noise at all. The&#10;middle image looks as if the two digits (the initial seed and a digit&#10;from the target class) were morphed together in one image, or as if the&#10;second digit was added as a “ghost” image to the initial digit. These&#10;results suggest that models with large powers not only learn&#10;semantically meaningful set of minima of the objective function, but&#10;also learn semantically meaningful deformations between the classes.&#10;This is achieved by training the network on the pixel permutation&#10;invariant classification task, not by training it as a generative model.&#10;In principle, the target class does not have to necessarily coincide&#10;with the second choice of the initial classification decision. This&#10;convention is used for convenience. To the best of our knowledge, the&#10;results do not change qualitatively if instead of the second choice&#10;label, the initial image is deformed towards any other label but the top&#10;one.">
</outline>
<outline text="Transferability between the models with various" _note="The most intriguing finding about rubbish and adversarial images is that&#10;they can be transferred between a diverse set of machine learning&#10;models. In order to test this phenomenon within the DAM framework we&#10;designed two experiments: one concerns the transfer of adversarial&#10;images and the other one concerns the transfer of rubbish images.&#10;&#10;In the first experiment the MNIST test set was used to generate&#10;adversarial examples by the four models with , resulting in four&#10;datasets each having 10000 images. For each clean image from the test&#10;set the procedure described in section \[adversarial deformation&#10;section\] was used to generate an artificial image placed one iteration&#10;step behind the decision boundary defined in Fig.\[adv\_deform\]. The&#10;resulting image is thus classified as the second choice of the network&#10;on the original image. These four adversarial datasets were used for the&#10;cross-classification by the same four models. The error rates are shown&#10;in Fig.\[true\_adv\_transfer\]. The diagonal elements of this table are&#10;close to , which is guaranteed by the design of the datasets[^1]. The&#10;most important aspect of this table is that the adversarial images&#10;generated by models with and do not transfer to the model with . These&#10;images share semantic similarity with the clean image that was used as&#10;an initial seed for crafting them, but do not generally have semantic&#10;features of the target class of the deformation. The model with can&#10;detect this similarity with the initial image and can still correctly&#10;classify of these cases. In contrast the adversarial images crafted&#10;using the model with can be transferred to models with . However, as we&#10;argued in the previous section this is expected since these images share&#10;semantic similarities with both the initial seed and the target class of&#10;the deformation. Thus, any machine learning algorithm or a human subject&#10;should misclassify a substantial fraction of them.&#10;&#10;In the second experiment an artificial dataset was created, so that data&#10;correspond to the minima of the 10 objective functions . For each sample&#10;a random noise image was generated from a gaussian distribution, which&#10;was then iteratively changed in the direction of the negative gradient&#10;according to (\[gradient rule\]) until convergence. The dataset has 100&#10;images of each label class, 1000 images in total. This procedure was&#10;repeated for each value of . The resulting four datasets were used for&#10;cross-classification by the same four models. The results are shown in&#10;Fig.\[transfer\_table\], but to discuss them we need to first introduce&#10;the notion of confidence.&#10;&#10;[^1]: The reason why the error rate is around 99% instead of 100% is&#10;    because the classification error on the clean dataset is about 1.5%,&#10;    and in about 1% of the cases the second choice of the network is the&#10;    correct answer. Thus in these rare cases the adversarial deformation&#10;    used to generate the dataset actually turns the incorrect answer&#10;    into the correct one.">
  <outline text="Confidence" _note="The output of the network (\[update\_rule\]) is a collection of numbers&#10;. For the following discussion it is convenient to define a measure of&#10;confidence that the neural network has in making a classification&#10;decision. If the actual outputs are approximately equal to the target&#10;output, the confidence is high. In contrast, if the outputs are all&#10;approximately zero, with one being slightly larger than the rest , the&#10;confidence is low. To quantify this effect, it is useful to pass the&#10;outputs through a softmax function to define a probability of each label&#10;given the input where the parameter is a number that regulates the&#10;steepness of the soft-max function. For each input image the confidence&#10;of the network is defined as the probability of the most probable label.&#10;Thus if the network is confident that the presented image belongs to a&#10;certain class , while if the network is unsure about what is shown in&#10;the image the distribution of labels is flat, and (in case of 10 classes&#10;for MNIST). The parameter should be chosen in such a way that the&#10;average confidence of the network on a dataset matches its error on the&#10;test set. If the outputs of the network were exactly equal to the target&#10;outputs for all inputs, then the average confidence on a dataset would&#10;be equal to (since there is one correct choice and nine incorrect ones)&#10;The error rate of all our models is of the order of , which defines the&#10;right hand side of the above equation. This results in . Since the&#10;outputs of the networks are in general slightly different from the&#10;target outputs, the actual values of are different for the four models&#10;that are discussed. They are: , , , . However, all the conclusions&#10;presented below remain true, for any values of as long as they stay&#10;within the range .&#10;&#10;The images from the MNIST test set can be binned into groups having&#10;certain confidence and the classification error can be calculated on&#10;each bin. The results are shown in Fig.\[error\_vs\_confidence\]. The&#10;probability of error decreases as the confidence increases, a result&#10;suggesting that the confidence measure is a meaningful quantity. Now we&#10;return to the discussion of the second experiment pertaining to the&#10;analysis of “rubbish” images.&#10;&#10;In Fig.\[transfer\_table\] the mean confidence of the 16&#10;cross-classification pairs is reported together with 10 sample images&#10;from each dataset, one for each class, for the four data sets. The&#10;bottom row corresponds to the data generated by the model with . The&#10;samples look like rubbish images, in accord with Fig.\[irreg\_images\].&#10;These rubbish images are recognized by the network with at the average&#10;confidence , which is the result of the particular choice of the value .&#10;These rubbish images can be transferred to the model with , which is&#10;also very confident that they correspond to the correct labels. However,&#10;if one tries to transfer these rubbish images to the models with or ,&#10;these higher order models immediately detect that these images do not&#10;correspond to any class and produce an average confidence . This&#10;accurately mimics the behavior of a human subject who would immediately&#10;detect that these rubbish images are semantically meaningless, thus&#10;should not belong to any class. The same conclusion applies to the&#10;dataset constructed by the model with . In contrast, the dataset&#10;constructed by the model with is composed of nice images of digits,&#10;which are recognizable by humans. These images can be transferred to any&#10;model with without loss in confidence. These results suggest that models&#10;with large better mimic human visual cognition, compared to models with&#10;small , both at generation and at testing.&#10;&#10;From the security perspective these results suggest that the models with&#10;large can be used to detect and stop a potential hacker attack, which is&#10;devised by exploiting conventional machine learning techniques (for&#10;example DNNs with ReLUs). At the same time, if the adversary tries to&#10;use models with large for mounting the attack and deploys it against&#10;models with any (small or large) , this does not possess a security&#10;issue, because the interpretations that any model gives to these images&#10;are consistent with human visual perception.&#10;&#10;From this perspective the models with large can be valuable even if&#10;their classification accuracy is slightly lower than that of models with&#10;small . They can be used in pair with another model that has a good&#10;classification accuracy, but that is vulnerable to adversarial/rubbish&#10;examples. The first model (with large ) detects a potential adversarial&#10;attack. In cases when it is confident, and its label disagrees with the&#10;other more accurate model, one can use the prediction of the more&#10;accurate model for the final classification decision. However if the&#10;large model is unconfident of a particular input, that input should be&#10;labeled as junk and not processed by the more accurate/vulnerable model.">
  </outline>
</outline>
<outline text="Discussion and conclusions" _note="Although modern machine learning techniques outperform humans on many&#10;classification tasks, there is a serious concern that they do not&#10;understand the structure of the training data. A clear demonstration of&#10;this lack of understanding was presented in , who showed two examples of&#10;nonsensical predictions of DNNs that contradict to human visual&#10;perception: adversarial images and rubbish images. In the present paper&#10;we propose that DAM with higher order interactions in the energy&#10;function produce more sensible interpretations (consistent with human&#10;vision) of adversarial and rubbish images. We argue that these models&#10;better mimic human visual perception than DNNs with ReLUs.&#10;&#10;A possible explanation of adversarial examples, pertaining to neural&#10;networks being too linear, was given in . Our explanation follows the&#10;same line of thought with some differences. One result of is that DAMs&#10;with large powers of the interaction vertex in the energy function are&#10;dual to feed-forward neural nets with highly non-linear activation&#10;functions - the rectified polynomials of higher degrees. From the&#10;perspective of this duality, one might expect that by simply replacing&#10;the ReLUs in DNNs by the higher rectified polynomials one might solve&#10;the problem of adversarial and rubbish images for a sufficiently large&#10;power of the activation function. We tried that and discovered that&#10;although DNNs with higher rectified polynomials alone perform better&#10;than DNNs with ReLUs from the adversarial perspective, they are worse&#10;than DAMs with the update rule (\[update\_rule\]). These observations&#10;need further comprehensive investigation. Thus, simply changing ReLUs to&#10;higher rectified polynomials is not enough to get rid of adversarial&#10;problems, and other aspects of the training algorithm presented in&#10;section \[DAM networks\] are important.&#10;&#10;For thinking about neurobiology the energy functions with higher order&#10;interactions considered in and in this paper should be thought of as&#10;effective theories that arise after excluding the auxiliary variables&#10;from the microscopic description. The microscopic realization in terms&#10;of biological neurons will be discussed elsewhere.&#10;&#10;There are two straightforward ideas for possible extensions of this&#10;work. First, it would be interesting to complement the proposed training&#10;procedure with adversarial training. In other words to train the network&#10;using the algorithm of section \[DAM networks\] but on a combination of&#10;clean images and adversarial images, along the lines of . We expect that&#10;this should further increase the robustness to adversarial examples and&#10;increase the classification accuracy on the clean images. Second, it&#10;would be interesting to investigate the proposed methods in the&#10;convolutional setting. Naively, one expects that the adversarial&#10;problems are more severe in the fully connected networks than in the&#10;convolutional networks. For this reason we used the fully connected&#10;networks for our experiments. We expect that the training algorithm of&#10;section \[DAM networks\] can be combined with convolutional layers to&#10;better describe images.&#10;&#10;Although more work is required to fully resolve the problem of&#10;adversarial and rubbish images, we believe that the present paper has&#10;identified a promising computational regime of the neural networks that&#10;significantly mitigates the vulnerability of DNNs to adversarial and&#10;rubbish images and that remains little investigated.&#10;&#10;[99]{} Szegedy, C., Zaremba, W., Sutskever, I., Bruna, J., Erhan, D.,&#10;Goodfellow, I. and Fergus, R., 2013. Intriguing properties of neural&#10;networks. arXiv preprint arXiv:1312.6199.&#10;&#10;Nguyen, A., Yosinski, J. and Clune, J., 2015, June. Deep neural networks&#10;are easily fooled: High confidence predictions for unrecognizable&#10;images. In 2015 IEEE Conference on Computer Vision and Pattern&#10;Recognition (CVPR) (pp. 427-436). IEEE.&#10;&#10;Papernot, N., McDaniel, P. and Goodfellow, I., 2016. Transferability in&#10;Machine Learning: from Phenomena to Black-Box Attacks using Adversarial&#10;Samples. arXiv preprint arXiv:1605.07277.&#10;&#10;Papernot, N., McDaniel, P., Goodfellow, I., Jha, S., Berkay Celik, Z.&#10;and Swami, A., 2016. Practical Black-Box Attacks against Deep Learning&#10;Systems using Adversarial Examples. arXiv preprint arXiv:1602.02697.&#10;&#10;Kurakin, A., Goodfellow, I. and Bengio, S., 2016. Adversarial examples&#10;in the physical world. arXiv preprint arXiv:1607.02533.&#10;&#10;Miyato, T., Maeda, S.I., Koyama, M., Nakae, K. and Ishii, S., 2015.&#10;Distributional smoothing with virtual adversarial training. stat, 1050,&#10;p.25.&#10;&#10;Gu, S. and Rigazio, L., 2014. Towards deep neural network architectures&#10;robust to adversarial examples. arXiv preprint arXiv:1412.5068.&#10;&#10;Huang, R., Xu, B., Schuurmans, D. and Szepesvári, C., 2015. Learning&#10;with a strong adversary. CoRR, abs/1511.03034.&#10;&#10;Nokland, A., 2015. Improving back-propagation by adding an adversarial&#10;gradient. arXiv preprint arXiv:1510.04189.&#10;&#10;Wang, Q., Guo, W., Zhang, K., Xing, X., Giles, C.L. and Liu, X., 2016.&#10;Random Feature Nullification for Adversary Resistant Deep Architecture.&#10;arXiv preprint arXiv:1610.01239.&#10;&#10;Krotov, D. and Hopfield, J.J., 2016. Dense Associative Memory for&#10;Pattern Recognition. Advances in Neural Information Processing Systems&#10;2016, in press, arXiv preprint arXiv:1606.01164.&#10;&#10;Simard, P.Y., Steinkraus, D. and Platt, J.C., 2003, August. Best&#10;practices for convolutional neural networks applied to visual document&#10;analysis. In ICDAR (Vol. 3, pp. 958-962).&#10;&#10;Goodfellow, I.J., Shlens, J. and Szegedy, C., 2014. Explaining and&#10;harnessing adversarial examples. arXiv preprint arXiv:1412.6572.">
</outline>
  </body>
</opml>