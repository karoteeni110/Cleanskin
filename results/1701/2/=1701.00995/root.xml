<?xml version="1.0" encoding="UTF-8"?>
<opml version="2.0">
  <head>
    <title>An Evaluation Framework and Database
for MoCap-Based Gait Recognition Methods[^1]

[^1]: This is a companion paper to our papers .</title>
    <abstract>As a contribution to reproducible research, this paper presents a
framework and a database to improve the development, evaluation and
comparison of methods for gait recognition from motion capture (MoCap)
data. The evaluation framework comprises source codes of
state-of-the-art human-interpretable geometric features as well as our
own approaches where gait features are learned by a modification of
Fisher’s Linear Discriminant Analysis with the Maximum Margin Criterion,
and by a combination of Principal Component Analysis and Linear
Discriminant Analysis. It includes a description and source codes of
a mechanism for evaluating class separability coefficients of feature
space and four classifier performance metrics. This framework also
contains a tool for learning a custom classifier and for classifying a
custom probe on a custom gallery. We provide an experimental database
along with source codes for its extraction from the general CMU MoCap
database. </abstract>
  </head>
  <body>
<outline text="Introduction" _note="Gait (walk) pattern has several attractive properties as a soft&#10;biometric trait. From a surveillance perspective, gait pattern&#10;biometrics is appealing in that it can be performed at a distance&#10;without requiring body-invasive equipment or subject cooperation.&#10;&#10;Many research groups investigate the discrimination power of gait&#10;pattern and develop models that are applied to the automatic recognition&#10;of walking people from MoCap data. A number of MoCap-based gait&#10;recognition methods have been introduced in the past few years and new&#10;ones continue to emerge. In order to move forward with this competitive&#10;research, it is necessary to compare their innovative approaches with&#10;the state-of-the-art and evaluate them against established evaluation&#10;metrics on a benchmark database. New frameworks and databases have been&#10;developed recently .&#10;&#10;As a contribution to reproducible research, this paper focuses on our&#10;framework for evaluating MoCap-based gait recognition methods and our&#10;benchmark MoCap gait database. We provide a large experimental database&#10;together with its extraction-and-normalization drive from the general&#10;CMU MoCap database, as specified in Section \[db\]. Implementation&#10;details of thirteen relevant methods are summarized in Section \[impl\].&#10;In Section \[eval\] we describe the evaluation mechanism and define four&#10;class separability coefficients and four rank-based classifier&#10;performance metrics. Finally, Section \[repro\] consists of a manual and&#10;comments on reproducing the experiments.">
</outline>
<outline text="Data" _note="MoCap technology provides video clips of individuals walking which&#10;contain structural motion data. The format keeps an overall structure of&#10;the human body and holds estimated 3D positions of major anatomical&#10;landmarks as the person moves. These MoCap data can be collected online&#10;by a system of multiple cameras (Vicon) or a depth camera (Microsoft&#10;Kinect). To visualize MoCap data (see Figure \[f1\]), a simplified stick&#10;figure representing the human skeleton (graph of joints connected by&#10;bones) can be recovered from body point spatial coordinates in time.&#10;Recent rapid improvement in MoCap sensor accuracy has brought affordable&#10;MoCap technology to assist human identification in such applications as&#10;access control and video surveillance.&#10;&#10;For evaluation purposes we have extracted a large number of gait samples&#10;from the MoCap database obtained from the CMU Graphics Lab , which is&#10;available under the Creative Commons license. It is a well-known and&#10;recognized database of structural human motion data and contains a&#10;considerable number of gait sequences. Motions are recorded with an&#10;optical marker-based Vicon system. People wear a black jumpsuit with&#10;41 markers taped on. The tracking space of is surrounded by 12 cameras&#10;with a sampling rate of at heights ranging from 2 to 4 meters above&#10;ground thereby creating a video surveillance environment. Motion videos&#10;are triangulated to get highly accurate 3D data in the form of relative&#10;body point coordinates (with respect to the root joint) in each video&#10;frame and are stored in the standard ASF/AMC data format. Each&#10;registered participant is assigned with their respective skeleton&#10;described in an ASF file. Motions in the AMC files store bone rotational&#10;data, which is interpreted as instructions about how the associated&#10;skeleton deforms over time.&#10;&#10;These MoCap data, however, contain skeleton parameters pre-calibrated by&#10;the CMU staff. Skeletons are unique to each walker and even a trivial&#10;skeleton check could result in  recognition. In order to fairly use the&#10;collected data, a prototypical skeleton is constructed and used to&#10;represent bodies of all subjects, shrouding the skeleton parameters.&#10;Assuming that all walking individuals are physically identical disables&#10;the skeleton check from being a potentially unfair classifier. Moreover,&#10;this is a skeleton-robust solution as all bone rotational data are&#10;linked to one specific skeleton. To obtain realistic parameters, it is&#10;calculated as mean of all skeletons in the provided ASF files.&#10;&#10;The raw data are in the form of bone rotations or, if combined with the&#10;prototypical skeleton, 3D joint coordinates. The bone rotational data&#10;are taken from the AMC files without any pre-processing. We calculate&#10;the joint coordinates using the bone rotational data and the&#10;prototypical skeleton. One cannot directly use raw values of joint&#10;coordinates, as they refer to absolute positions in the tracking space,&#10;and not all potential methods are invariant to person’s position or walk&#10;direction. To ensure such invariance, the center of the coordinate&#10;system is moved to the position of root joint for each time  and the&#10;axes are adjusted to the walker’s perspective: the X axis is from right&#10;(negative) to left (positive), the Y axis is from down (negative) to up&#10;(positive), and the Z axis is from back (negative) to front (positive).&#10;In the AMC file structure notation it is achieved by setting the root&#10;translation and rotation to zero (`root 0 0 0 0 0 0`) in all frames of&#10;all motion sequences.&#10;&#10;Since the general motion database contains all motion types, we&#10;extracted a number of sub-motions that represent gait cycles. First, an&#10;exemplary gait cycle was identified, and clean gait cycles were then&#10;filtered out using a threshold for their Dynamic Time Warping (DTW)&#10;distance on bone rotations in time. The distance threshold was&#10;explicitly set low enough so that even the least similar sub-motions&#10;still semantically represent gait cycles. Setting this threshold higher&#10;might also qualify sub-motions that do not resemble gait cycles anymore.&#10;Finally, subjects that contributed with less than 10 samples were&#10;excluded. The final database  has 54 walking subjects that performed&#10;3,843 samples in total, which results in an average of about 71 samples&#10;per subject.">
</outline>
<outline text="Implementation Details of Algorithms" _note="Recognizing a person from their gait involves capturing and normalizing&#10;their walk sample , extracting gait features to compose a template , and&#10;finally querying the gallery for a set of similar templates  – based on&#10;a distance function  – to report the most likely identity. This work&#10;focuses on extracting robust and discriminative gait features from raw&#10;MoCap data.&#10;&#10;Many geometric gait features have been introduced over the past few&#10;years. They are typically combinations of static body parameters (bone&#10;lengths, person’s height)  with dynamic gait features such as step&#10;length, walking speed, joint angles and inter-joint distances , along&#10;with various statistics (mean, standard deviation or maximum) of their&#10;signals . Clearly, these features are schematic and human-interpretable,&#10;which is convenient for visualizations and for intuitive understanding,&#10;but unnecessary for automatic gait recognition. Instead, our approach &#10;prefers learning features in a supervised manner that maximally separate&#10;the identity classes and are not limited by such dispensable factors.&#10;&#10;What follows is a detailed specification of the thirteen gait features&#10;extraction methods that we have reviewed in our work to date. Since the&#10;idea behind each method has some potential, we have implemented each of&#10;them for direct comparison.&#10;&#10;\#1&#10;&#10;**\#1**&#10;&#10;Ahmed by Ahmed [ET AL.]{}  extracts the mean, standard deviation and&#10;skew during one gait cycle of horizontal distances (projected on the&#10;Z axis) between feet, knees, wrists and shoulders, and mean and standard&#10;deviation during one gait cycle of vertical distances (Y coordinates) of&#10;head, wrists, shoulders, knees and feet, and finally the mean area&#10;during one gait cycle of the triangle of root and two feet. Ali by&#10;Ali [ET AL.]{}  measures the mean areas during one gait cycle of lower&#10;limb triangles. Andersson by Andersson [ET AL.]{}  calculates gait&#10;attributes as mean and standard deviation during one gait cycle of local&#10;extremes of the signals of lower body angles, step length as a maximum&#10;of feet distance, stride length as a length of two steps, cycle time and&#10;velocity as a ratio of stride length and cycle time. In addition, they&#10;extract the mean and standard deviation during one gait cycle of each&#10;bone length, and height as the sum of the bone lengths between head and&#10;root plus the averages of the bone lengths between root and both feet.&#10;Ball by Ball [ET AL.]{}  measures mean, standard deviation and maximum&#10;during one gait cycle of lower limb angle pairs: upper leg relative to&#10;the Y axis, lower leg relative to the upper leg, and the foot relative&#10;to the Z axis. Dikovski by Dikovski [ET AL.]{}  selects the mean during&#10;one gait cycle of step length, height, all bone lengths, then mean,&#10;standard deviation, minimum, maximum and mean difference of subsequent&#10;frames during one gait cycle of all major joint angles, and the angle&#10;between the lines of the shoulder joints and the hip joints. Gavrilova&#10;by Gavrilova [ET AL.]{}  chooses 20 joint relative distance signals and&#10;16 joint relative angle signals across the whole body, compared using&#10;the DTW. Jiang by Jiang [ET AL.]{}  measures angle signals between the&#10;Y axis and four major lower body (thigh and calf) bones. The signals are&#10;compared using the DTW. Krzeszowski by Krzeszowski [ET AL.]{}  observes&#10;the signals of rotations of eight major bones (humerus, ulna, thigh and&#10;calf) around all three axes, the person’s height and step length. These&#10;signals are compared using the DTW distance function. Kumar by Kumar [ET&#10;AL.]{}  extracts all joint trajectories around all three axes. Gait&#10;samples are compared by a distance function of their covariance&#10;matrices. Kwolek by Kwolek [ET AL.]{}  processes signals of bone angles&#10;around all axes, the person’s height and step length. The gait cycles&#10;are normalized to 30 frames. Preis by Preis [ET AL.]{}  takes height,&#10;length of legs, torso, both lower legs, both thighs, both upper arms,&#10;both forearms, step length and speed. Sedmidubsky by Sedmidubsky [ET&#10;AL.]{}  concludes that only the two shoulder-hand signals are&#10;discriminatory enough to be used for recognition. These temporal data&#10;are compared using the DTW distance function. Sinha by Sinha [ET AL.]{} &#10;combines all features of Ball and Preis with mean areas during one gait&#10;cycle of upper body and lower body, then mean, standard deviation and&#10;maximum distances during one gait cycle between the centroid of the&#10;upper body polygon and the centroids of four limb polygons.&#10;&#10;We are interested in finding an optimal feature space by maximizing its&#10;class separability, which is when gait templates are close to those of&#10;the same walker and far from those of other walkers. The method proposed&#10;in  learns gait features directly from joint coordinates by a&#10;modification of Fisher’s Linear Discriminant Analysis  with Maximum&#10;Margin Criterion. The framework allows learning from bone rotations as&#10;well.&#10;&#10;Let the model of a human body have  joints and all samples be linearly&#10;normalized to their average length . Labeled learning data in the sample&#10;(measurement) space are in the form where is a sample (gait cycle) in&#10;which are 3D spatial coordinates of a joint at time normalized with&#10;respect to the person’s position and direction. See that has&#10;dimensionality . Learning on bone rotations is analogical. Each learning&#10;sample falls strictly into one of the learning identity classes&#10;determined by . A class has samples. The classes are complete and&#10;mutually exclusive. We say that learning samples and share a common&#10;walker if and only if they belong to the same class, i.e., .&#10;&#10;Apart from Maximum Margin Criterion (MMC) we also investigated the&#10;fusion of Principal Component Analysis (PCA) with Linear Discriminant&#10;Analysis (LDA) that has been used for silhouette-based (2D) gait&#10;recognition by Su [ET AL.]{} . Feature extraction is given by a linear&#10;transformation (feature) matrix from a sample space of not necessarily&#10;labeled gait samples to a -dimensional feature space of gait templates&#10;where and gait samples are transformed into gait templates by .&#10;&#10;On given labeled learning data , Algorithm \[a1\] and Algorithm \[a2\]&#10;are efficient ways of learning the transforms for MMC and PCA+LDA,&#10;respectively. Both algorithms  are of quadratic complexity with respect&#10;to the number of learning identity classes due to the singular value&#10;decomposition and eigenvalue decomposition.&#10;&#10;split into classes of samples compute overall mean and individual class&#10;means compute compute compute compute eigenvectors and corresponding&#10;eigenvalues of through SVD of compute eigenvectors of through SVD of&#10;compute eigenvectors compute eigenvalues return transform as&#10;eigenvectors in that correspond to the eigenvalues of at least in&#10;&#10;split into classes of samples compute overall mean and individual class&#10;means compute compute compute eigenvectors of that correspond to largest&#10;eigenvalues compute eigenvectors of return transform&#10;&#10;In addition to the gait features extraction methods of our fellow&#10;researchers, we implemented our own methods as described below.&#10;Depending on whether the raw data are in the form of bone rotations or&#10;joint coordinates, the methods are referred to with BR or JC subscripts,&#10;respectively.&#10;&#10;\_MMC learns gait features by MMC (Algorithm \[a1\]) and the gait&#10;templates are compared by the Mahalanobis distance . \_PCALDA learns&#10;gait features by PCA+LDA (Algorithm \[a2\]) and the gait templates are&#10;also compared by the Mahalanobis distance . \_Random has no features and&#10;classification is performed by picking a random identity that is present&#10;in the gallery. **\_Raw** takes all raw data. The template vector,&#10;normalized to the average of frames, results in a large feature space&#10;dimensionality , which is why the raw data cannot be directly used for&#10;recognition on large databases.">
</outline>
<outline text="Evaluation" _note="Learning data of identities and evaluation data of identity classes have&#10;to be disjunct at all times. In the following, we introduce two setups&#10;of data separation: homogeneous and heterogeneous. The homogeneous setup&#10;learns the transformation matrix on samples of identities and is&#10;evaluated on templates derived from the other samples of the same&#10;identities. The heterogeneous setup learns the transform on all samples&#10;in identities and is evaluated on all templates derived from other&#10;identities. An abstraction of this concept is depicted in Figure \[f2\].&#10;Note that unlike in the homogeneous setup, no walker identity is ever&#10;used for both learning and evaluation at the same time in the&#10;heterogeneous setup.&#10;&#10;The homogeneous setup is parametrized by a single number of&#10;learning-and-evaluation identity classes, whereas the heterogeneous&#10;setup has the form specifying how many learning and how many evaluation&#10;identity classes are randomly selected from the database. The evaluation&#10;of each setup is repeated 3 times, selecting new random and identity&#10;classes each time and reporting the average result.&#10;&#10;In the homogeneous setup, all results are estimated with nested&#10;cross-validation (see Figure \[f7\]) that involves the outer 3-fold&#10;cross-validation loop where templates in one fold are used for learning&#10;the features, while templates in the remaining two folds are used for&#10;evaluations. In the heterogeneous setup, the learning and evaluation&#10;parts are selected at random based on the given and , respectively. For&#10;both setups, this model is frozen and ready to be evaluated for class&#10;separability coefficients. Evaluation of rank-based classifier&#10;performance metrics advances to the inner 10-fold cross-validation loop&#10;taking one dis-labeled fold as a testing set and the other nine labeled&#10;folds as gallery. Test templates are classified by the winner-takes-all&#10;strategy, in which a test template gets assigned with the label of the&#10;gallery’s closest identity class.&#10;&#10;Correct Classification Rate (CCR) is often perceived as the ultimate&#10;qualitative measure, however, if a method has a low CCR, we cannot&#10;directly say if the system is failing because of bad features or a bad&#10;classifier. It is more explanatory to provide an evaluation in terms of&#10;class separability of the feature space. The class separability measures&#10;give an estimate on the recognition potential of the extracted features&#10;and do not reflect an eventual combination with an unsuitable&#10;classifier:&#10;&#10;\#1 (\#2)&#10;&#10;Davies-Bouldin Index (DBI) where is the average distance of all elements&#10;in identity class to its centroid, and analogically for . Templates of&#10;low intra-class distances and of high inter-class distances have a low&#10;DBI. Dunn Index (DI) with from the above DBI. Since this criterion seeks&#10;classes with high intra-class similarity and low inter-class similarity,&#10;a high DI is more desirable. Silhouette Coefficient (SC) where is the&#10;average distance from to other samples within the same identity class&#10;and is the average distance of to the samples in the closest class. It&#10;is clear that and SC close to one means that classes are appropriately&#10;separated. Fisher’s Discriminant Ratio (FDR) High FDR is preferred for&#10;classes of low intra-class sparsity and high inter-class sparsity.&#10;&#10;Apart from analyzing the distribution of templates in the feature space,&#10;it is schematic to combine the features with a rank-based classifier and&#10;to evaluate the system based on distance distribution with respect to a&#10;probe. For obtaining a more applied performance evaluation, we evaluate:&#10;&#10;\#1 (\#2)&#10;&#10; &#10;&#10;Cumulative Match Characteristic (CMC) Sequence of Rank- (for on X axis&#10;from 1 up to ) recognition rates (Y axis) for measuring ranking&#10;capabilities of a recognition method. Its headline Rank-1 is the&#10;well-known **CCR**. False Accept Rate vs. False Reject Rate (FAR/FRR)&#10;Two sequences of the error rates (Y axis) as functions of discrimination&#10;threshold (X axis). Each method has a value  of this threshold giving&#10;Equal Error Rate (**EER**=FAR=FRR). Receiver Operating Characteristic&#10;(ROC) Sequence of True Accept Rate (**TAR**) and False Accept Rate&#10;(**FAR**) with a varied discrimination threshold. For a given threshold&#10;the system signals both TAR (Y axis) and FAR (X axis). Area Under Curve&#10;(**AUC**) is computed as the integral of the ROC curve. Recall vs.&#10;Precision (RCL/PCN) Sequence of rates with a varied discrimination&#10;threshold. For a given threshold the system signalizes both RCL (X axis)&#10;and PCN (Y axis). The value of Mean Average Precision (**MAP**) is&#10;computed as the area under RCL/PCN curve.&#10;&#10;These measures reflect how well the feature is class-separated and how&#10;much it takes to confuse the identities of two people. They do not, in&#10;fact, provide complementary information, however, a quality evaluation&#10;framework should be able to evaluate the most popular measures. Each&#10;measure is evaluated in the context of a particular application. For&#10;example, a hotel lobby authentication system could use a high Rank-3 at&#10;the CMC, while a city-level person tracking system is likely to need the&#10;ROC curve leaning towards the upper left corner.">
</outline>
<outline text="Reproducing the Experiments" _note="This section provides a description of the framework we implemented and&#10;the database we extracted. With this manual, a reader should be able to&#10;reproduce the evaluation and to use the implementation for recognizing&#10;people. All source codes including (1) database extraction drive,&#10;(2) implementations of the proposed and all relevant methods,&#10;(3) classifier learning and classification mechanisms and (4) evaluation&#10;mechanism and metrics, are available at our departmental Git&#10;repository . The original CMU MoCap database and extracted databases are&#10;available online at our research group web page .&#10;&#10;`Executor.java` is the main execution class. Set all parameters of&#10;evaluation and file locations and `distanceThreshold`, then select which&#10;actions to perform and finally, select the evaluation setups. The class&#10;contains the `main(String[] args)` method. It contains four methods to&#10;select for execution:&#10;&#10;`extractDatabase()`&#10;&#10;:   for extracting an experimental database from the original CMU MoCap&#10;    database –- this is also available for download at our page as&#10;    `original.zip`. To run this method, unzip to get the files&#10;    `gaitcycle.amc` (exemplary gait cycle) and `skeleton.asf`&#10;    (prototypical skeleton) and the directory `amcOriginal` (original&#10;    AMC files). Extraction begins with normalization with respect to a&#10;    person’s position and walk direction as provided in the&#10;    `normalized.zip` file. Clean gait cycles are then filtered out by&#10;    the distance threshold (see last paragraph of Section \[db\]) that&#10;    numerically expresses how much extracted motions resemble gait&#10;    cycles, that is, the lower the distance threshold, the fewer and&#10;    cleaner the gait cycles. Set a value for `distanceThreshold` to&#10;    produce a folder of the extracted database. Evaluations in  are set&#10;    with 302.0, extracting a database of 54 identities and&#10;    3[,]{}843 gait cycles. A higher distance threshold will qualify some&#10;    non-gait motions.&#10;&#10;`learnClassifiers()`&#10;&#10;:   for learning classifiers of all implemented methods on a&#10;    sub-database determined by the distance threshold. Set a value for&#10;    `distanceThreshold` (such as 302.0) and provide the corresponding&#10;    directory of the learning database (such as `amc302.0` in&#10;    `extracted-302.0.zip`) and the learned classifiers appear in the&#10;    `classifiers` folder.&#10;&#10;`performClassification()`&#10;&#10;:   for performing a classification of a custom probe/query on a custom&#10;    gallery with a custom classifier. Set file locations for the&#10;    classifier file `customClassifier`, the probe gait cycle&#10;    `customQueryFileAMC` and the gallery directory&#10;    `customGalleryDirectory`. Results are printed on the standard&#10;    output.&#10;&#10;`evaluateMethods()`&#10;&#10;:   for evaluating the implemented methods in homogeneous and&#10;    heterogeneous setups. To skip database extraction, one could supply&#10;    a provided extracted database (such as `amc302.0` in&#10;    `extracted-302.0.zip`) and a skeleton file (such as `skeleton.asf`&#10;    in any extracted database ZIP file). Our page provides additional&#10;    databases categorised according to various values of&#10;    `distanceThreshold`. The results are set to be printed on the&#10;    standard output but we suggest to redirect it to a CSV file. Results&#10;    of individual evaluation attempts vary slightly as different&#10;    learning, testing and gallery sets are randomly selected upon each&#10;    attempt.&#10;&#10;Compile to obtain `Gait.jar`. The main project location should also&#10;contain the `lib` directory and all necessary files and directories&#10;depending on which actions are to be executed. Run command&#10;`$ java -jar Gait.jar &gt; output.csv`.&#10;&#10;The output file (see structure in Table \[t1\]) contains the performance&#10;metrics as specified in Section \[eval\] and the information about&#10;average distance computation time (**DCT**) in milliseconds and average&#10;template dimensionality (**TD**). The evaluation results are in the form&#10;of one value per coefficient (see results in Table \[t2\]) and the&#10;sequences CMC, FAR/FRR, ROC=TAR/FAR and RCL/PCN. The CMC sequence&#10;contains values, one for each in the Rank- recognition rate. The other&#10;three pairs of sequences are normalized to 30 values by&#10;`method.setFineness(30)`. The FAR/FRR sequences of all methods are&#10;normalized to the discrimination threshold with respect to the first&#10;value of FAR=0 and FRR=1, and to the middle value that represents EER&#10;where all sequences cross. The ROC sequences are normalized with respect&#10;to the first value of TAR=FAR=0 and to the last value of TAR=FAR=1.&#10;Finally, the RCL/PCN sequences are normalized with respect to the first&#10;value of RCL=0 and to the last value of RCL=1.&#10;&#10;3.3pt&#10;&#10;To reproduce the experiments in Table \[t2\], follow the instructions in&#10;the README file at  in the `reproduce` folder. Please note that some&#10;methods are slow even on a leading edge hardware. Learning and&#10;evaluation times in Table \[t3\] were measured on a computer with&#10;Intel Xeon CPU E5-2650 v2 @ 2.60GHz and  RAM.&#10;&#10;4.2pt&#10;&#10;The goal of the MMC-based learning is to find a linear discriminant that&#10;maximizes the misclassification margin. This optimization technique&#10;appears to be more effective than designing geometric gait features.&#10;Table \[t2\] indicates the best results for the MMC on bone rotational&#10;data: highest SC, EER and AUC, and competitive DBI, DI, FDR, CCR and&#10;MAP. In terms of the Correct Classification Rate metric, our MMC method&#10;was only outperformed by the Raw method, which is implemented here as a&#10;form of baseline. We interpret the high scores as a sign of robustness.&#10;&#10;Apart from the performance merits, the MMC method is also efficient:&#10;relatively low-dimensional templates and Mahalanobis distance ensure&#10;fast distance computations and thus contribute to high scalability. Note&#10;that even if the Raw method has some of the best results, it can hardly&#10;be used in practice due to its extreme consumption of time and space&#10;resources. On the other hand, Random has no features but cannot be&#10;considered a serious recognition method. To illustrate the evaluation&#10;time, calculating the distance matrix (a matrix of distances between all&#10;evaluation templates) took a couple minutes for the MMC method, almost&#10;nothing for the Random method, and more than two weeks for the Raw&#10;method. To conclude, the MMC method on bone rotational data appears to&#10;be an optimal trade-off between effectiveness and efficiency, and thus&#10;the new state-of-the-art in feature extraction for MoCap-based gait&#10;recognition.">
</outline>
<outline text="Summary and Future Work" _note="As our contribution to reproducible research, we have provided&#10;implementation details and source codes  of our evaluation framework for&#10;gait recognition . The software implements the proposed method as well&#10;as all related methods. We include the evaluation database  together&#10;with source codes for its extraction from the general CMU MoCap&#10;database. We also attach the description and portable software for&#10;evaluating class separability coefficients of extracted features and&#10;classifier performance metrics. Finally, we provide documentation and&#10;installation instructions for easy and straightforward reproducibility&#10;of the experiments.&#10;&#10;As demonstrated by outperforming other methods in four class&#10;separability coefficients and four classification metrics, the proposed&#10;features learning mechanism has a strong potential in gait recognition&#10;applications. Even though we believe that MMC is the most suitable&#10;criterion for optimizing gait features, we continue to research further&#10;potential optimality criteria and machine learning approaches.&#10;&#10;We hope that the evaluation framework and database presented here will&#10;contribute to smooth development and evaluation of further novel&#10;MoCap-based gait recognition methods. All used data and source codes&#10;have been made available  under the Creative Commons Attribution license&#10;(CC-BY) for database and the Apache 2.0 license for software, which&#10;grant free use and allow for experimental evaluation. We encourage all&#10;readers and developers of MoCap-based gait recognition methods to&#10;contribute to the framework with new algorithms, data and improvements.">
  <outline text="Acknowledgments" _note="The authors thank to the anonymous reviewers and editor for their&#10;detailed commentary and suggestions. The data used in this project was&#10;created with funding from NSF EIA-0196217 and was obtained from&#10;&lt;http://mocap.cs.cmu.edu&gt; .">
  </outline>
</outline>
  </body>
</opml>