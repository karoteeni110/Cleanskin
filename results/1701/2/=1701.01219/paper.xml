<?xml version="1.0" encoding="UTF-8"?>
<opml version="2.0">
  <head>
    <title></title>
    <abstract> </abstract>
  </head>
  <body>
<outline text="Abstract" _note="It has been demonstrated that the statistical power of many neuroscience&#10;studies is very low, so that the results are unlikely to be robustly&#10;reproducible. How are neuroscientists and the journals in which they&#10;publish responding to this problem? Here I review the sample size&#10;justifications provided for all 15 papers published in one recent issue&#10;of the leading journal [NATURE NEUROSCIENCE]{}. Of these, only one&#10;claimed it was adequately powered. The others mostly appealed to the&#10;sample sizes used in earlier studies, despite a lack of evidence that&#10;these earlier studies were adequately powered. Thus, concerns regarding&#10;statistical power in neuroscience have mostly not yet been addressed.">
</outline>
<outline text="Introduction" _note="It is well-documented that the biomedical sciences are beset by bad&#10;statistical practices, and that this is one of the reasons for the&#10;current ‘reproducibility crisis’ . Prominent amongst these problems are&#10;n values that provide only low statistical power. Genuine effects that&#10;do actually exist are missed, and many effects that are found to be&#10;significant are likely to be just random chance. Neuroscience is no&#10;exception to this rule . Indeed, due to the particular challenges of&#10;this field, studies which can be completed within traditional parameters&#10;of time, cost and ethical approval are often restricted to low n values.&#10;Furthermore this data is then analysed post-hoc from many different&#10;perspectives in the hope of finding significant results, further&#10;increasing the probability of false positives.&#10;&#10;The purpose of the present article is not to review again these&#10;problems, which are well documented. Rather, I consider how the&#10;community of authors, reviewers and journal editors in neuroscience is&#10;responding to this clearly visible challenge. The leading neuroscience&#10;journal [NATURE NEUROSCIENCE]{} provides a good opportunity to do this,&#10;since (unlike most neuroscience journals) it requires authors to provide&#10;answers to some basic statistical questions about the design of their&#10;experiments. It is therefore of interest to see what answers have been&#10;forthcoming in recently published papers. Presumably, because the papers&#10;were published, the authors, reviewers and editors all thought these&#10;answers were acceptable.&#10;&#10;Here I reproduce the statements regarding sample size from all 15 papers&#10;published in the August 2016 issue, and find that all of them except one&#10;essentially confess they are probably statistically underpowered. I do&#10;not explicitly identify which papers these came from, because my goal is&#10;not to cast doubt on any specific work: this is simply a (somewhat)&#10;random subset to illustrate a very broad issue. Furthermore, there is no&#10;reason to think these problems are any different in other journals&#10;(though a recent study has argued that statistical power is&#10;[NEGATIVELY]{} correlated with journal impact factor ). What makes&#10;[NATURE NEUROSCIENCE]{} attractive for analysis in this regard, besides&#10;its current ranking as the highest-impact primary research journal in&#10;the field, is that it takes the trouble to require authors to make&#10;explicit comments about certain statistical matters.">
</outline>
<outline text="Statements of powerlessness" _note="These are ordered thematically, and do not reflect the ordering within&#10;the issue.&#10;&#10;1\. [THE SAMPLE SIZE FOR EACH EXPERIMENT WAS DETERMINED BASED ON&#10;PUBLISHED STUDIES USING SIMILAR EXPERIMENTAL DESIGNS TOGETHER WITH PILOT&#10;EXPERIMENTS FROM OUR LABORATORY. THIS ALLOWED US TO DETERMINE THE SAMPLE&#10;SIZE REQUIRED FOR EACH EXPERIMENT TO ENSURE A STATISTICAL POWER OF 0.8&#10;AND AN ALPHA LEVEL OF 0.05.]{}&#10;&#10;Here the authors clearly address the issue of statistical power.&#10;Although potentially one might want to see the evidence for the claim,&#10;this statement provides reassurance that these results are likely to be&#10;reproducible.&#10;&#10;2\. [SAMPLE SIZE WAS PREDETERMINED ON THE BASIS OF PUBLISHED STUDIES,&#10;EXPERIMENTAL PILOTS AND IN-HOUSE EXPERTISE.]{}&#10;&#10;It is encouraging that pilot studies were undertaken, but it is unclear&#10;how these pilots or the in-house expertise were used to determine&#10;statistical power.&#10;&#10;I now group several statements together, since they are all very&#10;similar.&#10;&#10;3\. [SAMPLE SIZES FOR EACH CONDITION OF THIS STUDY ARE SIMILAR TO THOSE&#10;GENERALLY EMPLOYED IN THE FIELD…AND WERE NOT PREDETERMINED BY A SAMPLE&#10;SIZE CALCULATION.]{}&#10;&#10;4\. [NO STATISTICAL METHODS WERE USED TO PREDETERMINE SAMPLE SIZES, BUT&#10;OUR SAMPLE SIZES ARE SIMILAR TO THOSE GENERALLY EMPLOYED IN THE&#10;FIELD.]{}&#10;&#10;5\. [SAMPLE SIZE CHOICE WAS BASED ON PREVIOUS STUDIES, NOT PREDETERMINED&#10;BY A STATISTICAL METHOD.]{}&#10;&#10;6\. [NO STATISTICAL TESTS WERE USED TO PREDETERMINE SAMPLE SIZES, BUT OUR&#10;SAMPLE SIZES ARE SIMILAR TO THOSE IN PREVIOUS STUDIES]{}&#10;&#10;7\. [NO STATISTICAL METHODS WERE USED TO PREDETERMINE SAMPLE SIZES, BUT&#10;OUR SAMPLE SIZES ARE SIMILAR TO THOSE GENERALLY EMPLOYED IN THE&#10;FIELD.]{}&#10;&#10;8\. [GROUP SAMPLE SIZES WERE CHOSEN ON THE BASIS OF PREVIOUS STUDIES.]{}&#10;&#10;9\. [NO STATISTICAL METHODS WERE USED TO PREDETERMINE SAMPLE SIZES, BUT&#10;OUR SAMPLE SIZES ARE SIMILAR TO THOSE PREVIOUSLY REPORTED.]{}&#10;&#10;10\. [NO STATISTICAL METHODS WERE USED TO PRE-DETERMINE SAMPLE SIZES BUT&#10;OUR SAMPLE SIZES ARE LARGER TO THOSE REPORTED IN PREVIOUS&#10;PUBLICATIONS.]{}&#10;&#10;The obvious problem with all these statements is that they do not&#10;address whether any of these previous studies demonstrated they were&#10;adequately powered (that previous work produced significant results says&#10;nothing about statistical power, a basic point that appears not to be&#10;widely appreciated). In addition, unless exactly the same experiments&#10;were performed, the variability and effects sizes are likely to be&#10;different, meaning that the sample size required to achieve adequate&#10;power will also be different.&#10;&#10;11\. [NO STATISTICAL METHODS WERE USED TO PREDETERMINE SAMPLE SIZES, BUT&#10;THE TISSUES WERE RANDOMLY CHOSEN IN EACH AGE GROUP…AND UNIFORMLY&#10;PROCESSED. ALSO, OUR SAMPLES SIZES ARE SIMILAR TO THOSE OF THE DISCOVERY&#10;SET OF A SIMILAR EXPERIMENTAL DESIGN IN A PREVIOUS PUBLICATION]{}&#10;&#10;Besides the problems mentioned above, this statement conflates&#10;statistical power with other issues of experimental design.&#10;&#10;12\. [NO STATISTICAL METHODS WERE USED TO PREDETERMINE SAMPLE SIZES.&#10;SAMPLE SIZE WAS DECIDED ON THE BASIS OF OUR PREVIOUS EXPERIENCE IN THE&#10;FIELD AND WAS NOT PRE-DETERMINED BY A SAMPLE SIZE CALCULATION. THE&#10;SAMPLE SIZE ARE SIMILAR TO THOSE GENERALLY EMPLOYED IN THE FIELD AND IS&#10;JUSTIFIED BY THE HIGH RATE OF EXCLUSION DUE TO THE DIFFICULTY OF THE&#10;COMBINED METHODOLOGICAL APPROACHES]{}&#10;&#10;Here the authors appeal to practical limitations on sample sizes. These&#10;limitations are real and worthy of acknowledgement. However this does&#10;not provide information pertinent to the statistical power, and thus&#10;reproducibility, of the results.&#10;&#10;13\. [NO ESTIMATES OF STATISTICAL POWER WERE PERFORMED BEFORE&#10;EXPERIMENTS; ANIMAL NUMBERS WERE MINIMIZED TO CONFORM TO ETHICAL&#10;GUIDELINES WHILE ACCURATELY MEASURING PARAMETERS OF ANIMAL&#10;PHYSIOLOGY.]{}&#10;&#10;Here the authors appeal to ethical limitations on sample sizes. Again,&#10;while real and worthy of acknowledgement, the same arguments apply as&#10;mentioned above.&#10;&#10;Finally, we come to perhaps the two most worrying statements.&#10;&#10;14\. [NO STATISTICAL TESTS WERE USED TO PREDETERMINE SAMPLE SIZES, BUT&#10;OUR SAMPLE SIZES ARE SIMILAR TO THOSE GENERALLY EMPLOYED IN THE FIELD.&#10;NORMAL DISTRIBUTION OF DATA WAS ASSUMED, BUT NOT FORMALLY TESTED.]{}&#10;&#10;15\. [NORMALITY OF THE DATA DISTRIBUTIONS WAS ASSUMED, BUT NOT FORMALLY&#10;TESTED.]{}&#10;&#10;The last makes no statement about sample sizes at all, despite this&#10;supposedly being a requirement of the journal. More importantly, both&#10;statements explicitly state that the authors do not know whether the&#10;statistical tests they applied were actually appropriate.&#10;&#10;For comparison the statements in the July 2016 issue were very similar.&#10;One article justified sample sizes in terms of a power calculation,&#10;while the remainder bar one (which simply stated that ‘no statistical&#10;methods were used to predetermine our sample sizes’) appealed to&#10;similarity with sample sizes used in previous studies, in one case in a&#10;different species.">
</outline>
<outline text="Discussion" _note="All of the statements reviewed above were approved by the authors,&#10;reviewers, and journal editors, and one must therefore conclude that&#10;they reflect currently accepted practice in the field. It is widely&#10;known and understood that statistical power is a key issue affecting&#10;reproducibility, yet 14/15 of these statements (93%) do not address&#10;statistical power. Most of them appeal to precedent for sample sizes,&#10;despite the facts that most neuroscience studies are underpowered , and&#10;that new experiments will most likely have different variances and&#10;effect sizes from previous work. It is clearly a step forward that&#10;[NATURE NEUROSCIENCE]{} requires authors to explicitly answer some key&#10;questions regarding statistical analysis. However, that the journal is&#10;willing to accept answers which are clearly inadequate, and even&#10;sometimes admissions that the statistical analysis performed was quite&#10;possibly wrong, suggests that the journal is still contributing to the&#10;problem rather than the solution. For comparison the relatively new&#10;journal [ENEURO]{} requires authors to provide the statistical power of&#10;each experiment reported. However this is merely the observed power,&#10;which provides little or no additional information beyond the observed p&#10;value , and thus this policy does not help matters much either.&#10;&#10;I am not attempting to single out these 14 papers as being of any more&#10;concern than any other work in the field, rather they simply provide a&#10;revealing window on community standards at the highest level.&#10;Neuroscientists seem willing to accept that work in the field generally&#10;uses low n values. Sometimes this might be reasonable: for instance the&#10;effect size of the difference in phenotype between a wild-type and&#10;knockout may be very large (though even in cases such as these power&#10;calculations are rarely provided). Certainly many important findings&#10;have been robustly reproduced, even though the statistical power of the&#10;original (or indeed subsequent) results was not established (e.g. ).&#10;However, in many experiments the effects are subtle, and low n values&#10;and thus power mean that, on average, reproducibility will be low.&#10;&#10;This is a very difficult problem (which I hasten to add I am also&#10;struggling with in my own research). Neuroscience experiments are often&#10;intrinsically long-term and low-throughput. For instance uncovering the&#10;function of a disease-related gene in a mouse model, or studying the&#10;neural correlates of consciousness in an awake behaving primate model,&#10;can require large resources and many years of work to obtain a single&#10;main result. Many of the latest techniques are extremely technically&#10;challenging, and therefore (as alluded to in one of the statements&#10;above) a large proportion of experiments fail. This can lead to a big&#10;mismatch between the n values at the start and end of the experiment.&#10;Funding is tight, and increasing n by even one animal for a particular&#10;experiment can have costly implications in time and money. Increasing&#10;ethical pressures on the use of animals in research (as alluded to&#10;above) add additional constraints on the n values that are practically&#10;achievable. However there is clearly also a cultural component to sample&#10;sizes in neuroscience: for instance work in organisms such as [C.&#10;ELEGANS]{} and [DROSOPHILA]{}, to which some of the above constraints&#10;are less applicable, also do not usually consider statistical power.&#10;&#10;What can be done? Clearly better education for neuroscientists (at all&#10;levels) regarding statistical issues is important to address the lack of&#10;statistical scepticism that apparently plagues the field, and books such&#10;as should be more widely read and understood (for instance many&#10;neuroscientists still appear to think that obtaining means it is 95%&#10;likely they have discovered something true, no matter how small the n&#10;value ). From the perspective of a field such as statistical genetics,&#10;one solution seems obvious: neuroscientists should collaborate in larger&#10;teams and share data, so that many small and weakly powered results can&#10;be replaced with a few strongly powered results. However, while data&#10;sharing in general should and is being broadly encouraged, there are&#10;problems with this general model for the community of neuroscientists at&#10;large (for an interesting discussion see ). How would everyone agree&#10;what were the right experiments to do? How would cutting-edge and often&#10;highly non-trivial methodologies be standardized between labs? How would&#10;this model not be detrimental to the entreprenurial spirit that has&#10;fuelled so many important discoveries in neuroscience? The risk is that&#10;progress in neuroscience, where publication rates are already low&#10;compared to some other fields of biomedical science (for the reasons&#10;mentioned above), could be reduced to an unviable level.&#10;&#10;Another approach is to replace the ubiquitous current statistical&#10;paradigm, of null hypothesis significance testing, with estimation .&#10;This can be done by confidence intervals and/or Bayesian approaches.&#10;Now, instead of there being a ‘bright line of truth’ at an arbitrarily&#10;chosen probability threshold relating to the null hypothesis (which,&#10;after all, is not what one is actually interested in), the focus is on&#10;determining degrees of confidence in quantities such as the effect size&#10;and the difference between means. Binary statements about ‘significance’&#10;versus ‘nonsignificance’ are replaced with graded confidence variations&#10;which are easier to interpret intuitively. However, this has yet to&#10;catch on in neuroscience.&#10;&#10;Staying within the confines of null hypothesis significance testing, I&#10;have argued that neuroscientists seem willing so far to accept the&#10;current situation regarding (lack of) statistical power. Perhaps that is&#10;indeed the best that can presently be achieved, given the current&#10;statistical paradigm and practical constraints in the field. Perhaps we&#10;should just accept that most studies will be likely underpowered and&#10;reproducibility will likely be a recurring issue, at least until some&#10;more mature stage of development is reached. However if this is the&#10;case, it would be helpful if authors, reviewers and journal editors more&#10;clearly acknowledged that underpowered studies lead to weak and&#10;potentially irreproducible results.&#10;&#10;[199]{}&#10;&#10;Ioannidis JP (2005). Why most published research findings are false.&#10;[PLOS MED]{}, [**8**]{}:e124.&#10;&#10;Colquhoun D (2014). An investigation of the false discovery rate and the&#10;misinterpretation of p-values. [R SOC OPEN SCI]{}, [**1**]{}:140216.&#10;&#10;Button KS, Ioannidis JP, Mokrysz C, Nosek BA, Flint J, Robinson ES,&#10;Munafo MR (2013). Power failure: why small sample size undermines the&#10;reliability of neuroscience. [NAT REV NEUROSCI.]{}, [**14**]{}, 365-376.&#10;&#10;Szucs D, Ioannidis JPA (2016). Empirical assessment of published effect&#10;sizes and power in the recent cognitive neuroscience and psychology&#10;literature. http://biorxiv.org/content/early/2016/08/25/071530&#10;&#10;Lenth, RV (2001). Some practical guidelines for effective sample size&#10;determination. [AM STAT]{}, [**55**]{}, 187-193.&#10;&#10;Bliss TVP &amp; Lomo T (1973). Long-lasting potentiation of synaptic&#10;transmission in the dentate area of the anaesthetized rabbit following&#10;stimulation of the perforant path. [J. PHYSIOL.]{}, [**232**]{},&#10;331-356.&#10;&#10;Reinhardt, A. (2015). Statistics Done Wrong: The Woefully Complete&#10;Guide. No Starch Press.&#10;&#10;Halsey LG, Curran-Everett D, Vowler SL, Drummond GB (2015). The fickle P&#10;value generates irreproducible results. [NAT METHODS]{}, [**12**]{},&#10;179-85.&#10;&#10;Mainen ZF, Häusser M &amp; Pouget A (2016). A better way to crack the brain.&#10;[NATURE]{}, [**539**]{}, 159-161.&#10;&#10;Cumming, G. &amp; Calin-Jageman, R. (2017). Introduction to the new&#10;statistics: estimation, open science, and beyond. Routledge.">
</outline>
  </body>
</opml>