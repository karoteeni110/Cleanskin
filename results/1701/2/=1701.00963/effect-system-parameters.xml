<?xml version="1.0" encoding="UTF-8"?>
<opml version="2.0">
  <head>
    <title></title>
    <abstract> </abstract>
  </head>
  <body>
<outline text="Setting the RADIUS Parameters" _note=" \[fig:Nsigma\]&#10;&#10; \[fig:ErrorMean\]&#10;&#10;In the previous section, we presented the details of the Bayesian&#10;thresholding and the supporting techniques in RADIUS. We now study the&#10;impact of the aforementioned parameters required in each individual&#10;technique on the system performance. Specifically, we elaborate the&#10;effect of the Bayes threshold parameter and then explore the parameter&#10;space of all the parameters involved in supporting techniques. Based on&#10;detailed analysis, we give insights on the best parameter setting for an&#10;indoor office environment.&#10;&#10;For this, we performed extensive experiments, collecting real data&#10;traces from an indoor testbed, whose details are reported in Section&#10;\[sec:evaluation\]. To capture different link conditions, we selected 8&#10;sender-receiver pairs (either line-of-sight or non-line-of-sight) at&#10;different locations with various environment dynamics (e.g., human&#10;movements and obstacles). In each experiment, we simulated a good link&#10;turning into a weak link by decreasing the transmission power of the&#10;sender node from the maximum level gradually to the minimum with a&#10;packet sending rate of 5 Hz. The receiver node records the RSSI and PDR&#10;traces for more than 15 minutes. We repeated the experiment 10 times for&#10;each link. The minimum PDR that decides whether a link is a good link or&#10;a weak link is set to 80% throughout the whole analysis.">
  <outline text="Bayesian Thresholding" _note="According to Equation \[equ:rssiTHD\], calculating the Bayes threshold&#10;requires a user-defined parameter: A PRIORI probability . Different from&#10;the general analysis depicted in Section \[sec:motivationBayes\], we&#10;present here the detailed analysis of the impacts of on the false&#10;positive rate (FPR), false negative rate (FNR) and the total error rate.&#10;&#10;Figure \[fig:EVA-ProbGood\] shows the change of the error rates with&#10;varying ranging in \[, \] for 5 representative links out of the 8&#10;analyzed links. We observe that the FPR always decreases with , while&#10;the FNR increases with . This is because a larger indicates a higher&#10;weight on the FPR in the computation of the Bayes error (see Equation&#10;\[equ:error\]). Hence, reducing FPR is more effective than reducing FNR&#10;to keep the Bayes error rate low for a larger .&#10;&#10;Moreover, we can see in Figure \[fig:EVA-ProbGood\] that the overall&#10;error rate mostly stays low regardless of the values of , except for the&#10;cases when the value of is extremely close to 0 or 1. The results&#10;confirm that the system performance with the Bayes threshold is&#10;insensitive to the setting of as long as extreme values are not&#10;considered. The reason for this is that the Bayesian thresholding&#10;approach always tries to balance between FPR and FNR for any setting.&#10;&#10;From the figure, we can further see that a global setting from a wide&#10;range (any value not close to 0 or 1) may not be the best setting for&#10;each individual link. However, it can provide for all different links&#10;near optimal detection accuracy at the same time. In other words, with a&#10;coarse global setting of for all DAs, the Bayesian thresholding ensures&#10;RADIUS to deliver near optimal accuracy for different links under&#10;diverse link conditions without the need of tuning for each of them. For&#10;our case, we select the initial setting of at 0.8 because we assume that&#10;the probability of the links being good is generally higher than that of&#10;being weak, in our deployment environment. Additionally, to avoid the&#10;significant increase of FNR caused by the over-adjustment due to the A&#10;PRIORI probability refinement, we limit the MAXIMUM to 0.99 for our&#10;deployment environment.">
  </outline>
  <outline text="Estimating the Minimal Training Set Size" _note="As discussed in Section \[sec:minTrainingSet\], the first task of a DA&#10;before generating a Bayes threshold is to estimate the minimal training&#10;set size for each link. A proper needs to achieve a good tradeoff&#10;between detection accuracy and training latency. To apply Equation&#10;\[equ:minSampleSize\], the computation of requires two parameters: (1)&#10;the number of first samples of RSSI for computing the standard deviation&#10;, and (2) the maximum error of the estimated mean . While the first&#10;samples of RSSI only give a quick indication, is the resultant minimal&#10;training set size, from which the DA estimates the RSSI mean and&#10;standard deviation for computing Bayes thresholds. is usually larger or&#10;at least equal to .&#10;&#10;We first study the impact of . A small may result in a partial view of&#10;the complete channel variation, while overly large may only increase the&#10;training delay. To understand the impact of , we plot in Figure&#10;\[fig:Nsigma\] the resultant standard deviation for various values of&#10;based on the RSSI traces of all 8 links. We observe that the values of&#10;initially have a larger variation and become more stable when is close&#10;to 250. The reason for this is that a small set of samples is&#10;insufficient to capture the overall temporal variations of RSSI,&#10;especially in an indoor environment where multi-path fading and&#10;interference are ubiquitous. Based on the result, we choose for our&#10;indoor environment. Then we focus on the impact of . According to&#10;Equation \[equ:minSampleSize\], the choice of has a tradeoff: smaller&#10;indicates higher estimation accuracy of the RSSI mean and thus higher&#10;detection accuracy; a smaller , however, may also increase the training&#10;set size significantly. By varying the estimated errors , we plot the&#10;resultant training set size and error rates for 5 representative links&#10;in Figure \[fig:ErrorMean\]. The figure shows that the total error rate&#10;decreases significantly with a smaller at the expense of a rapidly&#10;increasing training set size. To balance between the detection error and&#10;the training time, we choose dBm for our indoor environment, which&#10;causes only a slight increase of the detection error compared to that of&#10;dBm while at the same time keeping within the scale of a few hundred&#10;samples, achieving a good tradeoff between the training latency (several&#10;minutes with a sending frequency of 5 Hz) and detection accuracy.">
  </outline>
  <outline text="Data Smoothing" _note="As mentioned in Section \[sec:dataSmoothing\], smoothing the noisy data&#10;during the detection phase requires a sliding window of size to reduce&#10;the detection error caused by the normal RSSI randomness. To see the&#10;impact of , we demonstrate how the error rate changes with different&#10;values of (window size from 1 to 15) under two representative values.&#10;&#10;We observe from Figure \[fig:EVA-slidingWindow\] that, for both&#10;settings, increasing reduces FPR but increases FNR, which causes the&#10;total error rate to first decrease and then increase with a larger . The&#10;reason is that smoothing RSSI is effective to reduce false alarms.&#10;However, if keeps increasing, at some point, the real RSSI anomaly&#10;events are smoothed out, causing a significant increase in FNR. The&#10;impact of is also related to the setting of the minimal PDR () that&#10;defines a good link. In our case, as PDR is computed over a sliding&#10;window of 10 packets and is set to 80%, a small sliding window is&#10;preferred to avoid the significant increase in FNR. For our case, we&#10;choose , at which the total error rate is close to the lowest for both&#10;settings.">
  </outline>
  <outline text="Updating the Training Set" _note="To be adaptive to varying environment conditions, RADIUS updates the&#10;training set as discussed in Section \[sec:trainingSetUpdate\],&#10;dynamically generating new thresholds. For this, the relevant parameter&#10;is the update window size . We first show that the detection performance&#10;is enhanced with this updating technique, and then we discuss the impact&#10;of .&#10;&#10;In Figure \[fig:EVA-updateWindow\](a), we present an RSSI trace with a&#10;valley of RSSI values (between 300 and 500 seconds) indicating an&#10;abnormal situation that causes the monitored PDR to fall below the&#10;expected performance as shown in Figure \[fig:EVA-updateWindow\](b). By&#10;adapting the training set and consequently the threshold (see&#10;Figure \[fig:EVA-updateWindow\](c)), we can see from&#10;Figure \[fig:EVA-updateWindow\](d) that in this experiment, the&#10;detection error can be reduced of 3%-4% with updated thresholds, down&#10;from 18% to 14% approximately.&#10;&#10;Furthermore, we also observe from Figure \[fig:EVA-updateWindow\](d)&#10;that the impact of is not significant on the detection error (we set to&#10;be 50 for our example deployment). However, a larger may require a&#10;longer time to fill up the window making the threshold update less&#10;responsive in some cases. With such setting of , we observe the&#10;detection error can be reduced of 3% to 8% in all experiments.&#10;Considering that the total error rate in most of our experiments is less&#10;than 20%, such amount of reduction in the error rate is significant.">
  </outline>
  <outline text="Refinement of the A Priori Probability" _note="In addition to updating the training data set, one other situation that&#10;requires to generate a new threshold is when the detection accuracy&#10;degrades with an increasing number of false alarms, indicating the need&#10;of updating the A PRIORI probability. As described in Section&#10;\[sec:prioriRefinement\], we consider the maximum number of consecutive&#10;false alarms and the adjustment step of . We quantify the effects of&#10;these parameters in Figure \[fig:EVA-paramRefine\], where we compare the&#10;detection accuracy with and without the refinement of the A PRIORI&#10;probability. Specifically, we show the detection performance with&#10;varying and . We use the suggested values in the above sections for the&#10;other parameters.&#10;&#10;From Figure \[fig:EVA-paramRefine\], we can see that a smaller can&#10;reduce FPR but it may also cause a significant increase in FNR due to&#10;over-adjustment. On the other hand, a larger makes the system&#10;conservative on the A PRIORI probability refinement and hence the&#10;refinement less effective. The optimal choice of falls at the location&#10;where the total error rate is lowest. In addition, the choice of the&#10;parameter needs to consider a tradeoff: larger indicates a more&#10;effective adjustment but a higher risk of over-adjustment. In our&#10;example, we choose and . With such parameter settings, the analysis of&#10;all data traces shows that based on the accuracy improvement achieved by&#10;the training set updating technique, refining can further reduce the&#10;error rate in a range from 2% to 5%.">
  </outline>
</outline>
  </body>
</opml>