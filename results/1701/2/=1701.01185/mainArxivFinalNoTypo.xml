<?xml version="1.0" encoding="UTF-8"?>
<opml version="2.0">
  <head>
    <title>Efficient asymptotic variance reduction when estimating volatility in
high frequency data[^1]

[^1]: We would like to thank Markus Bibinger, Jia Li, Dacheng Xiu, Jean
    Jacod, Yacine Aït-Sahalia (the Editor), two anonymous referees and
    an anonymous Associate Editor, the participants of Keio Econometrics
    Workshop, the Workshop on Portfolio dynamics and limit order books
    in Ecole Centrale Paris, The Quantitative Methods in Finance 2016
    Conference in Sydney for helpful discussions and advice. The
    research of Yoann Potiron is supported by a special private grant
    from Keio University and Japanese Society for the Promotion of
    Science Grant-in-Aid for Young Scientists No. 60781119. All
    financial data is provided by the Chair of Quantitative Finance of
    the Ecole Centrale Paris. The research of Simon Clinet is supported
    by CREST Japan Science and Technology Agency and a special grant
    from Keio University.</title>
    <abstract>This paper shows how to carry out efficient asymptotic variance
reduction when estimating volatility in the presence of stochastic
volatility and microstructure noise with the realized kernels (RK) from
and the quasi-maximum likelihood estimator (QMLE) studied in . To obtain
such a reduction, we chop the data into blocks, compute the RK (or QMLE)
on each block, and aggregate the block estimates. The ratio of
asymptotic variance over the bound of asymptotic efficiency converges as
increases to the ratio in the parametric version of the problem, i.e.
1.0025 in the case of the fastest RK Tukey-Hanning 16 and 1 for the
QMLE. The impact of stochastic sampling times and jump in the price
process is examined carefully. The finite sample performance of both
estimators is investigated in simulations, while empirical work
illustrates the gain in practice. </abstract>
  </head>
  <body>
<outline text="Introduction" _note="Over the past decades, the availability of high frequency data has led&#10;to a better understanding of asset prices. The main object of interest,&#10;the quadratic variation, can be used for example as a proxy for the spot&#10;volatility or the volatility parameter of a time-varying model.&#10;Moreover, forecasts of future volatility can be improved with it.&#10;Without microstructure noise, the realized variance (RV) estimator (e.g.&#10;, , ) is both consistent and efficient. The convergence rate and the&#10;asymptotic variance (AVAR) were established in , and (see also and ).&#10;&#10;Under market frictions, the RV is no longer consistent. bring forward&#10;the Two-Scale Realized Volatility nonparametric estimator, the first&#10;consistent estimator in the presence of noise and with a relatively slow&#10;convergence rate of . modifies it to provide the Multi-Scale Realized&#10;Volatility (MSRV) which features the optimal rate of convergence as&#10;documented in . Other approaches consist in and are not limited to:&#10;pre-averaging (PAE) the observations (), advocates for the realized&#10;kernels (RK) and studies the quasi-maximum likelihood estimator (QMLE)&#10;which was originally considered in when volatility is constant. Those&#10;three approaches share the optimal rate property and only differ through&#10;edge-effects which impact their respective AVAR.&#10;&#10;The nonparametric AVAR bound of efficiency is equal to , where stands&#10;for the time horizon and corresponds to the noise variance. This was&#10;shown in under the deterministic volatility and Gaussian noise setting,&#10;but it is commonly assumed that it stays true under stochastic&#10;volatility. Subsequently, in a recent breakthrough paper, found an&#10;estimator based on the spectral approach introduced in which reaches the&#10;bound in a very general situation. More recently, proposed an adapted&#10;version of the pre-averaging estimator using local estimates as in which&#10;gave rise to estimators that are within 7% of the bound.&#10;&#10;To be fair when comparing several estimators, we need the candidates to&#10;be equipped with the same technology. Following closely the local&#10;technique used in and more recently in , we aim to adapt accordingly the&#10;RK and the QMLE. Indeed, although both estimators behave remarkably well&#10;when volatility is constant, i.e. in the parametric case the ratio of&#10;AVAR over the bound of asymptotic efficiency is 1.0025 when considering&#10;the most efficient Tukey-Hanning 16 RK and 1 for the QMLE, they can&#10;actually be highly inefficient in the non-parametric setting as&#10;documented in the following of this introduction and in Section 2. Under&#10;time-varying volatility, we aim to reduce significantly their AVAR and&#10;make them efficient. Although it would reduce the AVAR the same way, we&#10;did not implement the local version of the MSRV. In fact, MSRV and RK&#10;are asymptotically equivalent in the sense that they share the same&#10;asymptotic variance when considering the same kernel (see Section 2.2 in&#10;).&#10;&#10;To reduce the variance, we divide the interval into non-overlapping&#10;regular blocks , , , . We then compute the RK (QMLE) on each block, and&#10;take the sum of the estimates. We show that the nonparametric ratio of&#10;AVAR over the bound of efficiency converges to the parametric ratio as&#10;increases. More importantly for practical applications, the convergence&#10;is very fast, and the gain is already important in the case blocks.&#10;&#10;As an example, we focus on the RK Tukey-Hanning 16 and consider the&#10;(apparently innocuous) block constant model for and for . When choosing&#10;the optimal bandwidth, [^1] showed that the AVAR is equal to where is&#10;defined as with and where are constant functions of the kernel. We fix&#10;and we compute in that case , and . Thus, the bound of efficiency is&#10;equal to , whereas . This can be expressed as a loss of , which is to be&#10;compared to the loss in the parametric case[^2] . When fixing , the&#10;volatility on each block is constant and thus yields on the first block&#10;and on the second block. As both estimates are uncorrelated[^3], we&#10;obtain that the global AVAR is equal to , i.e. .25 % loss which&#10;corresponds exactly to the parametric loss.&#10;&#10;From (\[AVAR\]), we can see that the theoretical loss can be expressed&#10;as a deterministic function of the already well-known measure of&#10;volatility constancy and another connected quantity which we denote&#10;Details can be found in Section 2, along with an expression for the QMLE&#10;loss as well. In the previous example where the loss was about , the&#10;corresponding setting can be computed as and . Volatility on real data&#10;is moving more than on this toy example, corresponding to lower and . In&#10;their empirical study, daily estimate and find that the typical value is&#10;around 1.3, and about 1.6 when restricting to the top 10% days in terms&#10;of intraday variation of volatility. This corresponds respectively to&#10;estimates of as and . When taking respectively those two realistic&#10;values, the corresponding RK and QMLE losses are expected to be around&#10;20% (can go up to 100 %), depending on the other parameter value . With&#10;such highly inefficient estimators, we believe that there is a practical&#10;need for variance reduction. This is especially the case on days when&#10;the volatility is moving a lot.&#10;&#10;Clearly this estimator is related to local parametric methods in&#10;high-frequency data, i.e. aggregating local parametric estimates. For&#10;example, investigated the ex post adjustment involving asymptotic&#10;likelihood ratios to make when assuming constant local volatility.&#10;showed the asymptotic equivalence in Le Cam’s sense between the&#10;non-parametric and locally constant volatility experiment. To estimate&#10;quarticity and other functionals of volatility, estimated the volatility&#10;locally and plugged the value into the sum. Our work includes , , .&#10;&#10;The remainder of the paper is structured as follows. Section 2 stretches&#10;the limitations of the global approach by expressing the loss as a&#10;function of and . In Section 3, we provide the model, investigate the RK&#10;and the QMLE and their corresponding limit theory. Section 4&#10;investigates what happens to both methods when considering stochastic&#10;arrival times and adding jump in the price process. Section 5 performs a&#10;Monte Carlo experiment to assess finite sample performance and AVAR&#10;reduction. Section 6 provides an empirical illustration where we&#10;quantify the expected gain in practice. Theoretical details and proofs&#10;can be found in the Appendix.&#10;&#10;[^1]: see pp. 1494-1495 for more details.&#10;&#10;[^2]: Details can be found on Table II (p. 1495, ).&#10;&#10;[^3]: if we remove end-effects.">
</outline>
<outline text="Limitations of the global approach" _note="This section documents the performance of the global RK and QMLE. In&#10;particular, we show how it deteriorates as a function of&#10;heteroskedasticity. Finally, we diagnose the reasons and provide the&#10;solution to this relative failure.&#10;&#10;One crucial feature common to both estimators is that they behave&#10;remarkably well when volatility is constant. Indeed, the QMLE is&#10;efficient and the RK Tukey-Hanning 16 almost efficient in that case.&#10;Even the RK Tukey-Hanning 2, with an AVAR over the bound of efficiency&#10;ratio of less than 1.04, can be considered as “practically efficient”.&#10;To study what happens when volatility is time-varying, it is useful for&#10;to define to be measures of heteroskedasticity. In the following, we&#10;will be using and in place of and . The quantity was already introduced&#10;in and plays an important role in the AVAR of both RK and the QMLE.&#10;(Figure 1, p. 241) expresses the quotient of both AVARs as a function of&#10;, but does not assess their respective performance when compared to the&#10;(conjectured) bound of efficiency defined as In contrast, the other&#10;quantity is introduced to investigate that relative performance. More&#10;precisely, is needed to express the AVAR over the bound of efficiency&#10;ratio for both approaches since the AVAR does not feature the tricity,&#10;i.e. the integrated third moment of volatility, which is key in the&#10;bound of efficiency. Evidently, both measures and are very much&#10;connected and we can actually show that we have that&#10;&#10;Note that the equality for all corresponds to the parametric case. In&#10;particular, Eq. (\[RhoKappa\]) implies that for any given , the value is&#10;a.s. in a small boundary around . This is of particular interest because&#10;as far as the authors know under noisy observations the literature on&#10;quarticity estimation[^1] is far more abundant than the corresponding&#10;work on estimating tricity [^2], which implies that in practice can be&#10;estimated relatively easily, whereas would require more effort. From&#10;(Figure 7, p. 41), when taking a pre-averaging window equal to one&#10;minute (chosen consistently with their recommendation in Section 5.2.4&#10;on p. 34 where the authors argue that a reasonable choice of window&#10;should lie between 30 seconds and 2 minutes) we infer that the estimates&#10;of are about , and when considering respectively the bottom 10% days in&#10;terms of intraday variation of volatility, all days and the top 10% days&#10;in terms of intraday variation of volatility. Correspondingly, we will&#10;be using , , to refer respectively to high, regular and low values of&#10;throughout the rest of the paper. It is not surprising to find such low&#10;values on stocks data as it has been understood for several decades now&#10;that many stylized facts describe volatility as time-varying (see, e.g.,&#10;, ).&#10;&#10;When using the optimal bandwidth, is defined as where we have with&#10;constant functions of the kernel. Correspondingly, we give the formal&#10;definition of the RK loss as Obvious computations lead to . If we see as&#10;a function of , is equal to in the parametric case. The parametric&#10;values for several kernels can be directly inferred from (Table II, p.&#10;1495) and the loss is equal to .25 % when considering the Tukey-Hanning&#10;16, 3.625 % for the Tukey-Hanning 2, 6.75 % for the Parzen and 13 % for&#10;the Cubic kernel. We have that is an increasing function of , and thus&#10;the effect of and are reverse. Next we consider the AVAR of the QMLE&#10;expressed via The formula can actually be found in Box V (p. 240, ). The&#10;corresponding QMLE loss is defined in analogy with (\[loss\]) and can be&#10;expressed as Figure \[efficiencyGlob\] plots the feasible loss region&#10;for three typical RK, the QMLE and the PAE with triangle kernel. It is&#10;clear that they highly lose efficiency when is decreasing. The QMLE is&#10;dominated by the RK approach when is low, which was observed on Figure 1&#10;(p. 241, ).&#10;&#10;The problem behind this potentially high loss can be intuitively&#10;explained as follows. For the RK, although the optimal tuning parameter&#10;is robust to time-varying volatility, it suffers from the fact that one&#10;day[^3] is too long to “stay optimal”. This is a very similar situation&#10;to the PAE, which also features a tuning parameter. Subsequently, used&#10;block estimations to heavily reduce variance. As for the QMLE, which in&#10;contrast is designed in a parametric way yielding no choice of tuning&#10;parameter, the smaller and are, the further the misspecified model&#10;deviates from the truth. It is by nature a different estimator, but&#10;local methods are expected to reduce the misspecification as in . Thus,&#10;we aim to reduce the non-parametric loss into the parametric loss using&#10;adapted local methods. As we can see on Figure \[efficiencyGlob\], the&#10;QMLE will benefit the most as it is efficient in the parametric case and&#10;deteriorates more than the RK in the non-parametric case.&#10;&#10;[^1]: see, e.g., , , , and .&#10;&#10;[^2]: see the spectral approach AVAR estimator in , and .&#10;&#10;[^3]: or one week, one month, etc.">
</outline>
<outline text="Local estimation">
  <outline text="Model for the observations" _note="We assume that the latent log-price process and the volatility follow&#10;where is a 2 dimensional standard Brownian motion, the drift is&#10;componentwise locally bounded, the volatility matrix is componentwise&#10;locally bounded, itself an Itô process and a.s. We also assume that is a&#10;pure jump process of finite activity. This rules out jumps in , an issue&#10;addressed in Section \[robustness\]. In contrast the volatility process&#10;can include jumps (see, e.g., for empirical evidence). The observations&#10;are contaminated by the microstructure noise so that we observe where&#10;correspond to the observation times[^1] which are assumed to be&#10;regularly spaced, i.e. satisfying . Stochastic arrival times are also&#10;considered in Section \[robustness\]. Furthermore, we assume that the&#10;noise is independent and identically distributed (i.i.d), and&#10;independent of the other quantities, with null-mean, variance and finite&#10;fourth moment. Next the horizon time is defined as . Finally, we&#10;consider the high frequency asymptotics and assume that goes to&#10;infinity, where . In particular, the time gap goes to .&#10;&#10;[^1]: Note that , , etc. are implicitly assumed to depend on the index .&#10;    We sometimes refer to , when necessary.">
  </outline>
  <outline text="Realized Kernels">
    <outline text="Local RK definition" _note="We consider first the framework where the local RK coincides with the&#10;RK. The flat-top RK takes on the form where and the deterministic kernel&#10;is defined for . The realized autocovariance is defined as where .&#10;&#10;In the general case , for each we choose a bandwidth and define the&#10;estimate on the th block , where . On each block, we also assume that&#10;the number of observations is an integer for simplicity of exposition.&#10;Formally, all the considered quantities could be written with floor&#10;brackets, and all the results would still hold. We aggregate the local&#10;estimates to obtain the adapted version of the RK defined as The&#10;corresponding is now -dimensional in this case. We also adapt the&#10;jittering introduced in Section 2.6 (, p. 1487), i.e. for we assume that&#10;is an average of distinct observations on the interval .">
    </outline>
    <outline text="Asymptotic theory" _note="We define for -stable convergence. We further define as the&#10;noise-to-signal ratio, and refer to in the following. Finally, we define&#10;kernel weight functions that are two times continuously differentiable&#10;on and We recall the main asymptotic result with fastest rate of&#10;convergence about the RK which can be found in Theorem 4 (p. 1493) in .&#10;When , , and , we have where denotes a mixed normal distribution. A&#10;straightforward application of (\[RK00\]) on each block yields where is&#10;the tuning parameter used on the th block. Next we show that the AVAR&#10;associated to is equal to the sum of variance terms in&#10;(\[fastlocalAVAR\]).&#10;&#10;\[RKth\] (CLT for local RK) When , , and , we have&#10;&#10;The requirement that in (\[RKth2\]) is due to end-effects. The reader&#10;should refer to the discussion in (p. 1493) in the case . When is fixed,&#10;the relative contribution[^1] to the AVAR is proportional to , as it was&#10;already the case for the RK. documented that this magnitude can&#10;reasonably be ignored in practice.&#10;&#10;To determine the tuning parameters that minimize the AVAR in&#10;(\[RKth2\]), we can consider each local AVAR independently as they&#10;depend on one distinct tuning parameter. For that purpose, we follow&#10;Section 4.3 in (p. 1494-1496) and consider that The optimal values are&#10;then shown to be equal to The corresponding AVAR is equal to where is&#10;considered here as a function of and corresponds to the block length.&#10;&#10;We provide in what follows a consistent estimator for each tuning&#10;parameter. To pre-estimate on each block the integrated volatility and&#10;quarticity, we consider the pre-averaging estimators from . For each&#10;block we choose an integer and a real parameter which satisfy . We also&#10;consider a continuous function on , piecewise with a piecewise Lipschitz&#10;derivative such that , . We define and . We further define The&#10;pre-averaging estimators of integrated volatility and quarticity on each&#10;block take on the form We then estimate We provide now a consistent&#10;estimator of . We estimate the noise as and the asymptotic variance as&#10;The feasible CLT is given in the following theorem.&#10;&#10;\[RKcor\] (feasible CLT for local RK) When , , and with , we have and&#10;&#10;Finally, we show that when choosing the optimal values, the AVAR&#10;associated to goes to when . The constant , when normalized to ,&#10;corresponds to the parametric loss and depends solely on the shape of&#10;the kernel. The rationale of such result is that when increases we have&#10;the volatility roughly constant on each block and thus Next we obtain by&#10;a Riemann sum argument that which can be expressed as . The formal&#10;result is given in the following proposition.&#10;&#10;\[propAVARRK\] (Convergence of local RK AVAR) When , we have&#10;&#10;\[rklossprop\] In particular, the asymptotic loss for is , which is&#10;always smaller than when using the RK with . The proof of this statement&#10;can be found in Appendix (Section \[proofrk\]).&#10;&#10;[^1]: The corresponding expression can be found in the second term in&#10;    (\[th34\]).">
    </outline>
  </outline>
  <outline text="QMLE" _note="In analogy with Section \[RK\], we provide in this section a definition&#10;of the local estimator and equivalent asymptotic results in the case of&#10;the QMLE.">
    <outline text="Local QMLE definition" _note="We consider first the setting where the local QMLE is equal to the&#10;global QMLE. We recapitulate the parametric approach, and introduce the&#10;quasi-estimator. studied the parametric case assuming that the latent&#10;efficient log price process satisfies The observed log returns are&#10;following a MA(1) process in that situation. If we postulate that the&#10;noise distribution is Gaussian, then the log likelihood function for can&#10;be expressed as where We define the corresponding MLE which maximizes&#10;(\[loglik\]) as and the estimator of integrated volatility as . When the&#10;log price features stochastic volatility and drift as in Section&#10;\[model\] and/or when the noise is not normally distributed, is seen as&#10;the QMLE.&#10;&#10;When , we define for each block a local QMLE estimator which maximizes&#10;the expression applied to the observations on only, along with the local&#10;integrated volatility estimator . We then construct the aggregate&#10;version of the QMLE as">
    </outline>
    <outline text="Asymptotic theory" _note="We state the main result in (Box V, p. 240). If we assume that with , we&#10;have where we recall that Also, refers to the fourth cumulant of . An&#10;obvious application of (\[QMLE00\]) for each block gives us that We show&#10;in the following theorem that the AVAR associated to can be decomposed&#10;as a sum of local AVARs scaled by .&#10;&#10;\[CLTQMLE\] (CLT for local QMLE) We have&#10;&#10;We define which is estimated via The feasible theorem follows.&#10;&#10;\[QMLEcor\] (feasible CLT for local QMLE) We have and&#10;&#10;We show now that the AVAR associated to goes to when increases.&#10;&#10;\[propAVARQMLE\] (Convergence of local QMLE AVAR) When , we have">
    </outline>
  </outline>
</outline>
<outline text="Is the local method robust to stochastic sampling times and jump in the price process?" _note="We discuss in this section what happens to both approaches when&#10;considering stochastic arrival times and adding jump to the price&#10;process. Related work in the global case include for the QMLE and for&#10;the RK. We further inspect the AVAR behavior when in this situation. The&#10;results are mitigated. Reduction (conjectured to be efficient) is&#10;obtained in the case of stochastic arrival times on the one hand, but&#10;there are additional terms in the AVAR as when adding jumps on the other&#10;hand.">
  <outline text="Central limit theory when  is fixed" _note="We assume that the latent log-price process is now an Itô semimartingale&#10;defined by where and satisfy the same conditions as in Section&#10;\[model\], and is a pure jump process of finite activity.&#10;&#10;For the observation times, we adopt the random discretization scheme of&#10;(see Section 14.1) and we assume that there exists an Itô semimartingale&#10;which satisfies Assumption 4.4.2 (p. 115) in and is locally bounded away&#10;from , and i.i.d that are both independent of the other quantities, ,&#10;such that t\_0 &amp; = &amp; 0,\&#10;t\_i &amp; = &amp; t\_[i-1]{} + \_[t\_[i-1]{}]{} U\_i, where we recall that .&#10;Finally, we assume that , and that for any , as . Note that, defining ,&#10;the number of observations before as , we have as and \^[u.c.p]{}&#10;\_0\^t[\_s\^[-1]{}ds]{}, \[eqNumberJumps\] where the convergence means&#10;uniformly in probability on for any .[^1] We further define .&#10;&#10;As pointed out in (p. 431), any deterministic grid satisfies the above&#10;conditions. Actually, this model can be considered as more general than&#10;the time deformation proposed by (Section 5.3, pp. 1505-1507) in the&#10;sense that more complex arrival times, such as a Poisson process&#10;independent of the other quantities fall under the model. On the&#10;contrary, assuming the existence of the quadratic variation of time&#10;(see, e.g., Assumption A on p. 1939 in ) is too general as our proofs&#10;require the existence of the quadratic covariation of time lags for all&#10;lags[^2].&#10;&#10;Since the price process features possible jumps, the two estimators are&#10;no longer consistent to the integrated volatility, but they converge to&#10;the quadratic variation where corresponds to the size of the jump if&#10;there is a jump at time and 0 otherwise. Correspondingly we define on&#10;each block the new local target as The AVARs obtained in the robust&#10;theorems feature in place of integrated volatility, and the following&#10;quantity as an alternative for quarticity: Correspondingly, we define&#10;substitutes for the measure of heteroskedasticity and the noise-to-ratio&#10;measure as Moreover, we also introduce R\_[(i)]{} := , which corresponds&#10;to the asymptotic ratio of the total number of observations over the&#10;number of observations on the block as we have . Finally, we define and&#10;refer to for stable convergence with respect to . We provide the CLT for&#10;the two approaches in what follows.&#10;&#10;\[RKthjumps\] (robust CLT for local RK) When , , , and , we have where&#10;&#10;The new optimal bandwidth is given by with local and global optimal&#10;variances respectively defined as&#10;&#10;As for the QMLE, the log likelihood function when keeps the same form&#10;(\[loglik\]) but we replace by in the definition of now defined as where&#10;. Each local QMLE estimator is now defined as a maximizer of where is&#10;the vector of price returns on the th block, , and with . If we assume&#10;that we obtain the following theorem.&#10;&#10;\[CLTQMLEjumps\] (robust CLT for local QMLE) We have where&#10;&#10;[^1]: We can prove (\[eqNumberJumps\]) using Lemma 14.1.5 in . The&#10;    uniformity is a consequence of the fact that and are increasing&#10;    processes and Property (2.2.16) in .&#10;&#10;[^2]: To see a condition on the first lag, one can look at Assumption&#10;    B.vii on p. 37 in . This does not include other lags.">
  </outline>
  <outline text="The good case: robustness to stochastic arrival times" _note="Here we assume a no-jump setting, i.e. . The following two propositions&#10;provide the AVAR asymptotic behavior when for the two methods. The limit&#10;is very similar to that in the regular observation case, and thus the&#10;local method is robust to stochastic observation times. Note that the&#10;conjectured bound of efficiency is affected by the setting and takes the&#10;form&#10;&#10;\[RKcorRobust\] (Asymptotic behavior of local RK AVAR when sampling&#10;times are stochastic) When , we have&#10;&#10;\[QMLEcorRobust\] (Asymptotic behavior of local QMLE AVAR when sampling&#10;times are stochastic) When , we have">
  </outline>
  <outline text="The bad case: adding jumps to the price process" _note="In this section, the price process can feature jumps. Actually in such&#10;setting the AVAR of the RK tends to a big value as increases, and that&#10;of QMLE explodes. This sheds light on a weak point of the local method&#10;in this case.&#10;&#10;\[RKcorRobust2\] (Asymptotic behavior of local RK AVAR when ) As ,&#10;AVAR\^[(RK,rob)]{} &amp;&amp;8g(1)a\_0ł(\_0\^T\_s\^[-1]{}ds)\^[1/2]{}&#10;\_0\^T[\_s\^[1/2]{}\_s\^3]{}ds\&#10;&amp;+&amp;a\_0ł( + ) ł(\_0\^T\_s\^[-1]{}ds)\^[1/2]{}\_[0 &amp;lt; s T]{} J\_s\^2&#10;ł(\_s\^2\_s + \_[s-]{}\^2\_[s-]{})\^[1/2]{} .&#10;&#10;\[QMLEcorRobust2\](Asymptotic behavior of local QMLE AVAR when ) As ,">
  </outline>
</outline>
<outline text="Numerical study">
  <outline text="Goal of the study" _note="In this section, we discuss theoretical AVAR reduction and we examine&#10;the performance of the local RK and the local QMLE in a finite sample&#10;context for several values of . We carry out Monte Carlo simulations for&#10;three different volatility models having realistic values of . We then&#10;check whether asymptotic approximations of several statistics correctly&#10;kick in to illustrate to what extent the theory is affected when the&#10;sample data is finite of size . First, we assess the central limit&#10;theories for the two infeasible statistics Z\_n\^[\_B]{} = Z\_n\^[\_B]{}&#10;= , and the two feasible statistics \_n\^[\_B]{} = \_n\^[\_B]{} = , for&#10;. In particular, we investigate how increasing affects the standard&#10;normal approximation of these two studentizations for several levels of&#10;sampling. Second, we compare the relative performance of the local RK&#10;and the local QMLE. To do so, we report the empirical loss defined as&#10;\_B\^[(RK)]{} = \_M ł\[\] -1 \_B\^[(QMLE)]{} = \_M ł\[\] -1 where&#10;denotes the sample mean of based on the Monte Carlo simulations and we&#10;recall that is the bound of efficiency for the asymptotic variance. We&#10;also define the theoretical loss as and report the sample mean of the&#10;theoretical loss for . Note that is close to the mean loss if is large&#10;enough. The empirical loss , which gives us a simple criterion to&#10;compare the estimators, can be decomposed as">
  </outline>
  <outline text="Simulation design" _note="We implement the above procedures for Monte Carlo simulations of&#10;intraday returns on the time interval , year (that is working day). One&#10;working day is in turn subdivided in seconds corresponding to hours of&#10;trading activity. For each model, the corresponding trajectories are&#10;generated from a classical Euler scheme based on intervals, that is one&#10;observation every seconds. We simulate more observations prior and post&#10;main trading period in order to compute properly different that are&#10;necessary for the RK. Indeed, using their truncated versions tend to&#10;generate a non-negligible bias as pointed out in (see Table 2 on p.&#10;243), so that we prefer to overcome this issue with a few minutes of&#10;out-of-sample data. Finally, we also use observations based on sparsely&#10;sampled versions of the original trajectories, for a number of intervals&#10;taking on the values , , and , the latter corresponding to having one&#10;observation every seconds, which still corresponds to a fairly heavily&#10;traded stock. We do not report the results for lower frequencies, but&#10;the theory still kicks in for sparser samplings too.&#10;&#10;We consider three stochastic volatility models to simulate the intraday&#10;returns, along with three levels of mean noise-to-signal ratios , and .&#10;The three values are empirically corroborated in , where the authors&#10;report empirical values of for several stocks ranging from to (see Table&#10;3 on p. 147). We introduce now the volatility models, which have been&#10;designed to reflect different average values of ranging from&#10;(corresponding to a high value) for Model 1, (corresponding to a regular&#10;value) for Model 2 to (corresponding to a low value) for Model 3 as&#10;reported on Table \[tableRho\]. The three models can all be represented&#10;as a Heston stochastic volatility model (SV) with U-shape intraday&#10;volatility pattern and a possible jump whose occurrence time is picked&#10;up uniformly randomly on a subinterval of . Except for the jump&#10;component, this general model is directly inspired from Model 4 in , and&#10;(see Section 6.1 on p. 242). We assume that the log price process and&#10;the volatility process follow the dynamics with Here and are two&#10;standard Brownian motions with . Note that jumps at time , that we&#10;define as a uniform random variable on . controls the size of the jump.&#10;The choice of making jumps, instead of the global volatility , is merely&#10;a way to ensure that remains positive. Finally, the drift parameter and&#10;the stochastic volatility part remain constant for each model. The&#10;corresponding parameters are chosen consistently with the ones from&#10;Section 6.1 (p. 242) in , that is , , , , . Finally, is sampled from a&#10;Gamma distribution of parameters , which corresponds to the stationary&#10;distribution of the CIR process.">
    <outline text="Model 1: SV + steep U (HIGH )" _note="The first model does not incorporate the jump in volatility, i.e. we set&#10;. The parameters of the U-shape part are set to generate a steep slope,&#10;which in turn lowers somewhat the value of compared to Model 4 in where&#10;we find that the corresponding mean value is too high to be consistent&#10;with (which we recall is the empirical high value reported in Section&#10;\[limitations\]). With , , , , , this model presents a sample mean value&#10;of , which is slightly bigger than . We are conservative in this first&#10;model to show what happens to the local method in a very unlikely bad&#10;situation for AVAR reduction, i.e. a very high .">
    </outline>
    <outline text="Model 2: SV + normal U + 1 Jump (REGULAR )" _note="In this model, the U-shape intraday volatility parameters are set to&#10;values that are consistent with those chosen in Model 4 in , that is , ,&#10;, and . The jump size parameter is set to , that is a jump of 50% in&#10;size at the random time . We set , and thus let take values on the whole&#10;time interval. Such friction in the volatility process leads to lower&#10;values of and compared to Model 1, with a sample mean equal to . This is&#10;thus a very realistic model in terms of measure of heteroskedasticity as&#10;. It is also possible to obtain in an alternative continuous volatility&#10;model with normal U by taking a 2-factor stochastic volatility model&#10;(SV2F) as in (Section 6.2, p. 1511), with parameters tuned such that the&#10;trajectories are rough enough. The results from Section \[results\]&#10;would be similar. As a byproduct, Model 2 shows that a jump in the&#10;volatility can lower significantly the measures of heteroskedasticity&#10;and .">
    </outline>
    <outline text="Model 3: SV + steep U + 1 Jump (LOW )" _note="This last model is a combination of the first two models. U-shape&#10;volatility parameters are set to give the same sloap as for Model 1, and&#10;the jump size parameter is set to as in Model 2. However, to keep the&#10;positivity of we restrain the values of the jump time and set , . This&#10;third scenario is designed to reach volatility paths presenting an&#10;heteroskedasticity with a low value of and we report the sample mean ,&#10;which is almost equal to . We are in the situation where the global&#10;estimators should deviate the most from the bound of efficiency.&#10;&#10;We now turn to the estimation procedure. First, to estimate on , we work&#10;with the Tukey-Hanning 2 kernel as for the numerical study in (Section&#10;6, pp. 1510-1513) since it requires reasonable bandwidth sizes , which&#10;makes the estimator computable in an acceptable amount of time.&#10;Moreover, we do not need too many out-of-period data to compute . We&#10;implement the feasible adaptive estimator. We arbitrary set the tuning&#10;parameters equal to 30 seconds and the triangular kernel . In practice,&#10;we find that the realized kernel is not very sensitive to the dispersion&#10;of in terms of RMSE, so that it is not absolutely necessary to get very&#10;accurate pre-estimators. Such robustness proved to be crucial in our&#10;procedure as it is well known that estimators for the quarticity can be&#10;unstable in finite sample when the amount of data is not large. On each&#10;block , we do the same procedure and obtain the corresponding by&#10;aggregation. Finally, we compute the QMLE by a numerical maximization of&#10;the quasi-likelihood function given in Section \[localQMLEsub\]. This&#10;gives us and the local estimates .">
    </outline>
  </outline>
  <outline text="Discussion on theoretical AVAR reduction" _note="In this section, we propose to look at the theoretical AVAR reduction as&#10;a function of , and investigate the practical question of how fast the&#10;convergence in (\[eqAVARRK\]) and (\[eqAVARQMLE\]) is. The model&#10;considered for volatility is a deterministic U shape + 1 Jump, which&#10;corresponds to Model 2 without the stochastic volatility part. Here we&#10;generate different values of as a function of the jump time, which we&#10;restrict to be in so that each can be associated to a distinct jump time&#10;on that interval. We choose this particular model because the sample&#10;mean of is .77 which corresponds to a regular value, and the panel of&#10;generated values is sufficiently large compared to the other two models.&#10;&#10;The values of and are plotted as a function of in the upper panels of&#10;Figure \[AVARblockmod2\] for a realistic continuous U-shape with one&#10;jump volatility model where the sample mean corresponds to a regular&#10;value of [^1]. As we can see, the convergence in (\[eqAVARRK\]) is very&#10;fast. When , the QMLE loss is almost divided by 4 when considering 2&#10;blocks instead of 1, with and . In the same setting the RK loss goes&#10;from to . If we consider the lower value , the QMLE losses for the first&#10;four values of are equal to , , and . The corresponding RK values are ,&#10;, and . This suggests that the convergence to the loss bounds (which we&#10;recall to be equal to when considering the RK Tukey-Hanning 2 and for&#10;the QMLE) is very fast for both approaches. Actually for any reasonable&#10;taken to be between 0.5 and 1, choosing is big enough for the loss to&#10;stay within (or ), and it is usually far below this threshold with&#10;regular and high values of .&#10;&#10;Moreover, we can see on the left lower panel in Figure \[AVARblockmod2\]&#10;that when is relatively high, the QMLE outperforms the RK approach when&#10;considering , and the gap gets bigger as we increase . In contrast when&#10;, the QMLE is outperformed when considering only one block, but&#10;eventually makes it back when incrementing the value of . The actual&#10;value required to fill up the gap is getting bigger as decreases. This&#10;suggests that both approaches are complementary to each other. Finally,&#10;the lower left panel in Figure \[AVARblockmod2\] documents that both&#10;approaches dominate the PAE regardless of the number of blocks.&#10;&#10;[^1]: Useful details on this model can be found in Section&#10;    \[simulations\].">
  </outline>
  <outline text="Results" _note="We first report the finite sample properties of the four statistics in&#10;Table \[stdRK\]-\[stdQMLEfeasible\] for Model 2 under the noise level .&#10;We can see that the results are promising at any level of sampling, as&#10;the RMSE of the -statistic does not suffer much from the increasing in&#10;the number of blocks, especially for the QMLE for which the RMSE of&#10;stays closely in line with . The results also indicate that the&#10;asymptotic theory eventually kicks in for all the estimators as the&#10;standard deviation of the statistics decreases to when the sampling&#10;frequency increases. Nevertheless, we can see a slight over dispersion&#10;compared to what was reported in and . For the QMLE, this is due to the&#10;strong difference with the noise-to-signal ratio that was used in where&#10;. Concerning the RK, the difference in the studentization is due to the&#10;fact that the authors in do not employ for the studentization, but a&#10;non-asymptotic variance as documented in Section 4.4 (pp. 1496-1498) of&#10;their work. The feasible statistics are slightly biased, and this is due&#10;to the estimation of the AVAR procedure.&#10;&#10;We then report the theoretical loss values and the empirical loss for&#10;two levels of sampling and , and three levels of noise-to-signal ratios&#10;, and in Table \[tableLossXimedium\]. First, we can note that the&#10;theoretical loss behaves in a very similar way as in Section&#10;\[AVARreduction\] for the three models. In particular, this implies that&#10;neither the SV part nor the steep U component seems to have a bad impact&#10;for the local method. Also, one can see that when choosing the&#10;theoretical loss is at most more than the parametric loss (which we&#10;recall to be equal to 3.625 % for the RK Tukey-Hanning 2 and 0% in the&#10;case of the QMLE), which are in line with the threshold found in Section&#10;\[AVARreduction\].&#10;&#10;Second, the loss due to the finite sample behaves in a very proper way&#10;when increases. For any setting and both estimators, it is roughly&#10;constant as a function of , although suffering more when is higher and&#10;smaller. This is perfectly in line with the findings in Table \[stdRK\]&#10;and Table \[stdQMLE\]. In particular for Model 2 and Model 3, the finite&#10;sample effect is almost not moving as increases. For Model 1, this is&#10;basically the same picture for the QMLE, but the empirical loss seems to&#10;stagnate between and when using the RK. This is not surprising as the RK&#10;suffers more from the finite sample effect than the QMLE as seen in&#10;Table \[stdRK\] and Table \[stdQMLE\].&#10;&#10;Third, note that the decomposition where denotes the sample variance of&#10;based on the Monte Carlo simulations, is numerically well-verified and&#10;gives an intuitive interpretation of the main sources of deviation from&#10;the bound in practice. For instance, consider on Model 2, with , . In&#10;that case, the previous decomposition (\[vardec\]) gives for the left&#10;hand side, and for the right hand side which is very close to the other&#10;value indeed.&#10;&#10;Finally, this simulation study indicates that the local version of RK&#10;and the QMLE perform very well in practice, with the QMLE slightly more&#10;robust to the values of and as free of tuning parameters.">
  </outline>
</outline>
<outline text="Empirical illustration" _note="We conclude this study by the application of our method on transaction&#10;log prices of Intel Corporation (INTC) shares recorded on the NASDAQ&#10;stock market over the year 2015. We exclude January 1, the day after&#10;Thanksgiving and December 24 which are less active, thus this leaves us&#10;with 250 trading days of data. Moreover, we only keep transactions that&#10;were carried out between 9:30am and 4pm. Finally, we consider the data&#10;in tick time, for an average of daily trades. The most active days&#10;include more than trades.&#10;&#10;We first estimate the theoretical gain in AVAR. As for the numerical&#10;study, we do not cap by . Across the days, values of and blocks&#10;corresponding to an overall of 5,250 estimates, the value was crossed&#10;only a few times. We report in Table \[tableRho2\] key statistics for&#10;AVAR reduction. We get a global estimate of around , which is very close&#10;to . Across the year the estimates of ranged from around to , and&#10;actually crossed for two days where it reached and . When increases, we&#10;find as expected that , the mean estimated value of across days and&#10;blocks, also increases to reach a value of for blocks. Accordingly, the&#10;mean estimated ratios of AVAR decreases from to for the QMLE, and from&#10;to for the RK. Moreover, we find that those ratios are consistently&#10;smaller than for the 250 days and different values of bigger than 1, so&#10;that the local method never deteriorates the AVAR of the estimator. Note&#10;that the same ratios for Model 2 in our simulation study range from to&#10;for the QMLE and from to for the RK. The slight disparity between the&#10;empirical study and Model 2 can be explained in several ways. For&#10;example, it is likely that we still under-evaluate the difference&#10;between and , or that the theoretical model is a little too optimistic&#10;about how fast gets close to on local blocks. To sum up, the results are&#10;approximately in line with what was expected, and present a substantial&#10;gain in terms of AVAR for both the QMLE and the RK.&#10;&#10;The last column in Table \[tableRho2\] shows the empirical correlation&#10;between the correction terms and for several values of . The positive&#10;correlation indicates that the local method tends to correct the global&#10;estimates in the same direction for both the QMLE and the RK. Moreover,&#10;increasing the number of blocks amplifies the phenomenon. Table&#10;\[tableEstEmp\] shows the empirical mean and standard deviation of the&#10;estimators. Note that the main source of randomness being the target&#10;value itself, it is not surprising to find the mean and standard values&#10;very close to each other. We have reported in the last column the&#10;correlation between each estimator and the global QMLE. We find results&#10;very close to for all estimators. One should note that the global RK is&#10;less correlated to the QMLE than all the local QMLE . This indicates&#10;that the order of magnitude of the correction induced by the local&#10;method is smaller than the difference between the two global estimators.&#10;&#10;Finally, Figure \[CI\] shows daily 95% theoretical confidence intervals&#10;for , , and in May 2015. We can see that the confidence intervals for&#10;the local estimators are often shorter than their counterpart. Moreover,&#10;over the year the global and the local estimates confidence intervals&#10;always overlap, corroborating the fact that the local estimates are in&#10;line with their global versions.">
</outline>
<outline text="Conclusion" _note="In this paper, we have looked at the efficiency of local methods to&#10;estimate integrated volatility. We have shown that for the RK and the&#10;QMLE, if we chop the data into blocks we can reduce the AVAR when is&#10;fixed and retrieve the parametric loss when goes to infinity. We have&#10;also seen that the theoretical gain is mostly preserved when looking at&#10;finite sample results. Finally, we have documented that the gain is&#10;substantial in practice.&#10;&#10;Given how simple to implement the methodology is, we expect that it will&#10;be very helpful for practitioners. Our hope is that this simple and&#10;natural technique will be used on the QMLE and the RK, but also&#10;considered for a wider class of estimators. It is clear that the theory&#10;would work for the PAE and the MSRV, but econometricians should also try&#10;it on their own favorite estimator. Actually, the technique can be&#10;applied to other problems, such as the high-frequency covariance&#10;estimation, the estimation of functions of volatility, the leverage&#10;effect, the volatility of volatility, etc.">
</outline>
<outline text="Appendix: proofs">
  <outline text="Simplification of the problem" _note="Since we want to prove stable convergence, in view of the componentwise&#10;local boundedness of the matrix and because , we can without loss of&#10;generality assume that for all there exists some nonrandom constants and&#10;such that by using a standard localization argument (e.g., Section 2.4.5&#10;of ). One can further suppress as in Section 2.2 (pp. 1407-1409) of ,&#10;and act as if is a martingale. Also, we follow a similar procedure to&#10;localize the random variables as, e.g, in the proof of Lemma 14.1.5&#10;p.435, Equation (14.1.13), in . Consequently, we will assume in the&#10;following of the proof:&#10;&#10;**(H)** We have . Moreover , , , , , , , are bounded. Given an A PRIORI&#10;number , we also have .&#10;&#10;In particular, **(H)** implies, taking small enough, that , for large&#10;enough.&#10;&#10;We define the -field that generates the observation times and which is&#10;independent of . We will often have to use the conditional expectation ,&#10;that we hereafter denote for convenience . We also define the discrete&#10;filtration , and recall the continuous version where is the canonical&#10;filtration associated to . Note that by independence from , admits the&#10;same Itô semimartingale dynamics in the extension .&#10;&#10;Note also that, by virtue of Lemma 14.1.5 in , recalling , and we have&#10;&amp;gt;0 n\^[1-]{}\_t\^n 0. \[eqStepsize\] Throughout the proofs, we write&#10;for . We also define , for some to be adjusted, and we let be a positive&#10;constant that may vary from one line to the other. Finally we often&#10;refer to the continuous part of defined as \_t := X\_0 + \_0\^t[b\_s&#10;ds]{} + \_0\^t[\_s dW\_s]{}.">
  </outline>
  <outline text="Proof of ([RhoKappa])" _note="We first show the left hand side inequality, that can be reformulated as&#10;. Note that by an immediate application of Hölder’s inequality we have&#10;\_r\^s[\_u\^2du]{}(s-r)\^[1/3]{}ł(\_r\^s[\_u\^3du]{})\^[2/3]{}. Thus,&#10;\_[r,s]{} &amp;=&amp;\&#10;&amp;&amp; = \_[r,s]{}\^[2/3]{}. For the right hand side inequality, we first&#10;consider the domination \_r\^s[\_u\^3du]{}&#10;ł(\_r\^s[\_u\^2du]{})\^[1/2]{}ł(\_r\^s[\_u\^4du]{})\^[1/2]{}, which is&#10;obtained by Cauchy-Schwarz inequality. Then we inject this expression in&#10;and we get \_[r,s]{} &amp;=&amp;\&#10;&amp;&amp; = \_[r,s]{}\^[1/2]{}.">
  </outline>
  <outline text="Estimates for the efficient price" _note="Hereafter, we adopt the following notation convention. For a process&#10;(including the noise process by a slight “abuse of notation”), and we&#10;write , and . Finally, for interpolation purpose we sometimes write the&#10;continuous version , along with the time increment . We introduce the&#10;two following quantities: \_[i,t]{}\^n := (\_[i,t]{}\^n)\^2&#10;-\_[t\_[i-1]{}\^n]{}\^2t\_[i,t]{}\^n, [and]{.nodecor} |\_[i,t]{}\^n :=&#10;ł\[\_[i,t]{}\^n | \_[i-1]{}\^n\]. We have the following estimates&#10;&#10;\[lemmaEstimateX\] We have, for some constant independent of , &amp; &amp;&#10;Ln\^[-p/2]{}(U\_i\^n)\^[p/2]{} , \[eqDeltaX\]\&#10;||\_[i,t]{}\^n | &amp; &amp; L n\^[-3/2]{} (U\_i\^n)\^[3/2]{} , \[eqZetaTilde\]\&#10;ł\[ ł. ł(\_[t,i]{}\^n)\^p | \_[i-1]{}\^n \] &amp; &amp; Ln\^[-p]{}&#10;(U\_i\^n)\^[p]{}, \[eqZeta2\]\&#10;&amp; &amp; Ln\^[-3p/2]{}(U\_i\^n)\^[3p/2]{}. \[eqDevSigma\]&#10;&#10;For (\[eqDeltaX\]), this is a consequence of the fact that by the&#10;conditional Burkholder-Davis-Gundy inequality, we have ł\[ ł. \_[t ]{}&#10;ł| \_[[t\_[i-1]{}\^n]{} t]{}\^[[t\_[i]{}\^n]{} t]{}[\_sdW\_s]{}|\^p |&#10;\_[i-1]{}\^n \] &amp;&amp; ł\[ ł. \_[t ]{} ł|\_[[t\_[i-1]{}\^n]{}&#10;t]{}\^[[t\_[i]{}\^n]{} t]{}[\_s\^2ds]{}|\^[p/2]{} | \_[i-1]{}\^n \]\&#10;&amp;&amp; L ł([t\_[i]{}\^n]{} -[t\_[i-1]{}\^n]{})\^[p/2]{}. Since is bounded by&#10;assumption **(H)**, and since , we get (\[eqDeltaX\]). The other&#10;estimates are straightforwardly obtained using the same line of&#10;reasoning and Itô formula.">
  </outline>
  <outline text="Proof of Theorem [CLTQMLE] and Theorem [CLTQMLEjumps]" _note="We adopt the general setting introduced in Section \[robustness\] and&#10;Section \[simplification\]. We start by showing the consistency of the&#10;QMLE along with other estimates in the case . We then adapt and combine&#10;those results in the case to derive the central limit theorem stated in&#10;Theorem \[CLTQMLEjumps\]. As a byproduct, Theorem \[CLTQMLE\] will also&#10;be proven.&#10;&#10;When , we recall that for any , , we have, up to a constant term l\_n()&#10;= - [log det]{.nodecor}() -Y\^T \^[-1]{} Y, with . The exact definition&#10;of the coefficients can be found in e.g. (28), p. 245 of , replacing by&#10;. We define the approximate log-likelihood random field as |[l]{}\_n() =&#10;-[log det]{.nodecor}() -[Tr]{.nodecor}ł(\^[-1]{} ł{\_0\^c + \_0\^d } ),&#10;with \_0\^c &amp;=&amp; (&#10;&#10;\_0\^[t\_[1]{}\^n]{}[\_s\^2ds]{} + 2 a\^2 &amp; - a\^2 &amp; 0 &amp; &amp; 0\&#10;- a\^2 &amp; \_[t\_[1]{}\^n]{}\^[t\_[2]{}\^n]{}[\_s\^2ds]{} + 2 a\^2 &amp; -&#10;a\^2 &amp; &amp;\&#10;0 &amp; - a\^2 &amp; \_[t\_[2]{}\^n]{}\^[t\_[3]{}\^n]{}[\_s\^2ds]{}+ 2 a\^2 &amp; &amp;&#10;0\&#10;&amp; &amp; &amp; &amp; - a\^2\&#10;0 &amp; &amp; 0 &amp; - a\^2 &amp;&#10;\_[t\_[N\_n-1]{}\^n]{}\^[t\_[N\_n]{}\^n]{}[\_s\^2ds]{} + 2 a\^2&#10;&#10;),\&#10;and \_0\^d = [diag]{.nodecor}ł( \_[0 &amp;lt; st\_1\^n]{} [J\_s\^2]{}&#10;,\_[t\_1\^n &amp;lt; st\_2\^n]{} [J\_s\^2]{},, \_[t\_[N\_n-1]{}\^n &amp;lt;&#10;st\_[N\_n]{}\^n]{} [J\_s\^2]{}). We further define the diagonal scaling&#10;matrix \_n = [diag]{.nodecor}(N\_n\^[1/2]{},N\_n), and consider for the&#10;scaled score functions We start by showing the consistency of the QMLE.&#10;Before stating the result, we give a few definitions. For a matrix , we&#10;associate the matrix and whose components respectively satisfy \_[i,j]{}&#10;= a\_[i+1,j]{} - a\_[i,j]{}, and \_[i,j]{} = \_[i,j+1]{} - \_[i,j]{} =&#10;a\_[i+1,j+1]{} - a\_[i,j+1]{} + a\_[i,j]{} - a\_[i+1,j]{}, with the&#10;convention when or . This will be useful to disentangle some quadratic&#10;expressions using the following result.&#10;&#10;\[lemmaTransfo\] Let , with , . We define , and the same way. Then we&#10;have the by-part summation identities&#10;&#10;We now show a preliminary lemma to get the consistency of the QMLE.&#10;&#10;\[lemmaUnifConvScore\] (Asymptotic score) For any , let \_() = ł(&#10;&#10;-ł(\_0\^T\_s\^2ds+ \_[0&amp;lt;sT]{}J\_s\^2 -\^2T)-ł(a\^2-a\_0\^2)\&#10;ł(a\^2-a\_0\^2)&#10;&#10;). We have&#10;&#10;\_ ł|\_n()-\_()| 0. \[eqConsistency0\]&#10;&#10;We start by treating the case where the jump part . We have the&#10;decomposition with R\_n() &amp; = &amp; ł(&#10;&#10;ł{ Y\^T Y - [tr]{.nodecor} ł( ł{\_0\^c + \_0\^d })}\&#10;ł{ Y\^T Y - [tr]{.nodecor} ł( ł{\_0\^c + \_0\^d })}&#10;&#10;). By a straightforward adaptation of the proof of Lemma 1-2 and Theorem&#10;4 in , we have immediately that uniformly in the parameters since the&#10;step size of the observation grid by (\[eqStepsize\]). Thus it is&#10;sufficient to show that we have Equality (\[eqConsistency0\]) is then a&#10;direct consequence of equations (38) and (40) pp. 247-248 in that are&#10;obtained following exactly the same proof as pp.247-248 for an irregular&#10;grid such that .&#10;&#10;When there are jumps, there is an additional term in (\[Psin\]) which is&#10;equal to A\_n() = ł(&#10;&#10;ł{ł(J\^n)\^T J\^n + 2ł(J\^n)\^T ł{\^n + \^n} }\&#10;ł{ł(J\^n)\^T J\^n + 2ł(J\^n)\^T ł{\^n + \^n} }&#10;&#10;), so that it is sufficient to show that we have \_ ł|A\_n()+ł(&#10;&#10;\_[0&amp;lt;sT]{}J\_s\^2\&#10;0&#10;&#10;)| 0. We first compute the limit of the term . Recalling that is the -th&#10;index of , we provide the following decomposition: ł(J\^n)\^T J\^n =&#10;\_[i = 1]{}\^[N\_n]{} ł(J\_[i]{}\^n)\^2 + \_[1 j &amp;lt; i N\_n]{}[&#10;J\_[i]{}\^nJ\_[j]{}\^n]{}. Now, we define the jump times of , where is&#10;the random number of jumps of on . Since is finite, there exists a&#10;random number such that for we have \_[i = 1]{}\^[N\_n]{}&#10;ł(J\_[i]{}\^n)\^2 = \_[k=1]{}\^[N\^J]{} J\_[\_k]{}\^2. By direct&#10;calculation from the expression of the coefficients of in (28) p. 245 in&#10;, we easily deduce that for each we have uniformly in . Since the sum is&#10;finite, this yields the uniform convergence \_[ł| ł(J\^n)\^T J\^n +&#10;\_[0&amp;lt;sT]{}J\_s\^2 |]{} 0. By a similar argument, we also have for&#10;that exponentially so that we have uniformly. As for , on the one hand&#10;the same computation yields that the leading term of is&#10;\_[k=1]{}\^[N\^J]{} \_[O\_(1)]{} J\_[\_k]{} \_[o\_(1)]{}, where , so&#10;that as the sum is finite the expression is negligible. On the other&#10;hand, we also have by Lemma \[lemmaTransfo\] that the leading term of&#10;the noise part is -\_[k=1]{}\^[N\^J]{} \_[O\_(N\_n\^[-1/2]{})]{}&#10;J\_[\_k]{} \_[O\_(1)]{} since by direct calculation. Finally, similar&#10;reasoning shows that the second component of is negligible because of&#10;the scaling in instead of , and we are done.&#10;&#10;Now we turn to the consistency of the QMLE.&#10;&#10;(consistency). If is the QMLE, we have&#10;&#10;\_n \_0:= ł(\_0\^2, a\_0\^2), \[eqConsistency\] where we recall that .&#10;\[thmConsistencyH1\]&#10;&#10;To get (\[eqConsistency\]), it is sufficient to have \_ ł|\_n()-\_()| 0,&#10;which has been proven in Lemma \[lemmaUnifConvScore\], and for any&#10;\_[:-\_0]{} \_()\^2&amp;gt;0 = \_(\_0 )\^2 []{.nodecor} [-a.s]{.nodecor},&#10;\[conditionIdentifiability\] by a classical statistical argument (see&#10;e.g. , Theorem 5.9). Given the form of , the equality is immediate. Note&#10;also that the left hand side inequality of&#10;(\[conditionIdentifiability\]) will be automatically satisfied if we&#10;show that as soon as by a continuity argument since is compact. Let us&#10;then take such that , and assume first that . In that case, we have&#10;which leads to a contradiction. Similarly, the first component of leads&#10;to the domination so that we can conclude .&#10;&#10;We now turn to the convergence of the Fisher information related to our&#10;likelihood field. Let and be the scaled Hessian matrices of the&#10;likelihood fields, defined for any as&#10;&#10;H\_n() = -\_n\^[-1/2]{} \_n\^[-1/2]{} [and]{.nodecor} \_n() =&#10;-\_n\^[-1/2]{} \_n\^[-1/2]{}. \[defFisherMatrix\]&#10;&#10;(Asymptotic Fisher information) \[lemmaFisherConsistency\] Let be the&#10;matrix (\_0) = ł(&#10;&#10;&amp; 0\&#10;0 &amp;&#10;&#10;). We have, for any ball centred on , shrinking to , \_[\_n V\_n]{} ł&#10;H\_n(\_n) - (\_0) 0.&#10;&#10;First note that a small adaptation of Lemma 1-2 with second order&#10;derivatives of from yields since . Now, , bottom of p. 247 and after&#10;equation (41) on p. 248, can be easily adapted to our case replacing by&#10;as in the previous lemma, so that we have \_ ł{\_n() - \_()} 0, with&#10;\_() := ł(&#10;&#10;+ + &amp;0\&#10;0&amp;-&#10;&#10;). It is immediate to check that \_(\_0) = ł(&#10;&#10;&amp;0\&#10;0&amp;&#10;&#10;).&#10;&#10;We now adopt similar notations to in the proof of Lemma 3 (p. 248) and&#10;define the processes involved in the derivation of the central limit&#10;theorem. For , and , we define M\_[1]{}\^[()]{}(t) &amp; := &amp;&#10;\_[i=1]{}\^[N\_n(t)]{}[ł{ł(X\_[i,t]{}\^n)\^2 - \_[t\_[i-1]{}\^n t&#10;]{}\^[t\_[i]{}\^n t]{}[\_s\^2ds]{} - \_[t\_[i-1]{}\^n t &amp;lt;s&#10;t\_[i]{}\^n t]{} J\_s\^2}]{}, \[eqM1\]\&#10;M\_[2]{}\^[()]{}(t) &amp; := &amp; \_[i=1]{}\^[N\_n(t)]{} ł{\_[1 j &amp;lt; i]{}&#10;X\_[j,t]{}\^n} X\_[i,t]{}\^n, \[eqM2\]\&#10;M\_[3]{}\^[()]{}(t) &amp; := &amp;- 2\_[i=0]{}\^[N\_n(t)]{} ł{\_[j =&#10;1]{}\^[N\_n(t)]{} X\_[j,t]{}\^n} \_[t\_i\^n]{}, \[eqM3\]\&#10;M\_[4]{}\^[()]{}(t) &amp; := &amp; \_[i=0]{}\^[N\_n(t)]{}[ł{\_[t\_i\^n]{}\^2 -&#10;a\_0\^2} ]{} + 2\_[i=0]{}\^[N\_n(t)]{} ł{\_[0 j &amp;lt; i]{} \_[t\_j\^n]{}}&#10;\_[t\_i\^n]{}, \[eqM4\] where in all the definitions&#10;(\[eqM1\])-(\[eqM4\]), the terms involving the parameters such as , , ,&#10;are evaluated at point , for some . We also define the two-dimensional&#10;vectors for . Note that we have the key decomposition&#10;2\_n\^[1/2]{}ł{\_n() - |\_n()} = \_n\^[-1/2]{}&#10;ł{M\_1(T)+2M\_2(T)+M\_3(T)+M\_4(T) }. In the next few lemmas we&#10;investigate the limit of each one of those terms. In the presence of&#10;jumps and random observation times, we will see that they are not mere&#10;extensions of Lemma 3 in and that additional variance terms appear in&#10;the limits. We start by .&#10;&#10;\[lemmaM1\] We have&#10;&#10;\_n\^[-1/2]{} M\_1(T) 0.&#10;&#10;We have to show and . We start with the case where . We are going to&#10;show that for any we actually have . To do so, note that we can write&#10;M\_1\^[()]{}(T) = \_[i=1]{}\^[N\_n]{}[\_i\^n]{}, where \_i\^n =&#10;ł{ł(X\_[i]{}\^n)\^2 - \_[t\_[i-1]{}\^n ]{}\^[t\_[i]{}\^n]{}[\_s\^2ds]{}&#10;}. Now, since , . Moreover, , thus by Lemma 2.2.11 in , it is sufficient&#10;to show that . By Burkholder-Davis-Gundy inequality, we have&#10;N\_n\^[-1/2]{}\_[i=1]{}\^[N\_n]{}ł\[(\_i\^n)\^2 | \_[i-1]{}\^n \] &amp;&amp;&#10;4N\_n\^[-1/2]{}\_[i=1]{}\^[N\_n]{}ł()\^2 \_[[t\_[i-1]{}\^n]{}&#10;]{}\^[[t\_[i]{}\^n]{} ]{}[ł\[ł.ł(X\_[i,s]{}\^n)\^2\_s\^2&#10;|\_[i-1]{}\^n\]ds]{}\&#10;&amp;&amp; K N\_n\^[1/2]{}n\^[-1+]{}\_[i=1]{}\^[N\_n]{} ([t\_[i]{}\^n]{} -&#10;[t\_[i-1]{}\^n]{} )\&#10;&amp;&amp; K N\_n\^[1/2]{}n\^[-1+]{} \^ 0, where we have used the fact that&#10;uniformly in . In the presence of jumps, it remains to show that the&#10;additional terms and are negligible. From the finite activity property,&#10;note that the first one is identically for sufficiently large. Again,&#10;for sufficiently large, defining the finite number of jumps of on , we&#10;can write the second term as&#10;2N\_n\^[-1/4]{}\_[k=1]{}\^[N\^J]{}\_[O\_(N\_n\^[1/2]{})]{} J\_[\_k]{}&#10;\_[O\_(n\^[-1/2+1/2]{})]{} 0. where is such that , and where we have&#10;used **(H)**. This concludes the proof.&#10;&#10;\[lemmaM2\] We have -stably in law that&#10;&#10;N\_n\^[-1/4]{} M\_[2]{}\^[(\^2)]{}(T) ł( 0,&#10;\_0\^T[\_s\^[-1]{}ds]{}ł{\_0\^T[\_s\^4 \_sds]{} + \_[0 s T]{} J\_s\^2&#10;(\_s\^2\_s + \_[s-]{}\^2\_[s-]{}) }) and N\_n\^[-1/2]{}&#10;M\_[2]{}\^[(a\^2)]{}(T) 0.&#10;&#10;As usual, we start by the case with no jumps, that is . We show the&#10;result for . The proof is conducted in three steps.&#10;&#10;**Step 1.** We consider , and we define as \_2\^[(\^2)]{}(t) :=&#10;\_[i=1]{}\^[N\_n]{} ł{\_[(i- L\_n) 1 j &amp;lt; i]{} \_[j,i-L\_n-1,t]{}\^n}&#10;\_[i,i-L\_n-1,t]{}\^n, \[eqM2tilde\] that is when the increments are&#10;replaced by variables of the form , where is the value of the volatility&#10;process at the beginning of the truncated sum. We show that we have . We&#10;decompose N\_n\^[-1/4]{}ł{M\_2\^[(\^2)]{} -\_2\^[(\^2)]{} } =&#10;R\_n\^[(1)]{}+R\_n\^[(2)]{}+R\_n\^[(3)]{}, with R\_n\^[(1)]{} &amp; = &amp;&#10;N\_n\^[-1/4]{} \_[i=1]{}\^[N\_n]{}\_[1 j &amp;lt; i - L\_n]{} X\_[j,t]{}\^n&#10;X\_[i,t]{}\^n, \[eqR1\]\&#10;R\_n\^[(2)]{} &amp; = &amp; N\_n\^[-1/4]{} \_[i=1]{}\^[N\_n]{}\_[(i- L\_n) 1 j&#10;&amp;lt; i ]{} X\_[j,t]{}\^n (X\_[i,t]{}\^n - \_[i,i-L\_n-1,t]{}\^n),&#10;\[eqR2\]\&#10;R\_n\^[(3)]{} &amp; = &amp; N\_n\^[-1/4]{} \_[i=1]{}\^[N\_n]{}\_[(i- L\_n) 1 j&#10;&amp;lt; i]{} ( X\_[j,t]{}\^n - \_[j,i-L\_n-1,t]{}\^n )&#10;\_[i,i-L\_n-1,t]{}\^n. \[eqR3\] Now, proving that is negligible is&#10;immediate because when , we have the domination for some so that by an&#10;easy application of Cauchy-Schwarz inequality and estimates from Lemma&#10;\[lemmaEstimateX\] we get . Now we show the negligibility of . Assume&#10;first that has no jumps, i.e . being a sum of martingale increments, it&#10;is sufficient to show that where . Introducing , , we thus need to show&#10;that Itô’s formula applied to when yields so that defining for , we now&#10;show N\_n\^[-1/2]{}\_[i=1]{}\^[N\_n]{}\_ 0. \[convDeltaL\] For , we have&#10;by **(H)**, and thus (\[convDeltaL\]) boils down to showing that&#10;N\_n\^[-1/2]{}n\^[-2+2]{}L\_n\_[i=1]{}\^[N\_n]{}\_ 0. Using for , we&#10;deduce N\_n\^[-1/2]{}n\^[-2+2]{}L\_n\_[i=1]{}\^[N\_n]{}\_&amp;=&amp;&#10;N\_n\^[-1/2]{}n\^[-2+2]{}L\_n\_[i=1]{}\^[N\_n]{}&#10;\_[j=(i-L\_n)1]{}\^[i-1]{}[ł()\^2 \_ł\[ł(X\_[j]{}\^n)\^2\]]{}\&#10;&amp;&amp; L N\_n\^[-1/2]{}n\^[-3+3]{}L\_n\_[i=1]{}\^[N\_n]{}&#10;\_[j=(i-L\_n)1]{}\^[i-1]{}[ł()\^2]{}\&#10;&amp;&amp; L N\_n\^[2]{} n\^[-3+3]{}L\_n 0, where we have used that by direct&#10;calculation we have , and that . For , we split (\[convDeltaL\]) into&#10;two terms&#10;&#10;N\_n\^[-1/2]{}\_[i=1]{}\^[N\_n]{}\_= P\_n\^[(1)]{}+ P\_n\^[(2)]{},&#10;\[decompositionP1P2\] where P\_n\^[(1)]{} &amp;= &amp;&#10;N\_n\^[-1/2]{}\_\_[i=1]{}\^[N\_n]{} \_[(i- L\_n) 1 j &amp;lt; i]{} ł()\^2&#10;ł(X\_[j]{}\^n)\^2 \_[i,L\_n,t]{}\^[(1)]{},\&#10;P\_n\^[(2)]{} &amp; = &amp; N\_n\^[-1/2]{}\_\_[i=1]{}\^[N\_n]{} \_[(i- L\_n) 1 j&#10;k &amp;lt; i]{} X\_[j]{}\^nX\_[k]{}\^n \_[i,L\_n,t]{}\^[(1)]{}. We have by&#10;Cauchy-Schwarz inequality P\_n\^[(1)]{} &amp;&amp;&#10;N\_n\^[-1/2]{}\_[i=1]{}\^[N\_n]{} \_[(i- L\_n) 1 j &amp;lt; i]{} ł()\^2 ł(\_&#10;ł\[ ł(X\_[j]{}\^n)\^4 \] \_ \[(\_[i,L\_n,t]{}\^[(1)]{})\^2\])\^[1/2]{},\&#10;&amp;&amp; L N\_n\^[-1/2]{}n\^[-3+3]{}L\_n \_[i=1]{}\^[N\_n]{} \_[(i- L\_n) 1 j&#10;&amp;lt; i]{} ł()\^2\&#10;&amp;&amp; L N\_n\^[2]{}n\^[-3+3]{}L\_n 0, as by (\[eqDeltaX\]), and by the same&#10;estimate as for (\[eqDeltaX\]) for the Itô semimartingale . For , we&#10;first note that for we have ł|\_ł\[X\_k\^n X\_j\^n&#10;\_[i,L\_n,t]{}\^[(1)]{} \]| &amp;&amp; \_ł\[|X\_k\^n|&#10;ł|\_[[t\_[i-1]{}\^n]{}t]{}\^[[t\_[i]{}\^n]{}t]{} ł\[ł. X\_j\^n&#10;u\_[i,L\_n,s]{}\^[(1)]{} | \_[j-1]{}\^n\]ds | \]\&#10;&amp;&amp; L \_ł\[|X\_k\^n| \_[[t\_[i-1]{}\^n]{} t]{}\^[[t\_[i]{}\^n]{} t]{}&#10;ł|ł\[ł. \_[[t\_[j-1]{}\^n]{}]{}\^[[t\_[j]{}\^n]{}]{} v\_[i,L\_n,u]{}&#10;\_u\^[(1)]{} \_u du | \_[j-1]{}\^n\]ds| \]\&#10;&amp;&amp; L n\^[-3 + 3]{} L\_n\^[1/2]{}, where the last step is obtained using&#10;**(H)** as for the previous estimates. Overall, we get P\_n\^[(2)]{} &amp;&amp;&#10;L N\_n\^[-1/2]{}n\^[-3 + 3]{} L\_n\^[1/2]{} \_[i=1]{}\^[N\_n]{}&#10;\_[(i-L\_n) 1 j k &amp;lt; i]{}\&#10;&amp;&amp; L N\_n\^[-1/2]{}n\^[-3 + 3]{} L\_n\^[3/2]{} \_[i=1]{}\^[N\_n]{}&#10;\_[(i-L\_n)1 j &amp;lt; i]{} ł()\^2\&#10;&amp;&amp; L N\_n\^[2]{}n\^[-3 + 3]{} L\_n\^[3/2]{} 0. Finally, when , we write&#10;the same decomposition as (\[decompositionP1P2\]), and we note that the&#10;exact same calculation as in the case for remains valid. Moreover,&#10;following closely the calculation above, we get by orthogonality of the&#10;Brownian motions and . When has jumps of finite activity, we easily show&#10;as for previous calculations that an additional negligible term appears&#10;in , and thus combining all those results we have . Finally, is proven&#10;following the same line of reasoning as for .\&#10;&#10;**Step 2.** We are going to apply Theorem 2-1 p. 238 from to the&#10;continuous martingale . Condition (2.8) is automatically satisfied with&#10;. We now show the variance condition (2.9). This boils down to showing&#10;that there exists an increasing limit process such that for any&#10;&#10;ł\_2\^[(\^2)]{},\_2\^[(\^2)]{} \_t C\_t, \[convBracketM2\] and . We&#10;introduce L\_n\^[(1)]{} &amp; := &amp; N\_n\^[-1/2]{}\_[i=1]{}\^[N\_n]{}\_[&#10;(i-L\_n) 1 j &amp;lt; i]{} [ł()\^2&#10;\_[[t\_[i-L\_n-1]{}\^n]{}]{}\^4ł(W\_[j,t]{}\^n)\^2[t\_[i,t]{}\^n]{}]{},\&#10;L\_n\^[(2)]{} &amp; := &amp; N\_n\^[-1/2]{}\_[i=1]{}\^[N\_n]{} \_[ (i-L\_n) 1 j&#10;k &amp;lt; i]{} [\_[[t\_[i-L\_n-1]{}\^n]{}]{}\^4W\_[j,t]{}\^nW\_[k,t]{}\^n&#10;[t\_[i,t]{}\^n]{}]{}.\&#10;we have , so that our strategy to show (\[convBracketM2\]) will be to&#10;prove that L\_n\^[(1)]{} &amp; &amp; C\_t \[convLi1\]\&#10;L\_n\^[(2)]{} &amp; &amp; 0.\[convLi2\] For , we have directly that is equal to&#10;N\_n\^[-1]{} \_\_&#10;\_ł\[\_[[t\_[i\_1-L\_n-1]{}\^n]{}]{}\^4\_[[t\_[i\_2-L\_n-1]{}\^n]{}]{}\^4\]&#10;t\_[j,t]{}\^n t\_[k,t]{}\^n t\_[i\_1,t]{}\^nt\_[i\_2,t]{}\^n, where we&#10;have used that for , we have when and , and the expectation is null&#10;otherwise. Now, using the boundedness of and the fact that by assumption&#10;**(H)**, we obtain \_ł\[ł(L\_n\^[(2)]{})\^2 \] L&#10;N\_n\^[-1]{}n\^[-4+4]{}\_\_, which by direct calculation on the&#10;coefficients yields \_ł\[ł(L\_n\^[(2)]{})\^2 \] &amp;&amp; L N\_n\^[-1]{}&#10;n\^[-4+4]{} N\_n\^4 L\_n\&#10;&amp;&amp; N\_n\^[7/2 +]{}n\^[-4+4]{}0, for and small enough. Now we turn to&#10;(\[convLi1\]). We define , and we further decompose into L\_n\^[(1)]{} -&#10;C\_t = \_[i=1]{}\^[6]{} B\_n\^[(i)]{}, with B\_n\^[(1)]{} &amp; = &amp;&#10;N\_n\^[-1/2]{} \_[i=1]{}\^[N\_n]{} \_[ (i-L\_n) 1 j &amp;lt; i]{} [ł()\^2&#10;\_[[t\_[i-L\_n-1]{}\^n]{}]{}\^4 ł(ł(W\_[j,t]{}\^n)\^2 -&#10;t\_[j,t]{}\^n)ł([t\_[i]{}\^n]{}t - [t\_[i-1]{}\^n]{}t)]{},\&#10;B\_n\^[(2)]{} &amp; = &amp;N\_n\^[-1/2]{} \_n \_[i=1]{}\^[N\_n]{} \_[ (i-L\_n) 1&#10;j &amp;lt; i]{} [ł()\^2 \_[[t\_[i-L\_n-1]{}\^n]{}]{}\^4&#10;ł(\_[[t\_[j-1]{}\^n]{}]{}-\_[[t\_[i-L\_n-1]{}\^n]{}]{})U\_j\^nł([t\_[i]{}\^n]{}t&#10;- [t\_[i-1]{}\^n]{}t)]{},\&#10;B\_n\^[(3)]{} &amp; = &amp; N\_n\^[-1/2]{} \_n \_[i=1]{}\^[N\_n]{} \_[ (i-L\_n)&#10;1 j &amp;lt; i]{} [ł()\^2 \_[[t\_[i-L\_n-1]{}\^n]{}]{}\^4&#10;\_[[t\_[i-L\_n-1]{}\^n]{}]{} ł(U\_j\^n - 1 )ł([t\_[i]{}\^n]{}t -&#10;[t\_[i-1]{}\^n]{}t)]{},\&#10;B\_n\^[(4)]{} &amp; = &amp; \_[i=1]{}\^[N\_n]{} ł{N\_n\^[-1/2]{} \_n\_[ (i-L\_n)&#10;1 j &amp;lt; i]{} [ł()\^2 - \_0\^T[\_s\^[-1]{}ds]{}]{}}&#10;\_[[t\_[i-L\_n-1]{}\^n]{}]{}\^4 \_[[t\_[i-L\_n-1]{}\^n]{}]{}&#10;ł([t\_[i]{}\^n]{}t - [t\_[i-1]{}\^n]{}t),\&#10;B\_n\^[(5)]{} &amp; = &amp;&#10;\_0\^T[\_s\^[-1]{}ds]{}\_[i=1]{}\^[N\_n]{}[ł{\_[[t\_[i-L\_n-1]{}\^n]{}]{}\^4&#10;\_[[t\_[i-L\_n-1]{}\^n]{}]{} - \_[[t\_[i]{}\^n]{}]{}\^4&#10;\_[[t\_[i]{}\^n]{}]{} } ł([t\_[i]{}\^n]{}t - [t\_[i-1]{}\^n]{}t)]{},\&#10;B\_n\^[(6)]{} &amp; = &amp;&#10;\_0\^T[\_s\^[-1]{}ds]{}ł{\_[i=1]{}\^[N\_n]{}[\_[[t\_[i]{}\^n]{}]{}\^4&#10;\_[[t\_[i]{}\^n]{}]{} ł([t\_[i]{}\^n]{}t - [t\_[i-1]{}\^n]{}t)]{} -&#10;\_0\^t[\_s\^4\_sds]{}}. Using that if , and otherwise, we obtain the&#10;estimate \_ł\[ł(B\_n\^[(1)]{})\^2\] L N\_n\^[3]{}L\_n n\^[-4+4]{} 0.&#10;Moreover, by the same deviation inequality as (\[eqDeltaX\]) for (recall&#10;that is an Itô semimartingale) we have so that we obtain easily .&#10;Similar computation to that of shows that since and when . is a direct&#10;consequence of the fact that by a direct calculation we have uniformly&#10;in that and that by (\[eqNumberJumps\]), recalling that . is, again a&#10;simple consequence of the deviation inequality (\[eqDeltaX\]) for the&#10;Itô semimartingale , and finally is just the convergence of the Riemann&#10;sum toward the integral limit, and we are done. We show condition&#10;(2.10), i.e. that N\_n\^[-1/4]{} \_2\^[(\^2)]{}, W \_t 0. Note that&#10;N\_n\^[-1/4]{} \_2\^[(\^2)]{}, W \_t = N\_n\^[-1/4]{}&#10;\_[i=1]{}\^[N\_n]{} \_[ (i-L\_n) 1 j &amp;lt; i]{}&#10;\_[[t\_[i-L\_n-1]{}\^n]{}]{}\^2 W\_[j,t]{}\^n t\_[i,t]{}\^n, so that by&#10;a straightforward calculation on the Brownian motion increments we have&#10;\_ł\[ł(\_2\^[(\^2)]{}, W \_t)\^2\] L N\_n\^2 L\_n n\^[-3+3]{} 0.&#10;Moreover, condition (2.11) is satisfied because is continuous. Finally&#10;we show condition (2.12). But note that for any bounded martingale&#10;orthogonal to we have directly \_2\^[(\^2)]{}, \_t = 0 by&#10;(\[eqM2tilde\]), so that all the conditions required for the theorem&#10;hold.&#10;&#10;**Step 3.** In the presence of jumps, for large enough, an additional&#10;term appears in . First, since is of finite activity and by the&#10;Grigelionis decomposition for Itô-semimartingales (see e.g. Theorem&#10;2.1.2 in ), we can assume without loss of generality that the jump times&#10;of are a subset of the support of a Poisson random measure on for some&#10;arbitrary Polish space, adapted to , and with finite intensity measure .&#10;Let thus , ,, be an exhausting sequence for the jumps of . Since is of&#10;finite activity, for sufficiently large we cannot have more than a&#10;single jump on intervals of the form because by assumption **(H)**.&#10;Therefore, if is large enough, after a simple rearrangement of the terms&#10;that contain jumps, and by the previous calculation in the continuous&#10;case, we can write under the form M\_[2]{}\^[(\^2)]{}(t) =&#10;\_2\^[(\^2)]{}(t) + A\_n\^+(t) + A\_n\^-(t) + o\_(1), \[eqM2J\] with&#10;A\_n\^+(t) = \_[p 1]{}[J\_[\_p]{}\_[j = i\_p + 1]{}\^[i\_p + L\_n]{}&#10;\_[j,t]{}\^n ]{} [and]{.nodecor} A\_n\^-(t) = \_[p 1]{}[J\_[\_p]{}\_[j =&#10;i\_p - L\_n ]{}\^[i\_p - 1 ]{} \_[j,t]{}\^n ]{}, where is such that . We&#10;define&#10;&#10;M\_n\^+(t,p) = \_[j = i\_p + 1]{}\^[i\_p + L\_n]{} \_[j,t]{}\^n&#10;[and]{.nodecor} M\_n\^-(t,p) = \_[j = i\_p - L\_n ]{}\^[i\_p - 1 ]{}&#10;\_[j,t]{}\^n, along with the following infinite dimensional vector such&#10;that , and are i.i.d standard normal random variables. We can assume&#10;that and are rich enough to include such random variables information&#10;without loss of generality, since we can always construct a very good&#10;filtered extension as explained in pp. 36-37 of . Now define V\_:=&#10;\_0\^T[\_s\^[-1]{}ds]{}, M\_\^+(p) :=&#10;\_[\_p]{}\_[\_p]{}\^[1/2]{}V\_\^[1/2]{}R\_\^+(p) [and]{.nodecor}&#10;M\_\^-(p) := \_[\_p-]{}\_[\_p-]{}\^[1/2]{}V\_\^[1/2]{} R\_\^-(p), and :=&#10;C\_T\^[1/2]{} G, where was defined in (\[convBracketM2\]). We are going&#10;to show that -stably in law, we have the convergence N\_n\^[-1/4]{}&#10;(\_[2]{}\^[(\^2)]{}(T), (M\_n\^+(T,p),M\_n\^-(T,p))\_[p 1]{}) (,&#10;(M\_\^+(p),M\_\^-(p))\_[p 1]{}). \[vectorStable\] As the subset of&#10;finite dimensional cylinders is a convergence determining class for the&#10;product topology of , it is sufficient to show that the above&#10;convergence holds for all finite families of the form , . Now, let us&#10;consider the filtration which is the smallest filtration containing and&#10;the jump times of , . By independence of and the Wiener process , is&#10;also a continuous Itô process with respect to the filtration , so that&#10;is a multi-dimensional continuous -martingale. Now, for large enough and&#10;by the finite activity property, we have for any , M\_n\^+(.,p\_i),&#10;M\_n\^+(.,p\_j) \_t = M\_n\^-(.,p\_i), M\_n\^-(.,p\_j) \_t = 0&#10;[a.s,]{.nodecor} and M\_n\^+(.,p\_i), M\_n\^-(.,p\_i) \_t = 0&#10;[a.s.]{.nodecor} Moreover&#10;N\_n\^[-1/2]{}M\_n\^+(.,p\_i),M\_n\^+(.,p\_i)\_t = N\_n\^[-1/2]{} \_[j =&#10;i\_p +1]{}\^[i\_p + L\_n]{} ł()\^2 \_[[t\_[j-1]{}\^n]{}&#10;t]{}\^[[t\_[j]{}\^n]{} t]{}[\_s\^2ds]{}, since the random index is&#10;-measurable. By a similar (but easier) calculation than for above, we&#10;have N\_n\^[-1/2]{}M\_n\^+(.,p\_i),M\_n\^+(.,p\_i)\_T&#10;\_[\_[p\_i]{}]{}\^2\_[\_[p\_i]{}]{} \_0\^T[\_s\^[-1]{}ds]{}, and also&#10;N\_n\^[-1/2]{}M\_n\^-(.,p\_i),M\_n\^-(.,p\_i)\_T&#10;\_[\_[p\_i]{}-]{}\^2\_[\_[p\_i]{}-]{} \_0\^T[\_s\^[-1]{}ds]{}. Finally&#10;we show the negligibility of and . We have&#10;&#10;N\_n\^[-1/2]{} \_2\^[(\^2)]{} ,M\_n\^+(.,p\_i)\_t = N\_n\^[-1/2]{}&#10;\_[j=i\_p + 1]{}\^[i\_p + L\_n]{} \_[k=(j-L\_n) 1 ]{}\^[j-1]{}&#10;\_[k,t]{}\^n \_[j-L\_n-1]{}\^2 t\_[j,t]{}\^n, so that by Assumption&#10;**(H)** we have bounded by &amp; &amp; L N\_n\^[-1]{} n\^[-2+2]{} \_[j\_1,j\_2 =&#10;i\_p + 1]{}\^[i\_p + L\_n]{} \_[k = (j\_1 j\_2) - L\_n 1 ]{}\^[j\_1 j\_2&#10;-1]{} \_ł\[ ł(\_[k,t]{}\^n)\^2\]\&#10;&amp;&amp; L N\_n\^[-1]{} n\^[-3+3]{}\_[j\_1,j\_2 = i\_p + 1]{}\^[i\_p + L\_n]{}&#10;\_[k = (j\_1 j\_2) - L\_n 1 ]{}\^[j\_1 j\_2 -1]{}\&#10;&amp;&amp; L N\_n\^2 L\_n n\^[-3+3]{} 0, and thus the bracket is negligible. By&#10;a similar calculation we get that the bracket involving is also&#10;negligible. Moreover, the convergence of was shown in&#10;(\[convBracketM2\]). Finally, as above we easily check the bracket of&#10;each martingale with either or a bounded martingale orthogonal to is&#10;negligible so that by another application of Theorem 2-1 in we have&#10;(\[vectorStable\]). From the representation&#10;N\_n\^[-1/4]{}M\_2\^[(\^2)]{}(T) = N\_n\^[-1/4]{} ł{\_2\^[(\^2)]{}(T) +&#10;\_[p 1]{} J\_[\_p]{} (M\_n\^+(T,p)+M\_n\^+(T,p)) } + o\_(1), along with&#10;the fact that is finite, we deduce by the stable convergence&#10;(\[vectorStable\]) that (and A FORTIORI ) stably in law&#10;N\_n\^[-1/4]{}M\_2\^[(\^2)]{}(T) + \_[p 1]{} J\_[\_p]{}&#10;(M\_\^+(p)+M\_\^-(p)), which is equal to the claimed distribution.&#10;&#10;Finally, to show the convergence , note that and are equivalent up to a&#10;constant term so that all the above computations apply, and thus the&#10;scaling in instead of yields the negligibility of this term.&#10;&#10;Before turning to the limiting distribution of the other terms, we&#10;recall that for a -field , a random vector and a sequence of random&#10;vectors in , we say that converges in law toward conditioned on if we&#10;have for any&#10;&#10;ł\[ł.e\^[iu\^TZ\_n]{} | \] ł\[ł.e\^[iu\^T Z]{} | \]. Moreover, we recall&#10;in the following proposition a key result to combine stable convergence&#10;and conditional convergence. The proof of the result can be consulted in&#10;(proof of Proposition 5 on p. 1524).&#10;&#10;\[propositionConditionalStable\] Let be a given sub--field, and let and&#10;be sequences of random vectors, such that each is -measurable and the&#10;sequence converges -stably toward a limiting distribution , and&#10;converges in law conditioned on to some . Then -stably in distribution.&#10;&#10;\[lemmaM3\] We have conditioned on the convergence in distribution&#10;N\_n\^[-1/4]{} M\_[3]{}\^[(\^2)]{}(T) ł( 0, ), and N\_n\^[-1/2]{}&#10;M\_[3]{}\^[(a\^2)]{}(T) 0, \[eqM3a\] where we recall the definition .&#10;&#10;We start with . We apply a conditional version of Theorem 5.12 from (p.&#10;92). Accordingly, we note that can be written as&#10;N\_n\^[-1/4]{}M\_[3]{}\^[(\^2)]{}(T) = \_[i=0]{}\^[N\_n]{} \_i\^n, where&#10;, are rowwise conditionally independent and centered given . To get the&#10;theorem, it is thus sufficient to show that \_[i=1]{}\^[N\_n]{} ł\[ł.&#10;ł(\_i\^n)\^2 |\_T\] ł{\_0\^T\_s\^2ds + \_[0sT]{}J\_s\^2},&#10;\[convVarianceM3\] and the Lindeberg condition, for any ,&#10;\_[i=0]{}\^[N\_n]{} ł\[ł. ł(\_i\^n)\^2 \_[{|\_i\^n| }]{} |\_T\] 0. For&#10;(\[convVarianceM3\]), we can write with T\_n\^[(1)]{} = 4a\_0\^2&#10;N\_n\^[-1/2]{} \_[i=0]{}\^[N\_n]{}\_[j=1]{}\^[N\_n]{}&#10;ł()\^2ł(X\_j\^n)\^2, and T\_n\^[(2)]{} =4a\_0\^2 N\_n\^[-1/2]{}&#10;\_[i=0]{}\^[N\_n]{}\_[jk=1]{}\^[N\_n]{} X\_j\^nX\_k\^n, and using same&#10;techniques as for the proof of Lemma \[lemmaM2\] we easily get by direct&#10;calculation on the coefficients that we have , and . As for the&#10;Lindeberg condition, it is sufficient to notice that by independence of&#10;the Brownian increments and similar computation we have . Finally, for ,&#10;all the previous calculation holds but now the scaling in implies that .&#10;&#10;\[lemmaM4\] We have conditioned on the convergence in distribution&#10;\_n\^[-1/2]{} M\_4(T) ł(0,ł(&#10;&#10;&amp;0\&#10;0&amp; +&#10;&#10;)).&#10;&#10;This is an immediate adaptation of (45) and (47) pp.248-249 in&#10;conditioned on in lieu of , since is independent of .&#10;&#10;We consider now the general case , and accordingly we define for the&#10;local QMLE , and , the score functions on the block where all quantities&#10;are taken in the time interval . We also introduce the notation , and .&#10;The next lemma states the limit distribution of the vector . Finally we&#10;introduce the scaling factors along with the global scaling matrix .&#10;&#10;\[lemmaCLTPsi\] We have for any , taking , stably in , the convergence&#10;in distribution&#10;&#10;\_n\^[1/2]{}ł{\_n() - \_n()} ł(0,ł(&#10;&#10;V\_[(1)]{} &amp; 0 &amp; &amp; 0\&#10;0 &amp; V\_[(2)]{} &amp; 0 &amp;\&#10;&amp;0&amp;&amp;\&#10;0&amp;&amp;&amp;V\_[(B)]{}&#10;&#10;)), where for , is the two dimensional matrix defined by V\_[(i)]{} :=&#10;ł(&#10;&#10;ł(+ + ) &amp; 0\&#10;0 &amp; +&#10;&#10;), with and we recall that&#10;&#10;First, for , we define the processes following the definitions&#10;(\[eqM1\])-(\[eqM4\]) adapted to the time interval of length .&#10;Accordingly, for , we denote by the vector process , and we note that we&#10;have the decomposition&#10;&#10;2\_n\^[1/2]{}ł{\_n() - \_n()} = \_n\^[-1/2]{}&#10;ł{\_1(T)+2\_2(T)+\_3(T)+\_4(T) }. For , we consider the two terms and .&#10;By independence of with the other processes we deduce that the&#10;conditional covariance term between those two processes is null. We use&#10;this fact along with the marginal convergences obtained in Lemma&#10;\[lemmaM3\] and Lemma \[lemmaM4\] to obtain the convergence in law&#10;conditioned on&#10;&#10;\_[n,(i)]{}\^[-1/2]{} ł{M\_[3,(i)]{}(T)+M\_[4,(i)]{}(T) } ł(0,ł(&#10;&#10;ł( + ) &amp; 0\&#10;0 &amp; +&#10;&#10;)), where . Now, by Slutsky’s lemma, Lemma \[lemmaM1\] and Lemma&#10;\[lemmaM2\] we also have the -stable convergence in distribution&#10;Finally, by application of Proposition \[propositionConditionalStable\]&#10;with sub--field since is -measurable, we deduce the joint -stable&#10;convergence of hence the convergence of toward a mixed normal&#10;distribution of random variance . Finally, as blocks are non&#10;overlapping, we deduce that for any , for any the martingales and are&#10;orthogonal so that we have automatically the joint convergence of to a&#10;mixed normal with block diagonal random variance matrix whose&#10;submatrices are , and we are done.&#10;&#10;Finally, we derive a central limit theorem for to the limit , and as a&#10;byproduct Theorem \[CLTQMLEjumps\] (and Theorem \[CLTQMLE\]).&#10;&#10;We have -stably in law that \_n\^[1/2]{}ł{\_n-\_0 }ł(0,ł(&#10;&#10;V\_[(1)]{}\^[’]{} &amp; 0 &amp; &amp; 0\&#10;0 &amp; V\_[(2)]{}\^[’]{} &amp; 0 &amp;\&#10;&amp;0&amp;&amp;\&#10;0&amp;&amp;&amp;V\_[(B)]{}\^[’]{}&#10;&#10;)), where for , is the two dimensional matrix defined by&#10;V\_[(i)]{}\^[’]{} := ł(&#10;&#10;a\_0ł( + ) &amp; 0\&#10;0 &amp; 2a\_0\^[4]{} + [cum]{.nodecor}\_4\[\]\&#10;&#10;). In particular, Theorem \[CLTQMLEjumps\] (and Theorem \[CLTQMLE\])&#10;hold.&#10;&#10;First, note that we can easily extend Lemma \[lemmaCLTPsi\] to get a&#10;central limit theorem at the point for by a generalization of Slutsky’s&#10;Lemma for stably convergent sequences (see e.g. Theorem 3.18 (b) in ),&#10;where now the submatrices in the asymptotic variance of the mixed normal&#10;distribution have the form&#10;&#10;V\_[(i)]{} = ł(&#10;&#10;ł(+ ) &amp; 0\&#10;0 &amp; +\&#10;&#10;). To derive the CLT for the -dimensional estimator , we follow the&#10;standard procedure and expand the score function around . Thus, starting&#10;from the first order conditions on the score functions, we have&#10;&#10;0 = \_nł(\_n ) = \_n(\_0) + \_n\^[-1/2]{} \_n(\_n)\_n\^[1/2]{}(\_n -&#10;\_0), \[eqTaylorScore\] for some , and where is the block diagonal&#10;matrix with submatrices , and for , is the scaled Hessian matrix of the&#10;log-likelihood field on block , defined as in (\[defFisherMatrix\])&#10;adapted to the time interval . In the same way, we define as the block&#10;diagonal matrix whose subcomponents are where \_[(i)]{}(\_[0,(i)]{}) :=&#10;ł(&#10;&#10;&amp; 0\&#10;0 &amp;&#10;&#10;), and . We can rewrite (\[eqTaylorScore\]) as&#10;(\_0)\^[-1]{}\_n(\_n)\_n\^[1/2]{}(\_n - \_0) = - (\_0)\^[-1]{}&#10;\_n\^[1/2]{}ł{\_nł(\_0) -\_nł(\_0 ) } + \_n\^[1/2]{} \_n(\_0).&#10;\[eqTaylorReforumulate\] Note that, again, by a direct adaptation of&#10;(38) and (40) in (pp. 247-248) to the case of an irregular grid with and&#10;on the interval we automatically get that each so that is negligible.&#10;Now, is consistent by application of Theorem \[thmConsistencyH1\] to&#10;each on block . Therefore, , and by virtue of Lemma&#10;\[lemmaFisherConsistency\] applied to each submatrix , we conclude on&#10;the one hand that where is the identity matrix, and on the other hand by&#10;Slutsky’s Lemma and the stable CLT for that the left-hand side of&#10;(\[eqTaylorReforumulate\]) tends -stably in law to a mixed normal&#10;distribution of block diagonal random variance matrix with submatrices&#10;of the form&#10;&#10;ł(&#10;&#10;&amp; 0\&#10;0 &amp; 4a\_0\^8\&#10;&#10;) &amp;&amp; ł(&#10;&#10;ł(+ ) &amp; 0\&#10;0 &amp; +\&#10;&#10;)\&#10;&amp;=&amp; ł(&#10;&#10;a\_0ł( + ) &amp; 0\&#10;0 &amp; 2a\_0\^[4]{} + [cum]{.nodecor}\_4\[\]\&#10;&#10;)\&#10;&amp;=&amp;V\_[(i)]{}\^[’]{}, and thus we have shown the CLT for . Now to get&#10;Theorem \[CLTQMLEjumps\], it is sufficient to notice that where has the&#10;form and from here we easily conclude that the left-hand side of&#10;(\[last\]) admits a CLT with the claimed asymptotic variance. Finally&#10;Theorem \[CLTQMLE\] is a particular case of Theorem \[CLTQMLEjumps\].">
  </outline>
  <outline text="Proof of Theorem [RKth]" _note="Some details of the proof are omitted as the techniques used are very&#10;close to the QMLE case. We need to introduce some notation. We consider&#10;the block constant processes defined as Condition (\[RKth2\]) in Theorem&#10;\[RKth\] can be re-expressed as We also define the kernels for general&#10;processes and as where the realized autocovariance is defined as with .&#10;We further define the estimate on the th block and we aggregate the&#10;local estimates to define the adapted version of as We follow the same&#10;line of reasoning as in the proof of Theorem 4 (p. 1530, ). Accordingly,&#10;we just need to show an adapted version of Theorem 3 (p. 1492). Theorem&#10;\[RKth\] will then follow from Lemma 1 (p. 1523) and Proposition&#10;\[propositionConditionalStable\]. From now on, we aim to show the&#10;adapted version of Theorem 3 (p. 1492, ) which is stated in what&#10;follows.&#10;&#10;(Adapted version of Theorem 3 in ) We assume that . As we have that In&#10;addition, when , the asymptotic variance of is equivalent to where with&#10;and if&#10;&#10;To show (\[th31\]), we consider the continuous interpolated martingale .&#10;As for the QMLE, we aim to use Theorem 2.1 (). To show condition (2.9),&#10;i.e. that , we express as , where are such that for , on and for . We&#10;can easily show that The case in the proof of Theorem 3 (p. 1528, ) is&#10;based on a martingale theorem which shows that In view of (\[MM\]) and&#10;(\[MiMi\]), we have thus shown that . We show condition (2.10), i.e.&#10;that , by a straightforward calculation on the Brownian motion&#10;increments. Also, condition (2.11) is satisfied because is continuous.&#10;Finally we show that condition (2.12) hold, i.e. for any bounded&#10;martingale orthogonal to we have that M, \_t = 0. This can be proven&#10;with the same line of reasoning as for Lemma \[lemmaM2\] for the QMLE.&#10;&#10;The proof for (\[th32\]) can adapt directly from the cross-term part in&#10;the proof of Theorem 3 (p. 1528, ). Indeed, on each block we have the&#10;convergence discussed on p. 1525, and it is clear that as the block&#10;terms are uncorrelated to each other conditioned on , we obtain the&#10;convergence of the vector block estimates, with correlation limit&#10;between two different block terms equal to 0.&#10;&#10;We aim to show now (\[th34\]). In view of (A.3) on p. 1528 in , we have&#10;where and and is due to end-effects. We have that and for some normally&#10;distributed variables and from the proof on p. 1529 in . Actually, we&#10;can show that the convergence still holds for the random vector and thus&#10;we have that where is equal to We have that , which shows the&#10;convergence to the first term in (\[th34\]). The second term is obtained&#10;as The other terms in (\[sumcov\]) go to 0, thus we have shown&#10;(\[th34\]). The convergence (\[th35\]) is obtained as a straightforward&#10;consequence of (\[th34\]).">
  </outline>
  <outline text="Proof of Theorem [RKthjumps]" _note="The proof adding jumps and stochastic observation times follows the same&#10;line of reasoning as for the QMLE case.">
  </outline>
  <outline text="Proof of Corollary [RKcor] and Corollary [QMLEcor]" _note="By Slutsky’s Lemma, both corollaries will be proved if we have the&#10;consistency of the AVAR estimators. This is a consequence of the&#10;consistency of the estimators and by Theorem 3.1 and Remark 4 in along&#10;with the consistency of by, e.g., (21) in .">
  </outline>
  <outline text="Proof of Proposition [propAVARRK]" _note=" takes on the form AVAR\_[\[\_[i-1]{}, \_i\]]{}\^[(RK,c\_i\^\*,2)]{} =&#10;a\_0 ł( \_B \_[T\_[i-1]{}]{}\^[T\_i]{} \_u\^4 du )\^[3/4]{} g&#10;(\_[\_[i-1]{},\_i]{}). In view of (\[smsp\]), we easily obtain where .&#10;This gives us the estimate for some .&#10;&#10;Let us define for , , the random set . Because the jumps in are of&#10;finite activity, almost surely the cardinal of , defined as , tends to a&#10;finite value. Thus we can get rid of the terms for which is contained&#10;into because \_[i J\_B]{} AVAR\_[\[\_[i-1]{},&#10;\_i\]]{}\^[(RK,c\_i\^\*,2)]{} |J\_B| \_B T\^[1/2]{}a\_0 \^3 0, and&#10;similarly g(1) a\_0 T\^[1/2]{}\_[i J\_B]{} \_[\_[i-1]{}]{}\^[\_i]{}&#10;\_u\^3 du 0. Thus, the proposition will be proved if we show \_[i&#10;J\_B]{}[AVAR\_[\[\_[i-1]{}, \_i\]]{}\^[(RK,c\_i\^\*,2)]{}]{} - g (1)&#10;a\_0 T\^[1/2]{}\_[i J\_B]{} \_[T\_[i-1]{}]{}\^[T\_i]{} \_u\^3 du 0.&#10;&#10;As the continuous part of is assumed to be an Itô process with bounded&#10;components, some calculation shows that for any , and uniformly in we&#10;have the following expansion \_[\_[i-1]{}]{}\^[\_i]{}[\_u\^p du]{} =&#10;\_[T\_[i-1]{}]{}\^p\_B+ O\_[\^q]{}(\_B\^[3/2]{}), where , means that is&#10;bounded. Thus, using again (\[smsp\]), we also obtain the expansions , ,&#10;and to get finally the estimate AVAR\_[\[\_[i-1]{},&#10;\_i\]]{}\^[(RK,c\_i\^\*,2)]{} = g(1) a\_0 T\^[1/2]{}&#10;\_[T\_[i-1]{}]{}\^[T\_i]{} \_u\^3 du + O\_[\^q]{}(\_B\^[3/2]{})&#10;uniformly in . At this stage we have thus proved that \_[i&#10;J\_B]{}[AVAR\_[\[\_[i-1]{}, \_i\]]{}\^[(RK,c\_i\^\*,2)]{}]{} - g(1) a\_0&#10;T\^[1/2]{}\_[i J\_B]{} \_[T\_[i-1]{}]{}\^[T\_i]{} \_u\^3 du =&#10;O\_[\^q]{}(\_B\^[1/2]{}). To get the almost sure convergence to 0, we&#10;define as the left hand side of the previous equality and note that for&#10;any . This gives us that and so , which completes the proof.">
  </outline>
  <outline text="Proof of Remark [rklossprop]" _note="We show the inequality for any admissible couple . Note that by the&#10;domination obtained on the account of (\[RhoKappa\]), it is sufficient&#10;to show that the function is decreasing on the interval . We let , and a&#10;short calculation shows us that has the same sign as . Therefore, the&#10;inequality implies that is decreasing if and only if is, which is&#10;obvious.">
  </outline>
  <outline text="Proof of Proposition [propAVARQMLE]" _note="This proof follows the same line of reasoning as for the proof of&#10;Proposition \[propAVARRK\].">
  </outline>
  <outline text="Proof of Proposition [RKcorRobust]" _note="When , this is a straightforward adaptation of the proof of Proposition&#10;\[propAVARRK\] using the new estimates for any and">
  </outline>
  <outline text="Proof of Proposition [RKcorRobust2]" _note="When , the situation is fairly different. Let us define the random set&#10;Since is of finite activity, by taking sufficiently large, we may assume&#10;that for any , jumps exactly once on . Splitting the sum of local&#10;variances AVAR\_B\^[(RK,rob)]{} &amp;=&amp; \_[i J\_B\^X]{}&#10;R\_[(i)]{}\^[1/2]{}AVAR\_[\[\_[i-1]{},&#10;\_i\]]{}\^[(RK,rob,\_i\^[\*]{})]{} + \_[i J\_B\^X]{}&#10;R\_[(i)]{}\^[1/2]{}AVAR\_[\[\_[i-1]{},&#10;\_i\]]{}\^[(RK,rob,\_i\^[\*]{})]{}\&#10;&amp;=&amp; I + II, again by the finite activity property of we easily deduce&#10;from the proof of Proposition \[RKcorRobust\] that Now we derive the&#10;limit of . We write the jump times of labeled such that for any , . For&#10;any , we have the estimates where for the latter expression we used the&#10;continuity of at time which is a consequence of the independence of and&#10;. We thus have Combined with , we deduce that">
  </outline>
  <outline text="Proof of Proposition [QMLEcorRobust] and Proposition [QMLEcorRobust2]" _note="The proofs follow exactly the same line of reasoning as the proofs of&#10;Proposition \[RKcorRobust\] and Proposition \[RKcorRobust2\].&#10;&#10;[@rccccccccc@]{} &amp; &amp; &amp; &amp; &amp; &amp; &amp; &amp; &amp;\&#10;\&#10;5,850 &amp; -0.042&amp;1.102&amp;1.103&amp;0.29&amp;1.75&amp;3.77&amp;96.62&amp;98.62&amp;99.85\&#10;11,700 &amp; -0.032&amp;1.067&amp;1.068&amp;0.39&amp;1.96&amp;3.98&amp;96.01&amp;98.20&amp;99.80\&#10;23,400 &amp; -0.030&amp;1.044&amp;1.044&amp;0.41&amp;2.13&amp;4.16&amp;95.63&amp;97.84&amp;99.70\&#10;46,800 &amp; -0.027&amp;1.041&amp;1.041&amp;0.46&amp;2.25&amp;4.35&amp;95.58&amp;98.18&amp;99.74\&#10;\&#10;5,850 &amp; -0.065&amp;1.105&amp;1.106&amp;0.24&amp;1.55&amp;3.53&amp;96.49&amp;98.43&amp;99.82\&#10;11,700 &amp; -0.048&amp;1.069&amp;1.070&amp;0.32&amp;1.85&amp;3.65&amp;95.89&amp;98.20&amp;99.71\&#10;23,400 &amp; -0.042&amp;1.048&amp;1.049&amp;0.37&amp;2.01&amp;3.91&amp;95.52&amp;97.88&amp;99.65\&#10;46,800 &amp; -0.037&amp;1.044&amp;1.045&amp;0.43&amp;2.11&amp;4.10&amp;95.54&amp;98.06&amp;99.71\&#10;\&#10;5,850 &amp; -0.105&amp;1.110&amp;1.115&amp;0.21&amp;1.38&amp;3.02&amp;96.25&amp;98.37&amp;99.81\&#10;11,700 &amp; -0.082&amp;1.074&amp;1.077&amp;0.28&amp;1.54&amp;3.33&amp;95.74&amp;98.15&amp;99.66\&#10;23,400 &amp; -0.069&amp;1.051&amp;1.053&amp;0.36&amp;1.77&amp;3.66&amp;95.22&amp;97.73&amp;99.65\&#10;46,800 &amp; -0.059&amp;1.043&amp;1.044&amp;0.37&amp;1.89&amp;3.89&amp;95.31&amp;97.96&amp;99.64\&#10;\&#10;5,850 &amp; -0.144&amp;1.115&amp;1.124&amp;0.19&amp;1.23&amp;2.72&amp;95.81&amp;98.33&amp;99.75\&#10;11,700 &amp; -0.114&amp;1.077&amp;1.083&amp;0.23&amp;1.40&amp;3.09&amp;95.37&amp;97.88&amp;99.67\&#10;23,400 &amp; -0.099&amp;1.054&amp;1.059&amp;0.31&amp;1.65&amp;3.49&amp;94.95&amp;97.57&amp;99.60\&#10;46,800 &amp; -0.086&amp;1.043&amp;1.047&amp;0.38&amp;1.65&amp;3.56&amp;94.95&amp;97.76&amp;99.57\&#10;\&#10;5,850 &amp;-0.193&amp;1.119&amp;1.136&amp;0.15&amp;1.03&amp;2.31&amp;95.40&amp;98.14&amp;99.75\&#10;11,700 &amp; -0.154&amp;1.080&amp;1.091&amp;0.21&amp;1.26&amp;2.83&amp;95.27&amp;97.88&amp;99.66\&#10;23,400 &amp; -0.128&amp;1.054&amp;1.062&amp;0.28&amp;1.52&amp;3.27&amp;94.91&amp;97.56&amp;99.59\&#10;46,800 &amp;-0.109&amp;1.042&amp;1.047&amp;0.32&amp;1.64&amp;3.51&amp;94.72&amp;97.56&amp;99.55\&#10;&#10;This table shows summary statistics and empirical quantiles benchmarked&#10;to the (0,1) distribution for the infeasible Z-statistics related to the&#10;global and local RK (Tukey-Hanning 2). The simulation design is Model 2&#10;with Monte-Carlo simulations.&#10;&#10;[@rccccccccc@]{} &amp; &amp; &amp; &amp; &amp; &amp; &amp; &amp; &amp;\&#10;\&#10;5,850 &amp;-0.024&amp;1.084&amp;1.084&amp;0.36&amp;2.09&amp;4.12&amp;96.48&amp;98.57&amp;99.84\&#10;11,700 &amp; -0.015&amp;1.058&amp;1.058&amp;0.43&amp;2.26&amp;4.51&amp;96.32&amp;98.34&amp;99.75\&#10;23,400 &amp; -0.012&amp;1.039&amp;1.039&amp;0.51&amp;2.19&amp;4.48&amp;95.87&amp;97.97&amp;99.70\&#10;46,800 &amp; -0.013&amp;1.034&amp;1.034&amp;0.59&amp;2.38&amp;4.67&amp;95.74&amp;98.06&amp;99.73\&#10;\&#10;5,850 &amp; -0.023&amp;1.086&amp;1.086&amp;0.34&amp;1.90&amp;4.03&amp;96.48&amp;98.46&amp;99.85\&#10;11,700 &amp; -0.011&amp;1.06&amp;1.06&amp;0.42&amp;2.08&amp;4.28&amp;96.24&amp;98.31&amp;99.72\&#10;23,400 &amp; -0.007&amp;1.042&amp;1.042&amp;0.54&amp;2.12&amp;4.31&amp;95.71&amp;98.01&amp;99.66\&#10;46,800 &amp; -0.009&amp;1.036&amp;1.036&amp;0.56&amp;2.22&amp;4.51&amp;95.71&amp;98.08&amp;99.63\&#10;\&#10;5,850 &amp; -0.016&amp;1.089&amp;1.089&amp;0.34&amp;1.98&amp;3.86&amp;96.42&amp;98.57&amp;99.82\&#10;11,700 &amp; -0.007&amp;1.063&amp;1.063&amp;0.42&amp;2.09&amp;4.19&amp;96.24&amp;98.34&amp;99.72\&#10;23,400 &amp; -0.002&amp;1.042&amp;1.042&amp;0.51&amp;2.16&amp;4.52&amp;95.64&amp;98.02&amp;99.65\&#10;46,800 &amp; -0.005&amp;1.035&amp;1.035&amp;0.56&amp;2.20&amp;4.74&amp;95.60&amp;98.08&amp;99.69\&#10;\&#10;5,850 &amp; -0.012&amp;1.089&amp;1.089&amp;0.36&amp;2.01&amp;3.93&amp;96.46&amp;98.56&amp;99.82\&#10;11,700 &amp; -0.002&amp;1.062&amp;1.062&amp;0.43&amp;2.01&amp;4.26&amp;96.33&amp;98.33&amp;99.74\&#10;23,400 &amp; -0.0&amp;1.041&amp;1.041&amp;0.51&amp;2.10&amp;4.63&amp;95.60&amp;98.06&amp;99.71\&#10;46,800 &amp; -0.004&amp;1.033&amp;1.033&amp;0.57&amp;2.21&amp;4.72&amp;95.64&amp;98.07&amp;99.69\&#10;\&#10;5,850 &amp; -0.014&amp;1.093&amp;1.093&amp;0.36&amp;1.82&amp;3.83&amp;96.42&amp;98.63&amp;99.82\&#10;11,700 &amp; -0.005&amp;1.066&amp;1.066&amp;0.40&amp;1.94&amp;4.15&amp;96.33&amp;98.37&amp;99.74\&#10;23,400 &amp; -0.001&amp;1.043&amp;1.043&amp;0.47&amp;2.05&amp;4.53&amp;95.64&amp;98.15&amp;99.67\&#10;46,800 &amp; -0.003&amp;1.033&amp;1.033&amp;0.57&amp;2.29&amp;4.67&amp;95.60&amp;98.10&amp;99.66\&#10;&#10;This table shows summary statistics and empirical quantiles benchmarked&#10;to the (0,1) distribution for the infeasible Z-statistics related to the&#10;global and local QMLE. The simulation design is Model 2 with Monte-Carlo&#10;simulations.&#10;&#10;[@rccccccccc@]{} &amp; &amp; &amp; &amp; &amp; &amp; &amp; &amp; &amp;\&#10;\&#10;5,850 &amp; -0.117&amp;1.176&amp;1.182&amp;0.02&amp;0.66&amp;1.67&amp;95.42&amp;97.67&amp;99.66\&#10;11,700 &amp; -0.080&amp;1.128&amp;1.131&amp;0.01&amp;0.58&amp;2.01&amp;95.00&amp;97.08&amp;99.38\&#10;23,400 &amp; -0.083&amp;1.098&amp;1.101&amp;0.02&amp;0.52&amp;2.26&amp;94.89&amp;97.22&amp;99.04\&#10;46,800 &amp; -0.069&amp;1.087&amp;1.089&amp;0.02&amp;0.73&amp;3.44&amp;94.49&amp;97.27&amp;99.15\&#10;\&#10;5,850 &amp; -0.130&amp;1.148&amp;1.155&amp;0.02&amp;0.58&amp;1.76&amp;95.40&amp;97.27&amp;99.40\&#10;11,700 &amp; -0.099&amp;1.111&amp;1.115&amp;0.01&amp;0.66&amp;2.17&amp;94.53&amp;96.23&amp;99.08\&#10;23,400 &amp; -0.086&amp;1.083&amp;1.086&amp;0.05&amp;0.71&amp;2.28&amp;95.03&amp;97.17&amp;99.24\&#10;46,800 &amp; -0.072&amp;1.071&amp;1.073&amp;0.06&amp;1.17&amp;4.10&amp;94.74&amp;97.63&amp;99.41\&#10;\&#10;5,850 &amp; -0.173&amp;1.136&amp;1.149&amp;0.03&amp;0.64&amp;1.71&amp;94.56&amp;97.28&amp;98.98\&#10;11,700 &amp;-0.139&amp;1.107&amp;1.115&amp;0.01&amp;0.64&amp;1.78&amp;93.28&amp;96.21&amp;99.13\&#10;23,400 &amp; -0.107&amp;1.083&amp;1.089&amp;0.08&amp;0.88&amp;2.37&amp;94.97&amp;96.83&amp;99.03\&#10;46,800 &amp; -0.092&amp;1.079&amp;1.083&amp;0.03&amp;1.07&amp;3.93&amp;94.99&amp;97.64&amp;99.34\&#10;\&#10;5,850 &amp; -0.225&amp;1.145&amp;1.167&amp;0.02&amp;0.60&amp;1.25&amp;93.88&amp;96.93&amp;98.80\&#10;11,700 &amp; -0.177&amp;1.103&amp;1.117&amp;0.01&amp;0.53&amp;1.52&amp;93.21&amp;95.86&amp;99.04\&#10;23,400 &amp; -0.145&amp;1.077&amp;1.087&amp;0.04&amp;0.72&amp;1.91&amp;94.12&amp;96.61&amp;99.06\&#10;46,800 &amp;-0.122&amp;1.07&amp;1.077&amp;0.04&amp;1.01&amp;3.55&amp;94.82&amp;97.07&amp;99.35\&#10;\&#10;5,850 &amp;-0.270&amp;1.152&amp;1.183&amp;0.01&amp;0.45&amp;1.080&amp;94.41&amp;96.88&amp;98.66\&#10;11,700 &amp; -0.219&amp;1.106&amp;1.128&amp;0.02&amp;0.50&amp;1.65&amp;92.63&amp;95.76&amp;98.96\&#10;23,400 &amp;-0.176&amp;1.089&amp;1.103&amp;0.07&amp;0.55&amp;1.96&amp;93.63&amp;97.34&amp;98.97\&#10;46,800 &amp;-0.146&amp;1.078&amp;1.088&amp;0.03&amp;0.72&amp;3.27&amp;94.02&amp;96.98&amp;99.38\&#10;&#10;This table shows summary statistics and empirical quantiles benchmarked&#10;to the (0,1) distribution for the feasible Z-statistics related to the&#10;global and local RK (Tukey-Hanning 2). The simulation design is Model 2&#10;with Monte-Carlo simulations.&#10;&#10;[@rccccccccc@]{} &amp; &amp; &amp; &amp; &amp; &amp; &amp; &amp; &amp;\&#10;\&#10;5,850 &amp;-0.114&amp;1.200&amp;1.205&amp;0.01&amp;0.50&amp;1.29&amp;95.50&amp;97.99&amp;99.59\&#10;11,700 &amp;-0.090&amp;1.148&amp;1.152&amp;0.01&amp;0.35&amp;1.68&amp;95.36&amp;97.18&amp;99.12\&#10;23,400 &amp;-0.075&amp;1.109&amp;1.112&amp;0.01&amp;0.62&amp;2.00&amp;95.13&amp;96.91&amp;98.18\&#10;46,800 &amp;-0.062&amp;1.093&amp;1.095&amp;0.01&amp;0.61&amp;2.98&amp;94.38&amp;96.67&amp;99.08\&#10;\&#10;5,850 &amp;-0.099&amp;1.170&amp;1.174&amp;0.02&amp;0.56&amp;1.49&amp;95.70&amp;97.66&amp;99.38\&#10;11,700 &amp;-0.080&amp;1.130&amp;1.133&amp;0.01&amp;0.49&amp;1.76&amp;94.89&amp;96.81&amp;99.05\&#10;23,400 &amp;-0.057&amp;1.094&amp;1.095&amp;0.03&amp;0.91&amp;2.38&amp;95.25&amp;97.32&amp;98.61\&#10;46,800 &amp;-0.049&amp;1.079&amp;1.081&amp;0.02&amp;0.99&amp;3.61&amp;94.93&amp;97.24&amp;99.40\&#10;\&#10;5,850 &amp;-0.089&amp;1.150&amp;1.154&amp;0.04&amp;0.82&amp;1.62&amp;95.56&amp;97.50&amp;99.18\&#10;11,700 &amp; -0.077&amp;1.112&amp;1.114&amp;0.05&amp;0.56&amp;1.92&amp;94.80&amp;96.73&amp;99.17\&#10;23,400 &amp;-0.049&amp;1.083&amp;1.084&amp;0.06&amp;1.11&amp;2.91&amp;95.15&amp;97.36&amp;98.77\&#10;46,800 &amp;-0.046&amp;1.083&amp;1.084&amp;0.02&amp;1.16&amp;3.38&amp;95.23&amp;97.32&amp;99.53\&#10;\&#10;5,850 &amp;-0.090&amp;1.146&amp;1.149&amp;0.07&amp;0.91&amp;1.65&amp;95.89&amp;97.47&amp;99.00\&#10;11,700 &amp;-0.076&amp;1.105&amp;1.107&amp;0.05&amp;0.58&amp;2.36&amp;94.70&amp;96.98&amp;99.21\&#10;23,400 &amp; -0.050&amp;1.077&amp;1.078&amp;0.06&amp;1.07&amp;3.10&amp;95.10&amp;97.62&amp;98.78\&#10;46,800 &amp; -0.046&amp;1.076&amp;1.077&amp;0.03&amp;1.19&amp;3.69&amp;95.35&amp;97.39&amp;99.44\&#10;\&#10;5,850 &amp;-0.090&amp;1.145&amp;1.148&amp;0.08&amp;0.78&amp;1.96&amp;95.71&amp;97.39&amp;99.27\&#10;11,700 &amp; -0.073&amp;1.099&amp;1.101&amp;0.06&amp;0.68&amp;2.46&amp;94.83&amp;96.81&amp;99.11\&#10;23,400 &amp;-0.045&amp;1.076&amp;1.077&amp;0.08&amp;1.17&amp;2.51&amp;95.22&amp;97.50&amp;98.96\&#10;46,800 &amp;-0.046&amp;1.080&amp;1.081&amp;0.03&amp;1.46&amp;3.86&amp;95.39&amp;97.26&amp;99.46\&#10;&#10;This table shows summary statistics and empirical quantiles benchmarked&#10;to the (0,1) distribution for the feasible Z-statistics related to the&#10;global and local QMLE. The simulation design is Model 2 with Monte-Carlo&#10;simulations.&#10;&#10;Empirical losses and theoretical losses for the three models and the 10&#10;estimators. Two levels of sampling , and three noise-to-signal ratios ,&#10;and are considered.&#10;&#10;For , refers to the empirical mean value of estimates of on blocks&#10;across days and values of for INTC in 2015. The AVAR ratios are&#10;estimated by plugging estimates of the integrated volatility, the&#10;integrated quarticity and on blocks of different sizes. The last column&#10;shows the empirical correlation between the corrections induced by the&#10;local method.&#10;&#10;Sample means, standard deviations, and correlations with the global QMLE&#10;for the 10 estimators implemented for INTC data in 2015. The estimators&#10;are scaled by a factor .&#10;&#10;">
  </outline>
</outline>
  </body>
</opml>