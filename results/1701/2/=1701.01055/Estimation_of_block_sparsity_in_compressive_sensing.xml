<?xml version="1.0" encoding="UTF-8"?>
<opml version="2.0">
  <head>
    <title>Estimation of block sparsity in\
compressive sensing</title>
    <abstract>Explicitly using the block structure of the unknown signal can achieve
better recovery performance in compressive censing. An unknown signal
with block structure can be accurately recovered from underdetermined
linear measurements provided that it is sufficiently block sparse.
However, in practice, the block sparsity level is typically unknown. In
this paper, we consider a soft measure of block sparsity, and propose a
procedure to estimate it by using multivariate isotropic symmetric
-stable random projections without sparsity or block sparsity
assumptions. The limiting distribution of the estimator is given. Some
simulations are conducted to illustrate our theoretical results. </abstract>
  </head>
  <body>
<outline text="Introduction" _note="its introduction a few years ago , Compressive Sensing (CS) has&#10;attracted considerable interests (see the monographs for a comprehensive&#10;view). Formally, one considers the standard CS model, where is the&#10;measurements, is the measurement matrix, is the unknown signal, is the&#10;measurement error, and . The goal of CS is to recover the unknown signal&#10;by using only the underdetermined measurements and the matrix . Under&#10;the assumption of sparsity of the signal, that is has only a few nonzero&#10;entries, and the measurement matrix is properly chosen, can be recovered&#10;from by certain algorithms, such as the Basis Pursuit (BP), or&#10;-minimization approach, the Orthogonal Matching Pursuit (OMP) ,&#10;Compressive Sampling Matching Pursuit (CoSaMP) and the Iterative Harding&#10;Thresholding algorithm . Specifically, when the sparsity level of the&#10;signal is , if with some universal constant , and is subgaussian random&#10;matrix, then accurate or robust recovery can be guaranteed with high&#10;probability.&#10;&#10;The sparsity level parameter plays a fundamental role in CS, as the&#10;number of measurements, the properties of measurement matrix , and even&#10;some recovery algorithms all involve it. However, the sparsity level of&#10;the signal is usually unknown in practice. To fill the gap between&#10;theory and practice, very recently proposed a numerically stable measure&#10;of sparsity with , which is in ratios of norms. By random linear&#10;projections using i.i.d univariate symmetric -stable random variables,&#10;the author constructed the estimation equation for with by adopting the&#10;characteristic function method and obtained the asymptotic normality of&#10;the estimator.&#10;&#10;As a natural extension of the sparsity with nonzero entries arbitrarily&#10;spread throughout the signal, we can consider the sparse signals exhibit&#10;additional structure in the form of the nonzero entries occurring in&#10;clusters. Such signals are referred to as block sparse . Block sparse&#10;model appears in many practical scenarios, such as when dealing with&#10;multi-band signals , in measurements of gene expression levels , and in&#10;colour imaging . Moreover, block sparse model can be used to treat the&#10;problems of multiple measurement vector (MMV) and sampling signals that&#10;lie in a union of subspaces .&#10;&#10;To make explicit use of the block structure to achieve better sparse&#10;recovery performance, the corresponding extended versions of sparse&#10;representation algorithms have been developed, such as mixed -norm&#10;recovery algorithm , group lasso or adaptive group lasso , iterative&#10;reweighted recovery algorithms , block version of OMP algorithm and the&#10;extensions of the CoSaMP algorithm and of the Iterative Hard&#10;Thresholding to the model-based setting , which includes block sparse&#10;model as a special case. It was shown in that if the measurement matrix&#10;has small block-restricted isometry constants which generalizes the&#10;conventional RIP notion, then the mixed -norm recovery algorithm is&#10;guaranteed to recover any block sparse signal, irrespectively of the&#10;locations of the nonzero blocks. Furthermore, recovery will be robust in&#10;the presence of noise and modeling errors (i.e., when the vector is not&#10;exactly block sparse). showed that the block versions of CoSaMP and&#10;Iterative Hard Thresholding exhibit provable recovery guarantees and&#10;robustness properties. In addition, with the block-coherence of is&#10;small, the robust recovery of mixed -norm method, and the block version&#10;of the OMP algorithm are guaranteed in .&#10;&#10;The block sparsity level plays the same central role in recovery for&#10;block sparse signals as the sparsity level in recovery for sparse&#10;signals. Namely, the required number of measurements, properties of the&#10;recovery measurement matrix (Block RIP), and some recovery algorithms&#10;for signals with block structure all depend on the block sparsity level.&#10;However, in reality, the block sparsity level of the signals are also&#10;unknown. To obtain its estimator is very important from both the&#10;theoretical and practical views.">
  <outline text="Contributions" _note="First, as a extension of the soft sparsity measure with in , we propose&#10;a soft measure of block sparsity, that is .&#10;&#10;Second, we obtain an estimator for the block sparsity by using&#10;multivariate isotropic symmetric -stable random projections. When the&#10;block size is 1, our estimation procedure reduces to the case considered&#10;in . The asymptotic distributions of the estimators are obtained,&#10;similar to the results presented in .&#10;&#10;Finally, a series of simulation experiments are conducted to illustrate&#10;our theoretical results.">
  </outline>
  <outline text="Organization and Notations" _note="The remainder of the paper is organized as follows. In Section II, we&#10;introduce the definition of block sparsity and a soft measure of block&#10;sparsity. In Section III, we present the estimation procedure for the&#10;block sparsity measure and obtain the asymptotic properties for the&#10;estimators. In Section IV, we conduct some simulations to illustrate the&#10;theoretical results. Section V is devoted to the conclusion. Finally,&#10;the proofs are postponed to the Appendix.&#10;&#10;Throughout the paper, we denote vectors by boldface lower letters e.g.,&#10;, and matrices by upper letters e.g., . Vectors are columns by default.&#10;is the transpose of the vector . The notation denotes the -th component&#10;of . For any vector , we denote the -norm for . is the indicator&#10;function. is the expectation function. is the bracket function, which&#10;takes the maximum integer value. is the real part function. is the unit&#10;imaginary number. is the inner product of two vectors. indicates&#10;convergence in probability, while is convergence in distribution.">
  </outline>
</outline>
<outline text="Block Sparsity Measures">
  <outline text="Definitions" _note="We firstly introduce some basic concepts for block sparsity and propose&#10;a new soft measure of block sparsity.&#10;&#10;With , we define the -th block of a length- vector over . The -th block&#10;is of length , and the blocks are formed sequentially so that Without&#10;loss of generality, we assume that , then . A vector is called block&#10;-sparse over if is nonzero for at most indices . In other words, by&#10;denoting the mixed norm a block -sparse vector can be defined by .&#10;&#10;Despite the important theoretical role of the parameter , it has a&#10;severe practical drawback of being not sensitive to small entries of .&#10;For instance, if has large blocks and small blocks, then as soon as they&#10;are nonzero. To overcome this drawback, it is desirable to replace the&#10;mixed norm with a soft version. Specifically, we generalize the sparsity&#10;measure based on entropy to the block sparsity measure. For any non-zero&#10;signal given in (\[signal\]), it induces a distribution on the set of&#10;block indices , assigning mass at index , where . Then the entropy based&#10;block sparsity goes to where is the Rényi entropy of order . When , the&#10;Rényi entropy is given explicitly by , and the cases of are defined by&#10;evaluating limits, with being the ordinary Shannon entropy. Then, for&#10;and , we have the measure of block sparsity written conveniently in&#10;terms of mixed norm as where the mixed norm for . The cases of are&#10;evaluated as limits: , , and , where . When the block size equals 1, our&#10;block sparsity measure reduces to the nonblock sparsity measure given by&#10;.&#10;&#10;The fact that () is a sensible measure of the block sparsity for&#10;non-idealized signals is illustrated in Figure \[fig\_1\]. In the case&#10;that has large blocks and small blocks, we have , whereas .&#10;&#10;In addition, the quantity has some important properties similar as .&#10;&#10;Continuity:Unlike the mixed norm, the function is continuous on for all&#10;. Thus, it is stable with respective to small perturbations of the&#10;signal.&#10;&#10;Range equal to :For all given as (\[signal\]) and all , we have&#10;&#10;Scale-invariance:For all , it holds that . Scale-invariance encodes the&#10;idea that block sparsity should also be based on relative (rather than&#10;absolute) magnitudes of the entries of the signal as the sparsity.&#10;&#10;Non-increasing in :For any , we have&#10;&#10;">
  </outline>
  <outline text="Recovery results in terms of" _note="Before presenting the estimation procedure for the and with , we give&#10;the block sparse signal recovery results in terms of by using mixed&#10;-norm optimization algorithm.&#10;&#10;To recover the block sparse signal in CS model (\[1.1\]), we use the&#10;following mixed -norm optimization algorithm proposed in : where is a&#10;upper bound on the noise level . Then, we have the following result&#10;concerning on the robust recovery for block sparse signals.\&#10;[**Lemma 1**]{}(). Let be noisy measurements of a vector and fix a&#10;number . Let denote the best block -sparse approximation of , such that&#10;is block -sparse and minimizes over all the block -sparse vectors , and&#10;let be a solution to (\[2.1\]), a random Gaussian matrix of size with&#10;entries , and block sparse signals over , where for some integer . Then,&#10;there are constants , such that the following statement is true. If ,&#10;then with probability at least , we have&#10;&#10;[**Remark 1.**]{} Note that the first term in (\[2.2\]) is a result of&#10;the fact that is not exactly block -sparse, while the second term&#10;quantifies the recovery error due to the measurement noise. When the&#10;block size , this Lemma goes to the conventional CS result for sparse&#10;signals. Explicit use of block sparsity reduces the required number of&#10;measurements from to by times.\&#10;The limitation of the previous bound is that the ratio term is typically&#10;unknown. Thus, it is not clear how large should be chosen to guarantee&#10;that the relative -error 　is small with high probability. Next, we&#10;present an upper bound of the relative -error by an explicit function of&#10;and the new proposed block sparsity measure , which is estimable. The&#10;following result is an extension of Proposition 1 in . Its proof is left&#10;to Appendix.\&#10;[**Lemma 2.**]{} Let be noisy measurements of a vector , and let be a&#10;solution to (\[2.1\]), a random Gaussian matrix of size with entries ,&#10;and block sparse signals over , where for some integer . Then, there are&#10;constants , such that the following statement is true. If and satisfy ,&#10;then with probability at least , we have \&#10;">
  </outline>
</outline>
<outline text="Estimation Method for  and" _note="In this section, we mainly focus on the estimation of with . There are&#10;two reasons to consider this interval. One is that small is usually a&#10;better block sparsity measure than very large in applications. And we&#10;can approximate with very small as will be shown later. The other reason&#10;is that our estimation method relies on the -stable distribution, which&#10;requires to lie in . The core idea to obtain the estimators for and with&#10;is using random projections. Contrast to the conventional sparsity&#10;estimation by using projections with univariate symmetric -stable random&#10;variables , we use projections with the multivariate centered isotropic&#10;symmetric -stable random vectors for the block sparsity estimation.">
  <outline text="Multivariate Isotropic Stable Distribution" _note="We firstly give the definition of the multivariate centered isotropic&#10;symmetric -stable distribution.\&#10;[**Definition 1.**]{} For , a -dimensional random vector has a centered&#10;isotropic symmetric -stable distribution if there are constants and such&#10;that its characteristic function has the form We denote the distribution&#10;by , and is referred to as the scale parameter.\&#10;[**Remark 2.**]{} The most well-known example of multivariate isotropic&#10;symmetric stable distribution is the case of (Multivariate Independent&#10;Gaussian Distribution), and in this case, the components of the&#10;Multivariate Gaussian random vector are independent. Another case is&#10;(Multivariate Spherical Symmetric Cauchy Distribution ), unlike&#10;Multivariate Independent Gaussian case, the components of Multivariate&#10;Spherical Symmetric Cauchy are uncorrelated, but dependent. The&#10;perspective and contour plots for the densities of these two cases are&#10;illustrated in Figure \[fig\_2\]. The multivariate centered isotropic&#10;symmetric -stable random vector is a direct extension of the univariate&#10;symmetric -stable random variable, which is the special case when the&#10;dimension parameter . In applications, to simulation a -dimensional&#10;random vector from the multivariate centered isotropic symmetric -stable&#10;distribution , we can adopt the fact that , where is a independent&#10;univariate positive -stable random variable and is a standard&#10;-dimensional Gaussian random vector, see for more details.&#10;&#10;">
  </outline>
  <outline text="Estimation Procedure" _note="By random projections using i.i.d multivariate centered isotropic&#10;symmetric -stable random vectors, we can obtain the estimators for and&#10;with , which is presented as follows.&#10;&#10;We estimate the by using the random linear projection measurements:&#10;where is i.i.d random vector, and with i.i.d drawn from . The noise term&#10;are i.i.d from a distribution and assume its characteristic function is&#10;, the sets and are independent. are assumed to be symmetric about , with&#10;, but they may have infinite variance. The assumption of symmetry is&#10;only for convenience, it was explained how to drop it in Section III-B.e&#10;of . A minor technical condition we place on is that the roots of its&#10;characteristic function are isolated (i.e. no limit points). This&#10;condition is satisfied by many families of distributions, such as&#10;Gaussian, Student’s , Laplace, uniform, and stable laws. And we assume&#10;that the noise scale parameter and the distribution are treated as being&#10;known for simplicity.\&#10;Since our work involves different choices of , we will write instead of&#10;. Then the link with the norm hinges on the following basic lemma.\&#10;[**Lemma 3.**]{} Let be fixed, and suppose with i.i.d drawn from with&#10;and . Then, the random variable has the distribution .\&#10;[**Remark 3.**]{} When has different block lengths which are&#10;respectively, then we need choose the projection random vector with&#10;i.i.d drawn from . In that case, the conclusion in our Lemma and all the&#10;results in the followings still hold without any modifications. This&#10;Lemma is an extension of Lemma 1 in from i.i.d univariate symmetric&#10;-stable projection to i.i.d multivariate isotropic symmetric -stable&#10;projection.\&#10;By using this result, if we generate a set of i.i.d measurement random&#10;vectors as given above and let , then is an i.i.d sample from the&#10;distribution . Hence, in the special case of random linear measurements&#10;without noise, estimating the norm reduces to estimating the scale&#10;parameter of a univariate stable distribution from an i.i.d sample.\&#10;Next, we present the estimation procedure by using the characteristic&#10;function method . We use two separate sets of measurements to estimate&#10;and . The respective sample sizes of each measurements are denoted by&#10;and . To unify the discussion, we will describe just the procedure to&#10;obtain for any , since is a special case. The two estimators are&#10;combined to obtain the estimator for , which follows as: \&#10;In fact, the characteristic function of has the form: where . Then, we&#10;have By using the empirical characteristic function to estimate , we&#10;obtain the estimator of given by when and .\&#10;">
  </outline>
  <outline text="Asymptotic Properties" _note="Then, similar to the Theorem 2 in , we have the uniform central limit&#10;theorem (CLT) for . Before presenting the result, we introduce the&#10;noise-to-signal ratio constant \&#10;[**Theorem 1**]{} (Uniform CLT for Mixed Norm Estimator). Let . Let be&#10;any function of that satisfies as for some finite constant and . Then,&#10;we have as , where the limiting variance is strictly positive and&#10;defined according to the formula \&#10;For simplicity, we use the instead of the optimal in . Since it is&#10;simple to implement, and still gives a reasonably good estimator. To&#10;describe the pilot value, let be any number such that for all (which&#10;exists for any characteristic function). Also, we define the median&#10;absolute deviation statistic and define . Then we obtain the consistent&#10;estimator of a constant , where random variable (see the Proposition 3&#10;in ), and the consistent estimator of , . Therefore, the consistent&#10;estimator of the limiting variance is . Thus, we immediately have the&#10;following corollary to obtain the confidence intervals for .\&#10;[**Corollary 1**]{} (Confidence Interval for ). Under the conditions of&#10;Theorem 1, as , we have Then, it follows that the asymptotic confidence&#10;interval for is where is the -quantile of the standard normal&#10;distribution.\&#10;As a consequence, we can obtain a CLT and a confidence interval for by&#10;combining the estimators and with their respective . Before we present&#10;the main result, for each , we assume that there is a constant , such&#10;that ,&#10;&#10;[**Theorem 2**]{} (Asymptotic Property for ). Let and the conditions of&#10;Theorem 1 hold. Then as , where . And consequently, the asymptotic&#10;confidence interval for is where is the -quantile of the standard normal&#10;distribution.\&#10;">
  </outline>
  <outline text="Estimating  with  and Small" _note="Next, we present the approximation of to when is close to . To state the&#10;theorem, we define the block dynamic range of a non-zero signal given in&#10;(\[signal\]) as where is the smallest norm of the non-zero block of ,&#10;i.e. . When the block size , our goes to the defined in . The following&#10;result involves no randomness and is applicable to any estimator .\&#10;[**Theorem 3.**]{} Let , is non-zero signal given in (\[signal\]), and&#10;let be any real number. Then, we have \&#10;[**Remark 4.**]{} This theorem is a direct extension of Proposition 5 in&#10;, which corresponds the special case of the block size . When choosing&#10;to be the proposed estimator , the first term in (\[th3\]) is already&#10;controlled by Theorem 2. As pointed out in , the second term is the&#10;approximation error that improves for smaller choices of . When the&#10;norms of the signal blocks are similar, the quantity of will not be too&#10;large. In this case, the bound behaves well and estimating is of&#10;interest. On the other hand, if the norms of the signal blocks are very&#10;different, that is is large, then may not be the best measure of block&#10;sparsity to estimate.">
  </outline>
</outline>
<outline text="Simulation" _note="In this section, we conduct some simulations to illustrate our&#10;theoretical results. We focus on choosing , that is we use to estimate&#10;the block sparsity measure . When estimating , we requires a set of&#10;measurements by using multivariate isotropic symmetric cauchy&#10;projection, and a set of by using multivariate isotropic symmetric&#10;normal projection. We generated the samples and according to where ,&#10;with is i.i.d random vector, and with i.i.d drawn from , we let .&#10;Similarly, , with is i.i.d random vector, and with i.i.d drawn from , we&#10;let . The noise terms and are generated with i.i.d entries from a&#10;standard normal distribution. We considered a sequence of pairs for the&#10;sample sizes . For each experiments, we replicates 200 times.&#10;Consequently, we have 200 realizations of for each . We then averaged&#10;the quantity as an approximation of .">
  <outline text="Exactly Block Sparse Case" _note="First, we let our signal be a very simple exactly block sparse vector,&#10;that is where is a vector of length with entries all ones, is the zero&#10;vector. Then it is obvious that , while and depend on the block size&#10;that we choose.\&#10;a) We set . The simulation is conducted under several choices of the&#10;parameters, , and –with each parameter corresponding to a separate plot&#10;in Figure \[fig\_3\]. The signal dimension is set to 1000, except in the&#10;top left plot, where . We set in all cases, except in top right plot,&#10;where , corresponding the real value , which also equals , the exact&#10;block sparsity level of our signal with the block size to be . In turn,&#10;in all cases, except in the bottom plot where . In all three plots, the&#10;theoretical curves are computed in the following way. From Theorem 2, we&#10;have , where is a standard Gaussian random variable, and we set . Since&#10;, the theoretical curves are simply , as a function of . Note that&#10;depends on and , which is why there is only one theoretical curve in top&#10;left plot for error dependence on .&#10;&#10;From Figure \[fig\_3\], we can see that the black theoretical curves&#10;agree well with the colored empirical ones. In addition, the averaged&#10;relative error has no observable dependence on or (when is fixed), as&#10;expected from Theorem 2, and the dependence on the is mild.\&#10;b) Next, a simulation study is conducted to illustrate the asymptotic&#10;normality of our estimators in Corollary 1 and Theorem 2. We have 1000&#10;replications for these experiments, that is we have 1000 samples of the&#10;standardized statistics , and . We consider four cases, with and the&#10;noise is standard normal and which has infinite variance. In all the&#10;cases, we set , , and .&#10;&#10;Figure \[fig\_4\] shows that the density curves of the standardized&#10;statistics all are very close to the standard normal density curve,&#10;which verified our theoretical results. And these results hold even when&#10;the noise distribution is heavy-tailed. Comparing the four plots, we see&#10;that it leads to improve the normal approximation by increasing the&#10;sample size and reducing the noise variance.&#10;&#10;">
  </outline>
  <outline text="Nearly Block Sparse Case" _note="Second, we consider our signal to be not exactly block sparse but nearly&#10;block sparse, that is the entries of -th block all equal , with chosen&#10;so that . In this case, the norm of blocks decays like for . With the&#10;same settings as in the previous subsection, we obtain the similar&#10;simulation results as the exactly block sparse case in Figure \[fig\_5\]&#10;and Figure \[fig\_6\].&#10;&#10;">
  </outline>
  <outline text="Estimating  with  and Small" _note="Third, we consider the estimation of the mixed norm by using with . We&#10;consider the signals of the form with chosen so that . In this&#10;experiment, we set . To obtain , we generate the samples and , where ,&#10;with is i.i.d random vector, and with i.i.d drawn from , we let .&#10;Similarly, , with is i.i.d random vector, and with i.i.d drawn from , we&#10;let . The noise terms and are generated with i.i.d entries from a&#10;standard normal distribution. The noise level was set to . We considered&#10;a sequence of pairs for the sample sizes . For each experiments, we&#10;replicates 200 times. Then, we have 200 realizations of for each . We&#10;varied and , and averaged the quantity . Specifically, we considered the&#10;four cases .&#10;&#10;Figure \[fig\_7\] shows that estimates accurately over a wide range of&#10;the parameters and , and these parameters have a small effect on the&#10;relative estimate error , which is expected in Theorem 3.&#10;&#10;">
  </outline>
</outline>
<outline text="Conclusion" _note="In this paper, we proposed a new soft measure of block sparsity and&#10;obtained its estimator by adopting multivariate centered isotropic&#10;symmetric -stable random projections. The asymptotic properties of the&#10;estimators were presented. A series of simulation experiments&#10;illustrated our theoretical results.&#10;&#10;There are some interesting issues left for future research. Throughout&#10;the paper, we assume that the noise scale parameter and the&#10;characteristic function of noise are known. In practice, however, they&#10;are usually unknown and need to be estimated. Although considered the&#10;effects of adopting their estimators in estimation procedure, how to&#10;estimate these parameters based on our random linear projection&#10;measurements itself is still unknown. In addition, we have been&#10;considering the sparsity and block sparsity estimations for real-valued&#10;signals so far. It will be interesting to generalize the existing&#10;results to the case of complex-valued signals.">
</outline>
<outline text="Proofs" _note="Our main theoretical results Theorem 1 and Theorem 2 follow from Theorem&#10;2 and Corollary 1 in , since in both estimation procedures, the&#10;measurements without noise both have the univariate symmetric stable&#10;distribution but with different scale parameters after the random&#10;projection, for sparsity estimation, for block sparsity estimation.&#10;Therefore, the asymptotic results for the scale parameters estimators by&#10;using characteristic function method are rather similar. In order not to&#10;repeat, all the details are omitted. Next, we only present the proofs&#10;for Lemma 2, Lemma 3 and Theorem 3.\&#10;[**Proof of Lemma 2.**]{} The proof procedure follows from the proof of&#10;Proposition 1 in 　with some careful modifications. Let be as in Lemma&#10;1, and let be any number such that Define the positive number , and&#10;choose in Lemma 1. Note that when , this choice of is clearly at most ,&#10;and hence lies in . Then we have by using our assumption . Hence, our&#10;choice of ensures . To finish the proof, let be as in Lemma 1 so that&#10;the bound (\[2.2\]) holds with probability at least . Moreover, we have&#10;Let and be as in (\[2.2\]), then we have then the proof is completed by&#10;setting , and noticing the fact that .\&#10;[**Proof of Lemma 3.**]{} By using the independence of , for , the&#10;characteristic function of has the form: Then, the Lemma follows from&#10;the Definition 1.\&#10;[**Proof of Theorem 3.**]{} The triangle inequality implies As , we have&#10;&#10;Thus, it suffices to bound the last term on the right side. Since and is&#10;a non-increasing function of , then&#10;&#10;We now derive a bound on . For and , if we define the probability vector&#10;with , and defined as Section II.A, then it holds that by using and the&#10;formula for . Next, as , then for with , we have As this bound does not&#10;depend on and sum to 1, we obtain for all and , Finally, the basic&#10;integral result completes the proof.">
</outline>
<outline text="Acknowledgment" _note="This work is supported by the Swedish Research Council grant (Reg.No.&#10;340-2013-5342).&#10;&#10;[1]{}&#10;&#10;Baraniuk, R. G., Cevher, V., Duarte, M. F., Hegde, C. (2010).&#10;Model-based compressive sensing. IEEE TRANSACTIONS ON INFORMATION THEORY&#10;**56(4)** 1982–2001.&#10;&#10;Blumensath, T., Davies, M. E. (2009). Iterative hard thresholding for&#10;compressed sensing. APPLIED AND COMPUTATIONAL HARMONIC ANALYSIS&#10;**27(3)** 265–274.&#10;&#10;Blumensath, T., Davies, M. E. (2009). Sampling theorems for signals from&#10;the union of finite-dimensional linear subspaces. IEEE TRANSACTIONS ON&#10;INFORMATION THEORY **55(4)** 1872–1882.&#10;&#10;Candes, E. J., Romberg, J. K., Tao, T. (2006). Stable signal recovery&#10;from incomplete and inaccurate measurements. COMMUNICATIONS ON PURE AND&#10;APPLIED MATHEMATICS **59(8)** 1207–1223.&#10;&#10;Candes, E. J., Tao, T. (2005). Decoding by linear programming. IEEE&#10;TRANSACTIONS ON INFORMATION THEORY **51(12)** 4203–4215.&#10;&#10;Candes, E. J., Tao, T. (2006). Near-optimal signal recovery from random&#10;projections: Universal encoding strategies?. IEEE TRANSACTIONS ON&#10;INFORMATION THEORY **52(12)** 5406–5425.&#10;&#10;Chen, J., Huo, X. (2006). Theoretical results on sparse representations&#10;of multiple-measurement vectors. IEEE TRANSACTIONS ON SIGNAL PROCESSING&#10;**54(12)** 4634–4643.&#10;&#10;Cotter, S. F., Rao, B. D., Engan, K., Kreutz-Delgado, K. (2005). Sparse&#10;solutions to linear inverse problems with multiple measurement vectors.&#10;IEEE TRANSACTIONS ON SIGNAL PROCESSING **53(7)** 2477–2488.&#10;&#10;Donoho, D. L. (2006). Compressed sensing. IEEE TRANSACTIONS ON&#10;INFORMATION THEORY **52(4)** 1289–1306.&#10;&#10;Duarte, M. F., Eldar, Y. C. (2011). Structured compressed sensing: From&#10;theory to applications. IEEE TRANSACTIONS ON SIGNAL PROCESSING 59(9)&#10;4053–4085.&#10;&#10;Dudley, R. M. (1999). Uniform central limit theorems (Vol. 23).&#10;Cambridge: Cambridge University Press.&#10;&#10;Eldar, Y. C., Kuppinger, P., Bolcskei, H. (2010). Block-sparse signals:&#10;Uncertainty relations and efficient recovery. IEEE TRANSACTIONS ON&#10;SIGNAL PROCESSING **58(6)** 3042–3054.&#10;&#10;Eldar, Y. C., Kutyniok, G. (2012). Compressed Sensing: Theory and&#10;Applications. Cambridge University Press. Eldar, Y. C., Mishali, M.&#10;(2009). Robust recovery of signals from a structured union of subspaces.&#10;IEEE TRANSACTIONS ON INFORMATION THEORY **55(11)** 5302–5316.&#10;&#10;Elhamifar, E., Vidal, R. (2012). Block-sparse recovery via convex&#10;optimization. IEEE TRANSACTIONS ON SIGNAL PROCESSING **60(8)**&#10;4094–4107.&#10;&#10;Foucart, S., Rauhut, H. (2013). A Mathematical Introduction to&#10;Compressive Sensing. New York, NY, USA: Springer-Verlag.&#10;&#10;Lopes, M. E. (2013). Estimating Unknown Sparsity in Compressed Sensing.&#10;In ICML (3) (pp. 217–225). Lopes, M. E. (2016). Unknown Sparsity in&#10;Compressed Sensing: Denoising and Inference. IEEE TRANSACTIONS ON&#10;INFORMATION THEORY **62(9)** 5145–5166.&#10;&#10;Lv, X., Bi, G., Wan, C. (2011). The group lasso for stable recovery of&#10;block-sparse signal representations. IEEE TRANSACTIONS ON SIGNAL&#10;PROCESSING **59(4)** 1371–1382.&#10;&#10;Majumdar, A., Ward, R. K. (2010). Compressed sensing of color images.&#10;SIGNAL PROCESSING **90(12)** 3122–3127.&#10;&#10;Markatou, M., Horowitz, J. L. (1995). Robust scale estimation in the&#10;error-components model using the empirical characteristic function.&#10;CANADIAN JOURNAL OF STATISTICS **23(4)** 369-381.&#10;&#10;Markatou, M., Horowitz, J. L., Lenth, R. V. (1995). Robust scale&#10;estimation based on the the empirical characteristic function.&#10;STATISTICS PROBABILITY LETTERS **25(2)** 185–192.&#10;&#10;Mishali, M., Eldar, Y. C. (2008). Reduce and boost: Recovering arbitrary&#10;sets of jointly sparse vectors. IEEE TRANSACTIONS ON SIGNAL PROCESSING&#10;**56(10)** 4692–4702. Mishali, M., Eldar, Y. C. (2009). Blind multiband&#10;signal reconstruction: Compressed sensing for analog signals. IEEE&#10;TRANSACTIONS ON SIGNAL PROCESSING **57(3)** 993–1009. Needell, D.,&#10;Tropp, J. A. (2009). CoSaMP: Iterative signal recovery from incomplete&#10;and inaccurate samples. APPLIED AND COMPUTATIONAL HARMONIC ANALYSIS&#10;**26(3)** 301–321.&#10;&#10;Nolan, J. P. (2013). Multivariate elliptically contoured stable&#10;distributions: theory and estimation. COMPUTATIONAL STATISTICS **28(5)**&#10;2067–2089.&#10;&#10;Parvaresh, F., Vikalo, H., Misra, S., Hassibi, B. (2008). Recovering&#10;sparse signals using sparse measurement matrices in compressed DNA&#10;microarrays. IEEE JOURNAL OF SELECTED TOPICS IN SIGNAL PROCESSING&#10;**2(3)** 275–285.&#10;&#10;Plan, Y., Vershynin, R. (2013). One‐Bit Compressed Sensing by Linear&#10;Programming. COMMUNICATIONS ON PURE AND APPLIED MATHEMATICS **66(8)**&#10;1275–1297.&#10;&#10;Press, S. J. (1972). Multivariate stable distributions. JOURNAL OF&#10;MULTIVARIATE ANALYSIS **2(4)** 444-462.&#10;&#10;Stojnic, M., Parvaresh, F., Hassibi, B. (2010). On the reconstruction of&#10;block-sparse signals with an optimal number of measurements. IEEE&#10;TRANSACTIONS ON SIGNAL PROCESSING **57(8)** 3075–3085.&#10;&#10;Tropp, J. A., Gilbert, A. C. (2007). Signal recovery from random&#10;measurements via orthogonal matching pursuit. IEEE TRANSACTIONS ON&#10;INFORMATION THEORY **53(12)** 4655–4666.&#10;&#10;Vershynin, R. (2015). Estimation in high dimensions: a geometric&#10;perspective. In SAMPLING THEORY, A RENAISSANCE (pp. 3-66). Springer&#10;International Publishing.&#10;&#10;Yuan, M., Lin, Y. (2006). Model selection and estimation in regression&#10;with grouped variables. JOURNAL OF THE ROYAL STATISTICAL SOCIETY: SERIES&#10;B (STATISTICAL METHODOLOGY) **68(1)** 49–67.&#10;&#10;Zeinalkhani, Z., Banihashemi, A. H. (2015). Iterative Reweighted&#10;Recovery Algorithms for Compressed Sensing of Block Sparse Signals. IEEE&#10;TRANSACTIONS ON SIGNAL PROCESSING **63(17)** 4516–4531.&#10;&#10;Zolotarev, V. M. (1986). One-dimensional stable distributions (Vol. 65).&#10;Providence, RI, USA: AMS, 1986.">
</outline>
  </body>
</opml>