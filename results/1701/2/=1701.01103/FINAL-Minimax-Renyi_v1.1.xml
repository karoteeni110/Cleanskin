<?xml version="1.0" encoding="UTF-8"?>
<opml version="2.0">
  <head>
    <title>Minimax Rényi Redundancy</title>
    <abstract>The redundancy for universal lossless compression of discrete memoryless
sources in Campbell’s setting is characterized as a minimax Rényi
divergence, which is shown to be equal to the maximal -mutual
information via a generalized redundancy-capacity theorem. Special
attention is placed on the analysis of the asymptotics of minimax Rényi
divergence, which is determined up to a term vanishing in blocklength.

**Keywords:** Universal lossless compression, generalized
redundancy-capacity theorem, minimax redundancy, minimax regret,
Jeffreys’ prior, risk aversion, Rényi divergence, -mutual information. </abstract>
  </head>
  <body>
<outline text="Introduction" _note="variable length source coding, expected code length is the usual cost&#10;function that one aims to minimize. For discrete memoryless sources,&#10;asymptotically, the minimal achievable per-letter expected code length&#10;is equal to the entropy. However, if is a discrete memoryless source&#10;distribution with an unknown parameter and the encoding system assumes a&#10;distribution , then one needs to pay an extra penalty for the mismatch&#10;given by[^1] where stands for the relative entropy between the&#10;probability measures and . In light of , the conventional worst-case&#10;measure of redundancy in universal lossless compression is where the&#10;infimization is over all distributions on , and the supremum is over all&#10;possible values of the unknown parameter. In this zero-sum game, is&#10;chosen by the code designer, and is chosen by nature.&#10;&#10;A relation between and the maximal mutual information is given by the&#10;REDUNDANCY-CAPACITY THEOREM (e.g., , and ) that states that where[^2]&#10;the supremization is over all probability distributions on the parameter&#10;space. Through , and , we see a pleasing relationship between entropy,&#10;relative entropy and mutual information in the context of lossless data&#10;compression.&#10;&#10;Let , and note that where the RELATIVE INFORMATION between the discrete&#10;probability measures and is defined as[^3] A much more stringent&#10;performance guarantee than the average of relative information is its&#10;pointwise maximum. In particular, if one replaces with in , the&#10;resulting quantity, i.e., is called the MINIMAX REGRET, which has found&#10;applications in various settings[^4], e.g., . An analogy to the&#10;Redundancy-Capacity Theorem is given by where denotes the -mutual&#10;information of infinite order, whose definition is given in .&#10;&#10;The average and pointwise formulations are two extremes of performance&#10;guarantees, which are not quite suitable for certain applications. For&#10;this reason, one seeks a compromise between those two. For example, in&#10;the economics literature, average and pointwise guarantees are referred&#10;as RISK-NEUTRAL and RISK-AVOIDING, respectively. Since the former is&#10;known to be too lenient and the latter is known to be too stringent for&#10;typical applications, the notion of RISK-AVERSION has been introduced to&#10;provide a more useful compromise between these two extremes , , which is&#10;known to be relevant for diverse applications . In this paper, we&#10;introduce the notion of risk-aversion within the universal source coding&#10;context and quantify its effect on the fundamental limit.&#10;&#10;In the non-universal setting, i.e., when the source distribution is&#10;known, a classical result of Campbell introduces such a risk-averse cost&#10;function in a discrete memoryless setting. Specifically, proposes to&#10;generalize the conventional notion of minimizing the expected code&#10;length with the cost function where , denotes the code, and denotes the&#10;length function. In this case, for a discrete memoryless source ,&#10;Campbell shows that the minimum per-letter cost asymptotically&#10;achievable by prefix codes is given by the RÉNYI ENTROPY . Notice that&#10;captures the notion of risk-aversion through the parameter since A&#10;natural way to introduce risk-aversion in universal source coding is to&#10;use Campbell’s formulation and characterize the penalty for the mismatch&#10;akin to . Indeed, about forty years after Campbell’s work, Sundaresan&#10;shows that if one uses as the cost function, the penalty paid for&#10;universality can be written as[^5] where denotes the RÉNYI DIVERGENCE OF&#10;ORDER , which is defined in , and denotes the SCALED DISTRIBUTION of :&#10;The distance measure is known as the SUNDARESAN DIVERGENCE of order&#10;between and . Following , the relevant measure of redundancy for&#10;universal lossless compression under Campbell’s performance criterion is&#10;The conventional minimax redundancy in corresponds to while the minimax&#10;regret in corresponds to . Although, in general, , we are able to&#10;establish a pleasing analog to the classical redundancy results such as&#10;, and , : where in is the -MUTUAL INFORMATION OF ORDER between and with&#10;, see , . Note that is analogous to with Rényi divergence replacing the&#10;relative entropy. Thus, we refer as the MINIMAX RÉNYI REDUNDANCY.&#10;Moreover, generalizes the Redundacy-Capacity Theorem to -mutual&#10;information thereby finding another operational meaning for the maximal&#10;-mutual information beyond those that have been shown in the literature&#10;on error probability bounds for data transmission (e.g. , ). Moreover,&#10;the -mutual information smoothly interpolates between two extremes,&#10;namely in and in . Finally, and , coupled with Campbell’s result ,&#10;provide a pleasing relationship between Rényi entropy, Rényi divergence&#10;and -mutual information in the context of universal lossless data&#10;compression.&#10;&#10;The asymptotic behaviors of the minimax redundancy and minimax regret&#10;have also received considerable attention in the literature (e.g., , , ,&#10;) since, in addition to compression, they are relevant in applications&#10;such as machine learning, finance, prediction, gambling, and so on. In&#10;particular, Xie and Barron in their key contributions , show that where&#10;and are the number of observations and the alphabet size, respectively,&#10;denotes the Gamma function, and vanishes as .&#10;&#10;While Merhav gives , we quantify asymptotically the effect of the&#10;risk-aversion parameter on the fundamental limit in universal source&#10;coding by providing a pleasing interpolation[^6] between and :&#10;&#10;In the remainder of the paper, Section \[sec:notation\] sets the basic&#10;notation and definitions. Section \[sec:main results\] states the main&#10;results and gives the outlines of their proofs, which are contained in&#10;Section \[sec:proofs\]. In the Appendices, we prove several lemmas that&#10;are used in Section \[sec:proofs\].&#10;&#10;[^1]: For prefix codes, is well known . On the other hand, the loss in&#10;    rate incurred due to the prefix condition is known to be&#10;    asymptotically negligible .&#10;&#10;[^2]:  is the mutual information between and with .&#10;&#10;[^3]: Unless otherwise stated, logarithms and exponentials are of&#10;    arbitrary basis.&#10;&#10;[^4]: For example, in lossless compression with prefix codes, is often&#10;    viewed as a proxy for the mismatch penalty incurred by assuming that&#10;    is drawn from rather than the true distribution . Such an&#10;    approximation can be justified asymptotically.&#10;&#10;[^5]: Campbell’s and Sundaresan’s results are still valid when .&#10;    However, such a formulation corresponds to a RISK-SEEKING scheme,&#10;    which falls outside the philosophy espoused in this paper.&#10;&#10;[^6]: In a fundamentally different setup, Hayashi considers the&#10;    counterpart of the Clarke and Barron result replacing relative&#10;    entropy with Rényi divergence.">
</outline>
<outline text="Notation and Definitions" _note="Let and denote the -dimensional simplex of probability mass functions&#10;defined on by For each parameter , we define our observation model such&#10;that[^1] and the independent identically distributed (i.i.d.) extension&#10;of this model such that where denotes the number of times appears in the&#10;vector , and therefore It can be verified that the Fisher information&#10;matrix (in nats) of for the parameter vector is[^2] where denotes an&#10;matrix all of whose entries are equal to 1. The determinant of the&#10;Fisher information matrix in satisfies An important probability measure&#10;on is JEFFREYS’ PRIOR defined as where denotes a special form of the&#10;Dirichlet integrals of type 1 which can be written in terms of the Gamma&#10;function: In particular,&#10;&#10;The source distribution we get by assuming Jeffreys’ prior on the&#10;parameter space is referred as JEFFREYS’ MIXTURE which is denoted by[^3]&#10;&#10;For discrete probability measures and on the set such that dominates ,&#10;i.e., , RÉNYI DIVERGENCE of order[^4] between and is defined as where .&#10;In particular, when , Rényi divergence of order between and can be&#10;expressed as Given , an analogous generalization can be made for mutual&#10;information resulting in the -MUTUAL INFORMATION[^5] : where ,&#10;independent of , and we have used the conventional notation for&#10;INFORMATION DENSITY . As shown in Lemma \[lem:explicit alpha mut info\]&#10;in Appendix \[appdx: Explicit alpha mutual info\], the infimum in can be&#10;solved explicitly.&#10;&#10;In parallel with the standard usage for relative entropy, it is common&#10;to define the conditional Rényi divergence as therefore, the&#10;unconditional Rényi divergence in can be written as .&#10;&#10;[^1]: As a special case, when , we use the shorthand notation instead of&#10;    .&#10;&#10;[^2]: Note that the Fisher information matrix is since there are free&#10;    parameters in the model. Nevertheless, it is notationally convenient&#10;    to denote the parameter vector as if it were -dimensional.&#10;&#10;[^3]: Whenever it is informative to explicitly show the dimensionality&#10;    of the parameter space in the notation for Jeffreys’ mixture, we do&#10;    so by replacing with .&#10;&#10;[^4]: We are not concerned with Rényi divergences of order . A more&#10;    general definition can be found in .&#10;&#10;[^5]: The definition of -mutual information in dates back to Sibson’s&#10;    information radius . Although, it should be noted that Sibson’s&#10;    motivation in is not the generalization of mutual information. See&#10;    for a more thorough discussion.">
</outline>
<outline text="Statement of the Results" _note="Theorem \[thm:Generalized Redundancy-Capacity Theorem\] states that&#10;under the minimax operation in the Sundaresan divergence can be replaced&#10;by the Rényi divergence. We further show that this minimax operation can&#10;be written as the maximization of the -mutual information, thus,&#10;providing a generalization to the Redundancy-Capacity Theorem in . In&#10;Theorem \[thm:Asymptotic Behavior of Minimax Renyi Redundancy\], we&#10;investigate the asymptotic behavior of the minimax Rényi redundancy&#10;between and , and we find its precise asymptotic expansion, thereby&#10;quantifying the effect of the risk-aversion parameter .&#10;&#10;\[thm:Generalized Redundancy-Capacity Theorem\] For any , and positive&#10;integer&#10;&#10;As we show in the proof in Section \[sec:proofs\], is due to the fact&#10;that scaling a distribution is a one-to-one operation that preserves&#10;memorylessness while the minimax theorem for Rényi divergence is the&#10;gateway to showing the generalized redundancy-capacity theorem in .&#10;&#10;Although Theorem \[thm:Generalized Redundancy-Capacity Theorem\] holds&#10;in great generality, we illustrate its use in the simple example below.&#10;&#10;Consider the Z-CHANNEL with crossover probability, see, e.g., . In this&#10;case, which is a concave function of for every value of , and is&#10;maximized when After some elementary algebra, plugging into yields&#10;Observe that as , the right side of converges to the capacity of the&#10;channel, namely, . On the other hand, to compute the minimax Rényi&#10;redundancy, note that Let be the distribution such that Since through&#10;and , as enforced by generalized redundancy-capacity theorem, we observe&#10;that the maximal -mutual information matches the minimax Rényi&#10;divergence.&#10;&#10;\[thm:Asymptotic Behavior of Minimax Renyi Redundancy\] For any&#10;&#10;We prove Theorem \[thm:Asymptotic Behavior of Minimax Renyi Redundancy\]&#10;in Section \[sec:proofs\] by dividing it into two parts: converse and&#10;achievability. In both parts, Jeffreys’ prior plays a significant role.&#10;However, it is known that Jeffreys’ prior dramatically emphasizes the&#10;lower dimensional faces of the simplex. While this is not a problem in&#10;proving the converse bound, Jeffreys’ prior achieves a suboptimal&#10;minimax value (see Lemma \[lem:jefreys mixture is not minimax\] in&#10;Appendix \[appdx:jeffreys’ mixture is not minimax\]). Similar issues&#10;arise in finding the exact asymptotic constant in minimax redundancy ,&#10;and in minimax regret . To overcome this problem, we modify Jeffreys’&#10;prior by placing masses near the faces of the simplex as in . Although&#10;this resolves the problem encountered in the minimax redundancy and&#10;minimax regret cases, the functional form of Rényi divergence becomes&#10;the second obstacle which forces us to show a uniform Laplace&#10;approximation thereby making the proof of achievability a much more&#10;involved task than that of the converse. For this reason, we start by&#10;presenting the achievability proof in the special case of binary&#10;alphabets, in which the notation is simplified considerably.">
</outline>
<outline text="Proofs">
  <outline text="Proof of Theorem [thm:Generalized Redundancy-Capacity Theorem]" _note="To establish , for any , define the bijection as where Then, for any and&#10;, the scaled version of the conditional distribution (see ) satisfies&#10;Therefore, for any given distribution on As a result of , where follows&#10;because every probability measure in is a scaled version of another&#10;probability measure in .&#10;&#10;In order to establish , note that where the expectation in is with&#10;respect to , and follows from , which holds when is finite. The right&#10;side of is the maximal -mutual information of order[^1] in the sense of&#10;Csiszár, see and , which is known to equal maximal (see , and ) in the&#10;discrete parameter case. To see that holds even when the parameter space&#10;is continuous, recall the definition of -mutual information, , which can&#10;be written as and note that where follows from Jensen’s inequality,&#10;follows from the fact that the maximin value is always less than or&#10;equal to the minimax value, and is again due to .&#10;&#10;[^1]: When both random variables are discrete, another generalization of&#10;    mutual information, whose maximum also coincides with , is put&#10;    forward by Arimoto . See for further discussion of the various&#10;    proposals of -mutual information.">
  </outline>
  <outline text="Proof of the Converse of Theorem [thm:Asymptotic Behavior of Minimax Renyi Redundancy]" _note="This section is devoted to the proof of for any . Define for any . Let ,&#10;and consider the following where is due to Theorem \[thm:Generalized&#10;Redundancy-Capacity Theorem\], follows from a more general result,&#10;although, for the sake of completeness, its proof is included in Lemma&#10;\[lem:explicit alpha mut info\] in Appendix \[appdx: Explicit alpha&#10;mutual info\], is due to the suboptimal choice of Jeffreys’ prior, and&#10;follows because .&#10;&#10;Using Robbins’ sharpening of Stirling’s approximation, one can show that&#10;where the entropy is in nats and denotes the empirical distribution of&#10;the vector . Since , particularizes to With the aid of and we can&#10;express the integral in the right side of as The gamma function&#10;generalization of Stirling’s approximation (shown to be valid for&#10;positive real numbers by Whittaker and Watson ) yields where . In&#10;particular, for , where It follows from and that Combining and , we can&#10;write where is due to the definition of , , the fact that for any&#10;positive constant , is a monotone increasing function of , and the fact&#10;that the error terms (see and ) satisfy Uniting the lower bounds in ,&#10;and , where Notice that where follows after noticing that each factor of&#10;goes to 1, and follows from the definition of the Riemann integral.&#10;Assembling , and , we obtain the desired bound in .">
  </outline>
  <outline text="Proof of the achievability of Theorem [thm:Asymptotic Behavior of Minimax Renyi Redundancy] when" _note="In this section, we prove in when , i.e., To that end, we modify&#10;Jeffreys’ prior by placing masses near the vertices of the simplex,&#10;i.e., , which, in turn, enables us to show that when the parameter[^1]&#10;takes values near the vertices of the simplex the value of the minimax&#10;Rényi redundancy grows strictly slower than . Thus, we focus on values&#10;of that are not close to the vertices of the simplex, thereby enabling&#10;us to argue that the minimax Rényi redundancy behaves as in .&#10;&#10;Inspired by Xie and Barron’s modified Jeffreys’ prior, for and ,&#10;consider the prior which differs from the one in in the location of the&#10;point masses. Because of the modification on Jeffreys’ prior, the&#10;corresponding marginal changes from in to In view of&#10;Theorem \[thm:Generalized Redundancy-Capacity Theorem\], where The&#10;following result shows that the first and the third supremizations in&#10;the right side of are both dominated by .&#10;&#10;\[prop:achievability k=2 theta in face of simplex\] If , then&#10;&#10;Assume that . We have where follows from , follows because Rényi&#10;divergence is monotone decreasing in (see Lemma \[lem:renyi divergence&#10;monotone decreasing\] in Appendix \[appdx:monotonicity of binary renyi&#10;divergence\]) and follows because, for , Using a symmetrical argument,&#10;one can show that the upper bound in still holds when .&#10;&#10;It remains to investigate the behavior of the second supremization in&#10;the right side of . Let and note that which follows from . The following&#10;proposition gives an asymptotic upper bound on .&#10;&#10;\[prop:achievability-k=2-hard-step\] Let . For any ,&#10;&#10;Let and . Without loss of generality, we may assume that , otherwise we&#10;may interchange the roles of and together with the roles of and below.&#10;Note that where Thanks to Lemma \[lem:edge cases of ti\] in&#10;Appendix \[appdx:edge cases of t\_i\], we know that for all sufficiently&#10;large satisfying we have where the explicit expressions for and are&#10;given in and , respectively. Hence, we may now focus attention on . Note&#10;that where and denote the binary entropy and the binary relative entropy&#10;functions in nats, respectively and the bound in follows from Stirling’s&#10;approximation, see . Note also that where also follows from an&#10;application of Stirling’s approximation, see .&#10;&#10;By substituting , , and into the right side of , we get where and Note&#10;that we can find an asymptotically suboptimal upper bound on that&#10;depends only on by invoking Lemma \[lem:unif\_upp\_bd\_on\_K\] in&#10;Appendix \[appdx:bounds on K\], which shows a non-asymptotic uniform&#10;upper bound on , and then by invoking Lemma \[lem:uniform bound on the&#10;sum\] in Appendix \[appdx:uniform upper bound on the sum\], which shows&#10;a non-asymptotic uniform upper bound on Finding the optimal upper bound,&#10;on the other hand, requires a uniform Laplace approximation on , which&#10;is introduced next. First, given , split as where In&#10;Lemmas \[lem:k=2\_sum\_S1\_asympt\], \[lem:k=2\_sum\_S2\_asympt\] and&#10;\[lem:k=2\_sum\_S3\_asympt\] in Appendix \[appdx:achievebility\_k=2\],&#10;we show each of the following properties: Since the left side of does&#10;not depend on , – imply, by letting , that Finally, it follows from , ,&#10;, and that Since , it also follows that Combining and gives us the&#10;promised result of Proposition \[prop:achievability-k=2-hard-step\].&#10;&#10;Invoking Proposition \[prop:achievability k=2 theta in face of&#10;simplex\], we see that the functions in and can be bounded by while&#10;thanks to and Proposition \[prop:achievability-k=2-hard-step\], it&#10;follows that Since , we see that the right side of asymptotically&#10;dominates the right sides of and . Due to , and –, the desired result in&#10;follows by choosing an arbitrarily small in .&#10;&#10;[^1]: Since , we have . To simplify the discussion, we prefer the&#10;    shorthand notation rather than .">
  </outline>
  <outline text="Proof of the achievability of Theorem [thm:Asymptotic Behavior of Minimax Renyi Redundancy] when" _note="In this section, we prove in when , i.e., To do so, we once again modify&#10;Jeffreys’ prior as in the previous section by placing masses near the&#10;lower dimensional faces of the simplex, i.e., , which, in turn, enables&#10;us to show that when the parameter vector takes values near the faces of&#10;the simplex, the value of the minimax Rényi redundancy grows strictly&#10;slower than . Hence, by focusing on the parameter values that are not&#10;close to the faces of the simplex, we show that the minimax Rényi&#10;redundancy behaves as in .&#10;&#10;Following the idea in , let and, for , define Accordingly, we define the&#10;probability measure with respect to , the Lebesgue measure on , as&#10;Finally, for , we define the prior distribution on the probability&#10;simplex as where is Jeffreys’ prior. Because of the modification on&#10;Jeffreys’ prior in , the corresponding marginal changes from in to where&#10;Define, for , Note that denotes the vectors none of whose coordinates&#10;are within close proximity of zero in the sense of .&#10;&#10;In view of Theorem \[thm:Generalized Redundancy-Capacity Theorem\], The&#10;following result shows that the supremizations over for in are all&#10;dominated by .&#10;&#10;\[prop:achievability-general-faces dont matter\] If , then for each&#10;where the explicit value of is given in .&#10;&#10;Thanks to the symmetry, it suffices to show the result for . To that&#10;end, define as and let denote the Jeffreys’ mixture when the underlying&#10;parameter space is the -dimensional simplex. Further define and note&#10;that where follows from&#10;Lemma \[lem:uniform\_upper\_bd\_on\_renyi\_divergence\_btw\_model\_and\_jeff\_mix\]&#10;in Appendix \[appdx:uniform upp bd on renyi div\]. For , where follows&#10;from , and is due to . Finally, the desired result follows because –&#10;imply&#10;&#10;It remains to investigate the supremization over in . Observe that which&#10;follows from the definition of in . Parallel to&#10;Proposition \[prop:achievability-k=2-hard-step\],&#10;Proposition \[prop:general-hard to prove\] characterizes the behavior of&#10;the supremum in the right side of .&#10;&#10;\[prop:general-hard to prove\] For any ,&#10;&#10;We are only interested in . Therefore, for all , where is a constant.&#10;Since there is an index such that , it simplifies notation without loss&#10;of generality that . Otherwise, the proof remains identical. For a given&#10;positive integer , let be a proper subset and note that where Thanks to&#10;Lemma \[lem:edge cases of ti\] in Appendix \[appdx:edge cases of t\_i\],&#10;we know that for all sufficiently large satisfying it follows that where&#10;is a constant depending only on and , which is explicitly given in the&#10;proof of Lemma \[lem:edge cases of ti\], see . Hence, we may now focus&#10;attention on . Note that and where both the entropy and relative entropy&#10;are in nats and the bound in follows from Stirling’s approximation, see&#10;. Note also that where also follows from an application of Stirling’s&#10;approximation, see .&#10;&#10;By substituting , and into the right side of , we get where and Observe&#10;once again that we can find an asymptotically suboptimal upper bound on&#10;that depends only on and by invoking Lemma \[lem:unif\_upp\_bd\_on\_K\]&#10;in Appendix \[appdx:bounds on K\], which shows a non-asymptotic uniform&#10;upper bound on , and then by invoking Lemma \[lem:uniform bound on the&#10;sum\] in Appendix \[appdx:uniform upper bound on the sum\], which shows&#10;a non-asymptotic uniform upper bound on Finding the optimal upper bound,&#10;on the other hand, requires a uniform Laplace approximation on , which&#10;is introduced next. First, given , recall the set as defined in , let&#10;and split as where In Lemmas \[lem:generela\_sum\_S1\]&#10;and \[lem:generela\_sum\_S2\] in&#10;Appendix \[appdx:achievability\_k&amp;gt;2\], we show that the following&#10;properties hold: Since the left side of does not depend on , and imply,&#10;by letting , that Finally, it follows from (\[aqzu\]),&#10;(\[eqn:thanks\_to\_thm5\]), (\[eqn:thanks\_to\_substitition\]), and ,&#10;that holds when as we wanted to show.&#10;&#10;Invoking Proposition \[prop:achievability-general-faces dont matter\],&#10;we see that for each while thanks to and Proposition \[prop:general-hard&#10;to prove\], it follows that Since , we see that, as , the right side of&#10;goes to whereas the right side of remains constant. In view of , and ,&#10;the desired result in follows by choosing an arbitrarily small in .">
  </outline>
</outline>
<outline text="Explicit Evaluation of -Mutual Information" _note="In the case of finite collection of arbitrary distributions, explicit&#10;evaluation of is provided by Sibson . A more general result that allows&#10;non-discrete alphabets can be found in .&#10;&#10;\[lem:explicit alpha mut info\] Let . Given an arbitrary input&#10;distribution on and a random transformation with finite output alphabet&#10;, the -mutual information of order induced by on satisfies&#10;&#10;Define and recall that for any distribution on . Capitalizing on , note&#10;that By the definition of the -mutual information, see ; implies the&#10;result in .">
</outline>
<outline text="Monotonicity of Binary Rényi Divergence" _note="\[lem:renyi divergence monotone decreasing\] Let denote a Bernoulli&#10;distribution with parameter . For any and , is a monotone decreasing&#10;function of on .&#10;&#10;Fix . Let . It suffices to prove that is a monotone decreasing function&#10;of on . To that end, note that where follows because implies">
</outline>
<outline text="Uniform Upper Bound on" _note="\[lem:uniform\_upper\_bd\_on\_renyi\_divergence\_btw\_model\_and\_jeff\_mix\]&#10;Let be an element in the -dimensional simplex and assume that we are&#10;given a discrete i.i.d. model . Then, for any and , the relative&#10;information between the model and Jeffreys’ mixture satisfies the&#10;following bound where Consequently, for any , where is given in .&#10;&#10;Immediate consequence of .">
</outline>
<outline text="Edge Cases of" _note="\[lem:edge cases of ti\] Let and for a given positive integer , let be a&#10;proper subset of . Then, for any satisfying and (defined in ) where is&#10;defined[^1] in and is a constant that only depends and .&#10;&#10;Denote and note that Regarding the last term within the summation in the&#10;right side of , where follows from the definition of the Dirichlet&#10;integrals in , and follows from the fact that . Now, observe that where&#10;is the remainder in Stirling’s approximation of in , and is due to the&#10;following elementary bounds: It follows that where Since , where is&#10;because and for any we have . Let It follows from and that Note that&#10;where denotes the Jeffreys’ mixture when the underlying parameter space&#10;is the -dimensional simplex. Using the uniform upper bound on Rényi&#10;divergence in&#10;Lemma \[lem:uniform\_upper\_bd\_on\_renyi\_divergence\_btw\_model\_and\_jeff\_mix\],&#10;we get where is as defined in . Since , , and for any integer , we can&#10;upper bound As a result, and follows after setting&#10;&#10;[^1]: The quantity defined in corresponds to the special case of where ,&#10;    .">
</outline>
<outline text="Uniform Upper Bound on" _note="The quantity defined[^1] in satisfies the following upper bound.&#10;&#10;\[lem:uniform bound on the sum\] where is explicitly given in .&#10;&#10;Define where Note that where follows from , and follows from Stirling’s&#10;approximations, and , as well as the fact that Regarding , one can check&#10;that Invoking&#10;Lemma \[lem:uniform\_upper\_bd\_on\_renyi\_divergence\_btw\_model\_and\_jeff\_mix\]&#10;in Appendix \[appdx:uniform upp bd on renyi div\] to upper bound the&#10;left side of and applying the bound in to results in .&#10;&#10;[^1]: The quantity defined in corresponds to the special case of where ,&#10;    .">
</outline>
<outline text="Bounds on" _note="The quantity defined[^1] in satisfies the following non-asymptotic&#10;bound.&#10;&#10;\[lem:unif\_upp\_bd\_on\_K\] Given , In particular,&#10;&#10;For , because the function in the left side of is an increasing&#10;function. On the other hand, because the function of the left side of is&#10;a decreasing function. Finally, follows from the fact that and .&#10;&#10;\[lem:limit\_of\_K\] Let , and be fixed and be an integer. Assume that&#10;(defined in ) satisfies . If for then where in satisfies Furthermore,&#10;&#10;Note that since , for which, in turn, imply that Hence, inequality&#10;follows. It is straightforward to see the limit in .&#10;&#10;[^1]: The quantity defined in corresponds to the special case of where ,&#10;    .">
</outline>
<outline text="Lemmas for the Proof in Section [sec:subsec:ach-k=2]" _note="In the proofs of&#10;Lemmas \[lem:k=2\_sum\_S1\_asympt\], \[lem:k=2\_sum\_S2\_asympt\]&#10;and \[lem:k=2\_sum\_S3\_asympt\], we use the following bound: for and ,&#10;in nats. In particular, when To show and , we rely on Taylor’s theorem:&#10;for some in between and .&#10;&#10;\[lem:k=2\_sum\_S1\_asympt\] Let and fix . where is defined in .&#10;&#10;Assume that is a sufficiently large integer, let be given. Then where is&#10;due to , the uniform upper bound on given in&#10;Lemma \[lem:unif\_upp\_bd\_on\_K\] in Appendix \[appdx:bounds on K\],&#10;and the fact that , follows because for , Since the supremum in is&#10;attained at , it follows that holds.&#10;&#10;\[lem:k=2\_sum\_S2\_asympt\] Let . where is defined in .&#10;&#10;Assume that is a sufficiently large integer, let be given and define We&#10;have where is due to , the bound on for the given range of (see&#10;Lemma \[lem:limit\_of\_K\] in Appendix \[appdx:bounds on K\]), and the&#10;fact that for , follows because for , In light of&#10;Lemma \[lem:limit\_of\_K\] in Appendix \[appdx:bounds on K\], Moreover,&#10;the Riemann sum in can be upper bounded as It follows that holds.&#10;&#10;\[lem:k=2\_sum\_S3\_asympt\] Let and fix . where is defined in .&#10;&#10;The proof of this lemma is more involved than that of&#10;Lemma \[lem:k=2\_sum\_S1\_asympt\]. To proceed, using Pinsker’s&#10;inequality (e.g., ), namely we first prove that where is a fixed&#10;constant. Then, we show that with the help of Lemma \[lem:bound on&#10;integral\] in Appendix \[appdx:bound on integral\]. Fix a constant , and&#10;assume that is a sufficiently large integer.&#10;&#10;First, let be arbitrary and note that where follows from&#10;Lemma \[lem:unif\_upp\_bd\_on\_K\] in Appendix \[appdx:bounds on K\],&#10;Pinsker’s inequality as in , and the fact that , follows because and for&#10;, Thus, implies that Since , and it follows that holds.&#10;&#10;Second, let be arbitrary and fix some constant . Further, separate into&#10;two sums as follows where Regarding , we have where follows from&#10;Lemma \[lem:unif\_upp\_bd\_on\_K\] in Appendix \[appdx:bounds on K\] and&#10;, follows because and for and . Hence, and Regarding , we have where&#10;follows from Lemma \[lem:unif\_upp\_bd\_on\_K\] in&#10;Appendix \[appdx:bounds on K\]. Let be the maximizer of the right side&#10;in .&#10;&#10;Note that where follows after noticing that for any , the function is a&#10;decreasing function in and therefore the corresponding Riemann sum in&#10;the left side of can be upper bounded by the integral in its right side&#10;and follows from Lemma \[lem:bound on integral\] in&#10;Appendix \[appdx:bound on integral\]. Hence, As a result of and , holds.&#10;The desired result follows since we have established and .">
</outline>
<outline text="Upper Bound for the Integral in" _note="\[lem:bound on integral\] Let and . Fix , and . For any&#10;&#10;Abbreviate Applying integration by parts yields For , we have because is&#10;a decreasing function and is an increasing convex function for the given&#10;range of . Hence, we see that where follows because implies and">
</outline>
<outline text="Lemmas for the Proof in Section [sec:subsec:ach-gen]" _note="In the proofs of Lemmas \[lem:generela\_sum\_S1\]&#10;and \[lem:generela\_sum\_S2\], we use the following bound: for and ,&#10;where denotes the Fisher information matrix, and To show , we rely on&#10;Taylor’s theorem: for some such that lies between and .&#10;&#10;\[lem:generela\_sum\_S1\] The function defined in satisfies&#10;&#10;Assume that is a sufficiently large integer, and let with be given.&#10;Define We invoke with Hence, where is due to , the bound on when (see&#10;Lemma \[lem:limit\_of\_K\] in Appendix \[appdx:bounds on K\]), and the&#10;fact that for , follows because implies In light of&#10;Lemma \[lem:limit\_of\_K\] in Appendix \[appdx:bounds on K\], Since the&#10;multi-variable Riemann sum in can be upper bounded as we can conclude&#10;that holds.&#10;&#10;\[lem:generela\_sum\_S2\] The function defined in satisfies&#10;&#10;Assume that is a sufficiently large integer, and let with be given.&#10;Recall the definition of in , and note that if then there must exist&#10;such that Moreover, by symmetry, we can write where (\[aqtz\]) is due to&#10;the uniform upper bound on in Lemma \[lem:unif\_upp\_bd\_on\_K\], in&#10;(\[aqty\]), and the function denoted by is defined in . By invoking&#10;Lemma \[lem:uniform bound on the sum\] in Appendix \[appdx:uniform upper&#10;bound on the sum\], we see that can be upper bounded by a constant&#10;depending only on and . On the other hand, the sum without the factor&#10;vanishes as (see Lemmas \[lem:k=2\_sum\_S1\_asympt\]&#10;and \[lem:k=2\_sum\_S3\_asympt\]). Therefore, follows.">
</outline>
<outline text="Jeffreys’ Mixture is not Minimax" _note="The fact that Jeffreys’ prior is capacity achieving (or least favorable)&#10;follows from the converse proof of Theorem \[thm:Asymptotic Behavior of&#10;Minimax Renyi Redundancy\]. Therefore, Jeffreys’ mixture is maximin for&#10;Rényi redundancy. Parallel to the results in and , Lemma \[lem:jefreys&#10;mixture is not minimax\] below proves that Jeffreys’ mixture is NOT&#10;minimax.&#10;&#10;\[lem:jefreys mixture is not minimax\] For any , where the supremization&#10;is over all that are on the face of the simplex so that at most of its&#10;components are known to be non-zero.&#10;&#10;Note that the third term in the right side of interpolates the extra&#10;constants when and when , shown in and , respectively.&#10;&#10;Assuming without loss of generality that the last entries of are equal&#10;to zero simplifies the notation. Otherwise, the proof remains identical.&#10;Define where denotes the -th entry of . Note that where denotes the&#10;Jeffreys’ mixture when the underlying parameter space is the&#10;-dimensional simplex, follows from the fact that and follows from&#10;Stirling’s approximation which can be seen in . Since where the&#10;supremization in the left side of is over all whose last entries are&#10;zero, the converse result in Section \[sec:converse\] with , and the&#10;fact that along with routine algebraic manipulations yield the desired&#10;result in .">
</outline>
<outline text="Acknowledgments" _note="This work has been supported by ARO-MURI contract number&#10;W911NF-15-1-0479 and in part by the Center for Science of Information,&#10;an NSF Science and Technology Center under Grant CCF-0939370.&#10;&#10;[Semih Yagli]{} received his Bachelor of Science degree in Electrical&#10;and Electronics Engineering in 2013, his Bachelor of Science degree in&#10;Mathematics in 2014 both from Middle East Technical University and his&#10;Master of Arts degree in Electrical Engineering in 2016 from Princeton&#10;University.&#10;&#10;Currently, he is pursuing his Ph.D. degree in Electrical Engineering in&#10;Princeton University under the supervision of Sergio Verdú. His research&#10;interest include information theory, optimization, and machine learning.&#10;&#10;[Yücel Altuğ]{} received the B.S. and M.S. degrees in electrical and&#10;electronics engineering from Boğaziçi University, Turkey, in 2006 and&#10;2008, respectively and the Ph.D. degree in electrical and computer&#10;engineering from Cornell University, in 2013, where he has been awarded&#10;the ECE Director’s Ph.D. Thesis Research Award. After postdoctoral&#10;appointments at Cornell University and Princeton University, he is&#10;currently a senior data scientist at Natera Inc. His research interests&#10;include Shannon theory, feedback communications, and stochastic modeling&#10;and algorithm design for next-generation DNA sequencing and genetic&#10;testing.&#10;&#10;[Sergio Verdú]{} received the Telecommunications Engineering degree from&#10;the Universitat Politècnica de Barcelona in 1980, and the Ph.D. degree&#10;in Electrical Engineering from the University of Illinois at&#10;Urbana-Champaign in 1984. Since then, he has been a member of the&#10;faculty of Princeton University, where he is the Eugene Higgins&#10;Professor of Electrical Engineering, and is a member of the Program in&#10;Applied and Computational Mathematics.&#10;&#10;Sergio Verdú is the recipient of the 2007 Claude E. Shannon Award, and&#10;the 2008 IEEE Richard W. Hamming Medal. He is a member of both the&#10;National Academy of Engineering and the National Academy of Sciences. In&#10;2016, Verdú received the National Academy of Sciences Award for&#10;Scientific Reviewing.&#10;&#10;Verdú is a recipient of several paper awards from the IEEE: the 1992&#10;Donald Fink Paper Award, the 1998 and 2012 Information Theory Paper&#10;Awards, an Information Theory Golden Jubilee Paper Award, the 2002&#10;Leonard Abraham Prize Award, the 2006 Joint Communications/Information&#10;Theory Paper Award, and the 2009 Stephen O. Rice Prize from the IEEE&#10;Communications Society. In 1998, Cambridge University Press published&#10;his book [MULTIUSER DETECTION,]{} for which he received the 2000&#10;Frederick E. Terman Award from the American Society for Engineering&#10;Education. He was awarded a Doctorate Honoris Causa from the Universitat&#10;Politècnica de Catalunya in 2005, and was elected corresponding member&#10;of the Real Academia de Ingeniería of Spain in 2013.&#10;&#10;Sergio Verdú served as President of the IEEE Information Theory Society&#10;in 1997, and on its Board of Governors (1988-1999, 2009-2014). He has&#10;also served in various editorial capacities for the [IEEE TRANSACTIONS&#10;ON INFORMATION THEORY]{}: Associate Editor (Shannon Theory, 1990-1993;&#10;Book Reviews, 2002-2006), Guest Editor of the Special Fiftieth&#10;Anniversary Commemorative Issue (published by IEEE Press as “Information&#10;Theory: Fifty years of discovery&quot;), and member of the Executive&#10;Editorial Board (2010-2013). He co-chaired the Europe-United States&#10;[FRONTIERS OF ENGINEERING]{} program, of the National Academy of&#10;Engineering during 2009-2013. He is the founding Editor-in-Chief of&#10;[FOUNDATIONS AND TRENDS IN COMMUNICATIONS AND INFORMATION THEORY]{}.&#10;Verdú served as co-chair of the 2000 and 2016 [IEEE INTERNATIONAL&#10;SYMPOSIA ON INFORMATION THEORY]{}.&#10;&#10;Sergio Verdú has held visiting appointments at the Australian National&#10;University, the Technion-Israel Institute of Technology, the University&#10;of Tokyo, the University of California, Berkeley, the Mathematical&#10;Sciences Research Institute, Berkeley, Stanford University, and the&#10;Massachusetts Institute of Technology.">
</outline>
  </body>
</opml>