<?xml version="1.0" encoding="UTF-8"?>
<opml version="2.0">
  <head>
    <title>\[0in\]\[0in\]\
Joint Semantic Synthesis and Morphological Analysis of the Derived Word</title>
    <abstract>Much like sentences are composed of words, words themselves are composed
of smaller units. For example, the English word [[QUESTIONABLY]{}]{} can
be analyzed as [[QUESTION]{}]{}[[ABLE]{}]{}[[LY]{}]{}. However, this [
STRUCTURAL]{} decomposition of the word does not directly give us a
SEMANTIC representation of the word’s meaning. Since morphology obeys
the principle of compositionality, the semantics of the word can be
systematically derived from the meaning of its parts. In this work, we
propose a novel probabilistic model of word formation that captures both
the ANALYSIS of a word into its constituent segments and the SYNTHESIS
of the meaning of from the meanings of those segments. Our model jointly
learns to SEGMENT words into morphemes and COMPOSE distributional
semantic vectors of those morphemes. We experiment with the model on
English CELEX data and German DErivBase data. We show that jointly
modeling semantics increases both segmentation accuracy and morpheme by
between 3% and 5%. Additionally, we investigate different models of
vector composition, showing that recurrent neural networks yield an
improvement over simple additive models. Finally, we study the degree to
which the representations correspond to a linguist’s notion of
morphological productivity. </abstract>
  </head>
  <body>
<outline text="#1">
</outline>
<outline text="Introduction" _note="In most languages, words decompose further into smaller units, termed&#10;morphemes. For example, the English word [[QUESTIONABLY]{}]{} can be&#10;analyzed as [[QUESTION]{}]{}[[ABLE]{}]{}[[LY]{}]{}. This [ STRUCTURAL]{}&#10;decomposition of the word, however, by itself is not a SEMANTIC&#10;representation of the word’s meaning;[^1] we further require an account&#10;of how to synthesize the meaning from the decomposition. Fortunately,&#10;words—just like phrases—to a large extent obey the principle of&#10;compositionality: the semantics of the word can be systematically&#10;derived from the meaning of its parts.[^2] In this work, we propose a&#10;novel joint probabilistic model of word formation that captures both&#10;STRUCTURAL DECOMPOSITION of a word into its constituent segments and the&#10;SYNTHESIS of ’s MEANING from the meaning of those segments.&#10;&#10;Morphological segmentation is a structured prediction task that seeks to&#10;break a word up into its constituent morphemes. The output segmentation&#10;has been shown to aid a diverse set of applications, such as automatic&#10;speech recognition , keyword spotting , machine translation and parsing&#10;. In contrast to much of this prior work, we focus on [SUPERVISED]{}&#10;segmentation, i.e., we provide the model with gold segmentations during&#10;training time. Instead of SURFACE segmentation, our model performs&#10;CANONICAL segmentation , i.e., it allows the induction of orthographic&#10;changes together with the segmentation, which is not typical. For the&#10;example [[QUESTIONABLY]{}]{}, our model can restore the deleted&#10;characters [[LE]{}]{}, yielding the canonical segments [[QUESTION]{}]{},&#10;[[ABLE]{}]{} and [[LY]{}]{}. In this work, our primary contribution lies&#10;in the integration of continuous semantic vectors into supervised&#10;morphological segmentation—we present a joint model of morphological&#10;analysis and semantic synthesis at the word-level.&#10;&#10;We experimentally investigate three novel aspects of our model.&#10;&#10;First, we show that jointly modeling continuous representations of the&#10;semantics of morphemes and words allows us to improve morphological&#10;analysis. On the English portion of CELEX , we achieve a 5 point&#10;improvement in segmentation accuracy and a 3 point improvement in&#10;morpheme . On the German DErivBase dataset we achieve a 3 point&#10;improvement in segmentation accuracy and a 3 point improvement in&#10;morpheme .&#10;&#10;Second, we explore improved models of vector composition for&#10;synthesizing word meaning. We find a recurrent neural network improves&#10;over previously proposed additive models. Moreover, we find that more&#10;syntactically oriented vectors are better suited for morphology than&#10;bag-of-word (BOW) models.&#10;&#10;Finally, we explore the productivity of English derivational affixes in&#10;the context of distributional semantics.&#10;&#10;[^1]: There are many different linguistic and computational theories for&#10;    interpreting the structural decomposition of a word. For example,&#10;    [[UN-]{}]{} often signifies negation and its effect on semantics can&#10;    then be modeled by theories based on logic. This work addresses the&#10;    question of structural decomposition and semantic synthesis in the&#10;    general framework of distributional semantics.&#10;&#10;[^2]: Morphological research in theoretical and computational&#10;    linguistics often focuses on noncompositional or less compositional&#10;    phenomena—simply because compositional derivation poses fewer&#10;    interesting research problems. It is also true that—just as many&#10;    frequent multiword units are not completely compositional—many&#10;    frequent derivations (e.g., [[REFUSAL]{}]{}, [[FITNESS]{}]{}) are&#10;    not completely compositional. An indication that non-lexicalized&#10;    derivations are usually compositional is the fact that standard&#10;    dictionaries like list derivational affixes with their compositional&#10;    meaning, without a hedge that they can also occur as part of only&#10;    partially compositional forms. See also , §5.3.6.">
</outline>
<outline text="Derivational Morphology" _note="Two important goals of morphology, the linguistic study of the internal&#10;structure of words, are to describe the relation between different words&#10;in the lexicon and to decompose them into [ MORPHEMES]{}, the smallest&#10;linguistic unit bearing meaning. Morphology can be divided into two&#10;types: [INFLECTIONAL]{} and [ DERIVATIONAL]{}. Inflectional morphology&#10;is the set of processes through which the word form outwardly displays&#10;syntactic information, e.g., verb tense. It follows that an inflectional&#10;affix typically neither changes the part-of-speech (POS) nor the&#10;semantics of the word. For example, the English verb [[TO RUN]{}]{}&#10;takes various forms: [[RUN]{}]{}, [[RUNS]{}]{}, [[RAN]{}]{} and&#10;[[RUNNING]{}]{}, all of which convey “moving by foot quickly”, but&#10;appear in complementary syntactic contexts.&#10;&#10;Derivation deals with the formation of new words that have semantic&#10;shifts in meaning (often including POS) and is tightly intertwined with&#10;lexical semantics . Consider the example of the English noun&#10;[[DISCONTENTEDNESS]{}]{}, which is derived from the adjective&#10;[[DISCONTENTED]{}]{}. It is true that both words share a close semantic&#10;relationship, but the transformation is clearly more than a simple&#10;inflectional marking of syntax. Indeed, we can go one step further and&#10;define a chain of words [[CONTENT]{}]{} [[CONTENTED]{}]{}&#10;[[DISCONTENTED]{}]{} [[DISCONTENTEDNESS]{}]{}.&#10;&#10;In the computational literature, derivational morphology has received&#10;less attention than inflectional. There are, however, two bodies of work&#10;on derivation in computational linguistics. First, there is a series of&#10;papers that explore the relation between lexical semantics and&#10;derivation . All of these assume a gold morphological analysis and&#10;primarily focus on the effect of derivation on distributional semantics.&#10;The second body of work, e.g., the unsupervised morphological segmenter&#10;[[Morfessor]{}]{} , does not deal with semantics and makes [NO&#10;DISTINCTION]{} between inflectional and derivational morphology.[^1]&#10;Even though the boundary between inflectional and derivational&#10;morphology is a continuum rather than a rigid divide , there is still&#10;the clear distinction that derivation changes meaning whereas inflection&#10;does not. Our goal in this paper is to develop an account of how the&#10;meaning of a word form can be computed jointly, combining these two&#10;lines of work.&#10;&#10;We highlight two related issues in derivation that motivated the&#10;development of our model: productivity and semantic coherence. Roughly,&#10;a [PRODUCTIVE]{} affix is one that can still actively be employed to&#10;form new words in a language. For example, the English nominalizing&#10;affix [[NESS]{}]{} ([[RED]{}]{}[[RED]{}]{}[[NESS]{}]{}) can be attached&#10;to just about any adjective, including novel forms. In contrast, the&#10;archaic English nominalizing affix [[TH]{}]{}&#10;([[DEAR]{}]{}[[DEAR]{}]{}[[TH]{}]{}, [[HEAL]{}]{}[[HEAL]{}]{}[[TH]{}]{},&#10;[[STEAL]{}]{}[[STEAL]{}]{}[[TH]{}]{}) does not allow us to form new&#10;words such as [[CHEAPTH]{}]{}. This is a crucial issue in derivational&#10;morphology since we would not in general want to analyze new words as&#10;having been formed from non-productive endings; e.g., we do not want to&#10;analyze [[HEARTH]{}]{} as [[HEAR]{}]{}[[TH]{}]{} (or [[WUGTH]{}]{} as&#10;[[WUG]{}]{}[[TH]{}]{}). Relations such as those between [[HEAL]{}]{} and&#10;[[HEALTH]{}]{} are [LEXICALIZED]{} since they no longer can be derived&#10;by productive processes .&#10;&#10;Under a generative treatment of morphology, productivity becomes a&#10;central notion since a grammar needs to account for active word&#10;formation processes in the language . Defining productivity precisely,&#10;however, is tricky; writes, [“ONE OF THE CENTRAL MYSTERIES OF&#10;DERIVATIONAL MORPHOLOGY … \[IS THAT\] … THOUGH MANY THINGS ARE POSSIBLE&#10;IN MORPHOLOGY, SOME ARE MORE POSSIBLE THAN OTHERS.”]{} Nevertheless,&#10;speakers often have clear intuitions about which affixes in the language&#10;are productive.[^2]&#10;&#10;Related to productivity is the notion of [SEMANTIC COHERENCE]{}. The&#10;principle of compositionality applies to interpretation of words just as&#10;it does to phrases. Indeed, compositionality is often taken to be a&#10;signal for productivity . When deciding whether to further decompose a&#10;word, asking whether the parts sum up to the whole is often a good&#10;indicator. In the case of [[QUESTIONABLY]{}]{}&#10;[[QUESTION]{}]{}[[ABLE]{}]{}[[LY]{}]{}, the compositional meaning is “in&#10;a manner that could be questioned”, which corresponds to the meaning of&#10;the word. Contrast this with the word [[UNQUIET]{}]{}, which means&#10;“restless”, rather than “not quiet” and the compound [[BLACKMAIL]{}]{},&#10;which does not refer to a letter written in black ink.&#10;&#10;The model we will describe in is a JOINT MODEL OF BOTH SEMANTIC&#10;COHERENCE AND SEGMENTATION; that is, an analysis is judged not only by&#10;character-level features, but also by the degree to which the word is&#10;semantically compositional. Implicit in such a treatment is the desire&#10;to only segment a word if the segmentation is derived from a productive&#10;process. While most prior work on morphological segmentation has not&#10;explicitly modeled productivity,[^3] we believe, from a computational&#10;modeling perspective, segmenting only productive affixes is preferable.&#10;This is analogous to the modeling of phrase compositionality in&#10;embedding models, where it can be better to not further decompose&#10;noncompositional multiword units like named entities and idiomatic&#10;expressions; see, e.g., , , , , and .[^4]&#10;&#10;In this paper, we refer to the semantic aspect of the model either as&#10;SEMANTIC SYNTHESIS or as COHERENCE. These are two ways of looking at&#10;semantics that are related as follows. If the synthesis (i.e.,&#10;composition) of the meaning of the derived form from the meaning of its&#10;parts is a regular application of the linguistic rules of derivation,&#10;then the meaning so constructed is coherent. These are the cases where a&#10;joint model is expected to be beneficial for both segmentation and&#10;interpretation.&#10;&#10;[^1]: also make no distinction between inflectional and derivational&#10;    morphology, but their model is an exception in that it includes&#10;    vector similarity as a semantic feature. See for discussion.&#10;&#10;[^2]: It is also important to distinguish productivity from [&#10;    CREATIVITY]{}—a non-rule-governed form of word formation . As an&#10;    example of creativity, consider the creation of portmanteaux, e.g.,&#10;    [[DRAMEDY]{}]{} and [[SOUNDSCAPE]{}]{}.&#10;&#10;[^3]: Note that segmenters such as [Morfessor]{} utilize the principle&#10;    of minimum description length, which implicitly encodes&#10;    productivity, in order to guide segmentation.&#10;&#10;[^4]: As a reviewer points out, productivity of an affix and semantic&#10;    coherence of the words formed from it are not perfectly aligned.&#10;    Nonproductive affixes can produce semantically coherent words, e.g.,&#10;    [[WARM]{}]{}[[WARM]{}]{}[[TH]{}]{}. Productive affixes can produce&#10;    semantically incoherent words, e.g.,&#10;    [[CANNY]{}]{}[[UN]{}]{}[[CANNY]{}]{}. Again, this is analogous to&#10;    multiword units. However, there is a strong correlation and our&#10;    experiments show that relying on it gives good results.">
</outline>
<outline text="A Joint Model" _note="From an NLP perspective, canonical segmentation is the task that seeks&#10;to algorithmically decompose a word into its [CANONICAL]{} sequence of&#10;morphemes. It is a version of morphological segmentation that requires&#10;the learner to handle orthographic changes that take place during word&#10;formation. We believe this is a more natural formulation of&#10;morphological analysis—especially for the processing of derivational&#10;morphology—as it draws heavily on linguistic notions (see ).&#10;&#10;The main innovation we present is the augmentation of canonical&#10;segmentation to take into account semantic coherence and productivity.&#10;Consider the word [[HYPERCURIOSITY]{}]{} and its canonical segmentation&#10;[[HYPER]{}]{}[[CURIOUS]{}]{}[[ITY]{}]{}; this canonical segmentation&#10;seeks to decompose the word into its constituent morphemes and account&#10;for orthographic changes. This amounts to a [STRUCTURAL]{} decomposition&#10;of the word, i.e., how do we break up the string of characters into&#10;chunks? This is similar to the decomposition of a sentence into a parse&#10;tree. However, it is also natural to consider the [SEMANTIC]{}&#10;compositionality of a word, i.e., how is the meaning of the word&#10;synthesized from the meaning of the individual morphemes?&#10;&#10;We consider both of these questions together in a single model, where we&#10;would like to place high probability on canonical segmentations that are&#10;also semantically coherent. Returning to [[HYPERCURIOSITY]{}]{}, we&#10;could further decompose it into&#10;[[HYPER]{}]{}[[CURE]{}]{}[[OUS]{}]{}[[ITY]{}]{} in analogy to, say,&#10;[[VICE]{}]{} [[VICIOUS]{}]{}. Nothing about the surface form of&#10;[CURIOUS]{} alone gives us a strong cue that we should rule out the&#10;segmentation [[CURE]{}]{}[[OUS]{}]{}. Turning to distributional&#10;semantics, however, it is the case that the contexts in which&#10;[[CURIOUS]{}]{} occurs are quite different from those in which&#10;[[CURE]{}]{} occurs. This gives us a strong cue which segmentation is&#10;correct.&#10;&#10;Formally, given a word string , where is a discrete alphabet of&#10;characters (in English this could be as simple as the 26 letter&#10;lowercase alphabet), and a word vector , where is a set of&#10;low-dimensional word embeddings, we define the model as: This model is&#10;composed of three factors: a composition factor, a segmentation factor&#10;and a transduction factor. The parameters of the model are , the&#10;function composes morpheme vectors together, is the segmentation, is the&#10;labeling of the segments, is the underlying representation and is the&#10;partition function. Note that the conditional distribution is Gaussian&#10;distributed by construction. A visualization of our model is found in .&#10;This model is a conditional random field (CRF) that is mixed, i.e., it&#10;is defined over both discrete and continuous random variables . We&#10;restrict the range of to be a subset of , where is an insertion limit .&#10;In this work, we take . Explicitly, the partition function is defined as&#10;which is guaranteed to be finite.[^1]&#10;&#10;A CRF is simply the globally renormalized product of several&#10;non-negative factors . Our model is composed of three: transduction,&#10;segmentation and composition factors—we describe each in turn.&#10;&#10;[^1]: Since we have capped the insertion limit, we have a finite number&#10;    of values that can take for any . Thus, it follows that we have a&#10;    finite number of canonical segmentations . Hence we take a finite&#10;    number of Gaussian integrals. These integrals all converge since we&#10;    have fixed the covariance matrix as , which is positive definite.">
  <outline text="Transduction Factor" _note="The first factor we consider is the transduction factor: , which scores&#10;a [ SURFACE REPRESENTATION]{} (SR) , the character string observed in&#10;raw text, and an [UNDERLYING REPRESENTATION]{} (UR), a character string&#10;with orthographic processes reversed. The aim of this factor is to place&#10;high weight on good pairs, e.g., the pair&#10;(=[[QUESTIONABLY]{}]{},=[[QUESTIONABLELY]{}]{}), so we can accurately&#10;restore character-level changes.&#10;&#10;We encode this portion of the model as a weighted finite-state machine&#10;for ease of computation. This factor generalizes probabilistic edit&#10;distance by looking at additional input and output context; see for&#10;details. As mentioned above and in contrast to , we bound the insertion&#10;limit in the edit distance model.[^1] Computing the score between two&#10;strings and requires a dynamic program that runs in . This is a&#10;generalization of the forward algorithm for Hidden Markov Models (HMMs)&#10;.&#10;&#10;We employ standard feature templates for the task that look at features&#10;of edit operations, e.g., substitute for , in varying context&#10;granularities. See for details. Recent work has also explored weighting&#10;of WFST arcs with scores computed by LSTMs , obviating the need for&#10;human selection of feature templates .&#10;&#10;[^1]: As our transduction model is an unnormalized factor in a CRF, we&#10;    do not require the local normalization discussed in —a weight on an&#10;    edge may be any non-negative real number since we will renormalize&#10;    later. The underlying model, however, remains the same.">
  </outline>
  <outline text="Segmentation Factor" _note="The second factor is the segmentation factor: . The goal of this factor&#10;is to score a segmentation of a UR . In our example, it scores the&#10;input-output pair ([[QUESTIONABLELY]{}]{},&#10;[[QUESTION]{}]{}[[ABLE]{}]{}[[LY]{}]{}). It additionally scores a&#10;labeling of the segmentation. Our label set in this work is . The proper&#10;labeling of the segmentation above is [[QUESTION]{}]{}[&#10;:STEM]{}[[ABLE]{}]{}[:SUFFIX]{}[[LY]{}]{}[:SUFFIX]{}. The labeling is&#10;critical for our composition functions : which vectors are used depends&#10;on the label given to the segment; e.g., the vectors of the prefix&#10;“post” and the stem “post” are different.&#10;&#10;We can view this factor as an unnormalized first-order semi-CRF .&#10;Computation of the factor again requires dynamic programming. The&#10;algorithm is a different generalization of the forward algorithm for&#10;HMMs, one that extends it to the semi-Markov case. This algorithm runs&#10;in .&#10;&#10;We again use standard feature templates for the task. We create atomic&#10;indicator features for the individual segments. We then conjoin the&#10;atomic features with left and right context features as well as the&#10;label to create more complex feature templates. We also include&#10;transition features that fire on pairs of sequential labels. See for&#10;details. Recent work has also showed that a neural parameterization can&#10;remove the need for manual feature design .">
  </outline>
  <outline text="Composition Factor" _note="The composition factor takes the form of an unnormalized multivariate&#10;Gaussian density: , where the mean is computed by the (potentially&#10;non-linear) composition function (See ) and the covariance matrix is a&#10;diagonal matrix. The goal of the composition function is to stitch&#10;together [ MORPHEME]{} embeddings to approximate the vector of the&#10;entire word.&#10;&#10;The simplest form of the composition function is ADD, an additive model&#10;of the morphemes. See : each vector refers to a morpheme-specific,&#10;label-dependent embedding. If , then represents a stem morpheme. Given&#10;that our segmentation is canonical, an that is a stem generally itself&#10;is an entry in the lexicon and . If , then we set to 0.[^1] We optimize&#10;over vectors with as they correspond to bound morphemes.&#10;&#10;We also consider a more expressive composition model, a recurrent neural&#10;network (RNN). Let be the number of segments. Then where is a hidden&#10;vector, defined by the recursion:[^2] . Again, we optimize the morpheme&#10;embeddings only when along with the other parameters of the RNN, i.e.,&#10;the matrices and .&#10;&#10;[^1]: This is not changed in training, so all such are 0 in the final&#10;    model. Clearly, this could be improved in future work as a reviewer&#10;    points out, e.g., by setting such to an average of a suitable chosen&#10;    set of known word vectors.&#10;&#10;[^2]: We do not explore more complex RNNs, e.g., LSTMs and GRUs as words&#10;    in our data have 7 morphemes. These architectures make the learning&#10;    of long distance dependencies easier, but are no more powerful than&#10;    an Elman RNN, at least in theory. Note that perhaps if applied to&#10;    languages with richer derivational morphology than English,&#10;    considering more complex neural architectures would make sense.">
  </outline>
</outline>
<outline text="Inference and Learning" _note="Exact inference is intractable since we allow arbitrary segment-level&#10;features on the canonicalized word forms . Since the semi-CRF factor has&#10;features that fire on substrings, we would need a dynamic programming&#10;state for each substring of each of the exponentially many settings of ;&#10;this breaks the dynamic program. We thus turn to approximate inference&#10;through an importance sampling routine .">
  <outline text="Inference by Importance Sampling" _note="Rather than considering all underlying orthographic forms and&#10;segmentations , we sample from a tractable proposal distribution —a&#10;distribution over canonical segmentations. In the following equations we&#10;omit the dependence on for notational brevity and define . Crucially,&#10;the partition function is [NOT]{} a function of parameter subvector and&#10;its gradient with respect to is 0.[^1] Recall that computing the&#10;gradient of the log-partition function is equivalent to the problem of&#10;marginal inference . We derive our estimator as follows:&#10;&#10;\_ Z &amp;= \_[(l, s,u) \~p]{} &amp;\&#10;&amp;= \_[l, s, u]{} p(l, s, u) [[[h]{}]{}]{}(l, s,u) &amp;\&#10;&amp;= \_[l, s, u]{} p(l, s, u) [[[h]{}]{}]{}(l, s,u) &amp;\&#10;&amp;= \_[(l, s, u) \~q]{},&#10;&#10;where we have omitted the dependence on (which we condition on) and&#10;(which we marginalize out). So long as has support everywhere does&#10;(i.e., ), the estimate is unbiased. Unfortunately, we can only&#10;efficiently compute up to a constant factor, . Thus, we use the INDIRECT&#10;IMPORTANCE SAMPLING ESTIMATOR, where and importance weights are defined&#10;as: This indirect estimator is biased, but consistent.[^2]&#10;&#10;The success of importance sampling depends on the choice of a “good”&#10;proposal distribution, i.e., one that ideally is close to . Since we are&#10;fully supervised at training time, we have the option of training&#10;locally normalized distributions for the individual components.&#10;Concretely, we train [ TWO]{} proposal distributions and that take the&#10;form of a WFST and a semi-CRF, respectively, using features identical to&#10;the joint model. Each of these distributions is tractable—we can compute&#10;the marginals with dynamic programming and thus sample efficiently. To&#10;draw samples , we sample sequentially from and then , conditioned on the&#10;output of .&#10;&#10;[^1]: The subvector is responsible for computing only the [MEAN]{} of&#10;    the Gaussian factor and thus has no impact on its normalization&#10;    coefficient .&#10;&#10;[^2]: Informally, the indirect importance sampling estimate converges to&#10;    the TRUE expectation as (the definition of statistical consistency).">
  </outline>
  <outline text="Learning" _note="We optimize the log-likelihood of the model using [[AdaGrad]{}]{} ,&#10;which is SGD with a special per-parameter learning rate. The full&#10;gradient of the objective for one training example is: where we use the&#10;importance sampling algorithm described in to approximate the gradient&#10;of the log-partition function, following . Note that depends on the&#10;composition function used. In the most complicated case when is a RNN,&#10;we can compute efficiently with backpropagation through time . We take&#10;importance samples; using so few samples can lead to a poor estimate of&#10;the gradient, but for our application it suffices. We employ&#10;regularization.">
  </outline>
  <outline text="Decoding" _note="Decoding the model is also intractable. To approximate the solution, we&#10;again employ importance sampling. We take 10,000 importance samples and&#10;select the highest weighted sample.">
  </outline>
</outline>
<outline text="Related Work" _note="The idea that vector semantics is useful for morphological segmentation&#10;is not new. Count vectors have been shown to be beneficial in the&#10;unsupervised induction of morphology . Embeddings were shown to act&#10;similarly . Our method differs from this line of research in two key&#10;ways. (i) We present a PROBABILISTIC model of the process of&#10;synthesizing the word’s meaning from the meaning of its morphemes. Prior&#10;work was either not probabilistic or did not explicitly model morphemes.&#10;(ii) Our method is supervised and focuses on derivation. and , being&#10;fully unsupervised, do not distinguish between inflection and derivation&#10;and focus on inflection. More recently, look at the unsupervised&#10;induction of “morphological chains” with semantic vectors as a crucial&#10;feature. Their goal is to jointly figure out an ordering of word&#10;formation and a morphological segmentation, e.g.,&#10;[[PLAY]{}]{}[[PLAYFUL]{}]{}[[PLAYFULNESS]{}]{}. While it is a rich model&#10;like ours, theirs differs in that it is unsupervised and uses vectors as&#10;features, rather than explicitly treating vector composition. All of the&#10;above work focuses on [SURFACE SEGMENTATION]{} and not [CANONICAL&#10;SEGMENTATION]{}, as we do.&#10;&#10;A related line of work that has different goals concerns morphological&#10;generation. Two recent papers that address this problem using deep&#10;learning are and . In an older line of work, and exploit log frequency&#10;ratios of inflectionally related forms to tease apart that, e.g., the&#10;past tense of [[SING]{}]{} is not [[SINGED]{}]{}, but instead&#10;[[SANG]{}]{}. Related work by uses a Dirichlet process to model a corpus&#10;as a “mixture of a paradigm”, allowing for the semi-supervised&#10;incorporation of distributional semantics into a structured model of&#10;inflectional paradigm completion.&#10;&#10;Our work is also related to recent attempts to integrate morphological&#10;knowledge into general embedding models. For example, train a&#10;log-bilinear language model that models the composition of morphological&#10;structure. Likewise, train a recursive neural network over a&#10;heuristically derived tree structure to learn morphological composition&#10;over continuous vectors. Our work is different in that we learn a joint&#10;model of segmentation and composition. Moreover, supervised&#10;morphological analysis can drastically outperform unsupervised analysis&#10;.&#10;&#10;Early work by can be interpreted as finite-state canonical segmentation,&#10;but it neither addresses nor experimentally evaluates the question of&#10;joint modeling of morphological analysis and semantic synthesis.&#10;Moreover, we may view canonicalization as an orthographic analogue to&#10;phonology. On this interpretation, the finite-state systems of , which&#10;computationally apply SPE-style phonological rules , may be run&#10;backwards to get canonical underlying forms.">
</outline>
<outline text="Experiments and Results" _note="We conduct experiments on English and German derivational morphology. We&#10;analyze our joint model’s ability to segment words into their canonical&#10;morphemes as well as its ability to compositionally derive vectors for&#10;new words. Finally, we explore the relationship between distributional&#10;semantics and morphological productivity.&#10;&#10;For English, we use the pretrained vectors of for all experiments. For&#10;German, we train word2vec skip-gram vectors on the German Wikipedia. We&#10;first describe our English dataset, the subset of the English portion of&#10;the CELEX lexical database that was selected by ; the dataset contains&#10;10,000 forms. This allows for comparison with previously proposed&#10;methods. We make two modifications. (i) make the TWO-MORPHEME&#10;ASSUMPTION: every word is composed of exactly two morphemes. In general,&#10;this is not true, so we further segment all complex words in the corpus.&#10;For example, [[FRIENDLESS]{}]{}[[NESS]{}]{} is further segmented into&#10;[[FRIEND]{}]{}[[LESS]{}]{}[[NESS]{}]{}. To nevertheless allow for fair&#10;comparison, we provide versions of our experiments with and without the&#10;two-morpheme assumption where appropriate. (ii) only provide a single&#10;train/test split. As we require a held-out development set for&#10;hyperparameter tuning, we randomly allocate a portion of the training&#10;data to select the hyperparameters and then retrain the model using&#10;these parameters on the original train split. We also report 10-fold&#10;cross validation results in addition to Lazaridou et al.’s train/test&#10;split.&#10;&#10;Our German dataset is taken from and is described in . It, again,&#10;consists of 10,000 derivational forms. We report results on 10-fold&#10;cross validation.">
  <outline text="Experiment 1: Canonical Segmentation" _note="For our first experiment, we test whether jointly modeling the&#10;continuous representations allows us to segment words more accurately.&#10;We assume that we are given an embedding for the target word. We&#10;estimate the model as described in with regularization . To evaluate, we&#10;decode the distribution . We perform approximate MAP inference with&#10;importance sampling—taking the sample with the highest score. In these&#10;experiments, we use the RNN with the dependency vectors, the combination&#10;of which performs best on vector approximation in .&#10;&#10;We follow the experimental design of . We compare against two baselines&#10;(marked “Baseline” in ): (i) a “Semi-CRF” segmenter that cannot account&#10;for orthographic changes and (ii) the full “Joint” model of .[^1] We&#10;additionally consider an “Oracle” setting, where we give the model the&#10;gold underlying orthographic form (“UR”) at both training and test time.&#10;This gives us insight into the performance of the transduction factor of&#10;our model, i.e., how much could we benefit from a richer model.&#10;&#10;Our hyperparameters are (i) the regularization coefficient and (ii) ,&#10;the variance of the Gaussian factor. We use grid search to tune them: ,&#10;.&#10;&#10;[width=.475]{}&#10;&#10;We use three metrics to evaluate segmentation accuracy. Note that the&#10;evaluation of canonical segmentation is hard since a system may return a&#10;sequence of morphemes whose concatenation is not the same length as the&#10;concatenation of the gold morphemes. This rules out metrics for surface&#10;segmentation like border , which require the strings to be of the same&#10;length.&#10;&#10;We now define the metrics. (i) SEGMENTATION ACCURACY measures whether&#10;every single canonical morpheme in the returned sequence is correct. It&#10;is inflexible: closer answers are penalized the same as more distant&#10;answers. (ii) MORPHEME takes the predicted sequence of canonical&#10;morphemes, turns it into a set, computes precision and recall in the&#10;standard way and based on that then computes . This metric gives credit&#10;if some of the canonical morphemes were correct. (iii) LEVENSHTEIN&#10;DISTANCE joins the canonical segments with a special symbol \# into a&#10;single string and computes the Levenshtein distance between predicted&#10;and gold strings.&#10;&#10;Results in show that jointly modeling semantic coherence improves our&#10;ability to analyze words. For test, our proposed joint model (“This&#10;Work”) outperforms the baseline supervised canonical segmenter, which is&#10;state-of-the-art for the task, by .05 (resp. .03) on accuracy and .03&#10;(resp. .03) on for English (resp. German). We also find that when we&#10;give the joint model an oracle UR the vectors generally help less: .01&#10;(resp. .02) on accuracy and .01 (resp. .03) on for English&#10;(resp. German). This indicates that the chief boon the vector&#10;composition factor provides lies in selection of an appropriate UR.&#10;Moreover, the up to .15 difference in English between systems with and&#10;without the oracle UR suggests that reversing orthographic changes is a&#10;particularly difficult part of the task, at least for English.&#10;&#10;[^1]: i.e., a model [WITHOUT]{} the Gaussian factor that scores vectors.">
  </outline>
  <outline text="Experiment 2: Vector Approximation" _note="We adopt the experimental design of . Its aim is to approximate a vector&#10;of a derivationally complex word using a learned model of composition.&#10;As assume a gold morphological analysis, we compare two settings: (i)&#10;oracle morphological analysis and (ii) inferred morphological analysis.&#10;To the best of our knowledge, (ii) is a novel experimental condition&#10;that no previous work has addressed.&#10;&#10;We consider four composition models (See ). (i) STEM, using just the&#10;stem vector. This baseline tells us what happens if we make the&#10;incorrect assumption that derivation behaves like inflection and is not&#10;meaning-changing. (ii) ADD, a purely additive model. This is arguably&#10;the simplest way of combining the vectors of the morphemes. (iii) LDS, a&#10;linear dynamical system. This is arguably the simplest sequence model.&#10;(iv) A (simple) RNN. Recurrent neural networks are currently the most&#10;widely used nonlinear sequence model and simple RNNs are the simplest&#10;such models.&#10;&#10;Part of the motivation for considering a richer class of models lies in&#10;our removal of the two-morpheme assumption. Indeed, it is unclear that&#10;the [WADD]{} and [FULLADD]{} models are useful models in the general&#10;case of multi-morphemic words—the weights are tied by [POSITION]{},&#10;i.e., the first morpheme’s vector (be it a prefix or stem) is always&#10;multiplied by the same matrix.&#10;&#10;To compare with , we use their exact train/test split. Those results are&#10;reported in . This dataset enforces that all words are composed of&#10;exactly two morphemes. Thus, a word like [[UNQUESTIONABLY]{}]{} is&#10;segmented as [[UN]{}]{}[[QUESTIONABLY]{}]{}, without further&#10;decomposition. The vectors employed by are high-dimensional count&#10;vectors derived from lemmatized and POS tagged text with a&#10;before-and-after window of size 2. They then apply pointwise mutual&#10;information (PMI) weighting and dimensionality reduction by non-negative&#10;matrix factorization. In contrast, we employ [[word2vec]{}]{} , a model&#10;that is also interpretable as the factorization of a PMI matrix . We&#10;consider three [[word2vec]{}]{} models: two bag-of-word (BOW) models&#10;with before-and-after windows of size 2 and 5 and DEPs , a&#10;dependency-based model whose context is derived from dependency parses&#10;rather than BOW.&#10;&#10;In general, the results indicate that the key to better vector&#10;approximation is not a richer model of composition, but rather lies in&#10;the vectors themselves. We find that our best model, the RNN, only&#10;marginally edges out the LDS. Additionally, looking at the “all” column&#10;and the DEPs vectors, the simple additive model is only .02 lower than&#10;LDS. In comparison, we observe large differences between the vectors.&#10;The RNN+DEPs model is .23 better than the BOW5 models (.81 vs. .58), .14&#10;better than the BOW2 models (.81 vs. .67) and .25 better than Lazaridou&#10;et al.’s best model (.81 vs. .56). A wider context for BOW (5 instead of&#10;2) yields worse results. This suggests that syntactic information or at&#10;least positional information is necessary for improved models of&#10;morpheme composition. The test vectors are annotated for relatedness,&#10;which is a proxy for semantic coherence. HR (high-relatedness) words&#10;were judged to be more compositional than LR (low-relatedness) words.&#10;&#10;As a further strong baseline, we consider a retrofitting approach based&#10;on character-level recurrent neural networks. Recently, running a&#10;recurrent net over the character stream has become a popular way of&#10;incorporating subword information into a model—empirical gains have been&#10;observed in a diverse set of NLP tasks: POS tagging , parsing and&#10;language modeling . To the best of our knowledge, character-level&#10;retrofitting is a novel approach. Given a vector for a word form , we&#10;seek a function to minimize the following objective where is the final&#10;hidden state of a recurrent neural architecture, i.e., where is a&#10;non-linearity and is the character in , is the previous hidden state and&#10;and are matrices. While we have defined the architecture for a vanilla&#10;RNN, we experiment with two more advanced recurrent architectures: GRUs&#10;and LSTMs as well as deep variants . Importantly, this model has [NO&#10;KNOWLEDGE]{} of morphology—it can only rely on representations it&#10;extracts from the characters. This gives us a clear ablation on the&#10;benefit of adding structured morphological knowledge. We optimize the&#10;depth and the size of the hidden units on development data using a&#10;coarse-grained grid search. We found a depth of 2 and hidden units of&#10;size 100 (in both LSTM and GRU) performed best. We trained all models&#10;for 100 iterations of Adam with regularization with regularization&#10;coefficient .&#10;&#10;shows that the two character-level models (“c-GRU” and “c-LSTM”) perform&#10;much worse than our models. This indicates that supervised morphological&#10;analysis produces higher-quality vector representations than&#10;“knowledge-poor” character-level models. However, we note that these&#10;character-level models have fewer parameters than our morpheme-level&#10;models—there are many more morphemes in a languages than characters.&#10;&#10;In general, the two-morpheme assumption is incorrect. We consider an&#10;expanded setting of ’s task, in which we fully decompose the word, e.g.,&#10;[[UNQUESTIONABLY]{}]{}[[UN]{}]{}[[QUESTION]{}]{}[[ABLE]{}]{}[[LY]{}]{}.&#10;These results are reported in (top block, “oracle”). We report mean&#10;cosine similarity. Standard deviations for 10-fold cross-validation (not&#10;shown) are small () with two exceptions: for the DEPs-joint-stem results&#10;(.411 and .412).&#10;&#10;The multi-morphemic results mirror those of the bi-morphemic setting of&#10;. (i) RNN+DEPs attains an average cosine similarity of around .80 for&#10;English. Numbers for German are lower, around .70. (ii) The RNN only&#10;marginally edges out LDS for English and is slightly worse for German.&#10;Again, this is not surprising as we are modeling short sequences. (iii)&#10;Certain embeddings lend themselves more naturally to derivational&#10;compositionality: BOW2 is better than BOW5, DEPs is the clear winner.&#10;&#10;The final setting we consider is the vector approximation task without&#10;gold morphology. In this case, we rely on the full joint model . At&#10;evaluation, we are interested in the marginal distribution . We then use&#10;importance sampling to approximate the mean of this marginal&#10;distribution as the predicted embedding, i.e., where are the importance&#10;weights defined in and and are the sampled labeling and segmentation,&#10;respectively.&#10;&#10;Surprisingly, (joint) shows that relying on the inferred morphology does&#10;not drastically affect the results. Indeed, we are often within .01 of&#10;the result with gold morphology. Our method can be viewed as a&#10;retrofitting procedure , so this result is useful: it indicates that&#10;joint semantic synthesis and morphological analysis produces&#10;high-quality vectors.">
  </outline>
  <outline text="Experiment 3: Derivational Productivity" _note="We now delve into the relation between distributional semantics and&#10;morphological productivity. The extent to which jointly modeling&#10;semantics aids morphological analysis will be determined by the inherent&#10;compositionality of the words within the vector space. We break down our&#10;results on the vector approximation task with gold morphology using the&#10;dependency vectors and the RNN composer in by selected affixes. We&#10;observe a wide range of scores: the most compositional ending [[LY]{}]{}&#10;gives rise to cosine similarities that are 20 points higher than those&#10;of the least compositional [[ER]{}]{}.&#10;&#10;On the left end of we see extremely productive suffixes. The affix&#10;[[IZE]{}]{} is used productively with relatively obscure words in the&#10;sciences, e.g., [[RAO-BLACKWELLIZE]{}]{}. Likewise, the affix&#10;[[NESS]{}]{} can be applied to almost any adjective without restriction,&#10;e.g., [[POISSONNESS]{}]{} ‘degree to which data have a Poisson&#10;distribution’. On the right end, we find [[-MENT]{}]{}, [[-ER]{}]{} and&#10;[[RE-]{}]{}. The affix [[-MENT]{}]{} is borderline productive —modern&#10;English tends to form novel nominalizations with [[NESS]{}]{} or&#10;[[ITY]{}]{}. More interesting are [[RE-]{}]{} and , both of which are&#10;very productive in English. For [[ER]{}]{}, many of the words bringing&#10;down the average are simply non-compositional. For example,&#10;[[HOMER]{}]{} ‘homerun in baseball’ is not derived from&#10;[[HOME]{}]{}[[ER]{}]{}—this is an error in data. We also see examples&#10;like [[CUTTER]{}]{}. It has a compositional reading (e.g., “box&#10;cutter”), but also frequently occurs in the non-compositional meaning&#10;‘type of boat’. Finally, proper nouns like [[HOMER]{}]{} and&#10;[[TURNER]{}]{} end in [[ER]{}]{} and in our experiments we computed&#10;vectors for lowercased words. The affix [[RE-]{}]{} similarly has a&#10;large number of non-compositional cases, e.g., [[REMOVE]{}]{},&#10;[[RELOCATE]{}]{}, [[REMARK]{}]{}. Indeed, to get the compositional&#10;reading of [[REMOVE]{}]{}, the first syllable (rather than the second)&#10;is typically stressed to emphasize the prefix.&#10;&#10;We finally note several limitations of this experiment. (i) The ability&#10;of our models—even the recurrent neural network—to model transformations&#10;between vectors is limited. (ii) Our vectors are far from perfect; e.g.,&#10;sparseness in the training data affects quality and some of the words in&#10;our corpus are rare. (iii) Semantic coherence is not the only criterion&#10;for productivity. An example is [[-TH]{}]{} in English. As noted&#10;earlier, it is compositional in a word like [[WARMTH]{}]{}, but it&#10;cannot be used to form new words.">
  </outline>
</outline>
<outline text="Conclusion" _note="We have presented a model of the semantics and structure of&#10;derivationally complex words. To the best of our knowledge, this is the&#10;first attempt to jointly consider, within a single model, (i) the&#10;morphological decomposition of the word form and (ii) the semantic&#10;coherence of the resulting analysis. We found that directly modeling&#10;coherence increases segmentation accuracy, improving over a strong&#10;baseline. Also, our models show state-of-the-art performance on the&#10;derivational vector approximation task introduced by .&#10;&#10;Future work will focus on the extension of the method to more complex&#10;instances of derivational morphology, e.g., compounding and&#10;reduplication, and on the extension to additional languages. We also&#10;plan to explore the relation between derivation and distributional&#10;semantics in greater detail.&#10;&#10;The first author was supported by a DAAD Long-Term Research Grant and an&#10;NDSEG fellowship and the second by a Volkswagenstiftung Opus Magnum&#10;grant. We would also like to thank action editor Regina Barzilay for&#10;suggesting several changes we incorporated into the work and the three&#10;anonymous reviewers.">
</outline>
  </body>
</opml>