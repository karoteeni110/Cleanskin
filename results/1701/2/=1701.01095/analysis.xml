<?xml version="1.0" encoding="UTF-8"?>
<opml version="2.0">
  <head>
    <title></title>
    <abstract> </abstract>
  </head>
  <body>
<outline text="Theoretical Analysis" _note="In this section we start by proving Prop. \[prop:mvn\_ts\] that provides&#10;a regret bound for TS with MVN priors that is independent from the&#10;preference function. Then we use the relations between the gap and the&#10;preference radius in three preference function families to obtain&#10;Theorem \[thm:mvn\_ts\].">
  <outline text="Proof of Prop. [prop:mvn_ts]" _note="The following analysis extends the work for the 1-dimensional setting &#10;to the -dimensional setting. We rewrite Eq. \[eqn:regret\] as where we&#10;control . The proof relies on several facts (see&#10;Appendix \[app:technical\_tools\]) that extend Chernoff’s inequalities&#10;and (anti-)concentration bounds from the -dimensional setting to the&#10;-dimensional setting using the concepts of Pareto-domination and&#10;preference radius. We introduce the following quantities and events to&#10;control the quality of mean estimations and the quality of samples.&#10;&#10;For each suboptimal action , we choose a quantity , where is a&#10;preference radius. By definition of the preference radii, we have .&#10;Recall that if . Hence we have .&#10;&#10;For each suboptimal action , define as the event that , and define as&#10;the event that . More specifically, they are the event that suboptimal&#10;action is well estimated and well sampled, respectively.&#10;&#10;Define filtration .&#10;&#10;For suboptimal action , we decompose and control each part separately.&#10;In (A), is played while being well estimated and well sampled. We&#10;control this by bounding poor estimation and poor samples for the&#10;optimal action. In (B), is played while being well estimated but poorly&#10;sampled. We control this using Gaussian concentration inequalities. In&#10;(C), is played while being poorly estimated. We control this using&#10;Chernoff inequalities. Gathering the following results together and&#10;summing over all suboptimal actions, we obtain Prop. \[prop:mvn\_ts\].">
    <outline text="Bounding (A)" _note="By definition of TS, for suboptimal to be played on episode , we must&#10;(at least) have . By definition of event and the preference radii, we&#10;have if . Let denote the time step at which action is selected for the&#10;time for , and let . Note that for any action , for and . Then&#10;&#10;The second inequality uses the fact that the sampling of is independent&#10;from the events and . The last inequality uses the observation that is&#10;fixed given and that it changes only when changes, that is only when&#10;action is played. The first sum counts the number of episodes required&#10;before action has been played times. The second counts the number of&#10;episodes where is badly sampled after having been played times. We use&#10;the following Lemma to control the first summation, see&#10;Appendix \[app:proof:counts\_before\_succ\].&#10;&#10;\[lem:counts\_before\_succ\] Let denote the time of the selection of&#10;action . Then, for any and , where is such that for .&#10;&#10;Now we bound the second summation in Eq. \[eqn:a\] by controlling the&#10;probability of poorly sampling when . Let denote the event that . Then&#10;we have The last inequality uses Facts \[fac:chernoffd\]&#10;and \[fac:concentration\]. With we obtain We use&#10;Lem. \[lem:counts\_before\_succ\] and&#10;Eq. \[eqn:a:prob\_bad\_theta\_star\] in Eq. \[eqn:a\] to obtain for ,&#10;where is such that for .">
    </outline>
    <outline text="Bounding (B)" _note="We control the probability of badly sampling suboptimal action given&#10;that it has been played at least times. Recall that filtration is such&#10;that holds. To that extent we decompose The first inequality uses the&#10;observation that is fixed given and the definition of event . The second&#10;inequality uses the fact that event holds. The last inequality uses&#10;Fact \[fac:concentration\]. With we obtain">
    </outline>
    <outline text="Bounding (C)" _note="Similarly to what has been done previously with (B), we can control the&#10;probability of badly estimating suboptimal action given that it has been&#10;played at least times. Then we have The second inequality uses the&#10;observation that is fixed given . The last inequality uses&#10;Fact \[fac:chernoffd\]. With we obtain">
    </outline>
  </outline>
  <outline text="Proof of Theorem [thm:mvn_ts]" _note="By definition of the preference radii, given a linear&#10;(Ex. \[ex:linear\]), Chebyshev (Ex. \[ex:chebyshev\]), or -constraint&#10;preference function (Ex. \[ex:epsilon-constraint\]), one can take , .&#10;Using these values in Prop. \[prop:mvn\_ts\], we obtain&#10;Theorem \[thm:mvn\_ts\]: Let , for . The regret is bounded by with ,&#10;that is of order . More specifically, for , the regret bound is of order&#10;.">
  </outline>
</outline>
  </body>
</opml>