<?xml version="1.0" encoding="UTF-8"?>
<opml version="2.0">
  <head>
    <title></title>
    <abstract> </abstract>
  </head>
  <body>
<outline text="Experiments" _note="Given that the preference function is known a priori, one might be&#10;tempted to formalize the problem under the TRADITIONAL,&#10;single-objective, bandits setting. This would correspond to optimizing&#10;over the expected value of the preference function, , instead of . In&#10;the following experiments, we compare the performance of the TS&#10;algorithm from MVN priors (Alg. \[alg:mvn\_ts\]) in the multi-objective&#10;bandits scheme (Alg. \[alg:mobandits\]) with the one-dimensional TS from&#10;Gaussian priors  applied to the multi-objective bandits problem&#10;formalized under the traditional bandits setting&#10;(Alg. \[alg:gaussian\_ts\]).&#10;&#10;sample play and observe&#10;&#10;We randomly generate a 10-action setting with objectives, such that the&#10;objective space is . We consider settings where outcomes are sampled&#10;from multivariate normal distributions with covariance for all and from&#10;multi-Bernoulli distributions. A sample from a -dimensional&#10;multi-Bernoulli distribution with mean is such that . Experiments are&#10;conducted using the linear preference function and the -constraint&#10;preference function Tab. \[tab:experiments:setting\] gives the expected&#10;outcomes for all actions along with the associated preference value and&#10;gap given the preference function. Fig. \[fig:experiments:setting\]&#10;shows the expected outcomes and illustrates the preference function. We&#10;observe that the optimal action is different for the two preference&#10;functions. Each experiment is conducted over episodes and repeated 100&#10;times. Repetitions have been made such that the noise is the same for&#10;all tested approaches on THE SAME REPETITION. Therefore we can compare&#10;the performance of different approaches on the same repetition. The goal&#10;is to minimize the cumulative regret (Eq. \[eqn:regret\]).&#10;&#10;\[tab:experiments:setting\]&#10;&#10;[0.41]{}&#10;&#10;[0.41]{}&#10;&#10;[0.49]{}&#10;&#10;[0.49]{}&#10;&#10;[0.49]{}&#10;&#10;[0.49]{}&#10;&#10;Fig. \[fig:experiments:results\] shows the cumulative regret of TS from&#10;MVN priors and TS from Gaussian priors (in the traditional bandits&#10;formulation) for both outcome distributions and preference functions. We&#10;observe that the cumulative regret growth rate for TS from MVN priors&#10;appears to match the order of the provided theoretical bounds&#10;(Theorem \[thm:mvn\_ts\]). Results also show that, though it might be&#10;appealing to address a multi-objective problem as a single-objective&#10;bandits problem, it is not a good idea. Consider the -constraint&#10;preference function used in this experiment. It is evaluated as 0 if ,&#10;otherwise to . With multi-Bernoulli outcomes, for example, this means&#10;that . Given that, . Since the action considered as optimal in the&#10;single-objective formulation is not the same as the optimal action in&#10;the multi-objective problem, TS with Gaussian priors converges to the&#10;WRONG action, hence the linear regret.">
</outline>
  </body>
</opml>