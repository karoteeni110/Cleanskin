<?xml version="1.0" encoding="UTF-8"?>
<opml version="2.0">
  <head>
    <title></title>
    <abstract> </abstract>
  </head>
  <body>
<outline text="Conclusion" _note="In this work, we have addressed the online multi-objective optimization&#10;problem under the multi-objective bandits setting. Unlike previous&#10;formulations, we work in the a priori setting, where there exists a&#10;preference function to be maximized. However, acting in the the proposed&#10;setting would not require the preference function to be KNOWN. Indeed,&#10;it would be sufficient for an expert user to pick her preferred estimate&#10;among a set of options with no requirement of providing an actual, real&#10;valued, evaluation of each option. We have introduced the concept of&#10;preference radius to characterize the difficulty of a multi-objective&#10;setting through the robustness of the preference function to the quality&#10;of estimations available. We have shown how this measure relates to the&#10;gap between the optimal action and the recommended action by a learning&#10;algorithm. We have used this new concept to provide a theoretical&#10;analysis of the Thompson sampling algorithm from multivariate normal&#10;priors in the multi-objective setting. More specifically, we were able&#10;to provide regret bounds for three families of preference functions.&#10;Empirical experiments confirmed the expected behavior of the&#10;multi-objective Thompson sampling in terms of cumulative regret growth.&#10;Results also highlight the important fact that one cannot simply reduce&#10;a multi-objective setting to a traditional, single-objective, setting&#10;since this might cause a change in the optimal action. Future work&#10;includes the application of the proposed approach to a real world&#10;application.">
</outline>
  </body>
</opml>