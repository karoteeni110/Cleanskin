<?xml version="1.0" encoding="UTF-8"?>
<opml version="2.0">
  <head>
    <title></title>
    <abstract> </abstract>
  </head>
  <body>
<outline text="Thompson Sampling" _note="The Thompson sampling (TS)  algorithm maintains a posterior distribution&#10;on the mean given a prior and the history of observations . On each&#10;episode , one option is sampled from each posterior distribution . The&#10;algorithm selects . Recall that . Therefore is proportionnal to the&#10;posterior probability that maximizes the preference function given the&#10;history . Let denote the number of times action has been played up to&#10;episode . Also let respectively denote the empirical mean and&#10;covariance, and let and denote priors. For MVN priors, the posterior&#10;over is given by a MVN distribution , where for the known covariance&#10;matrix . Since assuming that is known might be unrealistic in practice,&#10;one can consider the non-informative covariance . With non-informative&#10;priors and ,[^1] this corresponds to a direct extension of the&#10;one-dimensional TS from Gaussian priors . Alg. \[alg:mvn\_ts\] shows the&#10;resulting TS procedure from MVN priors.&#10;&#10;sample play and observe&#10;&#10;The following proposition provides general regret bounds for TS from MVN&#10;priors. The next theorem specializes these regret bounds for three well&#10;known preference function families using the relation between preference&#10;radii and the gap, as discussed in previous examples.&#10;&#10;\[prop:mvn\_ts\] Assuming -sub-Gaussian noise with , the expected regret&#10;of TS from MVN priors (Alg. \[alg:mvn\_ts\]) is bounded by where , are&#10;preference radii, , and is such that for (see&#10;Remark \[remark:const\_c\]).&#10;&#10;\[thm:mvn\_ts\] Assume either a linear (Ex. \[ex:linear\]), Chebyshev&#10;(Ex. \[ex:chebyshev\]), or -constraint (Ex. \[ex:epsilon-constraint\])&#10;preference function. Assuming -sub-Gaussian noise with , the expected&#10;regret of TS from MVN priors (Alg. \[alg:mvn\_ts\]) is bounded by where&#10;is such that for (see Remark \[remark:const\_c\]). This regret bound is&#10;of order , where . More specifically, for , it is of order .&#10;&#10;\[remark:const\_c\] For we can take . For we can take , for we can take&#10;, and so on for any .&#10;&#10;For , the order of the regret bounds given by Theorem \[thm:mvn\_ts\]&#10;match the order of the regret bounds for TS from Gaussian priors in the&#10;single-objective bandits setting , assuming -bounded outcomes. However&#10;we observe that the noise tolerance decreases linearly with the&#10;dimension of the objective space. This means that the more dimensions we&#10;have, the less noise we can bear in order for these bounds to hold,&#10;GIVEN THE PROVIDED ANALYSIS.&#10;&#10;[^1]:  indicates a -elements column vector and indicates a identity&#10;    matrix.">
</outline>
  </body>
</opml>