<?xml version="1.0" encoding="UTF-8"?>
<opml version="2.0">
  <head>
    <title></title>
    <abstract> </abstract>
  </head>
  <body>
<outline text="Introduction" _note="Multi-objective optimization (MOO)  is a topic of great importance for&#10;real-world applications. Indeed, optimization problems are characterized&#10;by a number of conflicting, even contradictory, performance measures&#10;relevant to the task at hand. For example, when deciding on the&#10;healthcare treatment to follow for a given sick patient, a trade-off&#10;must be made between the efficiency of the treatment to heal the&#10;sickness, the side effects of the treatment, and the treatment cost. MOO&#10;is often tackled by combining the objective into a single measure&#10;(a.k.a. scalarization). Such approaches are said to be A PRIORI, as the&#10;preferences over the objectives is defined before carrying out the&#10;optimization itself. The challenge lies in the determination of the&#10;appropriate scalarization function to use and its parameterization.&#10;Another way to conduct MOO consists in learning the optimal trade-offs&#10;(the so-called Pareto-optimal set). Once the optimization is completed,&#10;techniques from the field of multi-criteria decision-making are applied&#10;to help the user to select the final solution from the Pareto-optimal&#10;set. These A POSTERIORI techniques may require a huge number of&#10;evaluations to have a reliable estimation of the objective values over&#10;all potential solutions. Indeed, the Pareto-optimal set can be quite&#10;large, encompassing a majority, if not all, of the potential solutions.&#10;In this work, we tackle the MOO problem where the scalarization function&#10;EXISTS a priori, but might be unknown, in which case a user can act as a&#10;black box for articulating preferences. Integrating the user to the&#10;learning loop, she can provide feedback by selecting her preferred&#10;choice given a set of options – the scalarization function lying in her&#10;head.&#10;&#10;More specifically, we consider problems where outcomes are stochastic&#10;and costly to evaluate (e.g., involving a human in the loop). The&#10;challenge is therefore to identify the best solutions given random&#10;observations sampled from different (unknown) density distributions. We&#10;formulate this problem as multi-objective bandits, where we aim at&#10;finding the solution that maximizes the preference function while&#10;maximizing the performance of the solutions evaluated during the&#10;optimization. The Thompson sampling (TS)  technique is a typical&#10;approach for bandits problems, where potential solutions are tried based&#10;on a Bayesian posterior over their expected outcome. Here we consider TS&#10;from multivariate normal (MVN) priors for multi-objective bandits. We&#10;introduce the concept of preference radius providing the tolerance range&#10;over objective value estimations, such that the BEST OPTION given the&#10;preference function remains unchanged. We use this concept for providing&#10;a theoretical analysis of TS from MVN priors. Finally, we perform some&#10;empirical experiments to support the theoretical results and also&#10;highlight the importance of tackling multi-objective bandits problems as&#10;such instead of scalarizing those under the traditional bandit setting.">
</outline>
  </body>
</opml>