<?xml version="1.0" encoding="UTF-8"?>
<opml version="2.0">
  <head>
    <title></title>
    <abstract> </abstract>
  </head>
  <body>
<outline text="Preference Radius" _note="Let denote the estimation associated with action on episode and let&#10;denote the estimated Pareto front given these options. By definition,&#10;the optimal options are . Let denote a ball of center and radius . In&#10;order to characterize the difficulty of a multi-objective bandits&#10;setting, we introduce the following quantity.&#10;&#10;For each action , we define the preference radius as any radius such&#10;that if for all actions, then&#10;&#10;The radii correspond to the ROBUSTNESS of the preference function, that&#10;is to which extent can actions be poorly estimated simultaneously before&#10;the set of optimal options changes. The radius is directly linked to the&#10;gap . For a suboptimal action, a large radius indicates that this action&#10;is far from being optimal. Also, the preference radii of suboptimal&#10;actions depend on the preference radius of the optimal action(s). Larger&#10;optimal action radii imply smaller radii for suboptimal actions. Note&#10;that if all actions estimates stand in their preference balls, being&#10;greedy is optimal.&#10;&#10;Let denote WEIGHTS such that . The weighted metric with is often used to&#10;represent decision functions. This function is known as the linear&#10;scalarization when and as the Chebyshev scalarization when . The&#10;following examples show the link between the preference radii and the&#10;gap for these two common functions.&#10;&#10;[0.35]{}&#10;&#10;[0.35]{}&#10;&#10;\[ex:linear\] The linear scalarization function is given by Consider the&#10;optimal action and the suboptimal action . By definition of the&#10;preference radii, we have that&#10;Fig. \[fig:pref\_radius:example:weigthed\_sum\] shows examples of&#10;preference radii with a linear preference function.&#10;&#10;\[ex:chebyshev\] The Chebyshev scalarization  function is given by&#10;Consider the optimal and suboptimal actions and , and let By definition&#10;of the preference radii, we have that The difficulty here is that and&#10;respectively depend on and . Consider a 2-objective setting, we can&#10;define as thresholds such that&#10;Fig. \[fig:pref\_radius:example:chebyshev\] shows examples of preference&#10;radii with a Chebyshev preference function.&#10;&#10;[0.35]{}&#10;&#10;[0.35]{}&#10;&#10;Outside metrics, other scalarization functions are often based on&#10;constraints. For example, using the -constraint scalarization technique,&#10;a user assigns a constraint to every objective except a target objective&#10;. All options that fail to respect one of the contraints receive a value&#10;of 0, while the options that respect all constraints get a value of .&#10;The following example shows the relation between the preference radius&#10;and the gap given a preference function that is articulated as an&#10;-constraint scalarization technique.&#10;&#10;\[ex:epsilon-constraint\] The -constraint function is given by Consider&#10;the optimal and suboptimal actions and . By definition of the preference&#10;radii, we have that We decompose such that denotes the radius required&#10;in order for action a to respect the constraints, that is to obtain ,&#10;and denotes the leftover leading to a gap reduction. Finally, we have&#10;that Fig. \[fig:pref\_radius:example:econstraint\] shows examples of&#10;preference radii with -constraint preference functions.&#10;&#10;[0.35]{}&#10;&#10;[0.35]{}">
</outline>
  </body>
</opml>