<?xml version="1.0" encoding="UTF-8"?>
<opml version="2.0">
  <head>
    <title></title>
    <abstract> </abstract>
  </head>
  <body>
<outline text="Technical Tools" _note="\[fac:chernoffd\] Let be i.i.d. -sub-Gaussian variables with values in&#10;such that . Let . Then, as shown by , for any , Now consider the the&#10;multivariate setting where are i.i.d. -dimensional -sub-Gaussian&#10;variables such that and . Then for any ,&#10;&#10;\[fac:concentration\] Let be a Gaussian random variable with mean and&#10;standard deviation . The following concentration is derived  from for :&#10;Now consider the multivariate setting where denotes a -dimensional&#10;Gaussian random variable with mean and diagonal covariance . Then for ,&#10;&#10;\[fac:anti\_concentration\] Let be a Gaussian random variable with mean&#10;and standard deviation . The following concentration is derived  from&#10;for : Now consider the multivariate setting where denotes a -dimensional&#10;Gaussian random variable with mean and diagonal covariance . Then for ,">
</outline>
<outline text="Proof of Lemma [lem:counts_before_succ]" _note="Let denote a distributed multivariate normal random variable. Let be a&#10;geometric variable denoting the number of consecutive independent trials&#10;until . Then observe that We want to bound the expected value of by a&#10;constant for all . Consider any integer , let , and let denote the&#10;MAXIMUM PREFERENCE of independent samples of , that is . We abbreviate&#10;as and as in the following. Then Using Fact \[fac:anti\_concentration\],&#10;this gives where the second inequality uses that and the last inequality&#10;uses that . Also, using Fact \[fac:chernoffd\], we have Substituting, we&#10;obtain and where is such that for . We observe that is required in order&#10;for the sum to converge.">
</outline>
  </body>
</opml>