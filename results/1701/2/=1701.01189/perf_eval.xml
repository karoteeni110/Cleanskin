<?xml version="1.0" encoding="UTF-8"?>
<opml version="2.0">
  <head>
    <title></title>
    <abstract> </abstract>
  </head>
  <body>
<outline text="Performance Evaluation" _note="In this section we evaluate our multisplit methods and analyze their&#10;performance. First, we discuss a few characteristics in our simulations:">
  <outline text="Simulation Framework" _note="All experiments are run on a NVIDIA K40c with the Kepler architecture,&#10;and a NVIDIA GeForce GTX 1080 with the Pascal architecture&#10;(Table \[table:gpus\]). All programs are compiled with NVIDIA’s nvcc&#10;compiler (version 8.0.44). The authors have implemented all codes except&#10;for device-wide scan operations and radix sort, which are from CUB&#10;(version 1.6.4). All experiments are run over 50 independent trials.&#10;Since the main focus of this paper is on multisplit as a GPU primitive&#10;within the context of a larger GPU application, we assume that all&#10;required data is already in the GPU’s memory and hence no transfer time&#10;is included.&#10;&#10;Some server NVIDIA GPUs (such as Tesla K40c) provide an error correcting&#10;code (ECC) feature to decrease occurrence of unpredictable memory errors&#10;(mostly due to physical noise perturbations within the device in&#10;long-running applications). ECCs are by default enabled in these&#10;devices, which means that hardware dedicates a portion of its bandwidth&#10;to extra parity bits to make sure all memory transfers are handled&#10;correctly (with more probability). Some developers prefer to disable ECC&#10;to get more bandwidth from these devices. In this work, in order to&#10;provide a more general discussion, we opt to consider three main&#10;hardware choices: 1) Tesla K40c with ECC enabled (default), 2) Tesla&#10;K40c with ECC disabled, and 3) GeForce GTX 1080 (no ECC option).">
  </outline>
  <outline text="Bucket identification" _note="The choice of bucket identification directly impacts performance results&#10;of any multisplit method, including ours. We support user-defined bucket&#10;identifiers. These can be as simple as unary functions, or complicated&#10;functors with arbitrary local arguments. For example, one could utilize&#10;a functor which determines whether a key is prime or not. Our&#10;implementation is simple enough to let users easily change the bucket&#10;identifiers as they please.&#10;&#10;In this section, we assume a simple user-defined bucket identifier as&#10;follows: buckets are assumed to be of equal width and to partition the&#10;whole key domain (DELTA-BUCKETS). For example, for an arbitrary key ,&#10;bucket IDs can be computed by a single integer division (i.e., ). Later,&#10;in Section \[subsec:multisplit\_sort\] we will consider a simpler bucket&#10;identifier (IDENTITY BUCKETS): where keys are equal to their bucket IDs&#10;(i.e., ). This is particularly useful when we want to use our multisplit&#10;algorithm to implement a radix sort. In&#10;Section \[subsec:multisplit\_histogram\] we use more complicated&#10;identifiers as follows: given a set of arbitrary splitters , for each&#10;key , finding those splitters (i.e., bucket ) such that . This type of&#10;identification requires performing a binary search over all splitters&#10;per input key.">
  </outline>
  <outline text="Key distribution" _note="Throughout this paper we assume uniform distribution of keys among&#10;buckets (except in Section \[subsec:perf\_distribution\] where we&#10;consider other distributions), meaning that keys are randomly generated&#10;such that there are, on average, equal number of elements within each&#10;bucket. For delta-buckets and identity buckets (or any other linear&#10;bucket identifier), this criteria results in uniform distribution of&#10;keys in the key domain as well. For more complicated nonlinear bucket&#10;identifiers this does not generally hold true.">
  </outline>
  <outline text="Parameters" _note="In all our methods and for every GPU architecture we have used either:&#10;1) four warps per block (128 threads per block), where each warp&#10;processes 7 consecutive windows, or 2) eight warps per block where each&#10;warp processes 4 consecutive windows. Our key-only BMS for up to uses&#10;the former, while every other case uses the latter (including WMS and&#10;BMS for both key-only and key-value pairs). These options were chosen&#10;because they gave us the best performance experimentally.&#10;&#10;This is a trade off between easier inter-warp computations (fewer warps)&#10;versus easier intra-warp communications (fewer windows). By having fewer&#10;warps per block, all our inter-warp computations in BMS (segmented scans&#10;and reductions in Section \[subsubsec:block\_histogram\] and&#10;\[subsubsec:block\_offset\]) are directly improved, because each segment&#10;will be smaller-sized and hence fewer rounds of shuffles are required (&#10;rounds). On the other hand, we use subword optimizations to pack the&#10;intermediate results of 4 processed windows into a single 32-bit integer&#10;(a byte per bucket per window). This lets us communicate among threads&#10;within a warp by just using a single shuffle per 4 windows. Thus, by&#10;having fewer warps per block, if we want to load enough input keys to&#10;properly hide memory access latency, we would need more than 4 windows&#10;to be read by each warp (here we used 7), which doubles the total number&#10;of shuffles that we use.&#10;&#10;It is a common practice for GPU libraries, such as in CUB’s radix sort,&#10;to choose their internal parameters at runtime based on the GPU’s&#10;compute capability and architecture. These parameters may include the&#10;number of threads per block and the number of consecutive elements to be&#10;processed by a single thread. The optimal parameters may substantially&#10;differ on one architecture compared to the other. In our final API, we&#10;hid these internal parameters from the user; however, our experiments on&#10;the two GPUs we used (Tesla K40c with `sm_35` and GeForce GTX 1080 with&#10;`sm_61`) exhibited little difference between the optimal set of&#10;parameters for the best performance on each architecture.&#10;&#10;In our algorithms, we always use as many threads per warp as allowed on&#10;NVIDIA hardware (). Based on our reliance on warp-wide ballots and&#10;shuffles to perform our local computations (as discussed in&#10;Section \[subsec:histogram\]), using smaller-sized logical warps would&#10;mean having smaller sized subproblems (reducing potential local work and&#10;increasing global computations), which is unfavorable. On the other&#10;hand, providing a larger-sized warp in future hardware with efficient&#10;ballots and shuffles (e.g., performing ballot over 64 threads and&#10;storing results as a 64-bit bitvector) would directly improve all our&#10;algorithms.">
  </outline>
  <outline text="Common approaches and performance references">
    <outline text="Radix sort" _note="As we described in Section \[sec:init\_approaches\], not every&#10;multisplit problem can be solved by directly sorting input keys (or&#10;key-values). However, in certain scenarios where keys that belong to a&#10;lower indexed bucket are themselves smaller than keys belonging to&#10;larger indexed buckets (e.g., in delta-buckets), direct sorting results&#10;in a non-stable multisplit solution. In this work, as a point of&#10;reference, we compare our performance to a full sort (over 32-bit keys&#10;or key-values). Currently, the fastest GPU sort is provided by CUB’s&#10;radix sort (Table \[table:reference\]). With a uniform distribution of&#10;keys, radix sort’s performance is independent of the number of buckets;&#10;instead, it only depends on the number of significant bits.">
    </outline>
    <outline text="Reduced-bit sort" _note="Reduced-bit sort (RB-SORT) was introduced in&#10;Section \[sec:init\_approaches\] as the most competitive&#10;conventional-GPU-sort-based multisplit method. In this section, we will&#10;compare all our methods against RB-sort. We have implemented our own&#10;kernels to perform labeling (generating an auxiliary array of bucket&#10;IDs) and possible packing/unpacking (for key-value multisplit). For its&#10;sorting stage, we have used CUB’s radix sort.">
    </outline>
    <outline text="Scan-based splits" _note="Iterative scan-based split can be used on any number of buckets. For&#10;this method, we ideally have a completely balanced distribution of keys,&#10;which means in each round we run twice the number of splits as the&#10;previous round over half-sized subproblems. So, we can assume that in&#10;the best-case scenario, recursive (or iterative) scan-based split’s&#10;average running time is lower-bounded by (or ) times the runtime of a&#10;single scan-based split method. This ideal lower bound is not&#10;competitive for any of our scenarios, and thus we have not implemented&#10;this method for more than two buckets.">
    </outline>
  </outline>
  <outline text="Performance versus number of buckets: m less than or equal to 256" _note="In this section we analyze our performance as a function of the number&#10;of buckets (). Our methods differ in three principal ways: 1) how&#10;expensive are our local computations, 2) how expensive are our memory&#10;accesses, and 3) how much locality can be extracted by reordering.&#10;&#10;In general, our WMS method is faster for a small number of buckets and&#10;BMS is faster for a large number of buckets. Both are generally faster&#10;than RB-sort. There is a crossover between WMS and BMS (a number of&#10;buckets such that BMS becomes superior for all larger numbers of&#10;buckets) that may differ based on 1) whether multisplit is key-only or&#10;key-value, and 2) the GPU architecture and its available hardware&#10;resources. Key-value scenarios require more expensive data movements and&#10;hence benefit more from reordering (for better coalesced accesses). That&#10;being said, BMS requires more computational effort for its reordering&#10;(because of multiple synchronizations for communications among warps),&#10;but it is more effective after reordering (because it reorders larger&#10;sized subproblems compared to WMS). As a result, on each device, we&#10;expect to see this crossover with a smaller number of buckets for&#10;key-value multisplit vs. key-only.">
    <outline text="Average running time" _note="Table \[table:timing\] shows the average running time of different&#10;stages in each of our three approaches, and the reduced bit sort&#10;(RB-sort) method. All of our proposed methods have the same basic&#10;computational core, warp-wide local histogram, and local offset&#10;computations. Our methods differ in performance as the number of buckets&#10;increases for three major reasons (Table \[table:timing\]):&#10;&#10;Reordering process&#10;&#10;:   Reordering keys (key-values) requires extra computation and shared&#10;    memory accesses. Reordering is always more expensive for BMS as it&#10;    also requires inter-warp communications. These negative costs mostly&#10;    depend on the number of buckets , the number of warps per block ,&#10;    and the number of threads per warp .&#10;&#10;Increased locality from reordering&#10;&#10;:   Since block level subproblems have more elements than warp level&#10;    subproblems, BMS is always superior to WMS in terms of locality. On&#10;    average and for both methods, our achieved gain from locality&#10;    decreases by as increases.&#10;&#10;Global operations&#10;&#10;:   As described before, by increasing , the height of the matrix&#10;    increases. However, since BMS’s subproblem sizes are relatively&#10;    larger (by a factor of ), BMS requires fewer global operations&#10;    compared to DMS and WMS (because the smaller width of its ). As a&#10;    result, scan operations for both the DMS and WMS get significantly&#10;    more expensive, compared to other stages, as increases (as doubles,&#10;    the cost of scan for all methods also doubles).&#10;&#10;\&#10;&#10;Figure \[fig:avg\_time\] shows the average running time of our&#10;multisplit algorithms versus the number of buckets (). For small , BMS&#10;has the best locality (at the cost of substantial local work), but WMS&#10;achieves fairly good locality coupled with simple local computation; it&#10;is the fastest choice for small ( \[key-only, Tesla K40c\], \[key-value,&#10;Tesla K40c\], and \[key-only, GeForce GTX 1080\]). For larger , the&#10;superior memory locality of BMS coupled with a minimized global scan&#10;cost makes it the best method overall.&#10;&#10;Our multisplit methods are also almost always superior to the RB-sort&#10;method (except for the key-only case on Tesla K40c with ECC off). This&#10;is partly because of the extra overheads that we introduced for bucket&#10;identification and creating the label vector, and packing/unpacking&#10;stages for key-value multisplit. Even if we ignore these overheads,&#10;since RB-sort performs its operations and permutations over the label&#10;vector as well as original key (key-value) elements, its data movements&#10;are more expensive compared to all our multisplit methods that instead&#10;only process and permute original key (key-value) elements.[^1]&#10;&#10;For our user-defined delta-buckets and with a uniform distribution of&#10;keys among all 32-bit integers, by comparing&#10;Table \[table:reference\] and Table \[table:timing\] it becomes clear&#10;that our multisplit method outperforms radix sort by a significant&#10;margin. Figure \[fig:speedup\] shows our achieved speedup against the&#10;regular 32-bit radix sort performance (Table \[table:reference\]). We&#10;can achieve up to 9.7x (and 10.8x) for key-only (and key-value)&#10;multisplits against radix sort.&#10;&#10;\&#10;&#10;[^1]: In our comparisons against our own multisplit methods, RB-sort&#10;    will be the best sort-based multisplit method as long as our bucket&#10;    identifier cannot be interpreted as a selection of some consecutive&#10;    bits in its key’s binary representation (i.e., for some and ).&#10;    Otherwise, these cases can be handled directly by a radix sort over&#10;    a selection of bits (from the -th bit until the -th bit) and do not&#10;    require the extra overhead that we incur in RB-sort (i.e., sorting&#10;    certain bits from input keys is equivalent to a stable multisplit&#10;    solution). We will discuss this more thoroughly in&#10;    Section \[subsec:multisplit\_sort\].">
    </outline>
    <outline text="Processing rate, and multisplit speed of light" _note="It is instructive to compare any implementation to its “speed of light”:&#10;a processing rate that could not be exceeded. For multisplit’s speed of&#10;light, we consider that computations take no time and all memory&#10;accesses are fully coalesced. Our parallel model requires one single&#10;global read of all elements before our global scan operation to compute&#10;histograms. We assume the global scan operation is free. Then after the&#10;scan operation, we must read all keys (or key-value pairs) and then&#10;store them into their final positions. For multisplit on keys, we thus&#10;require 3 global memory accesses per key; 5 for key-value pairs. Our&#10;Tesla K40c has a peak memory bandwidth of 288 GB/s, so the speed of&#10;light for keys, given the many favorable assumptions we have made for&#10;it, is 24 Gkeys/s, and for key-value pairs is 14.4 G pairs/s. Similarly,&#10;our GTX 1080 has 320 GB/s memory bandwidth and similar computations give&#10;us a speed of light of 26.6 G keys/s for key-only case and 16 Gpairs/s&#10;for key-value pairs.&#10;&#10;Table \[table:ms\_rate\] shows our processing rates for 32M keys and&#10;key-value pairs using delta-buckets and with keys uniformly distributed&#10;among all buckets. WMS has the highest peak throughput (on 2 buckets):&#10;12.48 Gkeys/s on Tesla K40c (ECC on), 14.15 Gkeys/s on Tesla K40c (ECC&#10;off), and 18.93 Gkeys/s on GeForce GTX 1080. We achieve more than half&#10;the speed of light performance (60% on Tesla K40c and 71% on GeForce GTX&#10;1080) with 2 buckets. As the number of buckets increases, it is&#10;increasingly more costly to sweep all input keys to compute final&#10;permutations for each element. We neglected this important part in our&#10;speed of light estimation. With 32 buckets, we reach 7.57 G keys/s on&#10;Tesla K40c and 16.64 G keys/s on GeForce GTX 1080. While this is less&#10;than the 2-bucket case, it is still a significant fraction of our speed&#10;of light estimation (32% and 63% respectively).&#10;&#10;The main obstacles in achieving the speed of light performance are&#10;1) non-coalesced memory writes and 2) the non-negligible cost that we&#10;have to pay to sweep through all elements and compute permutations. The&#10;more registers and shared memory that we have (fast local storage as&#10;opposed to the global memory), the easier it is to break the whole&#10;problem into larger subproblems and localize required computations as&#10;much as possible. This is particularly clear from our results on the&#10;GeForce GTX 1080 compared to the Tesla K40c, where our performance&#10;improvement is proportionally more than just the GTX 1080’s global&#10;memory bandwidth improvement (presumably because of more available&#10;shared memory per SM).">
    </outline>
    <outline text="Performance on different GPU microarchitectures" _note="In our design we have not used any (micro)architecture-dependent&#10;optimizations and hence we do not expect radically different behavior on&#10;different GPUs, other than possible speedup differences based on the&#10;device’s capability. Here, we briefly discuss some of the issues related&#10;to hardware differences that we observed in our experiments.">
      <outline text="Tesla K40c" _note="It is not yet fully disclosed whether disabling ECC (which is a hardware&#10;feature and requires reboot after modifications) has any direct impact&#10;besides available memory bandwidth (such as available registers, etc.).&#10;For a very small number of buckets, our local computations are&#10;relatively cheap and hence having more available bandwidth (ECC off&#10;compared to ECC on) results in better overall performance&#10;(Table \[table:ms\_rate\]). The performance gap, however, decreases as&#10;the number of buckets increases. This is mainly because of computational&#10;bounds due to the increase in ballot, shuffle, and numerical integer&#10;operations as grows. CUB’s radix sort greatly improves on Tesla K40c&#10;when ECC is disabled (Table \[table:reference\]), and because of it,&#10;RB-sort improves accordingly. CUB has particular architecture-based&#10;fine-grained optimizations, and we suspect it is originally optimized&#10;for when ECC is disabled to use all hardware resources to exploit all&#10;available bandwidth as much as possible. We will discuss CUB further in&#10;Section \[subsec:multisplit\_sort\]. RB-sort’s speedups in&#10;Fig. \[fig:speedup\] are relatively less for when ECC is disabled&#10;compared to when it is enabled. The reason is not because RB-sort&#10;performs worse (Table \[table:ms\_rate\] shows otherwise), but rather&#10;because CUB’s regular radix sort (that we both use in RB-sort and&#10;compare against for speedup computations) improves when ECC is disabled&#10;(Table \[table:reference\]).">
      </outline>
      <outline text="GeForce GTX 1080" _note="This GPU is based on NVIDIA’s latest “Pascal” architecture. It both&#10;increases global memory bandwidth (320 GB/s) and appears to be better at&#10;hiding memory latency caused by non-coalesced memory accesses. The GTX&#10;1080 also has more available shared memory per SM, which results in more&#10;resident thread-blocks within each SM. As a result, it is much easier to&#10;fully occupy the device, and our results (Table \[table:ms\_rate\]) show&#10;this.">
      </outline>
    </outline>
  </outline>
  <outline text="Performance for more than 256 buckets" _note="So far, we have only characterized problems with buckets. As we noted in&#10;Section \[sec:radix\], we expect that as the number of buckets&#10;increases, multisplit converges to a sorting problem and we should see&#10;the performance of our multisplits and sorting-based multisplits&#10;converge as well.&#10;&#10;The main obstacle for efficient implementation of our multisplits for&#10;large bucket counts is the limited amount of shared memory available on&#10;GPUs for each thread-block. Our methods rely on having privatized&#10;portions of shared memory with integers per warp (total of bits/warp).&#10;As a result, as increases, we require more shared storage, which limits&#10;the number of resident thread-blocks per SM, which limits our ability to&#10;hide memory latency and hurts our performance. Even if occupancy was not&#10;the main issue, with the current GPU shared memory sizes (48 KB per SM&#10;for Tesla K40c, and 96 KB per SM to be shared by two blocks on GeForce&#10;GTX 1080), it would only be physically possible for us to scale our&#10;multisplit up to about (at most 12k buckets if we use only one warp per&#10;thread-block).&#10;&#10;In contrast, RB-sort does not face this problem. Its labeling stage (and&#10;the packing/unpacking stage required for key-value pairs) are&#10;independent of the number of buckets. However, the radix sort used for&#10;RB-sort’s sorting stage is itself internally scaled by a factor of ,&#10;which results in an overall logarithmic dependency for RB-sort vs. the&#10;number of buckets.">
    <outline text="Solutions for larger multisplits ()" _note="Our solution for handling more buckets is similar to how radix sort&#10;handles the same scalability problem: iterative usage of multisplit over&#10;buckets. However, this is not a general solution and it may not be&#10;possible for any general bucket identifier. For example, for&#10;delta-buckets with 257 buckets, we can treat the first 2 buckets&#10;together as one single super-bucket which makes the whole problem into&#10;256 buckets. Then, by two rounds of multisplit 1) on our new 256&#10;buckets, 2) on the initial first 2 buckets (the super-bucket), we can&#10;have a stable multisplit result. This approach can potentially be&#10;extended to any number of buckets, but only if our bucket identifier is&#10;suitable for such modifications. There are some hypothetical cases for&#10;which this approach is not possible (for instance, if our bucket&#10;identifier is a random hash function, nearby keys do not necessarily end&#10;up in nearby buckets).&#10;&#10;Nevertheless, if iterative usage is not possible, it is best to use&#10;RB-sort instead, as it appears to be quite competitive for a very large&#10;number of buckets. As a comparison with regular radix sort performance,&#10;on Tesla K40c (ECC on), RB-sort outperforms radix sort up to almost 32k&#10;keys and 16k key-value pairs. However, we reiterate that unlike RB-sort,&#10;which is always a correct solution for multisplit problems, direct usage&#10;of radix sort is not always a possible solution.">
    </outline>
  </outline>
  <outline text="Initial key distribution over buckets" _note="So far we have only considered scenarios in which initial key elements&#10;were uniformly distributed over buckets (i.e., a uniform histogram). In&#10;our implementations we have considered small subproblems (warp-sized for&#10;WMS and block-sized for BMS) compared to the total size of our initial&#10;key vector. Since these subproblems are relatively small, having a&#10;non-uniform distribution of keys means that we are more likely to see&#10;empty buckets in some of our subproblems; in practice, our methods would&#10;behave as if there were fewer buckets for those subproblems. All of our&#10;COMPUTATIONS (e.g., warp-level histograms) are data-independent and,&#10;given a fixed bucket count, would have the same performance for any&#10;distribution. However, our DATA MOVEMENT, especially after reordering,&#10;would benefit from having more elements within fewer buckets and none&#10;for some others (resulting in better locality for coalesced global&#10;writes). Consequently, the uniform distribution is the worst-case&#10;scenario for our methods.&#10;&#10;As an example of a non-uniform distribution, consider the binomial&#10;distribution. In general denotes a binomial distribution over buckets&#10;with a probability of success . For example, the probability that a key&#10;element belongs to bucket is . This distribution forces an unbalanced&#10;histogram as opposed to the uniform distribution. Note that by choosing&#10;, the expected number of keys within the th bucket will be . For&#10;example, with elements and total buckets, there will be on average&#10;almost 184 empty buckets (72%). Such an extreme distribution helps us&#10;evaluate the sensitivity of our multisplit algorithms to changes in&#10;input distribution.&#10;&#10;Figure \[fig:binomial\] shows the average running time versus the number&#10;of buckets for BMS and RB-sort with binomial and uniform distributions,&#10;on our Tesla K40c (ECC off). There are two immediate observations here.&#10;First, as the number of buckets increases, both algorithms become more&#10;sensitive to the input distribution of keys. This is mainly because, on&#10;average, there will be more empty buckets and our data movement will&#10;resemble situations where there are essentially much fewer number of&#10;buckets than the actual . Second, the sensitivity of our algorithms&#10;increases in key-value scenarios when compared to key-only scenarios,&#10;mainly because data movement is more expensive in the latter. As a&#10;result, any improvement in our data movement patterns (here caused by&#10;the input distribution of keys) in a key-only multisplit will be almost&#10;doubled in a key-value multisplit.&#10;&#10;To get some statistical sense over how much improvement we are getting,&#10;we ran multiple experiments with different input distributions, with&#10;delta-buckets as our bucket identifiers, and on different GPUs.&#10;Table \[table:distribution\] summarizes our results with buckets. In&#10;this table, we also consider a milder distribution where of total keys&#10;are uniformly distributed among buckets and the rest are within one&#10;random bucket (-uniform). BMS achieves up to 1.24x faster performance on&#10;GeForce GTX 1080 when input keys are distributed over buckets in the&#10;binomial distribution. Similarly, RB-sort achieve up to 1.15x faster on&#10;Tesla K40c (ECC off) with the binomial distribution. In general,&#10;compared to our methods, RB-sort seems to be less sensitive to changes&#10;to the input distribution.">
  </outline>
</outline>
  </body>
</opml>