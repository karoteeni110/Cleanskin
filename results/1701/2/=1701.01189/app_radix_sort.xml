<?xml version="1.0" encoding="UTF-8"?>
<opml version="2.0">
  <head>
    <title></title>
    <abstract> </abstract>
  </head>
  <body>
<outline text="Building a radix sort" _note="In Section \[sec:perf\_eval\], we concentrated on the performance&#10;evaluation of our multisplit methods with user-defined bucket&#10;identifiers, and in particular the delta-bucket example. In this&#10;section, we focus on identity buckets and how we can modify them to&#10;implement our own version of radix sort.">
  <outline text="Multisplit with identity buckets" _note="Suppose we have identity buckets, meaning that each key is identical to&#10;its bucket ID (i.e., , for ). In this case, sorting keys (at least the&#10;first bits of keys) turns out to be equivalent to the stable multisplit&#10;problem. In this case, there is no need for the extra overheads inherent&#10;in RB-sort; instead, a direct radix sort can be a competitive alternate&#10;solution.">
  </outline>
  <outline text="Radix sort" _note="In Section \[sec:radix\] we briefly discussed the way radix sort&#10;operates. Each round of radix sort sorts a group of bits in the input&#10;keys until all bits have been consumed. For CUB, for example, in the&#10;Kepler architecture (e.g., Tesla K40c) each group consists of 5&#10;consecutive bits, while for the more recent Pascal architecture (e.g.,&#10;GeForce GTX 1080), each group is 7 bits.[^1]&#10;&#10;[^1]: These stats are for CUB 1.6.4.">
  </outline>
  <outline text="Multisplit-sort" _note="Each round of radix sort is essentially bucketing its output based on&#10;the set of bits it considers in that round. If we define our buckets&#10;appropriately, multisplit can do the same. Suppose we have , where&#10;denotes bitwise shift to the right and is a bitwise AND operator.&#10;denotes our radix size (i.e., the size of the group of bits to be&#10;considered in each iteration). Then, with iterations of multisplit with&#10;as each iteration’s bucket identifier, we have built our own radix sort.">
  </outline>
  <outline text="High level similarities and differences with CUB radix sort" _note="At a high level, multisplit-sort is similar to CUB’s radix sort. In CUB,&#10;contiguous chunks of bits from input keys are sorted iteratively. Each&#10;iteration includes an up-sweep where bin counts are computed, a scan&#10;operation to compute offsets, and a down-sweep to actually perform the&#10;stable sorting on a selected chunk of bits (similar roles to our&#10;pre-scan, scan and post-scan stages in our multisplit). The most&#10;important differences are 1) shared memory privatization (CUB’s&#10;thread-level versus our warp-level), which decreases our shared memory&#10;usage compared to CUB, and 2) our extensive usage of warp-wide&#10;intrinsics versus CUB’s more traditional register-level computations.">
  </outline>
  <outline text="Performance Evaluation" _note="Our multisplit-based radix sort has competitive performance to CUB’s&#10;radix sort. Our experiments show that, for example on the GeForce GTX&#10;1080, our key-only sort can be as good as 0.9x of CUB’s performance,&#10;while our key-value sort can be as good as 1.1x faster than CUB’s.">
    <outline text="Multisplit with identity buckets" _note="We first compare our multisplit methods (WMS and BMS) with identity&#10;buckets to CUB’s radix sort over bits. Multisplit and CUB’s performance&#10;are roughly similar. Our multisplit methods are usually better for a&#10;small number of buckets, while CUB’s performance is optimized around the&#10;number of bits that it uses for its internal iterations (5 bits for&#10;Kepler, 7 bits for Pascal). Table \[table:sort\_bits\_rate\] shows our&#10;achieved throughput (billion elements sorted per second) as a function&#10;of the number of bits per key.&#10;&#10;There are several important remarks to make:&#10;&#10;Our multisplit methods outperform CUB for up to 4 bits on the Tesla K40c&#10;and up to 6 bits on the GeForce GTX 1080. We note CUB is highly&#10;optimized for specific bit counts: 5-bit radixes on Kepler (Tesla K40c)&#10;and 7-bit radixes on Pascal (GeForce GTX 1080).&#10;&#10;By comparing our achieved throughputs with those of delta-buckets in&#10;Table \[table:ms\_rate\], it becomes clear that the choice of bucket&#10;identifier can have an important role in the efficiency of our&#10;multisplit methods. In our delta-bucket computation, we used integer&#10;divisions, which are expensive computations. For example, in our BMS,&#10;the integer division costs 0.72x, 0.70x, and 0.90x geometric mean&#10;decrease in our key-only throughput for Tesla K40c (ECC on), Tesla K40c&#10;(ECC off) and GeForce GTX 1080 respectively. The throughput decrease for&#10;key-value scenarios is 0.87x, 0.82x and 0.98x respectively. The GeForce&#10;GTX 1080 is less sensitive to such computational load variations.&#10;Key-value scenarios also require more expensive data movement so that&#10;the cost of the bucket identifier is relatively less important.&#10;&#10;On the GeForce GTX 1080, our BMS method is always superior to WMS. This&#10;GPU appears to be better at hiding the latency of BMS’s extra&#10;synchronizations, allowing the marginally better locality from larger&#10;subproblems to become the primary factor in differentiating performance.">
    </outline>
    <outline text="Multisplit-sort" _note="Now we turn to characterizing the performance of sort using multisplit&#10;with identity buckets. It is not immediately clear what the best radix&#10;size () is for achieving the best sorting performance. As a result, we&#10;ran all choices of . Because our bucket identifiers are also relatively&#10;simple (requiring one shift and one AND), our multisplit performance&#10;should be close to that of identity buckets.&#10;&#10;Since BMS is almost always superior to WMS for&#10;(Table \[table:sort\_bits\_rate\]), we have only used BMS in our&#10;implementations. For sorting 32-bit elements, we have used iterations of&#10;r-bit BMS followed by one last BMS for the remaining bits. For example,&#10;for , we run 4 iterations of 7-bit BMS then one iteration of 4-bit BMS.&#10;Table \[table:sort\] summarizes our sort results.&#10;&#10;By looking at our achieved throughputs (sorting rates), we see that our&#10;performance increases up to a certain radix size, then decreases for any&#10;larger . This optimal radix size is different for each different GPU and&#10;depends on numerous factors, for example available bandwidth, the&#10;efficiency of warp-wide ballots and shuffles, the occupancy of the&#10;device, etc. For the Tesla K40c, this crossover point is earlier than&#10;the GeForce GTX 1080 (5 bits compared to 7 bits). Ideally, we would like&#10;to process more bits (larger radix sizes) to have fewer total&#10;iterations. But, larger radix sizes mean a larger number of buckets ()&#10;in each iteration, requiring more resources (shared memory storage, more&#10;register usage, and more shuffle usage), yielding an overall worse&#10;performance per iteration.">
    </outline>
    <outline text="Comparison with CUB" _note="CUB is a carefully engineered and highly optimized library. For its&#10;radix sort, it uses a persistent thread style of programming , where a&#10;fixed number of thread-blocks (around 750) are launched, each with only&#10;64 threads (thus allowing many registers per thread). Fine-tuned&#10;optimizations over different GPU architectures enables CUB’s&#10;implementation to efficiently occupy all available resources in the&#10;hardware, tuned for various GPU architectures.&#10;&#10;The major difference between our approach with CUB has been our choice&#10;of privatization. CUB uses thread-level privatization, where each thread&#10;keeps track of its local processed information (e.g., computed&#10;histograms) in an exclusively assigned portion of shared memory. Each&#10;CUB thread processes its portion of data free of contention, and later,&#10;combines its results with those from other threads. However, as CUB&#10;considers larger radix sizes, it sees increasing pressure on each&#10;block’s shared memory usage. The pressure on shared memory becomes worse&#10;when dealing with key-value sorts as it now has to store more elements&#10;into shared memory than before.&#10;&#10;In contrast to CUB’s thread privatization, our multisplit-based&#10;implementations instead target warp-wide privatization. An immediate&#10;advantage of this approach is that we require smaller privatized&#10;exclusive portions of shared memory because we only require a privatized&#10;portion per warp rather than per thread. The price we pay is the&#10;additional cost of warp-wide communications (shuffles and ballots)&#10;between threads, compared to CUB’s register-level communication within a&#10;thread.&#10;&#10;The reduced shared memory usage of our warp privatization becomes&#10;particularly valuable when sorting key-value pairs. Our key-value sort&#10;on GeForce GTX 1080 shows this advantage: when both approaches use 7-bit&#10;radixes, our multisplit-sort achieves up to a 1.10x higher throughput&#10;than CUB. On the other hand, CUB demonstrates its largest performance&#10;advantage over our implementation (ours has 0.78x the throughput of&#10;CUB’s) for key-only sorts on Tesla K40c (ECC off). In this comparison,&#10;our achieved shared memory benefits do not balance out our more costly&#10;shuffles.&#10;&#10;Our multisplit-based radix sort proves to be competitive to CUB’s radix&#10;sort, especially in key-value sorting. For key-only sort, our best&#10;achieved throughputs are 1.05x, 0.78x, and 0.88x times the throughput&#10;that CUB provides for Tesla K40c (ECC on), Tesla K40c (ECC off), and&#10;GeForce GTX 1080, respectively. For key-value sorting and with the same&#10;order of GPU devices, our multisplit-based sort provides 1.26x, 0.83x,&#10;and 1.10x times more throughput than CUB, respectively. Our highest&#10;achieved throughput is 3.0 Gkeys/s (and 2.1 Gpairs/s) on a GeFroce GTX&#10;1080, compared to CUB’s 3.4 Gkeys/s (and 1.9 Gpairs/s) on the same&#10;device.">
    </outline>
    <outline text="Future of warp privatized methods" _note="We believe the exploration of the difference between thread-level and&#10;warp-level approaches has implications beyond just multisplit and its&#10;extension to sorting. In general, any future hardware improvement in&#10;warp-wide intrinsics will reduce the cost we pay for warp privatization,&#10;making the reduction in shared memory size the dominant factor. We&#10;advocate further hardware support for warp-wide voting with a&#10;generalized ballot that returns multiple 32-bit registers, one for each&#10;bit of the predicate. Another useful addition that would have helped our&#10;implementation is the possibility of shuffling a dynamically addressed&#10;register from the source thread. This would enable the user to share&#10;lookup tables among all threads within a warp, only requesting the exact&#10;data needed at runtime rather than delivering every possible entry so&#10;that the receiver can choose.">
    </outline>
  </outline>
</outline>
  </body>
</opml>