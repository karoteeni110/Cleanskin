<?xml version="1.0" encoding="UTF-8"?>
<opml version="2.0">
  <head>
    <title></title>
    <abstract> </abstract>
  </head>
  <body>
<outline text="Related Work and Background">
  <outline text="The Graphics Processing Unit (GPU)" _note="The GPU of today is a highly parallel, throughput-focused programmable&#10;processor. GPU programs (“kernels”) launch over a GRID of numerous&#10;BLOCKS; the GPU hardware maps blocks to available parallel cores. Each&#10;block typically consists of dozens to thousands of individual THREADS,&#10;which are arranged into 32-wide WARPS. Warps run under SIMD control on&#10;the GPU hardware. While blocks cannot directly communicate with each&#10;other within a kernel, threads within a block can, via a&#10;user-programmable 48 kB SHARED-MEMORY, and threads within a warp&#10;additionally have access to numerous warp-wide instructions. The GPU’s&#10;global memory (DRAM), accessible to all blocks during a computation,&#10;achieves its maximum bandwidth only when neighboring threads access&#10;neighboring locations in the memory; such accesses are termed COALESCED.&#10;In this work, when we use the term “GLOBAL”, we mean an operation of&#10;device-wide scope. Our term “LOCAL” refers to an operation limited to&#10;smaller scope (e.g., within a thread, a warp, a block, etc.), which we&#10;will specify accordingly. The major difference between the two is the&#10;cost of communication: global operations must communicate through global&#10;DRAM, whereas local operations can communicate through lower-latency,&#10;higher-bandwidth mechanisms like shared memory or warp-wide intrinsics.&#10;Lindholm et al.  and Nickolls et al.  provide more details on GPU&#10;hardware and the GPU programming model, respectively.&#10;&#10;We use NVIDIA’s CUDA as our programming language in this work . CUDA&#10;provides several warp-wide voting and shuffling instructions for&#10;intra-warp communication of threads. All threads within a warp can see&#10;the result of a user-specified predicate in a bitmap variable returned&#10;by `__ballot(predicate)` . Any set bit in this bitmap denotes the&#10;predicate being non-zero for the corresponding thread. Each thread can&#10;also access registers from other threads in the same warp with&#10;`__shfl(register_name, source_thread)` . Other shuffling functions such&#10;as `__shfl_up()` or `__shfl_xor()` use relative addresses to specify the&#10;source thread. In CUDA, threads also have access to some efficient&#10;integer intrinsics, e.g., `__popc()` for counting the number of set bits&#10;in a register.">
  </outline>
  <outline text="Parallel primitive background" _note="In this paper we leverage numerous standard parallel primitives, which&#10;we briefly describe here. A REDUCTION inputs a vector of elements and&#10;applies a binary associative operator (such as addition) to reduce them&#10;to a single element; for instance, sum-reduction simply adds up its&#10;input vector. The SCAN operator takes a vector of input elements and an&#10;associative binary operator, and returns an output vector of the same&#10;size as the input vector. In exclusive (resp., inclusive) scan, output&#10;location contains the reduction of input elements 0 to (resp., 0 to ).&#10;Scan operations with binary addition as their operator are also known as&#10;PREFIX-SUM . Any reference to a multi- operator (multi-reduction,&#10;multi-scan) refers to running multiple instances of that operator in&#10;parallel on separate inputs. COMPACTION is an operation that filters a&#10;subset of its input elements into a smaller output array while&#10;preserving the order.">
  </outline>
  <outline text="Multisplit and Histograms" _note="Many multisplit implementations, including ours, depend heavily on&#10;knowledge of the total number of elements within each bucket (bin),&#10;i.e., histogram computation. Previous competitive GPU histogram&#10;implementations share a common philosophy: divide the problem into&#10;several smaller sized subproblems and assign each subproblem to a&#10;thread, where each thread sequentially processes its subproblem and&#10;keeps track of its own PRIVATIZED local histogram. Later, the local&#10;histograms are aggregated to produce a globally correct histogram. There&#10;are two common approaches to this aggregation: 1) using atomic&#10;operations to correctly add bin counts together (e.g., Shams and&#10;Kennedy ), 2) storing per-thread sequential histogram computations and&#10;combining them via a global reduction (e.g., Nugteren et al. ). The&#10;former is suitable when the number of buckets is large; otherwise atomic&#10;contention is the bottleneck. The latter avoids such conflicts by using&#10;more memory (assigning exclusive memory units per-bucket and&#10;per-thread), then performing device-wide reductions to compute the&#10;global histogram.&#10;&#10;The hierarchical memory structure of NVIDIA GPUs, as well as NVIDIA’s&#10;more recent addition of faster but local shared memory atomics (among&#10;all threads within a thread block), provides more design options to the&#10;programmer. With these features, the aggregation stage could be&#10;performed in multiple rounds from thread-level to block-level and then&#10;to device-level (global) results. Brown et al.  implemented both Shams’s&#10;and Nugteren’s aforementioned methods, as well as a variation of their&#10;own, focusing only on 8-bit data, considering careful optimizations that&#10;make the best use of the GPU, including loop unrolling, thread&#10;coarsening, and subword parallelism, as well as others. Recently,&#10;NVIDIA’s CUDA Unbound (CUB)  library has included an efficient and&#10;consistent histogram implementation that carefully uses a minimum number&#10;of shared-memory atomics to combine per-thread privatized histograms per&#10;thread-block, followed by aggregation via global atomics. CUB’s&#10;histogram supports any data type (including multi-channel 8-bit inputs)&#10;with any number of bins.&#10;&#10;Only a handful of papers have explored multisplit as a standalone&#10;primitive. He et al.  implemented multisplit by reading multiple&#10;elements with each thread, sequentially computing their histogram and&#10;local offsets (their order among all elements within the same bucket and&#10;processed by the same thread), then storing all results (histograms and&#10;local offsets) into memory. Next, they performed a device-wide scan&#10;operation over these histogram results and scattered each item into its&#10;final position. Their main bottlenecks were the limited size of shared&#10;memory, an expensive global scan operation, and random non-coalesced&#10;memory accesses.[^1]&#10;&#10;Patidar  proposed two methods with a particular focus on a large number&#10;of buckets (more than 4k): one based on heavy usage of shared-memory&#10;atomic operations (to compute block level histogram and intra-bucket&#10;orders), and the other by iterative usage of basic binary split for each&#10;bucket (or groups of buckets). Patidar used a combination of these&#10;methods in a hierarchical way to get his best results.[^2] Both of these&#10;multisplit papers focus only on key-only scenarios, while data movements&#10;and privatization of local memory become more challenging with key-value&#10;pairs.&#10;&#10;[^1]: On an NVIDIA 8800 GTX GPU, for 64 buckets, He et al. reported&#10;    134 Mkeys/sec. As a very rough comparison, our GeForce GTX 1080 GPU&#10;    has 3.7x the memory bandwidth, and our best 64-bucket implementation&#10;    runs 126 times faster.&#10;&#10;[^2]: On an NVIDIA GTX280 GPU, for 32 buckets, Patidar reported&#10;    762 Mkeys/sec. As a very rough comparison, our GeForce GTX 1080 GPU&#10;    has 2.25x the memory bandwidth, and our best 32-bucket&#10;    implementation runs 23.5 times faster.">
  </outline>
</outline>
  </body>
</opml>