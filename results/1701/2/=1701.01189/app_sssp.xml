<?xml version="1.0" encoding="UTF-8"?>
<opml version="2.0">
  <head>
    <title></title>
    <abstract> </abstract>
  </head>
  <body>
<outline text="The Single Source Shortest Path problem" _note="In Section \[sec:intro\] we argued that an efficient multisplit&#10;primitive would have helped Davidson et al.  in their delta-stepping&#10;formulation of the Single Source Shortest Path (SSSP) problem on the&#10;GPU. In this section, we show that by using our multisplit&#10;implementation, we can achieve significant speedups in SSSP computation,&#10;especially on highly connected graphs with low diameters.">
  <outline text="The Single Source Shortest Path (SSSP) problem" _note="Given an arbitrary graph , with non-negative weights assigned to each&#10;edge and a source vertex , the SSSP problem finds the minimum cost path&#10;from the source to every other vertex in the graph. As described in&#10;Section \[sec:intro\], Dijkstra’s  and Bellman-Ford-Moore’s  algorithms&#10;are two classical approaches to solve the SSSP problem. In the former,&#10;vertices are organized in a single priority queue and are processed&#10;sequentially from the lowest to the highest weight. In the latter, for&#10;each vertex we process all its neighbors (i.e., processing all edges).&#10;This can be done in parallel and is repeated over multiple iterations&#10;until convergence. Dijkstra is highly work-efficient but essentially&#10;sequential and thus unsuitable for parallelization. Bellman-Ford-Moore&#10;is trivially parallel but does much more work than necessary (especially&#10;for highly connected graphs).&#10;&#10;As an alternative algorithm between these two extremes (sequential&#10;processing of all vertices vs. processing all edges in parallel),&#10;delta-stepping allows the selective processing of a subset of vertices&#10;in parallel . In this formulation, nodes are put into different buckets&#10;(based on their assigned weights) and buckets with smaller weights are&#10;processed first. Davidson et al.  proposed multiple GPU implementations&#10;based on the delta-stepping formulation. Their two most prominent&#10;implementations were based on a NEAR-FAR strategy and a BUCKETING&#10;strategy. Both divide vertices into multiple buckets, which can be&#10;processed in parallel. Both use efficient load-balancing strategies to&#10;traverse all vertices within a bucket. Both iterate over multiple rounds&#10;of processing until convergence is reached. The main difference between&#10;the two is in the way they organize the vertices to be processed next&#10;(work frontiers):&#10;&#10;Near-Far strategy&#10;&#10;:   In this strategy the work queue is prioritized based on a variable&#10;    splitting distance. In every iteration, only those vertices less&#10;    than this threshold (the NEAR SET) are processed. Those falling&#10;    beyond the threshold are appended to a FAR PILE. Elements in the far&#10;    pile are ignored until work in the near set is completely exhausted.&#10;    When all work in the near set is exhausted (this could be after&#10;    multiple relaxation phases), this strategy increases the splitting&#10;    distance (by adding an incremental weight to it) and removes invalid&#10;    elements from the far pile (those which have been updated with&#10;    similar distances), finally splitting this resulting set into a new&#10;    near set and far pile. This process continues until both the near&#10;    set and far pile are empty (the convergence criterion).&#10;&#10;Bucketing strategy&#10;&#10;:   In this strategy, vertices are partitioned into various buckets&#10;    based on their weights (Davidson et al. reported the best&#10;    performance resulted from 10 buckets). This strategy does a more&#10;    fine-grained classification of vertices compared to Near-Far,&#10;    resulting in a greater potential reduction in work queue size and&#10;    hence less work necessary to converge. The downside, however, is the&#10;    more complicated bucketing process, which due to lack of an&#10;    efficient multisplit primitive was replaced by a regular radix sort&#10;    in the original work. As a result of this expensive radix sort&#10;    overhead, Near-Far was more efficient in practice .&#10;&#10;">
  </outline>
  <outline text="Multisplit-SSSP" _note="Now that we have implemented an efficient multisplit GPU primitive in&#10;this paper, we can use it in the Bucketing strategy explained above to&#10;replace the costly radix sort operation. We call this new Bucketing&#10;implementation MULTISPLIT-SSSP. Our Multisplit-SSSP should particularly&#10;perform well on highly connected graphs with relatively large out&#10;degrees and smaller diameters (such as in social graphs), causing fewer&#10;iterations and featuring large enough work fronts to make multisplit&#10;particularly useful. However, graphs with low average degrees and large&#10;diameters (such as in road networks) require more iterations over&#10;smaller work frontiers, resulting in high kernel launch overheads&#10;(because of repetitive multisplit usage) without large enough work&#10;frontiers to benefit from the efficiency of our multisplit. We note that&#10;this behavior for different graph types is not limited to our SSSP&#10;implementation; GPU graph analytics in general demonstrate their best&#10;performance on highly connected graphs with low diameters .">
  </outline>
  <outline text="Performance Evaluation" _note="In this part, we quantitatively evaluate the performance of our new&#10;Multisplit-SSSP compared to Davidson et al.’s Bucketing and Near-Far&#10;approaches. Here, we choose a set of graph datasets listed in&#10;Table \[table:graphs\].[^1] For those graphs that are not weighted, we&#10;randomly assign a non-negative integer weight between 0 and 1000 to each&#10;edge.&#10;&#10;Table \[table:sssp\_results\] shows the convergence time for Near-Far,&#10;Bucketing, and Multisplit-SSSP (in million traversed edges per second,&#10;MTEPS), with Multisplit-SSSP’s speedup against Near-Far. Multisplit-SSSP&#10;is always better than Bucketing, on both devices and on every graph we&#10;tested (up to 9.8x faster on Tesla K40c and 9.1x faster on the GeForce&#10;GTX 1080). This behavior was expected because of the performance&#10;superiority of our multisplit compared to a regular radix-sort&#10;(Fig. \[fig:speedup\]).&#10;&#10;Against Near-Far, our performance gain depends on the type of graph. As&#10;we expected, on highly connected graphs with low diameters (such as&#10;rmat), we achieve up to 1.58x and 2.17x speedup against Near-Far, on the&#10;Tesla K40c and GeForce GTX 1080 respectively. However, for high diameter&#10;graphs such as road networks (e.g., belgium\_osm), we are closer to&#10;Near-Far’s performance: Multisplit-SSSP is slower than Near-Far on Tesla&#10;K40c (0.93x) and marginally faster on GeForce GTX 1080 (1.04x). Road&#10;graphs have significantly higher diameter and hence more iterations. As&#10;a result, the extra overhead in each phase of Multisplit-SSSP on large&#10;diameters can become more important than the saved operations due to&#10;fewer edge re-relaxations.&#10;&#10;[^1]: All matrices except for rmat are downloaded from University of&#10;    Florida Sparse Matrix Collection . Rmat was generated with&#10;    parameters .">
  </outline>
</outline>
  </body>
</opml>