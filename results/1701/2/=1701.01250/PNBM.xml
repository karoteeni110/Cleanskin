<?xml version="1.0" encoding="UTF-8"?>
<opml version="2.0">
  <head>
    <title>A Probabilistic View of Neighborhood-based Recommendation Methods</title>
    <abstract>Probabilistic graphic model is an elegant framework to compactly present
complex real-world observations by modeling uncertainty and logical flow
(conditionally independent factors). In this paper, we present a
probabilistic framework of neighborhood-based recommendation methods
(PNBM) in which SIMILARITY is regarded as an unobserved factor. Thus,
PNBM leads the estimation of user preference to maximizing a posterior
over SIMILARITY. We further introduce a novel multi-layer SIMILARITY
descriptor which models and learns the joint influence of various
features under PNBM, and name the new framework MPNBM. Empirical results
on real-world datasets show that MPNBM allows very accurate estimation
of user preferences. </abstract>
  </head>
  <body>
<outline text="Introduction" _note="Collaborative filtering, which leverages user history information to&#10;predict users’ unknown preference, is one of the most successful&#10;techniques to build recommender systems . Matrix factorization (MF) and&#10;neighborhood-based methods (NBMs) are two representative approaches. MF&#10;family attracts more attention due to its ability of modeling influence&#10;of various features (e.g. ), thus to improve accuracy. However, it is&#10;difficult to provide explainable recommendation results. NBM family,&#10;shown as Fig. \[strnbm\], is very popular mainly due to the fact that it&#10;naturally explains recommendation results (e.g. An item which is similar&#10;with what you bought before). SIMILARITY serves as the basis of&#10;weighting neighbors which is crucial to the accuracy of NBM recommender&#10;systems. However, existing SIMILARITY computation scheme is incapable of&#10;capturing influence from different features which hampers further&#10;polishing SIMILARITY to improve accuracy. In this paper, we first&#10;present a basic probabilistic framework of NBM family (PNBM) which leads&#10;learning SIMILARITY to a regression problem. Then we introduce a novel&#10;multi-layer SIMILARITY descriptor which models and learns the joint&#10;influence of different features under PNBM.">
  <outline text="Related Work" _note="Commonly, NBMs are divided into two classes . One is user-based approach&#10;which predicts the rating that a user will assign to an unrated item by&#10;referring to other users who are similar to this user. The other is&#10;item-based approach which estimates a user’s preference to an unrated&#10;item based on other items that are similar to this unrated item. The two&#10;approaches follow the same principle. With respect to NBM, researches&#10;have mainly focused on SIMILARITY computation schemes and neighbor&#10;selection strategies . SIMILARITY also serves as the basis for neighbor&#10;selection, thus we concentrate upon SIMILARITY in this paper. Generally,&#10;there are two main approaches to compute SIMILARITY. One introduces&#10;different kinds of correlation coefficients as SIMILARITY , such as&#10;Pearson and Cosine correlations. However, some researchers argue that&#10;such kind of methods isolate the relations between two items without&#10;leveraging global information. The other approach learns SIMILARITY via&#10;regression models. introduce a way to learn similarity by minimizing&#10;mean squared error between observed ratings and their corresponding&#10;estimation. factors similarity matrix via low-rank approximations.&#10;presents a weighted error function which gives more weight to the users&#10;who rated items most similar to the estimated item. simplify standard&#10;neighborhood-based models to a simple linear regression problem for top-&#10;recommendation based on binary databases.&#10;&#10;A number of probabilistic models have been introduced to collaborative&#10;filtering. However, only a very small portion of them are NBM related.&#10;presents a generic Bayesian personalized ranking framework which is&#10;optimized for the area under ROC (AUC) metric. introduces a&#10;probabilistic memory-based collaborative filtering method in which they&#10;use a mixture Gaussian model built on the basis of a set of user&#10;profiles and use the posterior distribution of user ratings for&#10;prediction. builds a Markov network using Pearson-correlation NBM as&#10;basis. People also place probabilistic prior assumptions to observations&#10;to model uncertainty. Such as, places Dirichlet distribution on the&#10;absolute value of rating difference. uses different probabilistic&#10;density functions to sample neighbors from a predefined similarity&#10;matrix (vectors).&#10;&#10;Unfortunately, these models are incapable of representing complex&#10;features, and none of them discusses NBM family itself from a Bayesian&#10;perspective.">
  </outline>
  <outline text="Contribution" _note="In this paper, we present a probabilistic (Bayesian) framework of NBM&#10;family, and our contribution is twofold.&#10;&#10;First, we present a general graphical model of NBM family (PNBM) which&#10;leads the estimation of user preference to maximizing a posterior over&#10;SIMILARITY.&#10;&#10;Then, we introduce a novel multi-layer SIMILARITY descriptor which is&#10;capable of modeling and learning the joint influence of various features&#10;(e.g. rating, text, genre) under PNBM, and we name the new framework as&#10;MPNBM.&#10;&#10;MPNBM is evaluated on three popular real-world datasets via&#10;root-mean-square-error (RMSE) metric. Empirical results show that MPNBM&#10;consistently outperform state-of-art approaches on the datasets we&#10;choose.">
  </outline>
</outline>
<outline text="Preliminary" _note="Suppose we have a data set organized in form of matrix , it contains&#10;users and items. is item SIMILARITY matrix, denotes similarity between&#10;item and , we further assume . presents indicator matrix, and . if user&#10;rated item , otherwise . denotes all the observed ratings.&#10;&#10;So far, many neighborhood-based methods have been proposed, as surveyed&#10;in . For simplicity, we take a variant of mean-centering NBM as instance&#10;throughout the paper. The predication formula is defined in Equation&#10;(\[myeq:mc\]). where is rating score that user gave to item . denotes&#10;the estimation of user ’s preference on item . is the mean value of all&#10;the ratings given to item . presents a set containing all the items.&#10;&#10;For further simplicity, Equation (\[myeq:mc\]) is transformed into a&#10;vectorization form: where and . denotes SIMILARITY vector corresponding&#10;to item and represents rating vector of user . The multiplication&#10;denotes the inner product of the two vectors. is an indicator vector of&#10;user . The symbol means a vector that does not contain an item which is&#10;being predicted. For example, with regard to Equation (\[myeq:basic\]),&#10;denotes a vector does not contain . Moreover, we assume the testing set&#10;is excluded from the training set, when we predict in the testing set,&#10;in the training set is always zero.">
</outline>
<outline text="Probabilistic Framework of NBM" _note="In this section, we present a probabilistic graphical model of NBMs&#10;(PNBM), shown in Fig. \[gpnbm\]. It is a Bayesian network which&#10;describes the following factorization:&#10;&#10;In our context, placing prior distribution on hyper-parameters does not&#10;significantly improve accuracy while dramatically increasing time&#10;complexity. For the sake of simplicity and reduction of time complexity,&#10;we simply let be constant, and is also constant. So we can simplify&#10;Equation (\[myeq:pnbm\]) to&#10;&#10;We introduce a general Gaussian distribution (but not limited to, other&#10;distribution can be also applied to. It depends on real-world context.)&#10;to density function which naturally leads to a sum-of-square-error.&#10;&#10;More specifically, assume that an item’s similarity vector is&#10;independent from those of other items, and is sampled from a mean-zero&#10;spherical Gaussian distribution. Thus we have where denotes the Gaussian&#10;distribution for with mean and precision . We also assume that ratings&#10;are independent with each other. Combine with Equation (\[myeq:basic\]),&#10;we have following">
</outline>
<outline text="Multi-layer SIMILARITY Descriptor" _note="In Section \[asimple\], we introduced a general probabilistic (Bayesian)&#10;NBM framework which is simple and straightforward. However, like other&#10;similarity computation methods, PNBM falls short in feature&#10;representation which extremely limits the accuracy improvement. In this&#10;section, we present a multi-layer SIMILARITY descriptor (MLSD, shown in&#10;Fig.\[g:mlsd\] ) which is able to model and learn the joint influence of&#10;various features (e.g. ratings, text, genre, time). MLSD is&#10;mathematically defined as&#10;&#10;where denotes the SIMILARITY basis at -th layer. is ’s constraint matrix&#10;which presents an influence of observed features. For example, it can&#10;present the similarity of text description ( or time closeness ) between&#10;any two-item. Note that different influences may be generated from the&#10;same feature. denotes the importance of the feature-influence modeled at&#10;layer . is the number of layers (influence) employed to model&#10;SIMILARITY. In this paper, we don’t require , since we always have a&#10;normalization factor in the prediction equation, i.e. Equation&#10;(\[myeq:basic\]). denotes point-wise product operation (Hadamard&#10;product) on matrices and , e.g.&#10;&#10;MLSD can be smoothly integrated into PNBM, shown in Fig. \[g:mpnbm\],&#10;named MPNBM. The Bayesian network is mathematically describe as where .&#10;Follow the same assumptions in Section \[asimple\], we define the prior&#10;of layer ( ) as And we have the conditional distribution over observed&#10;ratings defined as&#10;&#10;">
</outline>
<outline text="Maximum a Posterior" _note="PNBM is a specific case of MPNBM, which only has one layer with&#10;constraint-matrix set to 1. In this section, we take MPNBM as example to&#10;present how we optimize SIMILARITY via maximizing a posterior.&#10;&#10;The log of Bayesian network defined in Equation (\[myeq:mpnbm\]) is&#10;given by&#10;&#10;In fact, it defines the posterior distribution over SIMILARITY. Combine&#10;it with Equation (\[myeq:mulsim\]) and Equation (\[myeq:rmul\]), we have&#10;&#10;Maximizing the above Bayesian network distribution with hyper-parameters&#10;being kept fixed is equivalent to minimizing an error function defined&#10;as&#10;&#10;where is the regularization parameter for layer .&#10;&#10;A simple linear Gaussian model sometimes makes prediction value fall out&#10;of the range of valid rating values. In order to force the predication&#10;values to fall into valid range, we pass the linear-Gaussian model&#10;through hyperbolic tangent function which makes prediction values be in&#10;range of \[-1,1\]. We map the centralized ratings to range \[-1, 1\]&#10;with Equation (\[myeq:mapf\]). where and are the max and min value of&#10;ratings, respectively. Since the ratings are centralized by their&#10;corresponding mean value, we always have and . As a result, the range of&#10;valid rating value align with the estimation produced by our models.&#10;&#10;The conditional distribution of observed ratings becomes We adopt&#10;stochastic gradient descent (SGD) as learning algorithm to train latent&#10;factors, shown in Algorithm \[alg:sgd\].&#10;&#10;rating matrix , error function. similarity basis , influence&#10;constraint-matrix , influence importance factor , learning rate ,&#10;regular parameter . Note that . Training: For each layer (),&#10;point-wisely update the similarity basis : where . prediction using&#10;Equation (\[myeq:mc\]) with top- the most similar neighbors.">
</outline>
<outline text="Experiments">
  <outline text="Description of Data Sets" _note="In the experiments, we evaluate our models and state-of-art methods over&#10;three different data sets, summarized in Table \[dataset-table\].&#10;&#10;\[dataset-table\]&#10;&#10;ML-20M, ML-10M are data sets provided by MovieLens . Netflix is a subset&#10;sampled from Netflix Prize data set such that each user rated 50-1500&#10;movies, and each movie is rated by 5-1800 users. Yahoo-R4 is a subset of&#10;the movie-rating data set provided by the Yahoo Labs Webscope Team such&#10;that each movie are at least rated by 5 users.&#10;&#10;We use ML-10M, Netflix and Yahoo-R4 to compare the models’ accuracy.&#10;&#10;We also compare each model’s accuracy on data sets with different&#10;densities. In order to avoid the inherent differences of data sets from&#10;different originations, we extract 10 subsets from one single data set&#10;(ML-20M) based on the users’ rating number. Precisely, each subset has&#10;similar amount of users and items, the number of users and items are in&#10;range \[10000, 15000\] and \[8000, 20000\] respectively. The density of&#10;each data set is from 0.28% to 2.67%.&#10;&#10;">
  </outline>
  <outline text="Models for Comparison" _note="In this paper, the following models are compared:&#10;&#10;[**RegSim:** ]{} Regression on SIMILARITY , a representative work which&#10;learns SIMILARITY via a regression method.&#10;&#10;[**SLIM:** ]{} Sparse linear methods , a regression model for top-&#10;recommendation on binary data set. We extend it to a arbitrary&#10;real-value prediction model by placing a jointly Gaussian-Laplace prior&#10;on similarity vectors. It has a very similar error function as SLIM,&#10;where is a non-zero mean value of the Gaussian prior.&#10;&#10;[**PCC:** ]{} NBM using Pearson correlation as SIMILARITY .&#10;&#10;[**COS:** ]{} NBM using Cosine correlation as SIMILARITY .&#10;&#10;[**PMF:** ]{} Probabilistic matrix factorization .&#10;&#10;[**MPNBM:** ]{} In this paper, we exploit influence from ratings as an&#10;instance to demonstrate MLSD’s ability of modeling various features,&#10;thus to improve accuracy. We use a 3-layer SIMILARITY descriptor in&#10;which layer-1 treats latent influence equally with constraint-matrix set&#10;to 1; Layer-2 adopts Pearson correlation as constraint-matrix that&#10;stresses the influence from those items which either have significant&#10;positive correlation or strong negative correlation with the item under&#10;predication; Layer-3 employs Jaccard index to form a constraint-matrix&#10;that amplifies the influence from those items which have similar rating&#10;history, alleviates the divergence from infrequent-rated items and&#10;frequent-rated items. [**Time Complexity.**]{} The computational time is&#10;mainly taken by updating SIMILARITY. At a single epoch, approximately&#10;similarities are updated, where is the size of training set, is the&#10;average rating number per users and is the number of influence layers.&#10;Intuitively, a single epoch takes about 4, 260, 340 seconds on Yahoo-R4,&#10;Netflix, ML-10M respectively.&#10;&#10;[**Tanh-MPNBM:** ]{} The model which we pass MPNBM through hyperbolic&#10;tangent function ( detailed in Section \[map-1\] ).&#10;&#10;EXPERIMENT SETTING. All models are implemented with Matlab, and run on a&#10;single core of a Intel (R) Xeon(R) 3.50 GHz machine with 16 GB memory.">
  </outline>
  <outline text="Parameters Setting" _note="For RegSim, MPNBM, Tanh-MPNBM and SLIM, we empirically choose parameters&#10;for each model after a grid search in which . The finally chosen&#10;parameters are summarized in Table \[setpara\] ( indicates a model does&#10;not have such a parameter).&#10;&#10;\[setpara\]&#10;&#10;For PMF, we choose latent feature dimension and the momentum of&#10;mini-batch SGD . Regularized parameters ( for user latent factors and&#10;latent item factors respectively) and learning rate are set to&#10;&#10;For Yahoo-R4, and .&#10;&#10;For Netflix, and .&#10;&#10;For ML-10M, and .&#10;&#10;For the first two ML-20M subsets shown in the left panel of Fig.&#10;\[diffdensity\], and .&#10;&#10;For the other eight ML-20M subsets shown in the left panel of Fig.&#10;\[diffdensity\], and .&#10;&#10;For PCC and COS, we use top-200 the most similar neighbors for&#10;prediction.">
  </outline>
  <outline text="Comparison Results" _note="During the test, we randomly divide each data set into training set&#10;(85%), validation set (5%) and testing set (10%). We adopt RMSE for&#10;evaluation. We repeat the experiments 5 times.&#10;&#10;\[accuracy-table-rmse\]">
    <outline text="Accuracy" _note="The comparison is performed over :&#10;&#10;Accuracy on different data sets.&#10;&#10;Accuracy on different density.&#10;&#10;Fig. \[fig:com\] presents the detail of training on different data sets.&#10;Table \[accuracy-table-rmse\] records the final accuracy comparison (the&#10;training process is conducted by validation set). RegSim is selected as&#10;baseline model, the accuracy improvement of each model is displayed in&#10;the INC % column. Fig. \[diffdensity\] shows the accuracy comparison on&#10;data sets with different density. MPNBM and Tanh-MPNBM consistently&#10;outperform outperform state-of-art models, especially on those extremely&#10;sparse data sets ( which have serious COLD START problem). For&#10;simplicity, we don’t draw SLIM on the graph, since SLIM has similar&#10;accuracy with RegSim.&#10;&#10;We are also interested in that how the layer importance-factor affects&#10;the MPNBM (Tanh-MPNBM). We use two strategies to select parameters for&#10;each layer, 1) we consistently choose for all the three data sets, named&#10;TMPN-V1; 2) letting , we assign higher value to the which corresponding&#10;has lower RMSE, named TMPN-V2. The comparison is shown in Fig.&#10;\[fig:toy\], and 1) the accuracy is not significantly influenced, MPNBM&#10;(Tanh-MPNBM) is able to balance the influence automatically. 2)&#10;Assigning proper weight to according to the RMSE of results in faster&#10;convergence.">
    </outline>
    <outline text="Stability" _note="Model based approach may easily over fit when increasing the number of&#10;parameters under training. The system can be beneficent from the&#10;stability of algorithms which is defined by&#10;&#10;converge speed: the first epoch where a model converges to the local&#10;best solution, denoted as .&#10;&#10;ability of models to maintain their best status: the number of epochs&#10;that a model stays in the local best solution, denoted as .&#10;&#10;Table \[stable-table\] shows the values of and of each model over&#10;different data sets.&#10;&#10;\[stable-table\]&#10;&#10;In the comparison of stability, we treat RMSE values , if . Note that&#10;with regard to a model which does not over fit after 200 epochs (value&#10;of prefixed with ), if the lowest RMSE value appears at least 10 epochs,&#10;it is seen as the local best solution. \* in Table \[stable-table\]&#10;means a model does not converge after 200 epochs on a data set. e.g,&#10;MPNBM does not converge on Netflix data set, also shown in Fig.&#10;\[fig:com\]. The experimental results show that MPNBM and Tanh-MPNBM&#10;stay in the local best solution for many ( 40) epochs which is better&#10;than PMF. With regard to converge speed, as shown in Table&#10;\[stable-table\], it seems that sometimes MPNBM and Tanh-MPNBM do not&#10;converge as fast as PMF. In fact, they achieve a considerable accuracy&#10;at a much earlier epoch.">
    </outline>
  </outline>
</outline>
<outline text="Conclusion" _note="In this paper, we have presented a probabilistic framework of NBM&#10;family, and introduced a multi-layer SIMILARITY descriptor under PNBM&#10;which is capable of modeling and learning the joint influence of various&#10;features. Our experiments show that MPNBM and Tanh-MPNBM allow accurate&#10;and stable estimation of user preferences.&#10;&#10;Privacy is a serious problem to recommender systems. Nowadays, applying&#10;differential privacy to recommendation algorithms attracts great&#10;attention. A common approach is adding noise to data set. Recently,&#10;people find that sampling from a posterior distribution achieves some&#10;extent of differential privacy “for free&quot; , and this idea has been&#10;already successfully applied to probabilistic matrix factorization .&#10;Following the same idea, our models can also provide such kind of “free&#10;privacy&quot;. We leave a detailed investigation as future work.">
</outline>
<outline text="Acknowledgments" _note="Both authors are supported by a CORE (junior track) grant from the&#10;National Research Fund, Luxembourg. Qiang Tang is also partially&#10;supported by an internal project from University of Luxembourg.">
</outline>
  </body>
</opml>