<?xml version="1.0" encoding="UTF-8"?>
<opml version="2.0">
  <head>
    <title></title>
    <abstract> </abstract>
  </head>
  <body>
<outline text="Discussion and Related Work" _note="As motivated in the introduction, SDP has explored probabilistic&#10;inference problems with a specific form of alternating maximization and&#10;expectation blocks. The main computational advantage comes from lifting&#10;in the sense of lifted inference in standard first order logic. Issues&#10;that arise from conditional summations over combinations random&#10;variables, common in probabilistic lifted inference, have been touched&#10;upon but not extensively. In cases where SDP has been shown to work it&#10;provides [GENERALIZED LIFTED INFERENCE]{} where the complexity of the&#10;inference algorithm is completely independent of the domain size (number&#10;of objects) in problem specification, and where the response to queries&#10;is either independent of that size or can be specified parametrically.&#10;This is a desirable property but to our knowledge it is not shared by&#10;most work on probabilistic lifted inference. A notable exception is&#10;given by the knowledge compilation result of (see Chapter 4 and Theorem&#10;5.5) and the recent work in , where a model is compiled into an&#10;alternative form parametrized by the domain and where responses to&#10;queries can be obtained in polynomial time as a function of . The&#10;emphasis in that work is on being [DOMAIN LIFTED]{} (i.e., being&#10;polynomial in domain size). Generalized lifted inference requires an&#10;algorithm whose results can be computed once, in time independent of&#10;that size, and then reused to evaluate the answer for specific domain&#10;sizes. This analogy also shows that SDP can be seen as a compilation&#10;algorithm, compiling a domain model into a more accessible form&#10;representing the value function, which can be queried efficiently. This&#10;connection provides an interesting new perspective on both fields.&#10;&#10;In this chapter we focused on one particular instance of SDP. Over the&#10;last 15 years SDP has seen a significant amount of work expanding over&#10;the original algorithm by using different representations, by using&#10;algorithms other than value iteration, and by extending the models and&#10;algorithms to more complex settings. In addition, several “lifted&quot;&#10;inductive approaches that do not strictly fall within the probabilistic&#10;inference paradigm have been developed. We review this work in the&#10;remainder of this section.">
  <outline text="Deductive Lifted Stochastic Planning" _note="As a precursor to its use in lifted stochastic planning, the term SDP&#10;originated in the propositional logical context  when it was realized&#10;that propositionally structured MDP transitions (i.e., dynamic Bayesian&#10;networks ) and rewards (e.g., trees that exploited context-specific&#10;independence ) could be used to define highly compact FACTORED MDPS;&#10;this work also realized that the factored MDP structure could be&#10;exploited for representational compactness and computational efficiency&#10;by leveraging symbolic representations (e.g., trees) in dynamic&#10;programming. Two highly cited (and still used algorithms) in this area&#10;of work are the SPUDD  and APRICODD  algorithms that leveraged algebraic&#10;decision diagrams (ADDs)  for, respectively, exact and approximate&#10;solutions to factored MDPs. Recent work in this area shows how to&#10;perform propositional SDP directly with ground representations in&#10;PPDDL , and develops extensions for factored action spaces .&#10;&#10;Following the seminal introduction of [LIFTED]{} SDP in , several early&#10;papers on SDP approached the problem with existential rewards with&#10;different representation languages that enabled efficient&#10;implementations. This includes the First-order value iteration (FOVIA) ,&#10;the Relational Bellman algorithm (ReBel) , and the FODD based&#10;formulation of .&#10;&#10;Along this dimension two representations are closely related to the&#10;relational expression of this chapter. As mentioned above, relational&#10;expressions are an abstraction of the GFODD representation which&#10;captures expressions using a decision diagram formulation extending&#10;propositional ADDs . In particular, paths in the graphical&#10;representation of the DAG representing the GFODD correspond to the&#10;mutually exclusive conditions in expressions. The aggregation in GFODDs&#10;and relational expressions provides significant expressive power in&#10;modeling relational MDPs. The GFODD representation is more compact than&#10;relational expressions but requires more complex algorithms for its&#10;manipulation. The other closely related representation is the case&#10;notation of . The case notation is similar to relational expressions in&#10;that we have a set of conditions (these are mostly in a form that is&#10;mutually exclusive but not always so) but the main difference is that&#10;quantification is done within each case separately, and the notion of&#10;aggregation is not fully developed. First-order algebraic decision&#10;diagrams (FOADDs)  are related to the case notation in that they require&#10;closed formulas within diagram nodes, i.e., the quantifiers are included&#10;within the graphical representation of the expression. The use of&#10;quantifiers inside cases and nodes allows for an easy incorporation of&#10;off the shelf theorem provers for simplification. Both FOADD and GFODD&#10;were used to extend SDP to capture additive rewards and exogenous events&#10;as already discussed in the previous section. While the representations&#10;(relational expression and GFODDs vs. case notation and FOADD) have&#10;similar expressive power, the difference in aggregation makes for&#10;different algorithmic properties that are hard to compare in general.&#10;However, the modular treatment of aggregation in GFODDs and the generic&#10;form of operations over them makes them the most flexible alternative to&#10;date for directly manipulating the aggregated case representation used&#10;in this chapter. The idea of SDP has also been extended in terms of the&#10;choice of planning algorithm, as well as to the case of partially&#10;observable MDPs. Case notation and FOADDs have been used to implement&#10;approximate linear programming  and approximate policy iteration via&#10;linear programming  and FODDs have been used to implement relational&#10;policy iteration . GFODDs have also been used for open world reasoning&#10;and applied in a robotic context . The work of and explore SDP&#10;solutions, with GFODDs and case notation respectively, to relational&#10;partially observable MDPs (POMDPs) where the problem is conceptually and&#10;algorithmically much more complex. Related work in POMDPs has not&#10;explicitly addressed SDP, but rather has implicitly addressed lifted&#10;solutions through the identification of (and abstraction over)&#10;symmetries in applications of dynamic programming for POMDPs .">
  </outline>
  <outline text="Inductive Lifted Stochastic Planning" _note="Inductive methods can be seen to be orthogonal to the inference&#10;algorithms in that they mostly do not require a model and do not reason&#10;about that model. However, the overall objective of producing lifted&#10;value functions and policies is shared with the previously discussed&#10;deductive approaches. We therefore review these here for completeness.&#10;As we discuss, it is also possible to combine the inductive and&#10;deductive approaches in several ways.&#10;&#10;The basic inductive approaches learn a policy directly from a teacher,&#10;sometimes known as behavioral cloning. The work of provided learning&#10;algorithms for relational policies with theoretical and empirical&#10;evidence for their success. Relational policies and value functions were&#10;also explored in reinforcement learning. This was done with pure&#10;reinforcement learning using relational regression trees to learn a&#10;Q-function , combining this with supervised guidance , or using Gaussian&#10;processes and graph kernels over relational structures to learn a&#10;Q-function . A more recent approach uses functional gradient boosting&#10;with lifted regression trees to learn lifted policy structure in a&#10;policy gradient algorithm .&#10;&#10;Finally, several approaches combine inductive and deductive elements.&#10;The work of combines inductive logic programming with first-order&#10;decision-theoretic regression, by first using deductive methods&#10;(decision theoretic regression) to generate candidate policy structure,&#10;and then learning using this structure as features. The work of shows&#10;how one can implement relational approximate policy iteration where&#10;policy improvement steps are performed by learning the intended policy&#10;from generated trajectories instead of direct calculation. Although&#10;these approaches are partially deductive they do not share the common&#10;theme of this chapter relating planning and inference in relational&#10;contexts.">
  </outline>
</outline>
  </body>
</opml>