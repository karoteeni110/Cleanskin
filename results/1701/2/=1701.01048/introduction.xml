<?xml version="1.0" encoding="UTF-8"?>
<opml version="2.0">
  <head>
    <title></title>
    <abstract> </abstract>
  </head>
  <body>
<outline text="Introduction" _note="In this chapter we illustrate that stochastic planning can be viewed as&#10;a specific form of probabilistic inference and show that recent symbolic&#10;dynamic programming (SDP) algorithms for the planning problem can be&#10;seen to perform “generalized lifted inference”, thus making a strong&#10;connection to other chapters in this book. As we discuss below, although&#10;the SDP formulation is more expressive in principle, work on SDP to date&#10;has largely focused on algorithmic aspects of reasoning in open domain&#10;models with rich quantified logical structure whereas lifted inference&#10;has largely focused on aspects of efficient arithmetic computations over&#10;finite domain (quantifier free) template-based models. The contributions&#10;in these areas are therefore largely along different dimensions.&#10;However, the intrinsic relationships between these problems suggest a&#10;strong opportunity for cross-fertilization where the true scope of&#10;generalized lifted inference can be achieved. This chapter intends to&#10;highlight these relationships and lay out a paradigm for generalized&#10;lifted inference that subsumes both fields and offers interesting&#10;opportunities for future research.&#10;&#10;To make the discussion concrete, let us introduce a running example for&#10;stochastic planning and the kind of generalized solutions that can be&#10;achieved. For illustrative purposes, we borrow a planning domain from&#10;Boutilier et. al.  that we refer to as &lt;span&#10;style=&quot;font-variant:small-caps;&quot;&gt;BoxWorld&lt;/span&gt;. In this domain,&#10;outlined in Figure \[fig:boxworld\], there are several cities such as ,&#10;etc., trucks , etc., and boxes , etc. The agent can load a box onto a&#10;truck or unload it and can drive a truck from one city to another. When&#10;any box has been delivered to a specific city, , the agent receives a&#10;positive reward. The agent’s planning task is to find a policy for&#10;action selection that maximizes this reward over some planning horizon.&#10;&#10;Our objective in lifted stochastic planning is to obtain an abstract&#10;policy, for example, like the one shown in&#10;Figure \[fig:vfun\_and\_policy\]. In order to get some box to , the&#10;agent should drive a truck to the city where the box is located, load&#10;the box on the truck, drive the truck to , and finally unload the box in&#10;. This is essentially encoded in the symbolic value function shown in&#10;Fig. \[fig:vfun\_and\_policy\], which was computed by discounting&#10;rewards time steps into the future by .&#10;&#10;Similar to this example, for some problems we can obtain a solution&#10;which is described abstractly and is independent of the specific problem&#10;instance or even its size — for our example problem the description of&#10;the solution does not depend on the number of cities, trucks or boxes,&#10;or on knowledge of the particular location of any specific truck.&#10;Accordingly, one might hope that computing such a solution can be done&#10;without knowledge of these quantities and in time complexity independent&#10;of them. This is the computational advantage of symbolic stochastic&#10;planning which we associate with lifted inference in this chapter.&#10;&#10;The next two subsections expand on the connection between planning and&#10;inference, identify opportunities for lifted inference, and use these&#10;observations to define a new setup which we call [GENERALIZED LIFTED&#10;INFERENCE]{} which abstracts some of the work in both areas and provides&#10;new challenges for future work.">
  <outline text="Stochastic Planning and Inference" _note="Planning is the task of choosing what actions to take to achieve some&#10;goals or maximize long-term reward. When the dynamics of the world are&#10;deterministic, that is, each action has exactly one known outcome, then&#10;the problem can be solved through logical inference. That is, inference&#10;rules can be used to deduce the outcome of individual actions given the&#10;current state, and by combining inference steps one can prove that the&#10;goal is achieved. In this manner a proof of goal achievement embeds a&#10;plan. This correspondence was at the heart of McCarthy’s seminal paper&#10;that introduced the topic of AI and viewed planning as symbolic logical&#10;inference. Since this formulation uses first-order logic, or the closely&#10;related situation calculus, lifted logical inference can be used to&#10;solve deterministic planning problems.&#10;&#10;When the dynamics of the world are non-deterministic, this relationship&#10;is more complex. In particular, in this chapter we focus on the&#10;stochastic planning problem where an action can have multiple possible&#10;known outcomes that occur with known state-dependent probabilities.&#10;Inference in this case must reason about probabilities over an&#10;exponential number of state trajectories for some planning horizon.&#10;While lifted inference and planning may seem to be entirely different&#10;problems, analogies have been made between the two fields in several&#10;forms . To make the connections concrete, consider a finite domain and&#10;the finite horizon goal-oriented version of the &lt;span&#10;style=&quot;font-variant:small-caps;&quot;&gt;BoxWorld&lt;/span&gt; planning problem of&#10;Figure \[fig:boxworld\], e.g., two boxes, three trucks, and four cities&#10;and a planning horizon of 10 steps where the goal is to get some box in&#10;. In this case, the value of a state, , corresponds to the probability&#10;of achieving the goal, and goal achievement can be modeled as a specific&#10;form of inference in a Bayesian network or influence diagram.&#10;&#10;We start by considering the [CONFORMANT PLANNING PROBLEM]{} where the&#10;intended solution is an explicit sequence of actions. In this case, the&#10;sequence of actions is determined in advance and action choice at the th&#10;step does not depend on the actual state at the th step. For this&#10;formulation, one can build a Dynamic Bayesian Network (DBN) model where&#10;each time slice represents the state at that time and action nodes&#10;affect the state at the next time step, as in Figure \[fig:PasI\](a).&#10;The edges in this diagram capture , where is the current state, is the&#10;current action and is the next state, and each of is represented by&#10;multiple nodes to show that they are given by a collection of predicates&#10;and their values. Note that, since the world dynamics are known, the&#10;conditional probabilities for all nodes in the graph are known. As a&#10;result, the goal-based planning problem where a goal must hold at the&#10;last step, can be modeled using standard inference. The value of&#10;conformant planning is given by marginal MAP (where we seek a MAP value&#10;for some variables but take expectation over the remaining variables) :&#10;The optimal conformant plan is extracted using argmax instead of max in&#10;the equation.&#10;&#10;The standard MDP formulation with a reward per time time step which is&#10;accumulated can be handled similarly, by normalizing the cumulative&#10;reward and adding a binary node whose probability of being true is a&#10;function of the normalized cumulative reward. Several alternative&#10;formulations of planning as inference have been proposed by defining an&#10;auxiliary distribution over finite trajectories which captures utility&#10;weighted probability distribution over the trajectories . While the&#10;details vary, the common theme among these approaches is that the&#10;planning objective is equivalent to calculating the partition function&#10;(or “probability of evidence&quot;) in the resulting distribution. This&#10;achieves the same effect as adding a node that depends on the cumulative&#10;reward. To simplify the discussion, we continue the presentation with&#10;the simple goal based formulation.&#10;&#10;The same problem can be viewed from a Bayesian perspective, treating&#10;actions as random variables with an uninformative prior. In this case we&#10;can use to observe that and therefore one can alternatively maximize the&#10;probability conditioned on .&#10;&#10;However, linear plans, as the ones produced by the conformant setting,&#10;are not optimal for probabilistic planning. In particular, if we are to&#10;optimize goal achievement then we must allow the actions to depend on&#10;the state they are taken in. That is, the action in the second step is&#10;taken with knowledge of the probabilistic outcome of the first action,&#10;which is not known in advance. We can achieve this by duplicating action&#10;nodes, with a copy for each possible value of the state variables, as&#10;illustrated in Figure \[fig:PasI\](b). This represents a separate policy&#10;associated with each horizon depth which is required because finite&#10;horizon problems have non-stationary optimal policies. In this case,&#10;state transitions depend on the identity of the current state and the&#10;action variables associated with that state. The corresponding inference&#10;problem can be written as follows: However, the number of random&#10;variables in this formulation is prohibitively large since we need the&#10;number of original action variables to be multiplied by the size of the&#10;state space.&#10;&#10;Alternatively, the same desideratum, optimizing actions with knowledge&#10;of the previous state, can be achieved without duplicating variables in&#10;the equivalent formulation In fact, this formulation is exactly the same&#10;as the finite horizon application of the value iteration (VI) algorithm&#10;for (goal-based) Markov Decision Processes (MDP) which is the standard&#10;formulation for sequential decision making in stochastic environments.&#10;The standard formulation abstracts this by setting The optimal policy&#10;(at ) can be obtained as before by recording the argmax values. In terms&#10;of probabilistic inference, the problem is no longer a marginal MAP&#10;problem because summation and maximization steps are constrained in&#10;their interleaved order. But it can be seen as a natural extension of&#10;such inference questions with several alternating blocks of expectation&#10;and maximization. We are not aware of an explicit study of such problems&#10;outside the planning context.">
  </outline>
  <outline text="Stochastic Planning and Generalized Lifted Inference" _note="Given that planning can be seen as an inference problem, one can try to&#10;apply ideas of lifted inference to planning. Taking the motivating&#10;example from Figure \[fig:boxworld\], let us specialize the reward to a&#10;ground atomic goal equivalent to for constants and . Then we can query&#10;to compute where is the concrete value of the current state.&#10;&#10;Given that Figure \[fig:boxworld\] implies a complex relational&#10;specification of the transition probabilities, lifted inference&#10;techniques are especially well-placed to attempt to exploit the&#10;structure of this query to perform inference in aggregate and thus avoid&#10;redundant computations. However, we emphasize that, even if lifted&#10;inference is used, this is a standard query in the graphical model where&#10;evidence constrains the value of some nodes, and the solution is a&#10;single number representing the corresponding probability (together with&#10;a MAP assignment to variables).&#10;&#10;However, Eq \[eq:VI\] suggests an explicit additional structure for the&#10;planning problem. In particular, the intermediate expressions include&#10;the values (the probability of reaching the goal in steps) for all&#10;possible concrete values of . Similarly, the final result includes the&#10;values for all possible start states. In addition, as in our running&#10;example we can consider more abstract rewards. This suggests a first&#10;generalization of the standard setup in lifted inference. Instead of&#10;asking about a ground goal and expecting a single number as a response,&#10;we can abstract the setup in two ways: first, we can ask about more&#10;general conditions such as and second we can expect to get a structured&#10;result that specifies the corresponding probability for every concrete&#10;state in the world. If we had two box instances and truck instances ,&#10;the answer for , i.e., the value for the goal based formulation with&#10;horizon one, might take the form:&#10;&#10;The significance of this is that the question can have a more general&#10;form and that the answer solves many problems simultaneously, providing&#10;the response as a case analysis depending on some properties of the&#10;state. We refer to this reasoning as [INFERENCE WITH GENERALIZED QUERIES&#10;AND ANSWERS]{}. In this context, the goal of lifted inference will be to&#10;calculate a structured form of the reply directly.&#10;&#10;A second extension arises from the setup of generalized queries. The&#10;standard form for lifted inference is to completely specify the domain&#10;in advance. This means providing the number of objects and their&#10;properties, and that the response to the query is calculated only for&#10;this specific domain instantiation. However, inspecting the solution in&#10;the previous paragraph it is obvious that we can at least hope to do&#10;better. The same solution can be described more compactly as&#10;&#10;Arriving at such a solution requires us to allow open domain reasoning&#10;over all potential objects (rather than grounding them, which is&#10;impossible in open domains), and to extend ideas of lifted inference to&#10;exploit quantifiers and their structure. Following through with this&#10;idea, we can arrive at a DOMAIN-SIZE INDEPENDENT VALUE FUNCTION AND&#10;POLICY as the one shown in Figure \[fig:vfun\_and\_policy\]. In this&#10;context, the goal of lifted inference will be to calculate an abstracted&#10;form of the reply directly. We call this problem [INFERENCE WITH&#10;GENERALIZED MODELS]{}. As we describe in this chapter, SDP algorithms&#10;are able to perform this type of inference.&#10;&#10;The previous example had enough structure and a special query that&#10;allowed the solution to be specified without any knowledge of the&#10;concrete problem instance. This property is not always possible. For&#10;example, consider a setting where we get one unit of reward for every&#10;box in : . In addition, consider the case where, after the agent takes&#10;their action, any box which is not on a truck disappears with&#10;probability . In this case, we can still potentially calculate an&#10;abstract solution, but it requires access to more complex properties of&#10;the state, and in some cases the domain size (number of objects) in the&#10;state. For our example this gives:&#10;&#10;Here we have introduced a new notation for count expressions where, for&#10;example, counts the number of boxes in Paris in the current state. To&#10;see this result note that any existing box in Paris disappears 20% of&#10;the time and that a box on a truck is successfully unloaded 90% of the&#10;time but remains and does not disappear only in 80% of possible futures&#10;leading to the value 7.2. This is reminiscent of the type of expressions&#10;that arise in existing lifted inference problems and solutions. Typical&#10;solutions to such problems involve parameterized expressions over the&#10;domain (e.g., counting, summation, etc.), and critically do not always&#10;require closed-domain reasoning (e.g., A PRIORI knowledge of the number&#10;of boxes). They are therefore suitable for inference with generalized&#10;models. Some work on SDP has approached lifted inference for problems&#10;with this level of complexity, including exogenous activities (the&#10;disappearing boxes) and additive rewards. But, as we describe in more&#10;detail, the solutions for these cases are much less well understood and&#10;developed.&#10;&#10;To recap, our example illustrates that stochastic planning potentially&#10;enables abstract solutions that might be amenable to lifted&#10;computations. SDP solutions for planning problems have focused on the&#10;computational advantages arising from these expressive generalizations.&#10;At the same time, the focus in SDP algorithms has largely been on&#10;problems where the solution is completely independent of domain size and&#10;does not require numerical properties of the state. These algorithms&#10;have thus skirted some of the computational issues that are typically&#10;tackled in lifted inference. It is the combination of these aspects, as&#10;illustrated in the last example, which we call [**generalized lifted&#10;inference**]{}. As the discussion suggests, generalized lifted inference&#10;is still very much an open problem. In addition to providing a survey of&#10;existing SDP algorithms, the goal of this chapter is to highlight the&#10;opportunities and challenges in this exciting area of research.">
  </outline>
</outline>
  </body>
</opml>