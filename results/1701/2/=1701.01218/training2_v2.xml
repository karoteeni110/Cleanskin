<?xml version="1.0" encoding="UTF-8"?>
<opml version="2.0">
  <head>
    <title></title>
    <abstract> </abstract>
  </head>
  <body>
<outline text="Equal-size Clustering" _note="There are recent algorithms that deal with size constraints in&#10;clustering. For example,&#10;&#10;  formulated the problem of clustering with size constraints as a linear&#10;programming problem. However such algorithms are not computationally&#10;efficient, especially for large scale datasets (e.g., Human3.6M). We&#10;study two efficient ways to generate equal size clusters; see&#10;Table \[tab:thcomp\] (last row) for their ODC-complexity.&#10;&#10;**Recursive Projection Clustering (RPC) .** In this method,&#10;&#10;the training data is partitioned to perform GPR prediction.&#10;&#10;Initially all data points are put in one cluster. Then, two points are&#10;chosen randomly&#10;&#10;and orthogonal projection of all the data onto the line connecting them&#10;is computed. Depending on the median value of the projections, The data&#10;is then split into two equal size subsets. The same process is then&#10;applied to each cluster to generate clusters after repetitions. The&#10;iterations stops once . As indicated, the number of clusters in this&#10;method has to be a power of two and it might produce long thin clusters.&#10;&#10;**Equal-Size K-means (EKmeans).** We propose a variant of k-means&#10;clustering  to generate equal-size clusters.&#10;&#10;The goal is to obtain disjoint partitioning of into clusters , similar&#10;to the k-means objective, minimizing the within-cluster sum of squared&#10;Euclidean distances, ,&#10;&#10;where is the mean of cluster , and is the squared distance. Optimizing&#10;this objective is NP-hard and k-means iterates between the assignment&#10;and update steps as a heuristic to achieve a solution; denotes number of&#10;iterations of kmeans. We add equal-size constraints .&#10;&#10;In order to achieve this partitioning, we propose an efficient heuristic&#10;algorithm, denoted by [ASSIGN AND BALANCE (AB) EKMEANS]{}. It mainly&#10;modifies the assignment step of the k-means to bound the size of the&#10;resulting clusters. We first assign the points to their closest see&#10;center as typically done in the assignment step of k-means. We use to&#10;denote the cluster assignment of a given point . This results in three&#10;types of clusters: balanced, overfull, and underfull clusters. Then some&#10;of the points in the overfull clusters are redistributed to the&#10;underfull clusters by assigning each of these points to the closest&#10;underfull cluster. This is achieved by initializing a pool of overfull&#10;points defined as ; see Figure \[figABkmeans57\].&#10;&#10;Let us denote the set of underfull clusters by . We compute the&#10;distances . Iteratively, we pick the minimum distance pair and assign to&#10;cluster instead of cluster . The point is then removed from the overfull&#10;pool. Once an underfull cluster becomes full it is removed from the&#10;underfull pool, once an overfull cluster is balanced, the remaining&#10;points of that cluster are removed from overfull pool. The intuition&#10;behind this algorithms is that, the cost associated with the initial&#10;optimal assignment (given the computed means) is minimally increased by&#10;each swap since we pick the minimum distance pair in each iteration.&#10;Hence the cost is kept as low as possible while balancing the clusters.&#10;We denote the the name of this Algoirthm as Assign and Balance EKmeans.&#10;Algorithm \[alg:ddclusterALg1\] illustrates the overall assignment step&#10;and Fig. \[fig\_ABKmeans\_balancing\] visualizes the balancing step.&#10;&#10;\[t!\]&#10;&#10;1- Assign the points initially to its closest center; this will put the&#10;clusters into 3 groups (1) balanced clusters (2) overflowed clusters (3)&#10;under-flowed clusters.\&#10;2- Create a matrix , where is the distance between the point to the&#10;cluster center; rows are restricted points belongs only to the&#10;overflowed clusters; columns are restricted to underflowed cluster&#10;centers\&#10;3- Get the coordinate that maps the smallest distance in .\&#10;4- Remove the row from matrix and mark it as assigned to the cluster\&#10;5- If the size of the cluster achieves the ideal size (i.e.   ), then&#10;remove the column from matrix .\&#10;6- Go to step 3 if there is still unassigned points">
</outline>
<outline text="Overlapping Domain Cover(ODC) Model" _note="Having generated the disjoint equal size clusters, we generate the ODC&#10;subdomains based on the overlapping ratio , such that points are&#10;selected from the neighboring clusters. Let’s assume that we select only&#10;the closest clusters to each cluster, is closer to than if . It is&#10;important to note that must be greater than in order to supply the&#10;required points; this is since number of points in each cluster is .&#10;Hence, the minimum value for is clusters. Hence, we parametrize as . We&#10;study the effect of in the experimental results section. Having computed&#10;from and , each subdomain is then created by merging the points in the&#10;cluster with points, retrieved from the neighboring clusters.&#10;Specifically, the points are selected by sorting the points in each of&#10;clusters by the distance to . The number of points retrieved for each of&#10;the neighboring clusters is inversely proportional to the distance of&#10;its center to . If a subset of the clusters are requested to retrieve&#10;more than its capacity (i.e., ), the set of the extra points are&#10;requested from the remaining clusters giving priority to the closer&#10;clusters (i.e., starting from the nearest neighboring cluster to the&#10;cluster on which the subdomain is created).&#10;&#10;As and increases, all points that belong to the clusters tends to be&#10;merged with . In our framework, we used FLANN   for fast NN-retrieval;&#10;see pseudo-code of ODC generation in Appendix C.&#10;&#10;After the ODC is generated, we compute the the sample normal&#10;distribution using the points that belong to each subdomain. Then, a&#10;local kernel machine is trained for each of the overlapping subdomains.&#10;We denote the point set normal distribution of the subdomains as ;&#10;&#10; is precomputed during the training for later use during the prediction.&#10;&#10;Finally, we factor out all the computations that does not depend on the&#10;test point (for GPR, TGP, IWTGP) and store them with each sub domain as&#10;its local kernel machine. We denote the training model for subdomain as&#10;, which is computed as follows for GPR and TGP respectively.&#10;&#10;**[GPR]{}.**&#10;&#10;Firstly, we precompute , where is an kernel matrix, defined on the input&#10;points in . Each dimension in the output could have its own&#10;hyper-parameters, which results in a different kernel matrix for each&#10;dimension . We also precompute for each dimension. Hence .&#10;&#10;**[TGP.]{}**&#10;&#10;The local kernel machine for each subdomain in TGP case is defined as ,&#10;where and are kernel matrices defined on the input points and the&#10;corresponding output points respectively, which belong to domain .&#10;&#10;**IWTGP.** It is not obvious how to factor out computations that does&#10;not depend on the test data in the case of IWTGP, since the&#10;computational extensive factor(i.e., , ) does depend on the test set&#10;since is computed on test time. To help factor out the computation, we&#10;used linear algebra to show that where is a diagonal matrix, is the&#10;identity matrix, and is the trace of matrix .&#10;&#10;Kenneth Miller  proposed the following Lemma on Matrix Inverse. Applying&#10;Miller’s lemma, where and , leads directly to Eq. \[eq:lem\].&#10;&#10;Mapping to [^1], to either of or , we can compute . Having computed on&#10;test time, , could be computed in quadratic time given following&#10;equation  \[eq:lem\], since the inverse and the power of has linear&#10;computational complexity since it is diagonal.&#10;&#10;[^1]:  is a diagonal matrix">
</outline>
  </body>
</opml>