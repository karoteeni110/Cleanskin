%

\section{Discussion and Related Work}

%
%
%
%
%
%
%
%
%

%

%
%

As motivated in the introduction, SDP has explored probabilistic inference problems with a specific form of alternating maximization and expectation blocks. The main computational advantage comes from lifting in the sense of lifted inference in standard first order logic. Issues that arise from conditional summations over combinations random variables,  common in probabilistic lifted inference, have been touched upon but not extensively. In cases where SDP has been shown to work it provides {\em generalized lifted inference} where the complexity of the inference algorithm is completely independent of the domain size (number of objects) in problem specification, and where the response to queries is either independent of that size or can be specified parametrically. 
This is a desirable property but to our knowledge it is not shared by most work on probabilistic lifted inference. A notable exception is given by the knowledge compilation result of \cite{vandenbroeck-thesis} (see Chapter 4 and Theorem 5.5) 
and the recent work in \cite{KazemiP16,KazemiKBP16}, where a model is compiled into an alternative form parametrized by the domain $D$ and where responses to queries can be obtained in polynomial time as a function of $D$. 
The emphasis in that work is on being {\em domain lifted} (i.e., being polynomial in domain size). Generalized lifted inference requires an algorithm whose results can be computed once, in time independent of that size, and then reused to evaluate the answer for specific domain sizes.
This analogy also shows that SDP can be seen as a compilation algorithm, compiling a domain model into  a more accessible form representing the value function, which can be queried efficiently. 
This connection provides an interesting new perspective on both fields.

In this chapter we focused on one particular instance of SDP. 
Over the last 15 years SDP has seen a significant amount of work expanding over the original algorithm 
by using different representations, by using algorithms other than value iteration, and by extending the models and algorithms to more complex settings. In addition, several ``lifted" inductive approaches that do not strictly fall within the probabilistic inference paradigm have been developed. 
We review this work in the remainder of this section. 

 
%
%
%
%


\subsection{Deductive Lifted Stochastic Planning}

As a precursor to its use in lifted stochastic planning, the term SDP
originated in the propositional logical
context~\cite{bout-dean-hanks,boutilier99dt} when it was realized that
propositionally structured MDP transitions (i.e., dynamic Bayesian
networks~\cite{dbn}) and rewards (e.g., trees that exploited
context-specific independence~\cite{csi}) could be used to define
highly compact \textit{factored MDPs}; this work also realized that the
factored MDP structure could be exploited for representational
compactness and computational efficiency by leveraging symbolic
representations (e.g., trees) in dynamic programming.  Two highly
cited (and still used algorithms) in this area of work are the
SPUDD~\cite{spudd} and APRICODD~\cite{apricodd} algorithms that
leveraged algebraic decision diagrams (ADDs)~\cite{BaharFrGaHaMaPaSo93} for,
respectively, exact and approximate solutions to factored MDPs.
Recent work in this area \cite{lesner:ppddl11} shows how to perform propositional SDP 
directly with  ground representations in PPDDL~\cite{ppddl}, and develops extensions
for factored action spaces \cite{raghavan2012planning,raghavan2013symbolic}.

%
%
%
%
%
%

%
%
%
%
%
%
%
%

Following the seminal introduction of {\it lifted} SDP
in~\cite{BoutilierRePr01}, several early papers on SDP approached the
problem with existential rewards with different representation
languages that enabled efficient implementations. This includes the
First-order value iteration
(FOVIA)~\citep{lao_fovia,HolldoblerKaSk2006}, the Relational Bellman
algorithm (ReBel)~\citep{KerstingOtDe04}, and the FODD based
formulation of \citep{WangJoKh08,JoshiKh08,JoshiKeKh10}.

Along this dimension two representations are closely related to the
relational expression of this chapter.  As mentioned above, relational
expressions are an abstraction of the GFODD representation
\citep{JoshiKeKh11,JoshiKhRaTaFe13,HescottKh15} which captures
expressions using a decision diagram formulation extending
propositional ADDs \cite{BaharFrGaHaMaPaSo93}.  In particular, paths
in the graphical representation of the DAG representing the GFODD
correspond to the mutually exclusive conditions in expressions. The
aggregation in GFODDs and relational expressions provides significant
expressive power in modeling relational MDPs. The GFODD representation
is more compact than relational expressions but requires more complex
algorithms for its manipulation.  The other closely related
representation is the case notation of
\cite{BoutilierRePr01,SannerBo09}.  The case notation is similar to
relational expressions in that we have a set of conditions (these are
mostly in a form that is mutually exclusive but not always so) but the
main difference is that quantification is done within each case
separately, and the notion of aggregation is not fully developed.
First-order algebraic decision diagrams
(FOADDs)~\citep{Sanner08,SannerBo09} are related to the case
notation in that they require closed formulas within diagram nodes,
i.e., the quantifiers are included within the graphical representation
of the expression.  The use of quantifiers inside cases and nodes
allows for an easy incorporation of off the shelf theorem provers for
simplification.
%
Both FOADD and GFODD were used to extend SDP to capture additive rewards and exogenous events as already discussed in the previous section.
While the representations (relational expression and GFODDs vs.\ case notation and FOADD) have similar expressive power, the difference in aggregation makes for different algorithmic properties that are hard to compare in general. 
However, the modular treatment of aggregation in GFODDs and the generic form of operations over them makes them the most flexible alternative to date for directly manipulating the aggregated case representation used in this chapter.
%

The idea of SDP has also been extended in terms of the choice of
planning algorithm, as well as to the case of partially observable
MDPs.  Case notation and FOADDs have been used to implement
approximate linear programming~\citep{foalp,SannerBo09} and
approximate policy iteration via linear programming~\citep{foapi} and
FODDs have been used to implement relational policy iteration
\cite{WangKh07}.  GFODDs have also been used for open world reasoning
and applied in a robotic context \cite{JoshiSKS12}.  The work of
\cite{WangK10} and \cite{SannerK10} explore SDP solutions, with GFODDs
and case notation respectively, to relational partially observable MDPs (POMDPs) where the
problem is conceptually and algorithmically much more complex.
Related work in POMDPs has not explicitly addressed SDP, but rather has
implicitly addressed lifted solutions through the identification of (and
abstraction over) symmetries in applications of dynamic programming
for POMDPs~\cite{doshi:permpomdp08,kim:sympomdp12}.

%
%
%
%
%
%


%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%

%
%
%
%
%
%
%
%

\subsection{Inductive Lifted Stochastic Planning}

Inductive methods can be seen to be orthogonal to the inference algorithms in that they mostly do not require a model and do not reason about that model. However, 
%
%
%
%
%
%
%
the overall objective of
producing lifted value functions and policies is shared with the
previously discussed deductive approaches.  
We therefore review these here for completeness. As we discuss, it is also possible  
to combine the inductive and deductive approaches in several ways.


%
%
%
%
The basic inductive approaches learn a policy directly from a teacher, sometimes known as behavioral cloning. 
The work of \cite{Khardon96,Khardon99,givan:uai02} provided learning algorithms for relational policies with theoretical and empirical evidence for their success. 
Relational policies and value functions were also explored in reinforcement learning.
This
was done with pure reinforcement learning using relational 
%
regression trees to learn a 
%
Q-function~\citep{dzeroski01},
combining this with supervised guidance~\citep{driessens02}, or using
Gaussian processes and graph kernels over relational structures to
learn a 
%
Q-function~\citep{driessens06}.
A more recent approach uses 
functional gradient boosting with lifted
regression trees to learn lifted policy structure in
a policy gradient algorithm~\cite{kersting:nppg}.

Finally, several approaches combine inductive and deductive elements. 
The work of 
\cite{gretton_thiebaux} combines inductive logic programming with first-order
decision-theoretic regression, by first using deductive methods (decision theoretic regression) to
generate candidate policy structure, and then learning using this structure as features. 
The work of  \citep{givan:jair06} shows how one can implement relational approximate policy iteration where policy improvement steps are performed by learning the intended policy from generated trajectories instead of direct calculation. 
Although these approaches are partially deductive they do not share the common theme of this chapter relating planning and inference in relational contexts.  


%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%

%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
