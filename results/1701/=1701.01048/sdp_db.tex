%

\section{Symbolic Dynamic Programming}

%
%
%
The SDP algorithm is a symbolic implementation of the value iteration algorithm. 
The algorithm repeatedly applies so-called decision-theoretic regression which is equivalent to one iteration of the value iteration algorithm. 

%

As input to SDP we get closed
relational expressions for $V_k$ and $R$.  In addition, assuming that we
are using the \emph{Endogenous Branching Transition} model of the
previous section, we get open expressions for the probabilistic choice
of actions $Pr(A_j(X)|A(X))$ and for the dynamics of deterministic
action variants as TVDs. The corresponding expressions for the running example are given respectively in 
Eq~(\ref{eq:reward}), 
Eq~(\ref{eq:stoch_act_ex}) and Eq~(\ref{eq:ssa_ex}).

The following SDP algorithm of \cite{JoshiKeKh11} modifies the earlier
SDP algorithm of~\cite{BoutilierRePr01} and implements Eq~(\ref{eq:viflat}) using
the following 4 steps:
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
\begin{enumerate}
\item \label{sdp_1} {\bf Regression:} The $k$ step-to-go value
  function $V_k$ is regressed over every deterministic variant
  $A_j(X)$ of every action $A(X)$ to produce $\Regr(V_k, A_j(X))$.
%
  %
  %
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
Regression is conceptually similar to goal regression in
deterministic planning. That is, we identify conditions that need to occur before the action is taken in order to arrive at other conditions (for example the goal) after the action.  
However, here we need to regress all the conditions in the relational expression capturing the value function, so that we must regress 
each case $c_i$
of $V_k$ separately.  This can be done efficiently by replacing every atom in
each $c_i$ by its corresponding positive or negated portion of the TVD without changing the aggregation
function.  
Once this substitution is done, logical simplification (at the
  propositional level) can be used to compress the cases  by
  removing contradictory cases and simplifying the formulas. 
Applying this to regress $\unloadS(B_1,T_1,C_1)$ over the reward function given by Eq~(\ref{eq:reward}) we get:
\begin{align*}
  [\max_B \, [ & (\BIn(B,\paris) \lor \\
      & ((B_1=B)\land (C_1=\paris)  \land \On(B_1,T_1) \land \TIn(T_1,C_1))): 10; \neg: 0]]
\end{align*}
and regressing $\unloadF(B_1,T_1,C_1)$ yields
\begin{equation*}
[\max_B \, [\BIn(B,\paris): 10; \neg: 0]]
\end{equation*}
This illustrates the utility of compiling the transition model into the TVDs which allow for a simple implementation of deterministic regression.
  %
  %
  
\item \label{sdp_2} {\bf Add Action Variants:} The Q-function
  $Q_k^{A(X)}$ $=$ $R$ $\oplus$ $[\gamma$ $\otimes$
  $\oplus_j(Pr(A_j(X))$ $\otimes$ $Regr(V_k, A_j(X)))]$ for each
  action $A(X)$ is generated by combining regressed diagrams using the
  binary operations $\oplus$ and $\otimes$ over expressions.
%
  Recall that probability expressions do not refer to additional
  variables. The multiplication can therefore be done directly on the
  open formulas without changing the aggregation function.  As argued by
  \cite{WangJoKh08}, to guarantee correctness, both summation steps
  ($\oplus_j$ and $R\oplus$ steps) must standardize apart the functions
  before adding them.
  
  For our running example and assuming $\gamma=0.9$, we would need to compute the following:
  \begin{align*}
    Q_k&^{\unload(B_1,T_1,C_1)}(S)  = \\  R&(S) \oplus 0.9 \cdot \\
     [&(\Regr(V_0, \unloadS(B_1,T_1,C_1)) \otimes P(\unloadS(B_1,T_1,C_1) | \unload(B_1,T_1,C_1))) \oplus\\
      &(\Regr(V_0, \unloadF(B_1,T_1,C_1)) \otimes P(\unloadF(B_1,T_1,C_1) | \unload(B_1,T_1,C_1)))].
  \end{align*}
We next illustrate some of these steps. The multiplication by probability expressions can be done by cross product of cases and simplification. For $\unloadS$ this yields
\begin{align*}
  [\max_B \, [ & ((\BIn(B,\paris) \lor ((B_1=B)\land (C_1=\paris))) \\
               &   \land \On(B_1,T_1) \land \TIn(T_1,C_1)): 9; \neg: 0]]
\end{align*}
and for $\unloadF$ we get 
\begin{align*}
%
%
%
%
  [\max_B \, [ & \BIn(B,\paris)   \land (\On(B_1,T_1) \land \TIn(T_1,C_1)): 1;  \\
& \BIn(B,\paris) \land  \neg (\On(B_1,T_1) \land \TIn(T_1,C_1)): 10; \\
& \neg:  0]].
\end{align*}
Note that the values here are weighted by the probability of occurrence. For example the first case in the last equation has value 1=10*0.1 because when the preconditions of $\unload$ hold the variant $\unloadF$ occurs with $10\%$ probability. 
The addition of the last two equations requires standardizing them apart, performing the safe operation through cross product of cases, and simplifying. Skipping intermediate steps, this yields 
  \begin{align*}
[\max_B \, [ & \BIn(B,\paris):10;  \\
  & \neg \BIn(B,\paris) \land (B_1=B)\land (C_1=\paris)  \land \On(B_1,T_1) \land \TIn(T_1,C_1): 9; \\
  & \neg: 0]].
  \end{align*}
Multiplying by the discount factor scales the numbers in the last equation by 0.9 and finally standardizing apart and adding the reward and simplifying (again skipping intermediate steps) yields
  \begin{align*}
Q_0&^{\unload(B_1,T_1,C_1)}(S) = \\
%
%
%
[& \max_B \, [ \BIn(B,\paris):19;  \\
  & \qquad \, \neg \BIn(B,\paris) \land (B_1=B)\land (C_1=\paris)  \land \On(B_1,T_1) \land \TIn(T_1,C_1): 8.1; \\
  & \qquad \, \neg: 0]].
  \end{align*}
  Intuitively, this result states that after executing a concrete stochastic $\unload$
  action with arguments $(B_1,T_1,C_1)$, we achieve the highest value (10 plus a discounted 0.9*10) if a box was already in Paris,
  the next highest value (10 occurring with probability 0.9 and discounted by 0.9) if unloading $B_1$ from $T_1$ in $C_1=\paris$, and a
  value of zero otherwise. 
  The main source of efficiency (or lack thereof) of SDP is the ability to perform such operations symbolically and simplify the result into a compact expression.
  
 
\item \label{sdp_3} {\bf Object Maximization:} 
Note that up to this point in the algorithm the action arguments are still considered to be concrete arbitrary objects,
$(B_1,T_1,C_1)$ in our example. 
However, we must make sure that in each of the (unspecified and possibly infinite set of possible) states we choose the best concrete action for that state, by specifying the appropriate action arguments. This is handled in the current step of the algorithm.


To achieve this, we maximize over the
  action parameters $X$ of $Q_{V_k}^{A(X)}$ to produce $Q_{V_k}^A$ for each
  action $A(X)$. This implicitly obtains the value achievable by the best
  ground instantiation of $A(X)$ in each state. This step is
  implemented by converting action parameters $X$ 
  %
  to variables, each associated with the $\max$ aggregation operator,
  and appending these operators to the head of the aggregation
  function. Once this is done, further logical simplification may be possible. This occurs in our running example where existential quantification (over $B_1,C_1$) which is constrained by equality can be removed, and the result is:
\begin{align*}
Q_0^{\unload}(S) = & \\
[\max_T, \max_B \, & [\BIn(B,\paris):19;  \\
    & \neg \BIn(B,\paris) \land \On(B,T) \land \TIn(T,\paris): 8.1; \\
    & \neg: 0]].
\end{align*}

%
%
%
%
%
%
%
%

\item \label{sdp_4} {\bf Maximize over Actions:} The $k\!+\!1$st step-to-go
  value function $V_{k+1}$ $=$ $\max_A Q_{V_k}^A$, is generated by
  combining the expressions using the binary operation $\max$.
  
  Concretely, for our running example, this means we would compute:
  \begin{align*}
  V_1(S) = \max( Q_0^{\unload}(S), \max( Q_0^{\load}(S), Q_0^{\drive}(S) ) ).
  \end{align*}
  While we have only shown $Q_0^{\unload}(S)$ above, we remark that
  the values achievable in each state by $Q_0^{\unload}(S)$ dominate
  or equal the values achievable by $Q_0^{\load}(S)$ and $Q_0^{\drive}(S)$
  in the same state.  Practically this implies that after simplification
  we obtain the following value function:
  \begin{align*}
    V_1(S) = Q_0^{\unload}&(S) = \\
[\max_T, \max_B & [\BIn(B,\paris):19;  \\
    & \neg \BIn(B,\paris) \land \On(B,T) \land \TIn(T,\paris): 8.1; \\
    & \neg: 0]].    
%
%
%
  \end{align*}
%
%
%
%
%
%
%
%
%
%
%
  Critically for the objectives of lifted
  stochastic planning, we observe that the value function derived by
  SDP is indeed lifted: it holds for any number of boxes, trucks and cities.
  
\end{enumerate}

SDP repeats these steps to the required depth, iteratively calculating 
%
$V_k$.  For example, Figure~\ref{fig:vfun_and_policy} illustrates $V_\infty$
for the \textsc{BoxWorld} example, which was computed by terminating the SDP
loop once the value function converged.

The basic SDP algorithm is an exact calculation whenever the model can
be specified using the constraints above and the reward function can
be specified with $\max$ and $\min$ aggregation \cite{JoshiKeKh11}. 
This is satisfied by
classical models of stochastic planning.  As illustrated, in these cases, the SDP solution conforms to our definition of {generalized lifted inference}.
%
%
%

\subfour{Extending the Scope of SDP}  
The algorithm above cannot handle models with more complex dynamics and rewards as motivated in the introduction. In particular, prior work has considered two important properties that appear to be relevant in many domains. The first is additive rewards, illustrated for example, in Eq~\ref{eq:reward-additive}.
The second property is exogenous branching transitions illustrated above by the disappearing blocks example. 
These represent two different challenges for the SDP algorithm. The first is that we must handle sum aggregation in value functions, despite the fact that this means that some of the operations are not {\em safe} and hence require a special implementation. The second is in modeling the exogenous branching dynamics which requires getting around potential conflicts among such events and between such events and agent actions. 
The introduction illustrated the type of solution that can be expected in such a problem where counting expressions, that measure the number of times certain conditions hold in a state, determine the value in that state. 
 
To date, exact abstract solutions for problems of this form have not been obtained.
The work of \cite{sanner:icaps07}
and 
\cite{Sanner08} (Ch. 6) considered additive rewards and
has formalized an expressive family of models with exogenous events. This work 
has
shown that some specific challenging domains can be handled using several algorithmic ideas, but did not provide a general algorithm that is applicable across problems in this class. 
The work of \cite{JoshiKhRaTaFe13} 
developed a model for ``service domains" which significantly constrains the type of exogenous branching. In their model, a transition includes an agent step whose dynamics use endogenous branching, followed by ``nature's step" where each object (e.g., a box) experiences a random exogenous action (potentially disappearing). 
Given these assumptions, they provide a generally applicable approximation algorithm as follows.
%
Their algorithm treats agent's actions exactly as in SDP above. To regress nature's actions we follow the following three steps: (1) the summation variables
are first ground using a Skolem constant $c$, then (2) a single exogenous event centered at $c$ is regressed using the same machinery, and finally (3) the Skolemization is reversed to yield another additive value function. 
The complete details are beyond the scope of this chapter.
%
The algorithm yields a solution that avoids counting formulas and is syntactically close to the one given by the original algorithm. Since such formulas are necessary, the result is an approximation but it was shown to be a conservative one in that it provides a monotonic lower bound on the true value. 
Therefore, this algorithm
conforms to our definition of {\em approximate generalized lifted inference}. 

In our example, starting with the reward of Eq~(\ref{eq:reward-additive}) we first replace the sum aggregation with a scaled version of average aggregation (which is safe w.r.t.\ summation)
\begin{equation*}
[n \cdot \mbox{avg}_B [\BIn(B,\paris): 10; \neg: 0]]
\end{equation*}
and then ground it to get
\begin{equation*}
[n \cdot [\BIn(c,\paris): 10; \neg: 0]].
\end{equation*}
The next step is to regress through the exogenous event at $c$. The problem where boxes disappear with probability 0.2 can be cast as having two action variants where ``disappearing-block" succeeds with probability 0.2 and fails with probability 0.8.
Regressing the success variant we get the expression $[0]$ (the zero function) and regressing the fail variant we get
$[n \cdot [\BIn(c,\paris): 10; \neg: 0]]$. Multiplying by the probabilities of the variants we get:
$[0]$ and  $[n \cdot [\BIn(c,\paris): 8; \neg: 0]]$ and adding them (there are no variables to standardize apart) we get
\begin{equation*}
[n \cdot  [\BIn(c,\paris): 8; \neg: 0]].
\end{equation*}
Finally lifting the last equation we get
\begin{equation*}
[n \cdot \mbox{avg}_B [\BIn(B,\paris): 8; \neg  \BIn(B,\paris): 0]].
\end{equation*}
Next we follow with the standard steps of SDP for the agent's action. The steps are analogous to the example of SDP given above. 
%
%
%
Considering the discussion in the 
introduction (recall that in order to simplify the reasoning in this case we omitted discounting and adding the reward) this algorithm produces 
  \begin{align*}
%
& [n \cdot  \max_T, \mbox{avg}_B, 
[\BIn(B,\paris):8;  \\
& (\neg \BIn(B,\paris) \land \On(B,T) \land \TIn(T,\paris)): 7.2; \neg: 0]] , 
  \end{align*}
which is identical to the exact expression given in the introduction.
%
As already mentioned, the result is not guaranteed to be exact in general. 
%
In addition, the maximization in step~iv of SDP requires some ad-hoc implementation because maximization is not safe with respect to average aggregation.

It is clear from the above example that the main difficulty in extending SDP is due to the
interaction of the counting formulas arising from exogenous events and
additive rewards with the first-order aggregation structure inherent
in the planning problem.  Relational expressions, their GFODD counterparts, and other representations that have been used to date are not able to combine these effectively. A representation that seamlessly supports both
relational expressions and operations on them along with counting expressions
might allow for more robust versions of generalized lifted inference to be realized.



%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%


%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
