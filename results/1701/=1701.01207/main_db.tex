\documentclass[11pt,letterpaper]{article}
\usepackage{fullpage}
\usepackage[pdftex]{graphicx}
\usepackage{graphicx}
\usepackage{epstopdf,subcaption}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{multicol,multirow}
\usepackage{algorithm,algcompatible}
%\usepackage{mathtools}
\usepackage[margin=1in]{geometry}
%\usepackage{natbib}

%\usepackage{amssymb}
%\usepackage{amsthm}
%\usepackage{amsmath}
%\usepackage{fullpage}
%\usepackage[pdftex]{graphicx}
%\usepackage{graphicx}
%\usepackage{epstopdf,subcaption}
%\usepackage[margin=1in]{geometry}


\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{definition}{Definition}
\newtheorem{claim}{Claim}
\newtheorem{conjecture}[theorem]{Conjecture}
\newtheorem{observation}[theorem]{Observation}
\newtheorem{problem}{Problem}
\newtheorem{notation}[theorem]{Notation}
\newtheorem{alemma}{Lemma}
\newtheorem{example}[theorem]{Example}

\def\endproof{\vbox{\hrule height0.6pt\hbox{%
   \vrule height1.3ex width0.6pt\hskip0.8ex
   \vrule width0.6pt}\hrule height0.6pt
  }}

% Venkat's Macros
\newcommand{\ones}{\mathbf{1}}
\newcommand{\bzero}{\mathbf{0}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\bx}{\mathbf{x}}
\newcommand{\Sym}{\mathbb{S}}
\newcommand{\C}{\mathcal{C}}
\newcommand{\A}{\mathcal{A}}
\newcommand{\ch}{\mathrm{conv}}
\newcommand{\ba}{\mathbf{a}}
\newcommand{\be}{\mathbf{e}}
\newcommand{\by}{\mathbf{y}}
\newcommand{\bz}{\mathbf{z}}
%\newcommand{\bfz}{\mathbf{z}}
\newcommand{\bu}{\mathbf{u}}
\newcommand{\bv}{\mathbf{v}}
\renewcommand{\L}{\mathcal{L}}
\newcommand{\M}{\mathcal{M}}
\newcommand{\T}{\mathfrak{T}}
\renewcommand{\S}{\mathbb{S}}
\newcommand{\argmin}{\mathrm{arg~min}}

\newcommand{\botimes}{\boldsymbol \otimes}

\newcommand{\bs}{\mathbf{s}}
\newcommand{\bw}{\mathbf{w}}
\newcommand{\bt}{\mathbf{t}}
\newcommand{\ca}{\mathcal{A}}
\newcommand{\bepsilon}{\mathbf{\epsilon}}

%%%

%\newcommand{\ca}{\mathcal{A}}
%\newcommand{\ct}{\mathcal{T}}
%\newcommand{\cp}{\mathcal{P}}
%\newcommand{\xx}{\mathfrak{X}}
%\newcommand{\yy}{\boldsymbol{Y}}
%\newcommand{\dd}{\mathfrak{D}}
%\newcommand{\bs}{\mathbf{s}}
%\newcommand{\bt}{\mathbf{t}}
%
%\newcommand{\sfe}{\mathsf{E}}
%\newcommand{\sfd}{\mathsf{D}}
%\newcommand{\sff}{\mathsf{F}}
%\newcommand{\sfh}{\mathsf{H}}
%\newcommand{\sfi}{\mathsf{I}}
%\newcommand{\sfl}{\mathsf{L}}
%\newcommand{\sfm}{\mathsf{M}}
%\newcommand{\sfn}{\mathsf{N}}
%\newcommand{\sfq}{\mathsf{Q}}
%\newcommand{\sfw}{\mathsf{W}}
%\newcommand{\bbw}{\mathbb{W}}
%\newcommand{\err}{G}
%\newcommand{\ddo}{f}
%\newcommand{\eu}{\mathsf{eu}}
%\newcommand{\roc}{\Omega}
%\newcommand{\aroc}{\gamma}
%\newcommand{\coveig}{\Lambda}
%\newcommand{\covsup}{\Delta}
%\newcommand{\samplecov}{\mathsf{\Sigma}}
%\newcommand{\sigmacanorm}{\|\ca^*\|_{2}}

% Yong Sheng's Macros
\newcommand{\ct}{\mathcal{T}}
\newcommand{\cp}{\mathcal{P}}
\newcommand{\xx}{\mathfrak{X}}
\newcommand{\yy}{Y}
\newcommand{\dd}{\mathfrak{D}}

\newcommand{\sfe}{\mathsf{E}}
\newcommand{\sfd}{\mathsf{D}}
\newcommand{\sff}{\mathsf{F}}
\newcommand{\sfh}{\mathsf{H}}
\newcommand{\sfi}{\mathsf{I}}
\newcommand{\sfl}{\mathsf{L}}
\newcommand{\sfm}{\mathsf{M}}
\newcommand{\sfn}{\mathsf{N}}
\newcommand{\sfq}{\mathcal{Q}}
\newcommand{\sfw}{\mathcal{M}}
\newcommand{\bbw}{\mathbb{W}}
\newcommand{\err}{G}
\newcommand{\ddo}{f}
\newcommand{\eu}{\mathsf{eu}}
\newcommand{\roc}{\Omega}
\newcommand{\aroc}{\gamma}
\newcommand{\coveig}{\Lambda}
\newcommand{\covsup}{\Delta}
\newcommand{\samplecov}{\mathsf{\Sigma}}

%%%

\title{A Matrix Factorization Approach for \\ Learning Semidefinite-Representable Regularizers}
%\title{Learning Semidefinite-Representable Norms via Matrix Factorization}

\author{Yong Sheng Soh$^\dag$ and Venkat Chandrasekaran$^{\dag,\ddag}$ \thanks{Email: ysoh@caltech.edu, venkatc@caltech.edu} \vspace{0.25in} \\ $^\dag$ Department of Computing and Mathematical Sciences\\ $^\ddag$ Department of Electrical Engineering \\ California Institute of Technology \\ Pasadena, CA 91125}
%\date{}

\bibliographystyle{plain}

\begin{document}

\maketitle

\begin{abstract}
Regularization techniques are widely employed in optimization-based approaches for solving ill-posed inverse problems in data analysis and scientific computing.  These methods are based on augmenting the objective with a penalty function, which is specified based on prior domain-specific expertise to induce a desired structure in the solution.  We consider the problem of learning suitable regularization functions from data in settings in which precise domain knowledge is not directly available.  Previous work under the title of `dictionary learning' or `sparse coding' may be viewed as learning a regularization function that can be computed via linear programming.  We describe generalizations of these methods to learn regularizers that can be computed and optimized via semidefinite programming.  Our framework for learning such semidefinite regularizers is based on obtaining structured factorizations of data matrices, and our algorithmic approach for computing these factorizations combines recent techniques for rank minimization problems along with an operator analog of Sinkhorn scaling.  Under suitable conditions on the input data, our algorithm provides a locally linearly convergent method for identifying the correct regularizer that promotes the type of structure contained in the data.  Our analysis is based on the stability properties of Operator Sinkhorn scaling and their relation to geometric aspects of determinantal varieties (in particular tangent spaces with respect to these varieties). The regularizers obtained using our framework can be employed effectively in semidefinite programming relaxations for solving inverse problems.
\end{abstract}

\emph{Keywords}: atomic norm, convex optimization, low-rank matrices, nuclear norm, operator scaling, representation learning.

\section{Introduction} \label{sec:intro}

%-- Representation learning key to further data analysis
%
%-- Motivation: one approach is to learn concise data descriptions; atomic sets and atomic norms; goal is to learn a convex set / atomic set to fit data
%
%-- Our Contributions: learning polytope (finite atomic set) same as sparse coding; this paper is about learning semidefinite representations; give objective in terms of structured matrix factorization; then state that we develop alternating minimization approach involving OSI; prove that this algorithm is locally convergent
%
%-- Comparison Between Polyhedral and Semidefinite Data Descriptions: give distance / Gram matrices showing blocked vs toroidal structure, give complexity in terms of number of paramaters
%
%-- May need to blend previous two pieces
%
%-- Paper Outline

%Concise data description useful --> sparsity --> atomic sets and norms --> can we learn this from data.
%
%Point not just to learn atomic set but also a tractable norm in the sense that the associated convex program is tractable to compute.
%a determinantal variety (sets of rank-constrained matrices).  From an algebraic perspective, such atomic sets correspond to zero-dimensional ideals.  From an optimization perspective, the associated atomic norms   The atomic norms associated to such atomic sets can be represented via semidefinite programming.  In contrast, previous work on sparse coding in which one identifies finite atomic sets (in algebraic terms, zero-dimensional ideals) lead to polyhedral atomic norms that can be represented via linear programming.



%with the atoms parametrized efficiently as projections of determinantal varieties.  In particular, we investigate atomic sets that are specified as projections of determinantal varieties
%
%
%
%Focusing on past work for context ...
%
%Sparse data representations play a central role in contemporary data analysis in many problem domains.  Characterizing complex, high-dimensional data via a small number of elements from a set of basic building blocks or atoms is useful analysis of modern high-dimensional datasets in many problem domains.
%
%Prior knowledge of such low-dimensional structure, typically obtained via domain expertise,
%
%In this paper, we consider the problem of learning investigate a new
%
%Concise data descriptions play a central role in inverse problems throughout science
%
%
%and can be exploited powerfully in a range of inverse problems such as model selection, denoisingExamples of settings in which models with such \emph{low-dimensional structure} are prevalent include  in low-dimensional structure due to underlying constraints .  Useful in a range of data analysis tasks.  Sparse representations among the most powerful formalism for specifying structural constraints as they lead to algorithms for ....  Concretely, sparse data descriptions given as follows via atomic sets.  One of the virtues of such a description is that the norm defined by the convex hull ... is useful a regularizer in inverse problems such as denoising ....  Leads to tractable convex optimization based approaches for inverse problems involving structurally constrained dataIn this paper, we consider  and useful structural constraints.
%
% such as Markovianity, manifold structure, and sparsity.MArkovianitymanifold structure, markovianity,
%
%Sparse data representations play a central role in contemporary data analysis in many problem domains.  Characterizing complex, high-dimensional data via a small number of elements from a set of basic building blocks or atoms is useful analysis of modern high-dimensional datasets in many problem domains.  Specifying complex datasets Modern high-dimensional datasets often consist of latent low-dimensional structure such as manifold structure, markovianity, ....  Prior knowledge of this low-dimensional structure based on domain expertise is extremely useful and can be exploited powerfully in a range of inverse problems such as model selection, denoising, ...  Focus of this paper on learning the low-dimensional structure from data.
%
%Concretely, a prominent type of structured families are based on sparse data descriptions.  Consider a set of building blocks, or atoms.

%Despite the We ... From an optimization viewpoint, the particular families of algebraic varieties we consider are such that their gauge functions can be effectively represented via semidefinite programming; in contrast, finite atomic sets lead to polyhedral atomic norms that are described via linear programming.  Consequently, despite the potential computational difficulties that may arise with general infinite atomic sets, the specific families on which we focus can be employed
%
%images of determinantal varieties, whereas the previous efforts on learning finite atomic sets correspond to identifying zero-dimensional ideals.  Despite the seeming additional complexity due to the infinite atomic sets that we consider, these sets can nonetheless be employed in a computationally tractable convex optimization approach subsequently in tasks such as ..... ,
%
%tomic sets consisting of infinitely many elements.  We consider atomic sets specified as a particular type of algebraic variety with the property that the convex hulls of these varieties can be characterized tractably via semidefinite programming. investigate the problem of learning atomic sets that are
%
%In particular, we investigate the problem of learning an atomic set that is specified as a projection of rank-one matrices in a larger-dimensional space.  From an algebraic perspective, our approach corresponds to learning atomic sets that are images of determinantal varieties, whereas the previous efforts on learning finite atomic sets correspond to identifying zero-dimensional ideals.  From an optimization viewpoint, finite atomic sets lead to polyhedral norms that are described via linear programming, while our approach leads to atomic norms that can be represented via semidefinite programming.  Consequently, despite the transition from a finite atomic set to an infinite one, the parametrization of the types of the sets and we consider is such that we retain computational efficiency in the description of the convex hull.
%
%Formally, focu
%
%
%High-dimensional datasets in contemporary problem domains are often constrained structurally so that they contain far fewer degrees of freedom than their ambient dimension.  For example, natural images are often well-approximated by a small number of wavelet coefficients, financial time series may be characterized by low-complexity factor models, and a small number of genes may constitute a signature for disease.  Such low-dimensional structure can be leveraged in a range of data analysis tasks such as compression, denoising, and classification [CITE].  One of the most powerful formalisms for specifying and exploiting low-dimensional structure in data is based on the framework of \emph{sparse representations}.
%
%is based on the framework of \emph{sparse representations}High-dimensional datasets in contemporary problem domains are often constrained structurally so that they contain far fewer degrees of freedom than their ambient dimension.    .
%
%The influence of these methods is far-reachin as they provide In this paper, we study the question of \emph{learning} suitable regularization methods from data in settings in which prior domain expertise may not be available.  As such, these methods are widely used to , which is based on prior domain expertiseand theypresent methods for to avoid overfitting and enforcing prior domain expertise.  Sparse representations offer a powerful way to describe domain expertise and to transform such prior knowledge to convex regularizers that can be employed in a range of data analysis tasks.
%
%In this paper, we consider optimization-based computational procedures for solving inverse problems in which regularization methods take the form of penalty functions added to the objective to induce a desired structure in the solution.
%
%for inducing general and flexible notion of structure, namely those given by sparse representations, which lead naturally to structure-inducing regularizers that can be computed via
%
%they lend themselves naturally to computationally tractable convex relaxation techniques for inverse problems.

Regularization techniques are widely employed in the solution of inverse problems in data analysis and scientific computing due to their effectiveness in addressing difficulties due to ill-posedness.  In their most common manifestation, these methods take the form of penalty functions added to the objective in optimization-based approaches for solving inverse problems.  The purpose of the penalty function is to induce a desired structure in the solution, and these functions are specified based on prior domain-specific expertise.  For example, regularization is useful for promoting smoothness, sparsity, low energy, and large entropy in solutions to inverse problems in image analysis, statistical model selection, and the geosciences \cite{BDE:09,CanRec:09,CRT:06,CRPW:12,CDS:98,Don:06,MeiBuh:06,RFP:10,Tib:94}.  In this paper, we study the question of \emph{learning} suitable regularization functions from data in settings in which precise domain knowledge is not directly available.  The regularizers obtained using our framework are specified as convex functions that can be computed efficiently via semidefinite programming, and therefore they can be employed in tractable convex optimization approaches for solving inverse problems.

%to begin with, we consider the features that are desirable in a regularizer, and in particular how a regularizer encodes the structure desired in the eventual solution.  specifically, we discuss how a specify regularizers that promote a general form of sparsity structure in the desired solution.

We begin our discussion by highlighting the geometric aspects of regularizers that make them effective in promoting a desired structure.  In particular, we focus on a family of convex regularizers that are useful for inducing a general form of sparsity in solutions to inverse problems.  Sparse data descriptions provide a powerful formalism for specifying low-dimensional structure in high-dimensional data, and they feature prominently in a range of problem domains.  For example, natural images are often well-approximated by a small number of wavelet coefficients, financial time series may be characterized by low-complexity factor models, and a small number of genetic markers may constitute a signature for disease.  Concretely, suppose $\A \subset \R^d$ is a (possibly infinite) collection of elementary building blocks or atoms.  Then $\by \in \R^d$ is said to have a sparse representation using the atomic set $\A$ if $\by$ can be expressed as follows:
\begin{equation*}
\by = \sum_{i=1}^k c_i \ba_i, ~~~ \ba_i \in \A, c_i \geq 0,
\end{equation*}
for a relatively small number $k$.  As an illustration, if $\A =\{\pm \be^{(j)}\}_{j=1}^d \subset \R^d$ is the collection of signed standard basis vectors in $\R^d$, then concisely described objects with these atoms are those vectors in $\R^d$ consisting of a small number of nonzero coordinates.  Similarly, if $\A$ is the set of rank-one matrices, then the corresponding sparsely represented entities are low-rank matrices; see \cite{CRPW:12} for a more exhaustive collection of examples.  An important virtue of sparse descriptions based on an atomic set $\A$ is that employing the \emph{atomic norm} induced by $\A$ --- the gauge function of the atomic set $\A$ --- as a regularizer in inverse problems offers a natural convex optimization approach for obtaining solutions that have a sparse represention using $\A$ \cite{CRPW:12}.  Continuing with the examples of vectors with few nonzero coordinates and of low-rank matrices, regularization with the $\ell_1$ norm (the gauge function of the signed standard basis vectors) and with the matrix nuclear norm (the gauge function of the unit-norm rank-one matrices) are prominent techniques for promoting the corresponding sparse descriptions in solutions to inverse problems \cite{CanRec:09,CRT:06,CDS:98,Don:06,Faz:02,MeiBuh:06,RFP:10,Tib:94}.  The reason for the effectiveness of atomic norm regularization is the favorable facial structure of the convex hull of $\A$, which has the feature that all its low-dimensional faces contain points that have a sparse description using $\A$.  Indeed, in many contemporary data analysis applications the solutions of regularized optimization problems with generic input data tend to lie on low-dimensional faces of sublevel sets of the regularizer \cite{CT:06,Don:06,RFP:10}.  Based on this insight, atomic norm regularization has been shown to be effective in a range of tasks such as statistical denoising, model selection, and system identification \cite{BTR:13,OymHas:16,SBTR:12}.

%More generally, regularizing via the atomic norm is computationally tractable if the convex hull of $\A$ has an efficient description, and this method has been shown to be effective in a range of tasks such as statistical denoising, model selection, system identification, and the design of architecturally simple controllers [CITE].

%Such low-dimensional structure can be leveraged in a range of data analysis tasks such as compression, denoising, and classification [CITE].

%Of course, employing the appropriate atomic norm regularizer requires knowledge of a good atomic set with respect to which one seeks solutions with a sparse representation.  Such prior domain expertise may be available sometimes but difficult to obtain in general.  In the absence of such prior knowledge, we consider how to learn a suitable regularizer given observations ....  As motivated by the preceding discussion, our objective is to learn a norm such that ... Equivalently, we seek a collection of atoms -- the extreme points of the atomic norm ball -- such that each of the data points has a sparse representation using this atomic set.
%
%the extreme significant difficulty in employing this strategy is knowing a good atomic set, i.e., the extreme points of the atomic norm ball.  while this may be known, difficult in modern settings.  the goal of this paper is to learn suitable atomic norm regularizers in a data-driven manner.  In other words, given a collection of data-points, the objective is to learn a regularizer such that the the data-points all lie on or near low-dimensional faces of unit ball of the regularizer.  More precisely, .... From a convex-geometric perspective, identifying a good atomic set corresponds to specifying

The difficulty with employing an atomic norm regularizer in practice is that one requires prior domain knowledge of the atomic set $\A$ -- the extreme points of the atomic norm ball -- that underlies a sparse description of the desired solution in an inverse problem.  While such information may be available based on domain expertise in some problems (e.g., certain classes of signals having a sparse representation in a Fourier basis), identifying a suitable atomic set is challenging for many contemporary datasets that are high-dimensional and are typically presented to an analyst in an unstructured fashion.  In this paper, we study the question of learning a suitable regularizer directly from observations $\{\by^{(j)}\}_{j=1}^n \subset \R^d$ of a collection of structured signals or models of interest.  Specifically, as motivated by the preceding discussion, our objective is to identify a norm $\|\cdot\|$ in $\R^d$ such that each $\by^{(j)} / \|\by^{(j)}\|$ lies on a low-dimensional face of the unit ball of $\|\cdot\|$.  An equivalent formulation of this question in terms of extreme points is that we want to obtain an atomic set $\A$ such that each $\by^{(j)}$ has a sparse representation using $\A$; the corresponding regularizer is simply the atomic norm induced by $\A$.  A regularizer with these characteristics is adapted to the structure contained in the data $\{\by^{(j)}\}_{j=1}^n$, and it can be used subsequently as a regularizer in inverse problems to promote solutions with the same type of structure as in the collection $\{\by^{(j)}\}_{j=1}^n$.

%accrued in high dimensions due to conce and to to exploit the full power of concentration tasks sparse representations using atomic sets that  the Euclidean norm is a pretty stated iAddressing this In particular, given a dataset $\{\by^{(j)}\}_{j=1}^n$  If ... is a suitable norm, then ... is also a suitable norm.  Hence require some notion of normalization.  More importantly, a key requirement is that of computational tractability -- i.e., want to learn a norm that is efficient to compute and optimize over.

When considered in full generality, our question is somewhat ill-posed for several reasons.  First, if $\|\cdot\|$ is a norm that satisfies the properties described above with respect to the data $\{\by^{(j)}\}_{j=1}^n$, then so does $\alpha \|\cdot\|$ for any positive scalar $\alpha$.  This issue is addressed by learning a norm from a suitably scaled class of regularizers.  A second source of difficulty is that the Euclidean norm $\|\cdot\|_{\ell_2}$ trivially satisfies our requirements for a regularizer as each $\by^{(j)} / \|\by^{(j)}\|_{\ell_2}$ is an extreme point of the Euclidean norm ball in $\R^d$; indeed, this is the regularizer employed in ridge regression.  The atomic set in this case is the collection of all points with Euclidean norm equal to one, i.e., the dimension of this set is $d-1$.  However, datasets in many applications throughout science and engineering are well-approximated as sparse combinations of elements of atomic sets of much smaller dimension \cite{Bar:93,BDE:09,CRPW:12,DevTem:96,Jon:92,OlsFie:96,Pis:81}.  Identifying such lower-dimensional atomic sets is critical in inverse problems arising in high-dimensional data analysis in order to address the curse of dimensionality; in particular, as discussed in some of these preceding references, the benefits of atomic norm regularization in problems with large ambient dimension $d$ are a consequence of measure concentration phenomena that crucially rely on the small dimensionality of the associated atomic set in comparison to $d$.  We circumvent this second difficulty in learning a regularizer by considering atomic sets with appropriately bounded dimension.  A third challenge with our question as it is stated is that the gauge function of the set $\{\pm \by^{(j)} / \|\by^{(j)}\|_{\ell_2}\}_{j=1}^n$ also satisfies the requirements for a suitable atomic norm as each $\by^{(j)} / \|\by^{(j)}\|_{\ell_2}$ is an extreme point of the unit ball of this regularizer.  However, such a regularizer suffers from overfitting and does not generalize well as it is excessively tuned to the dataset $\{\by^{(j)}\}_{j=1}^n$.  Further, for large $n$ this gauge function becomes intractable to characterize and it does not offer a computationally efficient approach for regularization.  We overcome this complication by considering regularizers that have effectively parametrized sets of extreme points, and consequently are tractable to compute.

%The problem of learning a suitable polyhedral regularizer (i.e., an atomic norm with a unit ball that is a polytope) may be viewed as identifying a good finite set of atoms.  This question has been studied extensively under the title of `dictionary learning' or `sparse coding.'  Formally, ... describe dictionary learning and the resulting atomic norm.

%learning a polyhedral regularizer corresponds to the widely-studied problem of dictionary learning or sparse coding --> To see this connection, without loss of generality, suppose we parametrize a finite atomic set using a matrix so that the columns of L and their negations represent the set of atoms --> Learning a polyhedral regularizer from data y then corresponds to identifying a matrix L and a sparse vector x for each j such that ... --> This factorization question is precisely the question studied in the dictionary learning literature, although dictionary learning is typically not phrased in this manner --> e literature in dictionary learning is has received significant attention in the literature

The problem of learning a suitable polyhedral regularizer -- an atomic norm with a unit ball that is a polytope -- from data points $\{\by^{(j)}\}_{j=1}^n$ corresponds to identifying an appropriate \emph{finite} atomic set to concisely describe each $\by^{(j)}$.  This problem is equivalent to the question of `dictionary learning' (also called `sparse coding') on which there is a substantial amount of prior work \cite{AAJN:16,AAN:17,AEB:06,AGTM:15,AGM:14,BKS:15,OlsFie:96,SWW:12,SQW:16a,SQW:16b}.  To see this connection, suppose without loss of generality that we parametrize a finite atomic set via a matrix $L \in \R^{d \times p}$ so that the columns of $L$ and their negations specify the atoms.  The associated atomic norm ball is the image under $L$ of the $\ell_1$ ball in $\R^p$.  The columns of $L$ are typically scaled to have unit Euclidean norm to address the scaling issues mentioned previously (see Section \ref{sec:algorithm_cdl}).  The number of columns $p$ may be larger than $d$ (i.e., the `overcomplete' regime), and it controls the complexity of the atomic set as well as the computational tractability of describing the atomic norm.  With this parametrization, learning a polyhedral regularizer to promote the type of structure contained in $\{\by^{(j)}\}_{j=1}^n$ may be viewed as obtaining a matrix $L$ (given a target number of columns $p$) such that each $\by^{(j)}$ is well-approximated as $L \bx^{(j)}$ for a vector $\bx^{(j)} \in \R^p$ with few nonzero coordinates. Computing such a representation of the data is precisely the objective in dictionary learning, although this problem is typically not phrased as a quest for a polyhedral regularizer in the literature.  We remark further on some recent algorithmic developments in dictionary learning in Sections \ref{sec:relatedwork_dictionarylearning} and \ref{sec:algorithm_cdl}, and we contrast these with the methods proposed in the present paper.

%, although that literature usually does not phrase this factorization problem as a quest for a polyhedral regularizer

%corresponds to identifying an appropriate \emph{finite} atomic set to concisely describe the points $\{\by^{(j)}\}_{j=1}^n$.
%
%The problem of learning a suitable polyhedral regularizer -- an atomic norm with a unit ball that is a polytope -- corresponds to identifying an appropriate \emph{finite} atomic set to concisely describe the points $\{\by^{(j)}\}_{j=1}^n$.
%
%The problem of learning a suitable polyhedral regularizer -- an atomic norm with a unit ball that is a polytope -- corresponds to identifying an appropriate \emph{finite} atomic set to concisely describe the points $\{\by^{(j)}\}_{j=1}^n$.  A convenient parametrization for such finite sets is a matrix $L \in \R^{d \times q}$, with the columns of $L$ and their negations specifying the atoms.  usinvia the columns and their negations of a matrix $L \in \R^{d \times q}$.  Specifically,
%
%There is a substantial amount of prior work on this topic under the title of `dictionary learning' or `sparse coding' [CITE], although the dictionary learning literature .  Formally, the question is to find a matrix $L \in \R^{d \times q}$ such that each $\by^{(j)}$ is well-approximated as $L \bx^{(j)}$ for a vector $\bx^{(j)} \in \R^q$ with few nonzero coordinates.  The atomic set in this case is $\{L \be_i\}_{i=1}^q$, where $\be_i$ is the $i$'th standard basis vector, and the corresponding atomic norm ball is the image under $L$ of the $\ell_1$ ball in $\R^q$.  The columns of $L$ are scaled to have unit Euclidean norm to address the normalization issues mentioned previously.  The number of columns $q$ is usually larger than $d$ (i.e., the `overcomplete' regime), and it controls the complexity of the atomic set as well as the computational complexity of describing the atomic norm.

\subsection{From Polyhedral to Semidefinite Regularizers} \label{sec:intro_semidefleadup}
%Start by saying that we want something infinite.  Two difficulties.  Then say algebraic variety with a convex hull that is semidefinite representable.  Thus retain tractability.  Give formal description.

%polyhedral to non-polyhedral --> can solve convex optimization problems that are not linear + possibility of richer types of structure based on richer types of structure in set of extreme points --> two obstacles --> on the representation side, need an atomic set that is tractable to parametrize and for which we can identify sparsely described data --> on the computational side, need a convex hull that has an efficient description --> address both of these jointly by considering atomic set that is from a particular family of algebraic varieties that are nicely parametrized, and whose convex hulls can be described tractably via semidefinite programming --> Our focus on SDPs driven by the fact that they represent a significant generalization of linear programming, widely used in practice, can be solved using general-purpose software
%
%.. an effective parametrization of the set of extreme points as well as the low-dimensional faces.  Second, it is crucial that the algebraic varieties that we consider as candidate atomic sets have convex hulls that can be described in a computational tractable manner.

The objective of this paper is to investigate the problem of learning more general non-polyhedral atomic norm regularizers; in other words, the associated set of extreme points may be \emph{infinite}.  On the approximation-theoretic front, infinite atomic sets offer the possibility of concise descriptions of data sets with much richer types of structure than those with a sparse representation using finite atomic sets; in turn, the associated regularizers could promote a broader class of structured solutions to inverse problems than polyhedral regularizers.  On the computational front, many families of convex optimization problems beyond linear programs can be solved tractably and reliably \cite{NesNem:94}.  However, building on the challenges outlined previously, there are two important factors in identifying non-polyhedral regularizers from data.  First, it is crucial that any infinite atomic set $\A$ we consider has an effective parametrization so that it is tractable to characterize data that have a sparse representation using the elements of $\A$.  Second, we require that the convex hull of the atomic set $\A$ has an efficient description so that the associated atomic norm provides a computationally tractable regularizer.  As described next, we address these concerns by considering atomic sets that are efficiently parametrized as algebraic varieties (of a particular form) and that have convex hulls with tractable semidefinite descriptions.  Thus, previous efforts in the dictionary learning literature on identifying finite atomic sets may be viewed as learning zero-dimensional ideals, whereas our approach corresponds to learning atomic sets that are larger-dimensional varieties.  From a computational viewpoint, dictionary learning provides atomic norm regularizers that are computed via linear programming, while our framework leads to semidefinite programming regularizers.  Consequently, although our framework is based on a much richer family of atomic sets in comparison with the finite sets considered in dictionary learning, we still retain efficiency of parametrization and computational tractability based on semidefinite representability.

%Semidefinite programs are asignificant generalization of linear programs, and the class of regularizers that we consider obtained using our framework are much more general
%
%Semidefinite programs are useful for representing a significantly broader class of convex sets in comparison with linear programming, while  offer a significant generaOur focus on semidefinite-representable atomic norms is due to the fact that semidefinite programs are useful for representing a significantly  offer a bal....significant generalization over linear programs in terms of representing convex sets, while the types of convex sets that cnwith substantially more expressive power in terms of the types of convex sets that they can represent, while at the same time
%
%Our focus on semidefinite-representable atomic norms is driven by the fact that semidefinite programs offer a powerful framework for describing a broad class of non-polyhedral convex sets, while still being suitably structured so that
%
%

%We resolve this issue by focusing on varieties that have efficient semidefinite descriptions, which lead to atomic norm regularizers that can be computed tractably using semidefinite programming. In contrast, finite atomic sets from the dictionary learning literature lead to polyhedral atomic norms that are described via linear programming.  In this sense, our framework is based on a richer family of atomic sets in comparison with earlier work, while still retaining computational efficiency based on semidefinite representability.

%describe atomic set --> image of determinantal variety --> concisely described data are images of low-rank matrices --> parallel of dictionary learning case --> resolves representation issues --> on the computational side, the convex hull of the atoms given as image of nuclear norm ball --> as nuclear norm ball has semidefinite representation, we are done

Formally, we consider atomic sets in $\R^d$ that are images of rank-one matrices:
\begin{equation}
\A_{q}(\L) = \left\{\L (\bu \bv') ~|~ \bu, \bv \in \R^q, ~ \|\bu\|_{\ell_2} = 1, \|\bv\|_{\ell_2} = 1 \right\}, \label{eq:lowrankatoms}
\end{equation}
where $\L : \R^{q \times q} \rightarrow \R^d$ specifies a linear map. We focus on settings in which the dimension $q$ is such that $q^2 > d$, so the atomic sets $\A_{q}(\L)$ that we study in this paper are projections of rank-one matrices from a larger-dimensional space (in analogy to the overcomplete regime in dictionary learning).  By construction, elements of $\R^d$ that have a sparse representation using the atomic set $\A_{q}(\L)$ are those that can be specified as the image under $\L$ of \emph{low-rank matrices} in $\R^{q \times q}$.  As the convex hull of unit-Euclidean-norm rank-one matrices in $\R^{q \times q}$ is the nuclear norm ball in $\R^{q \times q}$, the corresponding atomic norm ball is given by:
\begin{equation}
\mathrm{conv}\left(\A_{q}(\L)\right) = \left\{ \L (X) ~|~ X \in \R^{q \times q}, ~ \|X\|_\star \leq 1 \right\}, \label{eq:nuclearimage}
\end{equation}
where $\|X\|_\star := \sum_{i} \sigma_i(X)$.  As the nuclear norm ball has a tractable semidefinite description \cite{Faz:02,RFP:10}, the atomic norm induced by $\A_{q}(\L)$ can be computed efficiently using semidefinite programming.

%This feature parallels the dictionary learning setup in which concisely described data are specified as images under a linear map of sparse vectors.

Given a collection of data points $\{\by^{(j)}\}_{j=1}^n \subset \R^d$ and a target dimension $q$, our goal is to find a linear map $\L : \R^{q \times q} \rightarrow \R^d$ such that each $\by^{(j)}$, upon normalization by the gauge function of $\A_{q}(\L)$, lies on a low-dimensional face of $\mathrm{conv}(\A_{q}(\L))$.  For each $\by^{(j)}$ to have this property, it must have a sparse representation using the atomic set $\A_{q}(\L)$; that is, there must exist a low-rank matrix $X^{(j)} \in \R^{q \times q}$ with $\by^{(j)} = \L (X^{(j)})$.  The matrix $X^{(j)}$ provides a concise description of $\by^{(j)} \in \R^d$ in the higher-dimensional space $\R^{q \times q}$.  Consequently, the problem of learning a semidefinite-representable regularizer with a unit ball that is a linear image of the nuclear norm ball may be phrased as one of \emph{matrix factorization}.  In particular, let $Y = [\by^{(1)} | \cdots | \by^{(n)}] \in \R^{d \times n}$ denote the data matrix, and let $\L_{i} \in \R^{q \times q}, ~ i=1,\dots,d$ be the matrix that specifies the linear functional corresponding to the $i$'th component of a linear map $\L : \R^{q \times q} \rightarrow \R^d$.  Then our objective can be viewed as one of finding a collection of matrices $\{\L_{i}\}_{i=1}^d \subset \R^{q \times q}$ specifying linear functionals and a set of low-rank matrices $\{X^{(j)}\}_{j=1}^n \subset \R^{q \times q}$ specifying concise descriptions such that:
\begin{equation} 
Y_{i,j} = \langle \L_{i}, X^{(j)} \rangle ~~~ i=1,\dots,d, ~ j=1,\dots,n. \label{eq:lowrankfactor}
\end{equation}
Note the distinction with dictionary learning in which one seeks a factorization of the data matrix $Y$ such that the $X^{(j)}$'s are sparse vectors as opposed to low-rank matrices as in our approach.  Figure \ref{fig:comparison} summarizes the key differences between dictionary learning and the present paper.


%In other words, our work represents a semidefinite programming generalization of dictionary learning.  In this sense,
%Our focus on SDPs driven by the fact that they represent a significant generalization of linear programming, widely used in practice, can be solved using general-purpose software

%In seeking a non-polyhedral extension of dictionary learning, our specific emphasis on semidefinite-representable regularizers as opposed to `arbitrary' convex regularizers may appear somewhat restrictive in scope.  The principal reason for this focus is that emphasis as we investigate atomic norms that can be computed via SDP is based on the observation that SDPs represent a significant generalization over LPs, regularizers in this paper is driven by their wide use in practice, the existence of general-purpose software for their solution, and

%In summary, our work is a semidefinite programming generalization of dictionary learning --> Table gives a comparison
%
%The focus of this paper is on the computational aspects of identifying semidefinite regularizers by learning suitable linear maps (given target dimension) to approximate the given data -->  Although not the focus of this paper, we remark briefly on the qualitative differences regarding the types of datasets that may be well-represented using the two types of representations, focusing on the geometric aspects of these datasets --> give details of comparison.
%
%
%Figure~\ref{fig:comparison}


%As a point of contrast, the dictionary learning literature has traditionally considered finite atomic sets; we adopt a somewhat non-standard parametrization to describe these sets to make the connection to $\A^{lr}_{q \times q}(\L)$ [REF] more transparent:
%\begin{equation*}
%\A^{sp}_q(L) = \left\{\pm L \be_i ~|~ \be_i \in \R^q ~\mathrm{is~the}~ i\mathrm{'th~standard~basis~vector}\right\}.
%\end{equation*}
%Here the map $L : \R^q \rightarrow \R^d$ is linear.  Writing $L$ as a $d \times q$ matrix, the atoms in $\A^{sp}_q(L)$ are the columns of $L$ and their negations.  A key setting of interest in dictionary learning is the so-called `overcomplete' regime in which $q > d$, i.e., there are more atoms (not counting negations) than the dimension $d$ of the data.  The unit ball of the atomic norm induced by the set $\A^{sp}_q(L)$ is the image of the $\ell_1$ ball in $\R^q$ under the linear map $L$; this atomic norm can be computed tractably via linear programming as the $\ell_1$ ball has an efficient linear programming representation.

\begin{figure*}
\centering
\small
\begin{tabular}{|c|c|c|}
\hline

{\multirow{2}{*}{}} & {\multirow{2}{*}{\bf Dictionary learning}} & {\multirow{2}{*}{\bf Our work}}
\\
& &
\\

\hline
\hline

{\multirow{4}{*}{Atomic set}} & $\{\pm L \be^{(i)} ~|~ \be^{(i)} \in \R^p ~\text{is~the}~ i\text{'th}$& {$\{ \L (\bu \bv') ~|~ \bu, \bv \in \R^q,$}
\\
& $\text{standard~basis~vector}\}$ & $\hspace{0.1in} \|\bu\|_{\ell_2} = \|\bv\|_{\ell_2} = 1 \}$
\\
& {\multirow{2}{*}{$L : \R^p \rightarrow \R^d$ (linear map)}} & {\multirow{2}{*}{$\L: \R^{q \times q} \rightarrow \R^d$ (linear map)}}
\\
& &
\\

\hline

Algebraic/geometric & {\multirow{2}{*}{Zero-dimensional ideal}} & {\multirow{2}{*}{Image of determinantal variety}}
\\
structure of atoms & &
\\

\hline

Concisely specified & {Image under $L$ of} & {Image under $\L$ of}
\\
data using atomic set & {sparse vectors} & {low-rank matrices}
\\

\hline

{\multirow{2}{*}{Atomic norm ball}} & {\multirow{2}{*}{$\left\{ L \bx ~|~ \bx \in \R^p, ~ \|\bx\|_{\ell_1} \leq 1 \right\}$}} & {\multirow{2}{*}{$\left\{ \L (X) ~|~ X \in \R^{q \times q}, ~ \|X\|_\star \leq 1 \right\}$}}
\\
 & &
\\

%{\multirow{2}{*}{Atomic norm ball}} & $\left\{ L x ~|~ \bx \in \R^q, ~ \|\bx\|_{\ell_1} \leq 1 \right\}$ & $\left\{ \L (X) ~|~ X \in \R^{q \times q}, ~ \|X\|_\star \leq 1 \right\}$
%\\
%& $(\subset \R^d)$ & $(\subset \R^d)$
%\\


\hline

Computing atomic & {\multirow{2}{*}{Linear programming}} & {\multirow{2}{*}{Semidefinite programming}}
\\
norm regularizer & &
\\

\hline

Learning regularizer & Identify $L$ and sparse $\bx^{(j)} \in \R^p$ & Identify $\L$ and low-rank $X^{(j)} \in \R^{q \times q}$
\\
from data $\{\by^{(j)}\}_{j=1}^n$ & such that $\by^{(j)} \approx L \bx^{(j)}$ for each $j$ & such that $\by^{(j)} \approx \L(X^{(j)})$ for each $j$
\\


\hline
\end{tabular}
\caption{A comparison between prior work on dictionary learning and the present paper.} \label{fig:comparison}
\end{figure*}

%To further illustrate these distinctions, we consider datasets that have sparse representations ...

%*** DO WE STILL WANT TO DESCRIBE GRAM MATRIX EXPERIMENT? ***



%We begin by describing finite atomic sets that have been considered previously in the dictionary learning literature; our parametrization suggests a transparent generalization to the atomic sets considered in this paper.  Formally, varities considered , and parametrizing.... Describe both types of sets.  describe matrix factorization approach.


\subsection{An Alternating Update Algorithm for Matrix Factorization} \label{sec:intro_am}

%The main objective of this paper is to identify a semidefinite-representable regularizer with unit ball of the form $\mathrm{conv}(\A_{q}(\L))$ to promote the structure contained in a given dataset. In other words

%e(of  the family considered here) from data can be phrased aim of finding a suitable linear map $\L$

A challenge with identifying a semidefinite regularizer by factoring a given data matrix as in \eqref{eq:lowrankfactor} is that such a factorization is not unique.  Specifically, consider any linear map $\mathcal{M} : \R^{q \times q} \rightarrow \R^{q \times q}$ that is a rank-preserver, i.e., $\mathrm{rank}(\mathcal{M}(X)) = \mathrm{rank}(X)$ for all $X \in \R^{q \times q}$; examples of rank-preservers include operators that act via conjugation by non-singular matrices and the transpose operation.  If each $\by^{(j)} = \L (X^{(j)})$ for a linear map $\L$ and low-rank matrices $\{X^{(j)}\}_{j=1}^n$, then we also have that each $\by^{(j)} = \L \circ \mathcal{M}^{-1} (\mathcal{M}(X^{(j)}))$, where by construction each $X^{(j)}$ has the same rank as the corresponding $\mathcal{M}(X^{(j)})$.  This non-uniqueness presents a difficulty as the image of the nuclear norm ball under a linear map $\L$ is, in general, different than it is under $\L \circ {\mathcal{M}}^{-1}$ for an arbitrary rank-preserver $\mathcal{M}$.  Consequently, due to its invariances the factorization \eqref{eq:lowrankfactor} does not uniquely specify a regularizer.  We investigate this point in Section \ref{sec:algorithm_normalization} by analyzing the structure of rank-preserving linear maps, and we describe an approach to associate a unique regularizer to a family of linear maps obtained from equivalent factorizations.  Our method entails putting linear maps in an appropriate `canonical' form using the Operator Sinkhorn iterative procedure, which was developed by Gurvits to solve certain quantum matching problems \cite{Gur:04}; this algorithm is an operator analog of the diagonal congruence scaling technique for nonnegative matrices developed by Sinkhorn \cite{Gur:04}.

%As the image of the nuclear norm ball in $R^{q \times q}$ under a linear map $\L$ is not the same as it is under $\L \circ \mathcal{M}^{-1}$, the factorization \eqref{eq:lowrankfactor} does not uniquely specify a regularizer.
%
%The lack of uniqueness in the factorization \eqref{eq:lowrankfactor} is due to linear maps $\mathcal{M} : \R^{q \times q} \rightarrow \R^{q \times q}$ that are rank-preservers, i.e., $\mathrm{rank}(\mathcal{M}(X)) = \mathrm{rank}(X)$ for all $X \in \R^{q \times q}$.  Examples of rank-preservers include operators that act via conjugation by non-singular matrices and the transpose operation.  Suppose each $\by^{(j)} = \L (X^{(j)}$ for a linear map $\L$ and low-rank matrices $\{X^{(j)}\}_{j=1}^n$.  Then for any rank-preserver $\mathcal{M}$ we have that each $\by^{(j)} = \L \circ \mathcal{M}^{-1} (\mathcal{M}(X^{(j)}))$, where by construction each $X^{(j)}$ has the same rank as the corresponding $\mathcal{M}(X^{(j)})$.  This non-uniqueness presents a difficulty as the image of the nuclear norm ball under a linear map $\L$ is, in general, different than it is under $\L \circ {\mathcal{M}}^{-1}$ for a rank-preserver $\mathcal{M}$ (note that ${\mathcal{M}}^{-1}$ is also a rank-preserver if $\mathcal{M}$ is a rank-preserver).  Thus, a regularizer cannot be obtained uniquely from a factorization due to the existence of equivalent factorizations that lead to non-equivalent regularizers.  To address this difficulty, we describe an approach to associate a \emph{unique} regularizer to a family of linear maps obtained from equivalent factorizations.



%Then state that we have an alternating minimization approach that involves solution of affine rank minimization problems and an application of the operator sinkhorn scaling algorithm.  We develop this algorithm in Section[REF].  Natural generalization of alternating minimization approaches developed in the sparse case.  In Section[REF], we show that our algorithm is locally convergent under suitable conditions.  Specifically, if the algorithm is presented with data satisfying ....

In Section \ref{sec:algorithm} we describe an alternating update algorithm to compute a factorization of the form \eqref{eq:lowrankfactor}.  With the $\L_{i}$'s fixed, updating the $X^{(j)}$'s entails the solution of affine rank minimization problems.  Although this problem is intractable in general \cite{Nat:93}, in recent years several tractable heuristics have been developed and proven to succeed under suitable conditions \cite{GM:11,JMD:10,RFP:10}.  With the $X^{(j)}$'s fixed, the $\L_{i}$'s are updated by solving a least-squares problem followed by an application of the Operator Sinkhorn iterative procedure to put the map $\L$ in a canonical form as described above.  Our alternating update approach is a generalization of methods that are widely employed in dictionary learning for identifying finite atomic sets (see Section \ref{sec:algorithm_cdl}).

Section \ref{sec:analysis} contains the main theorem of this paper on the local linear convergence of our alternating update algorithm.  Specifically, suppose a collection of data points $\{\by^{(j)}\}_{j=1}^n \subset \R^d$ is generated as $\by^{(j)} = \L^\star \left({X^{(j)}}^\star \right), ~ j=1,\dots,n$ for a linear map $\L^\star : \R^{q \times q} \rightarrow \R^d$ that is nearly isometric restricted to low-rank matrices (formally, $\L^\star$ satisfies a \emph{restricted isometry property} \cite{RFP:10}) and a collection $\left\{{X^{(j)}}^\star \right\}_{j=1}^n \subset \R^{q \times q}$ of low-rank matrices that is isotropic in a well-defined sense.  Given the data $\{\by^{(j)}\}_{j=1}^n$ as input, our alternating update approach is locally linearly convergent to a linear map $\hat{\L} : \R^{q \times q} \rightarrow \R^d$ with the property that the image of the nuclear norm ball in $\R^{q \times q}$ under $\hat{\L}$ is equal to its image under $\L^\star$, i.e., our procedure identifies the appropriate regularizer that promotes the type of structure contained in the data $\{\by^{(j)}\}_{j=1}^n$; see Theorem \ref{thm:localconvergence}.  Our analysis relies on geometric aspects of determinantal varieties (in particular tangent spaces with respect to these varieties) and their relation to stability properties of Operator Sinkhorn scaling.

We demonstrate the utility of our framework with a series of experimental results on synthetic as well as real data in Section \ref{sec:numexp}.

\subsection{Related Work} \label{sec:intro_relatedwork}

%\subsection{Connections to Lifts of Convex Sets}
%Topic of significant interest in the optimization literature is to describe convex sets in an efficient manner.  Informally a convex set is efficient to represent if it can be specified as the intersection of a small number of ``elementary'' convex set, each of which has a tractable representation.  However,

\subsubsection{Dictionary Learning} \label{sec:relatedwork_dictionarylearning}
As outlined above, our approach for learning a regularizer from data may be viewed as a semidefinite programming generalization of dictionary learning.  The alternating update algorithm we propose in Section \ref{sec:algorithm_am} for computing a factorization \eqref{eq:lowrankfactor} generalizes similar methods previously developed for dictionary learning \cite{AAJN:16,AEB:06,AGTM:15,OlsFie:96} (see Section \ref{sec:algorithm_cdl}), and the local convergence analysis of our algorithm in Section \ref{sec:analysis} also builds on previous analyses for dictionary learning \cite{AAJN:16,AGTM:15}.  In contrast to these previous results, the development and the analysis of our method in the present paper are more challenging due to the invariances and associated identifiability issues underlying the factorization \eqref{eq:lowrankfactor}, which necessitate the incorporation of the Operator Sinkhorn scaling procedure in our algorithm.

An unresolved matter in our paper -- one that has been investigated previously in the context of dictionary learning -- is the question of a suitable initialization for our algorithm.  In particular, our theory states that our algorithm exhibits linear convergence to the desired solution provided the initial guess is sufficiently close to a linear map that specifies the correct regularizer (in an appropriate metric).  We employ random initializations in our experiments with real data in Section \ref{sec:numexp_raw}, and these are useful in identifying effective semidefinite regularizers that outperform polyhedral regularizers obtained via dictionary learning.  Random initialization is the most common technique utilized in practice in dictionary learning as well as in many other structured matrix factorization problems arising in data analysis.  To build support for this idea, several researchers have proven that random initialization succeeds with high probability in recovering a desired factorization under suitable conditions in a number of problems \cite{GLM:16,SWQ:16c}, including in a restricted form of dictionary learning \cite{SQW:16a,SQW:16b} in which the polyhedral regularizer is specified as the image of the $\ell_1$ ball under an invertible linear map (as described previously, dictionary learning in full generality allows for polyhedral regularizers that may be specified as an image of the $\ell_1$ ball under a many-to-one linear map).  In a different direction, some recent papers also describe data-driven initialization strategies for dictionary learning based on variants of clustering \cite{AAN:17,AGM:14}.  It would be of interest to develop both these sets of ideas in our context, and we comment on this point in Section \ref{sec:discussion}.

%for two reasons.  First due to the larger class  the key
%
%With   indeed, as discussed in Section[REF] our approach reduces to previous ddescribed  in which the related work is to dictionary learning.

\subsubsection{Lifts of Convex Sets}
A second body of work with which our paper is conceptually related is the literature on lift-and-project representations (or extended formulations) of convex sets.  A tractable lift-and-project representation refers to a description of a `complicated' convex set in $\R^d$ as the projection of a more concisely specified convex set in $\R^{d'}$, with the lifted dimension $d'$ not being too much larger than the original dimension $d$.  As discussed in \cite{GPT:13,Yan:91}, obtaining a suitably structured factorization -- of a different nature than that considered in the present paper -- of the \emph{slack matrix} of a polytope (and more generally, of the slack operator of a convex set) corresponds to identifying an efficient lift-and-project description of the polytope.  On the other hand, we seek a structured factorization of a \emph{data matrix} to identify a convex set (i.e., the unit ball of a regularizer) with an efficient extended formulation and with the additional requirement that the data points (upon suitable scaling) lie on low-dimensional faces of the set.  This latter stipulation arises in our context from data analysis considerations, and it is a distinction between our setup and the optimization literature on extended formulations.

\subsubsection{Sinkhorn Scaling}
A third topic with which our paper has synergies -- and to which we make contributions in the course of our analysis -- is the literature on Sinkhorn scaling.  This algorithm is an iterative procedure for transforming an entrywise nonnegative matrix to a doubly stochastic matrix by diagonal congruence scaling \cite{Sin:64}.  There is a substantial body of work on the properties of this algorithm (see \cite{Ide:16} and the references therein) as well as on its applications in domains such as combinatorial optimization (approximating the permanent of a matrix \cite{LSW:00}) and data analysis (efficiently computing distances between probability distributions  \cite{Cut:13}).  The operator analog of Sinkhorn scaling was developed by Gurvits and this work was motivated by certain operator analogs of the bipartite matching problem that arise in matroid theory \cite{Gur:04}.  To the best of our knowledge, our work represents the first application of Operator Sinkhorn scaling in a problem in data analysis.  Further, in our investigation of the properties of Algorithm \ref{alg:osi}, we describe results on the stability of Operator Sinkhorn scaling; these may be of independent interest beyond the specific context of our paper (see Appendix \ref{apx:sinkhornstability}).


\subsection{Paper Outline}
In Section \ref{sec:algorithm} we discuss our alternating update algorithm for computing the factorization \eqref{eq:lowrankfactor} based on an analysis of the invariances arising in \eqref{eq:lowrankfactor}.  Section \ref{sec:analysis} gives the main theoretical result concerning the local linear convergence of the algorithm described in Section \ref{sec:algorithm}, and Section \ref{sec:numexp} describes numerical results obtained using our algorithm.  We conclude with a discussion of further research directions in Section \ref{sec:discussion}.

\paragraph{Notation} We denote the Euclidean norm of a vector by $\|\cdot\|_{\ell_2}$, of a matrix by $\|\cdot\|_F$, and of a more general linear operator by $\|\cdot\|_{\eu}$.  We denote the operator or spectral norm by $\|\cdot\|_2$.  The $k$'th largest singular value of a linear map is denoted by $\sigma_k(\cdot)$, and the largest and smallest eigenvalues of a self-adjoint linear map are denoted by $\lambda_{\max}(\cdot)$ and $\lambda_{\min}(\cdot)$ respectively.  The space of $q \times q$ symmetric matrices is denoted $\mathbb{S}^q$ and the set of $q \times q$ symmetric positive-definite matrices is denoted $\mathbb{S}^q_{++}$.  The projection map onto a subspace $\mathbb{V}$ is denoted $\cp_\mathbb{V}$.  The restriction of a linear map $M$ to a subspace $\mathbb{V}$ is denoted by $M_{\mathbb{V}}$.  Given a self-adjoint linear map $M : \mathbb{V} \rightarrow \mathbb{V}$ with $\mathbb{V}$ being a subspace of a vector space $\bar{\mathbb{V}}$, we denote the extension of $M$ to $\bar{\mathbb{V}}$ by $\cp_{\mathbb{V}} [M]_{\mathbb{V}} \cp_{\mathbb{V}} : \bar{\mathbb{V}} \rightarrow \bar{\mathbb{V}}$; the component in $\mathbb{V}$ of the image of any $\bx \in \bar{\mathbb{V}}$ under this map is $M \cp_{\mathbb{V}}(\bx)$, while the component in $\mathbb{V}^\perp$ is the origin.  Given a vector space $\mathbb{V}$, we denote the set of linear operators from $\mathbb{V}$ to $\mathbb{V}$ by $\mathbb{L}(\mathbb{V})$.  Given matrices $A, B \in \R^{q \times q}$, the linear map $A \boxtimes B \in \mathbb{L}(\R^{q \times q})$ is specified as $A \boxtimes B : X \rightarrow \langle B, X \rangle A$.  The Kronecker product between two linear maps is specified using the standard $\botimes$ notation.  For a collection of matrices $\{X^{(j)}\}_{j=1}^n \subset \R^{q \times q}$, the covariance is specified as $\mathsf{\Sigma}(\{X^{(j)}\}_{j=1}^n) = \frac{1}{n}\sum_{j=1}^n X^{(j)} \boxtimes X^{(j)}$.  Two quantities associated to this covariance that play a role in our analysis are $\Lambda(\{X^{(j)}\}_{j=1}^n) = \frac{1}{2}\left(\lambda_{\max}\left(\mathsf{\Sigma}(\{X^{(j)}\}_{j=1}^n)\right) + \lambda_{\min}\left(\mathsf{\Sigma}(\{X^{(j)}\}_{j=1}^n)\right) \right)$ and $\Delta(\{X^{(j)}\}_{j=1}^n) = \frac{1}{2}\left(\lambda_{\max}\left(\mathsf{\Sigma}(\{X^{(j)}\}_{j=1}^n)\right) - \lambda_{\min}\left(\mathsf{\Sigma}(\{X^{(j)}\}_{j=1}^n)\right) \right)$.  Given a matrix $X \in \R^{q \times q}$ of rank $r$, the tangent space at $X$ with respect to the algebraic variety of $q \times q$ matrices of rank at most $r$ is specified as\footnote{A rank-$r$ matrix $X \in \R^{q \times q}$ is a smooth point with respect to the variety of $q \times q$ matrices of rank at most $r$.}:
\begin{equation*}
\begin{aligned}
\ct(X) = \{ X A + B X ~|~ A,B \in \R^{q \times q} \}.
\end{aligned}
\end{equation*}




\section{An Alternating Update Algorithm for Learning Semidefinite Regularizers} \label{sec:algorithm}

%-- Invariances in Factorization: what are we looking for in a factorization; comparison to sparse case
%
%-- Normalization via the Operator Sinkhorn Iteration:
%
%-- An Alternating Minimization Algorithm:

In this section we describe an alternating update algorithm to factor a given data matrix $Y = [\by^{(1)} | \cdots | \by^{(n)}] \in \R^{d \times n}$ as in \eqref{eq:lowrankfactor}.  As discussed previously, the difficulty with obtaining a semidefinite regularizer using a factorization \eqref{eq:lowrankfactor} is the existence of infinitely many equivalent factorizations due to the invariances underlying \eqref{eq:lowrankfactor}.  We begin by investigating and addressing this issue in Sections \ref{sec:algorithm_identifiablity} and \ref{sec:algorithm_normalization}, and then we discuss our algorithm to obtain a regularizer in Section \ref{sec:algorithm_am}.  We contrast our method with techniques that have previously been developed in the context of dictionary learning in Section \ref{sec:algorithm_cdl}.

\subsection{Identifiability Issues} \label{sec:algorithm_identifiablity}

%to obtain a semidefinite regularizer is that such a regularizer is not uniquely specified due to the  lack of identifiability of a data matrix $Y \in \R^{d \times n}$ to define a semidefinite regularizer is that
%
%as in \eqref{eq:lowrankfactor} is that we face an identifiability issue as described previously. Specifically, for any linear map $\mathcal{M} : \R^{q \times q} \rightarrow \R^{q \times q}$ that is a rank-preserver, i.e.,  difficulty due to identifiability

%many factorizations due to rank-preservers -> one can check that equivalent factorizations lead to linear maps that give different regularizers -> need a way to associate a unique regularizer to family of equivalent linear maps -> to do this, we study structure of rank-preservers -> given by the following theorem -> although general rank-preservers are a problem, conjugation by transpose and orthogonal is not a problem -> suggests the following corollary that employs the polar decomposition -> based on the preceding discussion, the key source of lack of uniqueness in a regularizer based on a factorization is due to rank preservers that are specified as conjugation by pd matrices -> address this degree of freedom in factorizations by putting maps in a particular normalized form -> give definition of normalized maps -> value of this definition is the following theorem due to Gurvits -> give theorem -> thus, for a generic linear map, there is a unique conjugation by pd matrices that puts the map in normalized form -> this resolves the identifiability issue as follows -> given any element of the set of equivalent linear maps, obtain

Building on the discussion in the introduction, for a linear map $\L : \R^{q \times q} \rightarrow \R^d$ obtained from the factorization \eqref{eq:lowrankfactor} and for any linear rank-preserver $\mathcal{M} : \R^{q \times q} \rightarrow \R^{q \times q}$, there exists an equivalent factorization in which the linear map is $\L \circ \mathcal{M}$ (note that ${\mathcal{M}}^{-1}$ is also a rank-preserver if $\mathcal{M}$ is a rank-preserver).  As the image of the nuclear norm ball in $\R^{q \times q}$ is not invariant under an arbitrary rank-preserver, a regularizer cannot be obtained uniquely from a factorization due to the existence of equivalent factorizations that lead to non-equivalent regularizers.  To address this difficulty, we describe an approach to associate a \emph{unique} regularizer to a family of linear maps obtained from equivalent factorizations.  We begin by analyzing the structure of rank-preserving linear maps based on the following result \cite{MarMoy:59}:


%A challenge with identifying a semidefinite regularizer by factorizing a given data matrix as in \eqref{eq:lowrankfactor} is that such a factorization is not unique.  In particular, for any linear map $\mathcal{M} : \R^{q \times q} \rightarrow \R^{q \times q}$ that is a rank-preserver (e.g.,  a map that acts via conjugation by non-singular matrices), the factorization $Y_{i,j} = \langle \mathcal{M}^{-T} (\L^{(i)}), \mathcal{M}(X^{(j)}) \rangle$ for all $i, j$ has the same attributes as the original factorization \eqref{eq:lowrankfactor}.  As the image of the nuclear norm ball in $R^{q \times q}$ under a linear map $\L$ is not the same as it is under $\L \circ \mathcal{M}^{-1}$, the factorization \eqref{eq:lowrankfactor} does not uniquely specify a regularizer.
%
%The lack of uniqueness in the factorization \eqref{eq:lowrankfactor} is due to linear maps $\mathcal{M} : \R^{q \times q} \rightarrow \R^{q \times q}$ that are rank-preservers, i.e., $\mathrm{rank}(\mathcal{M}(X)) = \mathrm{rank}(X)$ for all $X \in \R^{q \times q}$.  Examples of rank-preservers include operators that act via conjugation by non-singular matrices and the transpose operation.  Suppose each $\by^{(j)} = \L (X^{(j)}$ for a linear map $\L$ and low-rank matrices $\{X^{(j)}\}_{j=1}^n$.  Then for any rank-preserver $\mathcal{M}$ we have that each $\by^{(j)} = \L \circ \mathcal{M}^{-1} (\mathcal{M}(X^{(j)}))$, where by construction each $X^{(j)}$ has the same rank as the corresponding $\mathcal{M}(X^{(j)})$.  This non-uniqueness presents a difficulty as the image of the nuclear norm ball under a linear map $\L$ is, in general, different than it is under $\L \circ {\mathcal{M}}^{-1}$ for a rank-preserver $\mathcal{M}$ (note that ${\mathcal{M}}^{-1}$ is also a rank-preserver if $\mathcal{M}$ is a rank-preserver).  Thus, a regularizer cannot be obtained uniquely from a factorization due to the existence of equivalent factorizations that lead to non-equivalent regularizers.  To address this difficulty, we describe an approach to associate a \emph{unique} regularizer to a family of linear maps obtained from equivalent factorizations.
%
%(note that ${\mathcal{M}}^{-1}$ is also a rank-preserver if $\mathcal{M}$ is a rank-preserver).



\begin{theorem}(\cite[Theorem 1]{MarMoy:59}, \cite[Theorem 9.6.2]{Tun:00}) \label{thm:external_rankpreserver}  An invertible linear operator $\mathcal{M}: \R^{q \times q} \rightarrow \R^{q \times q}$ is a rank-preserver if and only if $\mathcal{M}$ is of one of the following two forms for non-singular matrices $W_1,W_2 \in \R^{q \times q}$: $\mathcal{M}(X) = W_1 X W_2$ or $\mathcal{M}(X) = W_1 X' W_2$.
\end{theorem}

This theorem brings the preceding discussion into sharper focus, namely, that the lack of identifiability boils down to the fact that the nuclear norm is not invariant under conjugation of its argument by arbitrary non-singular matrices.  However, we note that the nuclear norm ball is invariant under the transpose operation and under conjugation by orthogonal matrices.  This observation leads naturally to the idea of employing the \emph{polar decomposition} to describe a rank-preserver:

\begin{corollary} \label{thm:external_rankpreserverpolardecomp}
An invertible linear operator $\mathcal{M}: \R^{q \times q} \rightarrow \R^{q \times q}$ is a rank-preserver if and only if $\mathcal{M}$ can be decomposed as $\mathcal{M} = \mathcal{M}^{\mathrm{or}} \circ \mathcal{M}^{\mathrm{pd}}$ for rank-preservers $\mathcal{M}^{\mathrm{pd}} : \R^{q \times q} \rightarrow \R^{q \times q}$ and $\mathcal{M}^{\mathrm{or}} : \R^{q \times q} \rightarrow \R^{q \times q}$ with the following properties:
\begin{itemize}
\item The operator $\mathcal{M}^{\mathrm{pd}}$ is specified as $\mathcal{M}^{\mathrm{pd}}(X) = P_1 X P_2$ for some positive-definite matrices $P_1, P_2 \in \S^q$.

\item The operator $\mathcal{M}^{\mathrm{or}}$ is of one of the following two forms for orthogonal matrices $U_1,U_2 \in \R^{q \times q}$: $\mathcal{M}^{\mathrm{or}}(X) = U_1 X U_2$ or $\mathcal{M}^{\mathrm{or}}(X) = U_1 X' U_2$.
\end{itemize}
\end{corollary}

\begin{proof}
The result follows by combining Theorem \ref{thm:external_rankpreserver} with the polar decomposition.
\end{proof}

We refer to rank-preservers of the type $\mathcal{M}^{\mathrm{pd}}$ in this corollary as \emph{positive-definite rank-preservers} and to those of the type $\mathcal{M}^{\mathrm{or}}$ as \emph{orthogonal rank-preservers}.  This corollary highlights the point that the key source of difficulty in identifying a regularizer uniquely from a factorization is due to positive-definite rank-preservers.  We address this complication by putting linear maps in a particular normalized form that removes this degree of ambiguity.

\begin{definition}
Let $\L : \R^{q \times q} \rightarrow \R^d$ be a linear map, and let $\L_{i} \in \R^{q \times q}, ~ i=1,\dots,d$ be the component linear functionals of $\L$.  Then $\L$ is said to be normalized if $\sum_{i=1}^d \L_{i} {\L_{i}}' = qI$ and $\sum_{i=1}^d {\L_{i}}' \L_{i} = qI$.
\end{definition}

%definition useful due to result by gurvits (more later) --> why is it useful --> define regularizer based on normalizer --> why is this good? -- now all that remains is to compute a normalization of a linear map

The utility of this definition in resolving our identifiability issue is based on a paper by Gurvits  \cite{Gur:04}.  Specifically, for a generic linear map $\L : \R^{q \times q} \rightarrow \R^d$, the results in \cite{Gur:04} imply that there exists a \emph{unique} positive-definite rank-preserver $\mathcal{N}_\L : \R^{q \times q} \rightarrow \R^q$ so that $\L \circ \mathcal{N}_\L$ is normalized.  Further, such a normalizing positive-definite rank-preserver can be computed using the Operator Sinkhorn iterative procedure developed in \cite{Gur:04}.  We discuss the algorithmic consequences of the results from \cite{Gur:04} in Section \ref{sec:algorithm_normalization}.  In the remainder of the present section, we describe how the existence of such unique normalizing positive-definite rank-preservers offers a conceptually natural approach for uniquely associating a regularizer to an equivalence class of factorizations.

\underline{\emph{Obtaining a regularizer from a linear map}}: Given a linear map $\L : \R^{q \times q} \rightarrow \R^d$ obtained from a factorization \eqref{eq:lowrankfactor}, the unit ball of the regularizer we associate to this factorization is the image of the nuclear norm ball in $\R^{q \times q}$ under the linear map $\L \circ \mathcal{N}_\L$; here $\mathcal{N}_\L$ is the unique positive-definite rank-preserver that normalizes $\L$ (as discussed in the sequel in Corollary \ref{thm:external_normalizablelinearmaps}, such unique normalizing rank-preservers exist for generic maps $\L$).

The soundness of this approach follows from the fact that linear maps from equivalent factorizations produce the same regularizer.  In particular, consider linear maps $\L$ and $\L \circ \mathcal{M}$, where $\mathcal{M}$ is any rank-preserver.  Suppose $\mathcal{N}_\L$ is the unique positive-definite rank-preserver such that $\L \circ \mathcal{N}_\L$ is normalized.  As $\mathcal{M}^{-1} \circ \mathcal{N}_{\mathcal{L}}$ is a rank-preserver, we can apply Corollary \ref{thm:external_rankpreserverpolardecomp} to conclude that $\mathcal{M}^{-1} \circ \mathcal{N}_{\mathcal{L}} = \bar{\mathcal{M}}^{\mathrm{or}} \circ \bar{\mathcal{M}}^{\mathrm{pd}}$, where $\bar{\mathcal{M}}^{\mathrm{or}}$ is an orthogonal rank-preserver and $\bar{\mathcal{M}}^{\mathrm{pd}}$ is a positive-definite rank-preserver.  We demonstrate that $\mathcal{N}_{\L \circ \mathcal{M}} = \mathcal{M}^{-1} \circ \mathcal{N}_{\L} \circ \bar{\mathcal{M}}^{\mathrm{or}'}$ is the unique positive-definite rank-preserver that normalizes $\L \circ \mathcal{M}$.  First one can check that $\mathcal{N}_{\L \circ \mathcal{M}} = \bar{\mathcal{M}}^{\mathrm{or}} \circ \bar{\mathcal{M}}^{\mathrm{pd}} \circ \bar{\mathcal{M}}^{\mathrm{or}'}$, which implies that $\mathcal{N}_{\L \circ \mathcal{M}}$ is a positive-definite rank-preserver.  Second we have that $\L \circ \mathcal{M} \circ \mathcal{N}_{\L \circ \mathcal{M}} = \L \circ \mathcal{N}_{\L} \circ \bar{\mathcal{M}}^{\mathrm{or}'}$ is also normalized.  Finally, the uniqueness of $\mathcal{N}_{\L \circ \mathcal{M}}$ follows from the uniqueness of $\mathcal{N}_\L$. Further, the image of the nuclear norm ball under $\L \circ \mathcal{N}_\L$ is the same as it is under $\L \circ \mathcal{M} \circ \mathcal{N}_{\L \circ \mathcal{M}}$ because the nuclear norm ball is invariant under the action of the action of the orthogonal rank-preserver $\bar{\mathcal{M}}^{\mathrm{or}'}$.  In summary, the approach described above for associating a regularizer to a (generic) linear map $\L$ associates the same regularizer to any other linear map obtained from an equivalent factorization.

%The uniqueness of $\mathcal{N}_{\L \circ \mathcal{M}}$ is easily seen.
%
%is normalized of the map $\L$ from a map $\L \circ \mathcal{M}$ the image of the nuclear norm ball under the normalized version of a linear map $\L$ is the same as it is under the normalized version of $\L \circ \mathcal{M}$ for a rank-preserver $\mathcal{M}$.  In particular, suppose there exists a unique normalizing rank-preserver $\mathcal{N}_\L$ so that $\L$ can be normalized by a unique rank-preserver $\mathcal{N}_\L$.  Then for any rank-preserver $\M$, there exists a corresponding unique normalizing rank-preserver $\mathcal{N}_{\L \circ \mathcal{M}}$ the map $\L \circ \mathcal{M}$ can also be normalized by a unique rank-preserver for a rank-indecomposable linear map $\L$ for any rank-preserver $\mathcal{M}$, the map $\L \circ \mathcal{M}$ also satisfies the rank-indecomposability condition.  Consequently, from Theorem[REF] there exists a unique normalizing rank-preserver $\mathcal{N}_{\L \circ \mathcal{M}}$ that puts $\L \circ \mathcal{M}$ in normalized form.  Specifically, if $\mathcal{M}$ is decomposed as $\mathcal{M} = \mathcal{M}^{\mathrm{or}} \circ \mathcal{M}^{\mathrm{pd}}$ as described in Corollary [REF], then we have that $\mathcal{N}_{\L \circ \mathcal{M}} = {\mathcal{M}}^{-1} \circ \mathcal{N}_{\L} \circ \mathcal{M}^{\mathrm{or}}$.  On the one hand, it is clear that the image of the nuclear norm ball under $\L \circ \mathcal{N}_\L$ is the same as it is under $\L \circ \mathcal{N}_{\L \circ \mathcal{M}}$; this follows from the fact that the nuclear norm ball is invariant under the action of $\mathcal{M}^{\mathrm{or}}$.  On the other hand, we have that $\mathcal{N}_{\L \circ \mathcal{M}} = {\mathcal{M}^\mathrm{pd}}^{-1} \circ {\mathcal{M}^\mathrm{or}}^{-1} \circ \mathcal{N}_\L \circ \mathcal{M}^\mathrm{or}$; one can check that each of the rank-preservers ${\mathcal{M}^\mathrm{pd}}^{-1}$ and ${\mathcal{M}^\mathrm{or}}^{-1} \circ \mathcal{N}_\L \circ \mathcal{M}^\mathrm{or}$ are specified as conjugations by positive-definite matrices, and therefore $\mathcal{N}_{\L \circ \mathcal{M}}$ is also specified as a conjugation by positive-definite matrices.  In summary, the approach described above for obtaining a regularizer based on a factorization \eqref{eq:lowrankfactor} associates a unique regularizer to an equivalence class of factorizations.

\subsection{Normalizing Maps via Operator Sinkhorn Scaling} \label{sec:algorithm_normalization}
From the discussion in the preceding section, a key step in associating a unique regularizer to a collection of equivalent factorizations is to normalize a given linear map $\L : \R^{q \times q} \rightarrow \R^d$.  In this section we describe how this may be accomplished by appealing to the work of Gurvits \cite{Gur:04}.

Given a linear operator $\T : \S^q \rightarrow \S^q$ that leaves the positive-semidefinite cone invariant, Gurvits consider the question of the existence (and computation) of positive-definite matrices $P_1,P_2 \in \S^q_{++}$ such that the rescaled operator $\tilde{\T} = (P_1 \botimes P_1) \circ \T \circ (P_2 \botimes P_2)$ has the property that $\tilde{\T}(I) = \tilde{\T}'(I) = I$, i.e., the identity matrix is an eigenmatrix of the rescaled operator $\tilde{\T}$ and its adjoint \cite{Gur:04}.  This problem is an operator analog of the classical problem of transforming entrywise square nonnegative matrices to doubly stochastic matrices by diagonal congruence scaling.  This \emph{matrix scaling} problem was originally studied by Sinkhorn \cite{Sin:64}, and he developed an iterative solution technique that is known as Sinkhorn scaling.   Gurvits developed an operator analog of classical Sinkhorn scaling that proceeds by alternately performing the updates $\T \leftarrow \left(\T(I)^{-\tfrac{1}{2}} \botimes \T(I)^{-\tfrac{1}{2}} \right) \circ \T$ and $\T \leftarrow \T \circ \left(\T'(I)^{-\tfrac{1}{2}} \botimes \T'(I)^{-\tfrac{1}{2}} \right)$; this sequence of operations is known as the \emph{Operator Sinkhorn iteration}.  The next theorem concerning the convergence of this iterative method is proved in \cite{Gur:04}.  Following the terminology in \cite{Gur:04}, a linear operator $\T : \S^q \rightarrow \S^q$ is \emph{rank-indecomposable} if it satisfies the inequality $\mathrm{rank}\left(\T(Z) \right) \geq \mathrm{rank}(Z)$ for all $Z \succeq 0$ with $1 \leq \mathrm{rank}(Z) < q$; this condition is an operator analog of a matrix being irreducible.

\begin{theorem} (\cite[Theorem 4.6 and 4.7]{Gur:04}) \label{thm:external_osiconverges}
Let $\T : \S^q \rightarrow \S^q$ be a rank-indecomposable linear operator.  There exist \emph{unique} positive-definite matrices $P_1, P_2 \in \S^q_{++}$ with $\det(P_1) = 1$ such that $\tilde{\T} = (P_1 \botimes P_1) \circ \T \circ (P_2 \botimes P_2)$ satisfies the conditions $\tilde{\T}(I) = \tilde{\T}'(I) = I$.  Moreover, the Operator Sinkhorn Iteration initialized with $\T$ converges to $\tilde{\T}$.
\end{theorem}

\noindent \textbf{Remark}. The condition $\det(P_1) = 1$ is imposed purely to avoid the ambiguity that arises from setting $P_1 \leftarrow \alpha P_1$ and $P_2 \leftarrow \tfrac{1}{\alpha} P_2$ for positive scalars $\alpha$.  Other than this degree of freedom, there are no other positive-definite matrices that satisfy the property that the rescaled operator $\tilde{\T}$ in this theorem as well as its adjoint both have the identity as a eigenmatrix.

\begin{algorithm}[t] 
    \caption{Normalizing a linear map via the Operator Sinkhorn iteration}
    \label{alg:osi}
  %\begin{algorithmic}[1]
    \textbf{Input}: A linear map $\L: \R^{q \times q} \rightarrow \R^d$ with component functionals $\L_{i}, ~i=1,\dots,d$ \\
    \textbf{Require}: A normalized map $\L \circ \mathcal{M}$ where $\mathcal{M}: \R^{q \times q} \rightarrow \R^d$ is a rank-preserver that acts via conjugation by positive-definite matrices \\
    %\OUTPUT Decomposed signal $\hat{x}_{\text{est}}$ after $k$ iteration, Residual $R^{(k)}$
%    \STATE \textbf{Initialization} $R^{(0)} = x$
    \textbf{Algorithm}: Repeat until convergence \\
    \textbf{1.} $R = \sum_{i=1}^d \L_{i} {\L_{i}}'$ \\
    \textbf{2.} $\L_{i} \leftarrow \sqrt{q} R^{-\tfrac{1}{2}} \L_{i}, ~ i=1,\dots,d$ \\
    \textbf{3.} $C = \sum_{i=1}^d {\L_{i}}' \L_{i}$ \\
    \textbf{4.} $\L_{i} \leftarrow \sqrt{q} \L_{i} C^{-\tfrac{1}{2}}, ~ i=1,\dots,d$

  %\end{algorithmic}
\end{algorithm}

These ideas and results are directly relevant in our context as follows.  For any linear map $\L : \R^{q \times q} \rightarrow \R^d$, we may associate an operator $\mathfrak{T}_\L : \S^q \rightarrow \S^q$ defined as $\T_\L(Z) = \frac{1}{q}\sum_{i=1}^d \L_{i} Z {\L_{i}}'$, which has the property that it leaves the positive-semidefinite cone invariant.  Rescaling the operator $\T_\L$ via positive-definite matrices $P_1,P_2 \in \S^q_{++}$ to obtain $\tilde{\T}_\L = (P_1 \botimes P_1) \circ \T \circ (P_2 \botimes P_2)$ corresponds to conjugating the component linear functionals $\{\L_{i}\}_{i=1}^d$ of $\L$ by $P_1$ and $P_2$.  Consequently, rescaling $\T_\L$ so that $\tilde{\T}_\L = (P_1 \botimes P_1) \circ \T_\L \circ (P_2 \botimes P_2)$ and its adjoint both have the identity as an eigenmatrix is equivalent to composing $\L$ by a positive-definite rank-preserver $\mathcal{N} = P_1 \botimes P_2$ so that $\L \circ \mathcal{N}$ is normalized.  Based on this correspondence Algorithm \ref{alg:osi} gives a specialization of the general Operator Sinkhorn Iteration to our setting for normalizing a linear map $\L$.  We also have the following corollary to Theorem \ref{thm:external_osiconverges}:

\begin{corollary} \label{thm:external_normalizablelinearmaps}
Let $\L : \R^{q \times q} \rightarrow \R^d$ be a linear map, and suppose $\mathrm{rank}\left(\sum_{i=1}^d \L_{i} Z {\L_{i}}' \right) \geq \mathrm{rank}(Z)$ for all $Z \succeq 0$ with $1 \leq \mathrm{rank}(Z) < q$ (i.e., the operator $\T_\L(Z) = \frac{1}{q} \sum_{i=1}^d \L_{i} Z {\L_{i}}'$ is rank-indecomposable).  There exists a \emph{unique} positive-definite rank-preserver $\mathcal{N}_\L : \R^{q \times q} \rightarrow \R^{q \times q}$ such that $\L \circ \mathcal{N}_\L$ is normalized.  Moreover, Algorithm \ref{alg:osi} initialized with $\L$ converges to $\L \circ \mathcal{N}_\L$.
\end{corollary}

\noindent \emph{Proof}: Follows from Theorem \ref{thm:external_osiconverges}. $\endproof$

Generic linear maps $\L : \R^{q \times q} \rightarrow \R^d$ (for $d \geq 2$) satisfy the condition $\mathrm{rank}\left(\sum_{i=1}^d \L_{i} Z {\L_{i}}' \right) \geq \mathrm{rank}(Z)$ for all $Z \succeq 0$ with $1 \leq \mathrm{rank}(Z) < q$.  Therefore, this assumption in Corollary \ref{thm:external_normalizablelinearmaps} is not particularly restrictive.  The polynomial-time complexity of the (general) Operator Sinkhorn iterative procedure -- in terms of the number of iterations required to obtain a desired accuracy to the fixed-point -- has recently been established in \cite{GGOW:15}.  In summary, this approach provides a computationally tractable method to normalize linear maps, and consequently to associate a unique regularizer to a collection of equivalent factorizations.

%
%Specializing the Operator Sinkhorn iteration to our setting, the process proceeds as follows:
%
%
%This known as the Operator Sinkhorn iteration, is a technique to transform cone into
%
%For a generic linear map $\L$, Gurvits proves that there exists a \emph{unique} rank-preserver $\mathcal{N}_\L$ that acts via conjugation by positive-definite matrices so that $\L \circ \mathcal{N}_\L$ is normalized.  Moreover, Gurvits also describes an iterative process known as the Operator Sinkhorn scaling procedure to compute the distinguished rank-preserver $\mathcal{N}_\L$.  Before stating this algorithm and result formally, we demonstrate how they resolve our difficulty with identifiability.  In particular, given a linear map $\L$ obtained from a factorization
%
%Gurvits' this resutl formsuch a distinguished rank-preserverputting a generic linear map $\L$ in normalized form by composing it
%
%In particular, for a linear map $\L : \R^{q \times q} \rightarrow \R^d$ with component linear functionals $\{\L^{(i)}\}_{i=1}^d$ and for a rank-preserver $\mathcal{M} : \R^{q \times q} \rightarrow \R^{q \times q}$, the component linear functionals of $\L \circ \mathcal{M}$ are given by $\{\M^\dag(\L^{(i)})\}_{i=1}^d$.  In other words, if $\mathcal{M}$ acts via conjugation by positive-definite matrices, then the components of $\L \circ \mathcal{M}$ are the components of $\L$ transformed by conjugation by positive-definite matrices.
%
%
%Following the terminology in [CITE], a linear map $\L : \R^{q \times q} \rightarrow \R^d$ is said to be \emph{rank-indecomposable} if it satisfies the condition $\mathrm{rank}\left(\sum_{i=1}^d \L^{(i)} Z {\L^{(i)}}^T \right) \geq \mathrm{rank}(Z)$ for all $Z \succeq 0$ with $1 \leq \mathrm{rank}(Z) < q$; this condition is an operator analog of a matrix being irreducible.
%
%\begin{theorem} [CITE]
%Let $\L : \R^{q \times q} \rightarrow \R^d$ be a rank-indecomposable linear map.  There exists a \emph{unique} rank-preserver $\mathcal{N}_\L : \R^{q \times q} \rightarrow \R^{q \times q}$ specified as the conjugation of positive-definite matrices such that $\L \circ \mathcal{N}_\L$ is normalized.
%\end{theorem}
%
%\noindent \textbf{Remark.} Gurvits' result [CITE] is stated more generally for operators that leave the positive semidefinite cone invariant (the operator $Z \mapsto \sum_{i=1}^d \L^{(i)} Z {\L^{(i)}}^T$ for $Z \in \S^q$ in the definition of rank-indecomposability preceding the theorem is one such example), but the above version suffices for our purposes.
%
%
%blah blah
%
%Generic linear maps (for $d \geq 2$) are rank-indecomposable, and therefore the requirement that a linear map obtained from a factorization be rank-indecomposable is not particularly restrictive.
%
%
%These observations lead to the following natural approach for associating a regularizer uniquely to an equivalence class of factorizations.  Specifically, given a linear map $\L : \R^{q \times q} \rightarrow \R^d$ derived from a factorization \eqref{eq:lowrankfactor} of a data matrix, the unit ball of the regularizer that we associate to this factorization is given by the image of the nuclear norm ball in $\R^{q \times q}$ under the map $\L \circ \mathcal{N}_\L$.  For any other equivalent factorization
%
%following associated class of maps obtained from equivalent factorizations:
%
%
%
%Gurvits' result is stated
%
%Formally, we consider the following set for a given linear map $\L : \R^{q \times q} \rightarrow \R^d$:
%\begin{equation*}
%\mathfrak{R}(\L) = \{\L \circ \tilde{\mathcal{M}} ~|~ \tilde{\mathcal{M}} : \R^{q \times q} \rightarrow \R^{q \times q} \mathrm{~is~a~linear~rank}\small{-}\mathrm{preserver}\},
%\end{equation*}
%which represents the entire family of linear maps obtained from equivalent factorizations.
%
%We describe next a procedure to To address the difficulty arising due to this , w
%
%
%$\mathfrak{L} \mathfrak{R} \mathfrak{X}$
%
%Specifically, for any non-singular linear map $\M: \R^{q \times q} \rightarrow \R^{q \times q}$ that is a rank-preserver -- i.e., $\mathrm{rank}(\mathcal{M}(X)) = \mathrm{rank}(X)$ for all $X \in \R^{q \times q}$ -- a factorization of the form $Y_{i,j} = \langle \M^{-T} (\L^{(i)}), \M (X^{(j)}) \rangle$ has the same attributes as the original factorization \eqref{eq:lowrankfactor}.  This non-uniqueness presents a difficulty as the image of the nuclear norm ball under a linear map $\L$ is, in general, different than it is under $\L \circ \mathcal{M}$.  Thus, a regularizer cannot be obtained uniquely from a factorization due to the existence of infinitely many equivalent factorizations.
%
%
%As discussed above, the image of the nuclear norm ball in $R^{q \times q}$ under a linear map $\L : \R^{q \times q} \rightarrow \R^d$ is not the same as it is under the map $\L \circ \mathcal{M}^{-1}$ for a rank-preserver $\mathcal{M} : \R^{q \times q} \rightarrow \R^{q \times q}$.  However, from Corollary[REF] and the preceding discussion, we note that the lack

\subsection{An Alternating Update Algorithm for Matrix Factorization} \label{sec:algorithm_am}

Given the resolution of the identifiability issues in the preceding two sections, we are now in a position to describe an algorithmic approach for computing a factorization \eqref{eq:lowrankfactor} of a data matrix $Y = [\by^{(1)} | \cdots | \by^{(n)}] \in \R^{d \times n}$ to obtain a semidefinite regularizer that promotes the type of structure contained in $Y$.  Specifically, given a target dimension $q$, our objective is to obtain a normalized linear map $\L : \R^{q \times q} \rightarrow \R^d$ and a collection $\{X^{(j)}\}_{j=1}^n$ of low-rank matrices such that $\sum_{i=1}^n \|\by^{(j)} - \L(X^{(j)})\|_{\ell_2}^2$ is minimized.  Our procedure is an alternating update technique that sequentially updates the low-rank $X^{(j)}$'s followed by an update of $\L$.  We assume that our algorithm is provided with a data matrix $Y \in \R^{d \times n}$, a target dimension $q$, and an initial guess for the normalized map $\L$.  Our method is summarized in Algorithm \ref{alg:amsdpreg}.

\subsubsection{Updating the low-rank matrices $\{X^{(j)}\}_{j=1}^n$} \label{sec:algorithm_am_lrmrecovery}

%such problems are ill-posed in general --> with low-rank restriction, become well-posed and known as the affine rank minimization problem --> in full generality, problem is NP-hard --> problem of significant practical interest, and has received much attention over the past decade --> several heuristics that are computationally tractable used in practice and are proven to succeed under suitable conditions on the underlying problem instance --> we describe two of the most popular approaches here --> give nuclear norm relaxation --> give singular value projection (and describe this in table) --> in our theoretical and experimental results, we employ singular value projection, but nuclear norm approach also possible

In this stage a normalized linear map $\L : \R^{q \times q} \rightarrow \R^d$ is fixed, and the objective is to find low-rank matrices $\{X^{(j)}\}_{j=1}^n$ such that $\by^{(j)} \approx \L(X^{(j)})$ for each $j=1,\dots,n$, i.e., for each $j = 1,\dots,n$, we are required to find a low-rank matrix near an affine space. Without the requirement that the $X^{(j)}$'s be low-rank, such linear inverse problems are ill-posed in our context as $q^2$ is typically taken to be larger than $d$.  With the low-rank restriction, this problem is well-posed and it is known as the \emph{affine rank minimization problem}.  This problem is NP-hard in general \cite{Nat:93}.  However, due to its prevalence in a range of application domains \cite{Faz:02,RFP:10}, significant efforts have been devoted towards the development of tractable heuristics that are useful in practice and that succeed on certain families of problem instances.  We describe next two popular heuristics for this problem.

The first approach -- originally proposed by Fazel in her thesis \cite{Faz:02} and subsequently analyzed in \cite{CanRec:09,RFP:10} -- is based on a convex relaxation in which the rank constraint is replaced by the nuclear norm penalty, which leads to the following convex program:
\begin{equation} \label{eq:nuclearnormlasso}
\begin{aligned}
\hat{X} = \underset{X \in \R^{q \times q}}{\argmin} ~~~ \tfrac{1}{2} \|\by - \L(X) \|_{\ell_2}^2 + \lambda \|X\|_\star.
\end{aligned}
\end{equation}
Here $\by \in \R^d$ and $\L : \R^{q \times q} \rightarrow \R^d$ are the problem data specifying the affine space near which we seek a low-rank solution, and the parameter $\lambda > 0$ provides a tradeoff between fidelity to the data (i.e., fit to the specified affine space) and rank of the solution $\hat{X}$.  This problem is a semidefinite program and it can solved to a desired precision in polynomial-time using standard software \cite{NesNem:94,sdpt3:1}.

\begin{algorithm}[t] 
    \caption{Obtaining a low-rank matrix near an affine space via Singular Value Projection}
    \label{alg:svp}
  %\begin{algorithmic}[1]
    \textbf{Input}: A linear map $\L: \R^{q \times q} \rightarrow \R^d$, a point $\by \in \R^d$, a target rank $r$, an initial guess $X \in \R^{q \times q}$, and a damping parameter $\nu \in (0,1]$ \\
    \textbf{Require}: A matrix $\hat{X}$ of rank at most $r$ such that $\|\by - \L(\hat{X}) \|_{\ell_2}$ is minimized, i.e., solve \eqref{eq:varietyconstrainedopt} \\
    %\OUTPUT Decomposed signal $\hat{x}_{\text{est}}$ after $k$ iteration, Residual $R^{(k)}$
    \textbf{Initialization} $X = 0$ \\
    \textbf{Algorithm}: Repeat until convergence \\
    \textbf{1.} $X \leftarrow X + \nu \L' (\by - \L(X))$ (i.e., take a gradient step with respect to the objective of \eqref{eq:varietyconstrainedopt}) \\
    \textbf{2.} Compute top-$r$ singular vectors and singular values of $X$: $U_r, V_r \in \R^{q \times r}, ~ \Sigma_r \in \R^{r \times r}$ \\
    \textbf{3.} $X \leftarrow U_r \Sigma_r V_r'$

  %\end{algorithmic}
\end{algorithm}


Another popular method for the affine rank minimization problem is based on directly attempting to solve the following non-convex optimization problem via alternating projection for a specified rank $r < q$:
\begin{equation} \label{eq:varietyconstrainedopt}
\begin{aligned}
\hat{X} = \underset{X \in \R^{q \times q}}{\argmin} & ~~~ \|\by - \L(X) \|_{\ell_2}^2 \\ \text{s.t.} & ~~~ \mathrm{rank}(X) \leq r.
\end{aligned}
\end{equation}
This problem is intractable to solve globally in general, but the heuristic described in Algorithm \ref{alg:svp} provides an approach that provably succeeds under certain conditions \cite{GM:11,JMD:10}.  The utility of this method in comparison to the convex program \eqref{eq:nuclearnormlasso} is that applying the procedure described in Algorithm \ref{alg:svp} is much more tractable in large-scale settings in comparison to solving \eqref{eq:nuclearnormlasso}.

The analyses in \cite{FCRP:08,GM:11,JMD:10,RFP:10} rely on the map $\L$ satisfying the following type of restricted isometry condition introduced in \cite{RFP:10}:
\begin{definition} Consider a linear map $\L : \R^{q \times q} \rightarrow \R^d$.  For each $k = 1,\dots,q$ the \emph{restricted isometry constant of order $k$} is defined as the smallest $\delta_k(\L)$ such that:
\begin{equation*}
1-\delta_k(\L) \leq \frac{\|\L(X)\|_{\ell_2}^2}{\|X\|_F^2} \leq 1 + \delta_k(\L)
\end{equation*}
for all matrices $X \in \R^{q \times q}$ with rank less than or equal to $k$.
\end{definition}
If a linear map $\L$ has a small restricted isometry constant for some order $k$, then the affine rank minimization problem is, in some sense, well-posed when restricted to matrices of rank less than or equal to $k$.  The results in \cite{FCRP:08,GM:11,JMD:10,RFP:10} go much further by demonstrating that if $\by = \L(X^\star) + \epsilon$ for $\epsilon \in \R^d$ and with $\mathrm{rank}(X^\star) \leq r$, and if the map $\L$ satisfies a bound on the restricted isometry constant $\delta_{4r}(\L)$, then both the convex program \eqref{eq:nuclearnormlasso} as well as the procedure in Algorithm \ref{alg:svp} applied to solve \eqref{eq:varietyconstrainedopt} provide solutions $\hat{X}$ such that $\|\hat{X} - X^\star\|_F \lesssim C \|\epsilon\|_{\ell_2}$.  Due to the qualitative similarity in the performance guarantees for these approaches, either of them is appropriate as a subroutine for updating the $X^{(j)}$'s in our alternating update method for computing a factorization of a given data matrix $Y \in \R^{d \times n}$.  Algorithm \ref{alg:amsdpreg} is therefore stated in a general manner to retain this flexibility.  In our main theoretical result in Section \ref{sec:analysis_mainresult}, we assume that the $X^{(j)}$'s are updated by solving \eqref{eq:varietyconstrainedopt} using the heuristic outlined in Algorithm \ref{alg:svp}; our analysis could equivalently be carried out by assuming that the $X^{(j)}$'s are updated by solving \eqref{eq:nuclearnormlasso}.
%
%then $\|\hat{X} - X^\star\|_F \leq C \epsilon$ for suitable choices of $\lambda$ in [REF] and $r$ in [REF].  This condition plays a significant role in the sequel in our analysis in Section[REF].


\subsubsection{Updating the linear map $\L$}
In this stage the low-rank matrices $\{X^{(j)}\}_{j=1}^n$ are fixed and the goal is to obtain a normalized linear map $\L$ such that $\sum_{i=1}^n \|\by^{(j)} - \L(X^{(j)})\|_{\ell_2}^2$ is minimized.  Our procedure for this update consists of two steps.  First we solve the following least-squares problem:
\begin{equation} \label{eq:dictionaryleastsquaresupdate}
\tilde{\L} = \underset{\substack{\bar{\L} : \R^{q \times q} \rightarrow \R^d \\ \bar{\L}~\text{is a linear map}}}{\argmin} ~~~ \sum_{i=1}^n \|\by^{(j)} - \bar{\L}(X^{(j)})\|_{\ell_2}^2
\end{equation}
This problem can be solved, for example, via a pseudoinverse computation.  Next, we apply the procedure described in Algorithm \ref{alg:osi} to the updated $\tilde{\L}$ obtained from \eqref{eq:dictionaryleastsquaresupdate} in order to normalize it.

\begin{algorithm}[t] 
    \caption{Computing a factorization via alternating updates}
    \label{alg:amsdpreg}
  %\begin{algorithmic}[1]
    \textbf{Input}: A data matrix $Y = [\by^{(1)} | \cdots | \by^{(n)}] \in \R^{d \times n}$, a target dimension $q$, an initial guess for a normalized linear map $\L: \R^{q \times q} \rightarrow \R^d$, a target rank $r < q$ \\
    \textbf{Require}: A normalized linear map $\hat{\L} : \R^{q \times q} \rightarrow \R^d$ and a collection of matrices $\{\hat{X}^{(j)}\}_{j=1}^n$ with rank at most $r$ such that $\sum_{i=1}^n \|\by^{(j)} - \hat{\L}(\hat{X}^{(j)})\|_{\ell_2}^2$ is minimized \\
    %\OUTPUT Decomposed signal $\hat{x}_{\text{est}}$ after $k$ iteration, Residual $R^{(k)}$
    \textbf{Algorithm}: Repeat until convergence \\
    \textbf{1.}[Update $X^{(j)}$'s; $\L$ fixed] Obtain matrices $\{X^{(j)}\}_{j=1}^n$ of rank at most $r$ such that $\sum_{i=1}^n \|\by^{(j)} - \L(X^{(j)})\|_{\ell_2}^2$ is minimized.  This can be accomplished either via Algorithm \ref{alg:svp} or by solving \eqref{eq:nuclearnormlasso} for a suitable choice of $\lambda$. \\
    \textbf{2.}[Update $\L$; $X^{(j)}$'s fixed] $\tilde{\L} \leftarrow \underset{\substack{\bar{\L} : \R^{q \times q} \rightarrow \R^d \\ \bar{\L}~\text{is a linear map}}}{\argmin} ~~~ \sum_{i=1}^n \|\by^{(j)} - \bar{\L}(X^{(j)})\|_{\ell_2}^2$ \\
    \textbf{3.}[Normalize $\L$] Normalize updated linear map from previous step using Algorithm \ref{alg:osi}.

  %\end{algorithmic}
\end{algorithm}

\subsection{Comparison with Dictionary Learning} \label{sec:algorithm_cdl}
As described in Section \ref{sec:intro_semidefleadup}, the dictionary learning literature considers the following factorization problem: given a collection of data points $\{\by^{(j)}\}_{j=1}^n \subset \R^d$ and a target dimension $p$, find a linear map $L : \R^p \rightarrow \R^d$ and a collection of sparse vectors $\{\bx^{(j)}\}_{j=1}^n \subset \R^p$ such that $\by^{(j)} = L \bx^{(j)}$ for each $j$.  As with \eqref{eq:lowrankfactor}, the linear map $L$ does not lead to a unique polyhedral regularizer.  Specifically, for any linear sparsity-preserver $M : \R^p \rightarrow \R^p$, there is an equivalent factorization in which the linear map is $L M$.  In parallel to Corollary \ref{thm:external_rankpreserverpolardecomp}, one can check that $M$ is a sparsity-preserver if and only if $M$ is a composition of a positive-definite diagonal matrix and a signed permutation matrix.  Since the $\ell_1$ ball is invariant under the action of a signed permutation, the main source of difficulty in obtaining a unique regularizer from a factorization is due to sparsity-preservers that are positive-definite diagonal matrices.  A common convention in dictionary learning that addresses this uniqueness issue is to require that each of the columns of $L$ has unit Euclidean norm; for a generic linear map $L$, there is a unique positive-definite diagonal matrix $D$ such that $LD$ consists of unit-norm columns.  Adopting a similar reasoning as in Section \ref{sec:algorithm_normalization}, one can check that this normalization resolves the issue of associating a unique regularizer to an equivalence of factorizations.

The most popular approach for computing a factorization in dictionary learning is based on alternately updating the map $L$ and the sparse vectors $\{\bx^{(j)}\}_{j=1}^n$.  For a fixed linear map $L$, updating the $\bx^{(j)}$'s entails the solution of a sparse linear inverse problem for each $j$.  That is, for each $j$ we seek a sparse vector $\bx^{(j)}$ in the affine space $\by^{(j)} = L \bx$.  Although this problem in NP-hard in general, there is a significant literature on tractable heuristics that succeed under suitable conditions \cite{CRT:06,CT:06,CDS:98,Don:06,Don:06b,DonHuo:01}; indeed, this work predates and served as a foundation for the literature on the affine rank minimization problem.  Prominent examples include the lasso \cite{Tib:94}, which is a convex relaxation approach akin to \eqref{eq:nuclearnormlasso}, and iterative hard thresholding \cite{BD:09}, which is analogous to Algorithm \ref{alg:svp}.  For a fixed collection $\{\bx^{(j)}\}_{j=1}^n$, the linear map $L$ is then updated by solving a least-squares problem followed by a rescaling of the columns so that they have unit Euclidean norm.

We note that each step in this procedure has a direct parallel to a corresponding step of Algorithm \ref{alg:amsdpreg}.  In summary, our proposed approach for obtaining a semidefinite regularizer via matrix factorization is a generalization of previous methods in the dictionary learning literature for obtaining a polyhedral regularizer.




\section{Convergence Analysis of Our Algorithm} \label{sec:analysis}

%-- Factors that govern convergence
%
%-- Main Result: give also proof ingredients
%
%-- Ensembles that Satisfy Conditions

This section describes the main theoretical result on the local convergence of our algorithm.  We begin by discussing the setup and an outline of our analysis in Sections \ref{sec:analysis_setup} and \ref{sec:analysis_proofhighlevel} respectively.  The statement of our main theorem with deterministic conditions is given in Section \ref{sec:analysis_mainresult}, and we describe natural random ensembles that satisfy these deterministic conditions with high probability in Section \ref{sec:analysis_randensemble}.  The proof of our theorem is discussed in Section \ref{sec:analysis_maintheoremproof}.

\subsection{Theoretical Setup} \label{sec:analysis_setup}

%we are given observations ... --> goal is to recover ... --> need a way to quantify convergence to correct regularizer --> describe how two linear maps can be related --> motivates following definition --> give definition of equivalence class metric --> we will give high-level proof strategy, and along the way, motivate the definitions of the key parameters required in the proof of our result --> start with error decomposition at some 't' and need to show that the error sequence goes down --> i.e., need to show that error at iteration 't+1' goes down relative to error at iteration 't' --> to show this, consider two pieces of our algorithm, first updating X's and second updating L -->

The setup underlying our main theorem is as follows.  We assume that we are given a collection of data points $\{{\by^{(j)}}^\star\}_{j=1}^n \subset \R^d$ with each ${\by^{(j)}}^\star = \L^\star\left({X^{(j)}}^\star\right)$, where $\L^\star : \R^{q \times q} \rightarrow \R^d$ is a linear map and $\{{X^{(j)}}^\star\}_{j=1}^n \subset \R^{q \times q}$ is a collection of low-rank matrices.  Without loss of generality, we may take $\L^\star$ to be normalized and surjective.  Our objective is to obtain a linear map $\hat{\L} : \R^{q \times q} \rightarrow \R^d$ with the property that the image of the nuclear norm ball in $\R^{q \times q}$ under $\L^\star$ is the same as it is under $\hat{\L}$.  To this end, we seek a linear map $\hat{\L}$ that can be expressed as the composition of $\L^\star$ with an orthogonal rank-preserver (recall that the nuclear norm ball is invariant under the action of an orthogonal rank-preserver).

As this goal is distinct from the more restrictive requirement that $\hat{\L}$ must equal $\L^\star$, we need an appropriate measure of the ``distance'' of a linear map to $\L^\star$.  A convenient approach to addressing this issue is to express a linear map $\L : \R^{q \times q} \rightarrow \R^d$ in terms of $\L^\star$ as follows, given any linear rank-preserver $\mathcal{M} : \R^{q \times q} \rightarrow \R^{q \times q}$:
\begin{equation} \label{eq:dicterr_rp_factorization}
\L = \L^\star \circ (\sfi + \sfe) \circ \mathcal{M},
\end{equation}
Here $\sfi \in \mathbb{L}(\R^{q \times q})$ is the identity map and the error term $\sfe = {\L^{\star}}^+ \circ (\L \circ \mathcal{M}^{-1} - \L^\star) \in \mathbb{L}(\R^{q \times q})$; the assumption that $\L^\star$ is surjective is key as ${\L^{\star}}^+$ is the right-inverse of $\L^\star$.  By varying the rank-preserver $\mathcal{M}$ in \eqref{eq:dicterr_rp_factorization} the error term $\sfe$ changes. If there exists an \emph{orthogonal} rank-preserver $\mathcal{M}$ such that the corresponding error $\sfe$ is small, then in some sense the image of the nuclear norm ball under $\L$ is close to the image under $\L^\star$.  This observation suggests that the closeness between $\L$ and $\L^\star$ may be measured as the smallest error $\sfe$ that one can obtain by varying $\mathcal{M}$ over the set of orthogonal rank-preservers.  The following result suggests that one can in fact vary $\mathcal{M}$ over \emph{all} rank-preservers, provided we have the additional condition that $\L$ is also normalized.  The additional flexibility provided by varying $\mathcal{M}$ over all rank-preservers is well-suited to characterizing the effects of normalization via Operator Sinkhorn scaling in our analysis, as described in the next section.

%The following result gives a bound on the distance between $\mathcal{M}$ and an orthogonal rank-preserver in terms of the size of the error $\sfe$:

\begin{proposition} \label{thm:normalizednearothogonal}
Suppose $\L, \L^\star : \R^{q \times q} \rightarrow \R^d$ are normalized linear maps such that $(i)$ $\L^\star$ satisfies the restricted isometry condition $\delta_1(\L^\star) \leq 1/10$, and $(ii)$ $\L = \L^\star \circ (\sfi + \sfe) \circ \mathcal{M}$ for a linear rank-preserver $\mathcal{M}$ with $\|\sfe\|_{\eu} \leq 1 / (150 \sqrt{q} \|\L^\star\|_2)$.  Then there exists an orthogonal rank-preserver $\mathcal{M}^{\mathrm{or}}$ such that $\|\mathcal{M}^{\mathrm{or}} - \mathcal{M}\|_2 \leq 300 \sqrt{q} \|\L^\star\|_2 \|\sfe\|_{\eu}$.
\end{proposition}

In words, if both $\L$ and $\L^\star$ are normalized and if there exists a rank-preserver $\mathcal{M}$ such that $\|\sfe\|_{\eu}$ is small in \eqref{eq:dicterr_rp_factorization}, then $\mathcal{M}$ is close to an orthogonal rank-preserver\footnote{The restricted isometry condition in Proposition \ref{thm:normalizednearothogonal} is a mild one; we require a stronger restricted isometry condition on $\L^\star$ in Theorem \ref{thm:localconvergence}.}; in turn, this implies that the image of the nuclear norm ball under $\L^\star$ is close to the image of the nuclear norm ball under $\L$.  These observations motivate the following definition as a measure of the distance between normalized linear maps $\L^\star, \L : \R^{q \times q} \rightarrow \R^d$ for surjective $\L^\star$:
\begin{equation} \label{eq:defn_distancebetweendicts}
\xi_{\L^\star}(\L) := \inf\{\|\sfe\|_{\eu} ~|~ \exists \sfe  \in \mathbb{L}(\R^{q \times q}) ~\text{and a rank-preserver}~ \mathcal{M} \in \R^{q \times q} ~\text{s.t.}~ \L = \L^\star \circ (\sfi + \sfe) \circ \mathcal{M} \}.
\end{equation}
In Section \ref{sec:analysis_mainresult}, our main result gives conditions under which the sequence of normalized linear maps obtained from Algorithm \ref{alg:amsdpreg} converges to $\L^\star$ in terms of the distance measure $\xi$.

%\noindent \textbf{Remarks.} (1) All linear maps $\ca$ that are in a neighborhood of the equivalence class of $\ca^*$ can be written in the above form; specifically, one would define $\sfe:= \ca^{*\dagger} \circ (\ca \circ W^{-1}_{R} \botimes W^{-1}_L - \ca^*)$ for some choice of invertible matrices $W_R,W_L$ that minimizes $\| \sfe \|_{\eu}$. %(2) The convergence rate is stated as the maximum of two quantities, of which the quantity $(q \sqrt{r} \sigmacanorm/c^{3/2})\| \frac{1}{\sigma^2 n} \sum_{i=1}^{n} X_i^* \boxtimes X_i^* - c \sfi \|$ can be made arbitrarily small with suitably large ensembles of low-rank matrices, while the quantity $\omega \sigma^2_{\max}(\ca^*)/c$ is purely in terms of the dimensions $q,r,d$.

%In an earlier remark we noted that any generic linear $\ca$ map can be expressed in the form $\ca = \ca^* \circ (\sfi + \sfe) \circ W_R \botimes W_L$; in particular, the linear map $\sfe$ measures the deviation of $\ca$ from the underlying $\ca^*$ modulo equivalences.

\subsection{An Approach for Proving a Local Convergence Result} \label{sec:analysis_proofhighlevel}

We describe a high-level approach for proving a local convergence result, which motivates the definition of the key parameters that govern the performance of our algorithm.  Our proof strategy is to demonstrate that under appropriate conditions the sequence of normalized iterates $\L^{(t)}$ obtained from Algorithm \ref{alg:amsdpreg} satisfies $\xi_{\L^\star}(\L^{(t+1)}) \leq \gamma \xi_{\L^\star}(\L^{(t)})$ for a suitable $\gamma < 1$.  To bound $\xi_{\L^\star}(\L^{(t+1)})$ with respect to $\xi_{\L^\star}(\L^{(t)})$, we consider each of the three steps in Algorithm \ref{alg:amsdpreg}.  Fixing notation before we proceed, let $\L^{(t)} = {\L^{\star}} \circ (\sfi + \sfe^{(t)}) \circ \mathcal{M}^{(t)}$ for some linear rank-preserver $\mathcal{M}^{(t)}$ and for a corresponding error term $\sfe^{(t)}$.  Our objective is to show that there exists a linear rank-preserver $\mathcal{M}^{(t+1)}$ and corresponding error term $\sfe^{(t+1)}$ with $\L^{(t+1)} = {\L^{\star}} \circ (\sfi + \sfe^{(t+1)}) \circ \mathcal{M}^{(t+1)}$, so that $\|\sfe^{(t+1)}\|_\eu$ is suitably bounded above in terms of $\sfe^{(t)}$.  By taking limits we obtain the desired result in terms of $\xi_{\L^\star}(\L^{(t)})$ and $\xi_{\L^\star}(\L^{(t+1)})$.

The first step of Algorithm \ref{alg:amsdpreg} involves the solution of the following optimization problem for each $j = 1,\dots,n$:
\begin{equation*}
\hat{X}^{(j)} = \argmin_{X \in \R^{q \times q}} ~ \left\|{\by^{(j)}}^\star - \L^{(t)} (X) \right\|_{\ell_2}^2 ~ \text{s.t.} ~ \mathrm{rank}(X) \leq r.
\end{equation*}
As $\L^{(t)} = \L^\star \circ (\sfi + \sfe^{(t)}) \circ \mathcal{M}^{(t)}$ and as ${\by^{(j)}}^\star = \L^\star \left({X^{(j)}}^\star\right)$, the preceding problem can be reformulated in the following manner:
\begin{equation*}
\begin{aligned}
\mathcal{M}^{(t)}(\hat{X}^{(j)}) = \argmin_{\tilde{X} \in \R^{q \times q}} ~ & \left\|\L^\star \circ (\sfi + \sfe^{(t)})(X^\star) - \L^\star \circ \sfe^{(t)} (X^\star) + \L^\star \circ (\sfi + \sfe^{(t)})(\tilde{X}) \right\|_{\ell_2}^2 \\ \text{s.t.} ~ & \mathrm{rank}(\tilde{X}) \leq r.
\end{aligned}
\end{equation*}
If $\L^{\star} \circ (\sfi + \sfe^{(t)})$ satisfies a suitable restricted isometry condition, then the results in \cite{GM:11,JMD:10} (as described in Section \ref{sec:algorithm_am_lrmrecovery}) imply that
$\mathcal{M}^{(t)}(\hat{X}^{(j)}) \approx {X^{(j)}}^\star$.  In other words, if $\|\sfe^{(t)}\|_{\eu}$ is small and if $\L^\star$ satisfies a restricted isometry condition, then $\mathcal{M}^{(t)}(\hat{X}^{(j)}) \approx {X^{(j)}}^\star$; the following result states matters formally:

\begin{proposition}\label{thm:varietyconstrainederr}
Let $\L^\star : \R^{q \times q} \rightarrow \R^d$ be a linear map such that $(i)$ $\L^\star$ is normalized, and $(ii)$ $\L^\star$ satisfies the restricted isometry condition $\delta_{4r}(\L^{\star}) \leq \frac{1}{20}$.  Suppose $\L = \L^\star \circ (\sfi + \sfe) \circ \mathcal{M}$ such that $(i)$ $\mathcal{M}$ is a linear rank-preserver, and $(ii)$ $\|\sfe\|_{\eu} \leq \frac{1}{50} \min\{\frac{1}{\sqrt{q}}, \frac{1}{\|\L^\star\|_2} \}$. Finally, suppose $\by = \L^\star(X^\star)$, where $X^\star \in \R^{q \times q}$ is a rank-$r$ matrix such that $\sigma_r(X^\star) \geq \sigma_1(X^\star) / 2$, and that $\hat{X}$ is the optimal solution to
\begin{equation} \label{eq:varietyoptimization}
\hat{X} = \argmin_{X \in \R^{q \times q}} ~ \left\|\by - \L(X) \right\|_{\ell_2}^2 ~ \text{s.t.} ~ \mathrm{rank}(X) \leq r.
\end{equation}
Then
\begin{equation*}
\mathcal{M}(\hat{X}) = X^\star -\cp_{\ct(X^\star)}\left[\left({\L^{\star\prime}_{\ct(X^\star)}} \L^\star_{\ct(X^\star)}\right)^{-1} \right]_{\ct(X^\star)}\cp_{\ct(X^\star)} \circ {\L^\star}' \L^\star \circ \sfe \left(X^\star\right) + \err,
\end{equation*}
where $\|\err\|_F \leq 1500 r^{5/2} \|\L^\star\|_2^2 \|X^\star\|_2 \| \sfe \|_{\eu}^2$.
\end{proposition}

In this proposition, the conclusion is well-defined as the linear map ${\L^{\star\prime}_{\ct(X^\star)}} \L^\star_{\ct(X^\star)} : \ct(X^\star) \rightarrow \ct(X^\star)$ is invertible due to the restricted isometry condition on $\L^\star$ (see Lemma \ref{thm:weakincoherencebound}).  The proof appears in Appendix \ref{apx:varietyanalysis}, and it relies primarily on the first-order optimality conditions of the problem \eqref{eq:varietyconstrainedopt}.  To ensure that the conditions required by this proposition hold, we assume in our main theorem in Section \ref{sec:analysis_mainresult} that $\L^\star$ satisfies the restricted isometry property for rank-$r$ matrices and that the initial guess $\L^{(0)}$ that is supplied to Algorithm \ref{alg:amsdpreg} is such that $\xi_{\L^\star}(\L^{(0)})$ is small (with a sufficiently good initial guess and by an inductive hypothesis, we have that there exists an error term $\sfe^{(t)}$ at iteration $t$ such that $\|\sfe^{(t)}\|_{\eu}$ is small).

The second step of Algorithm \ref{alg:amsdpreg} entails the solution of a least-squares problem.  To describe the implications of this step in detail, we consider the linear maps $\xx^\star : \bz \mapsto \sum_{j=1}^n {X^{(j)}}^\star \bz_j$ and $\hat{\xx} :   \bz \mapsto \sum_{j=1}^n \hat{X}^{(j)} \bz_j$ from $\R^n$ to $\R^{q \times q}$.  With this notation, the second step of Algorithm \ref{alg:amsdpreg} results in the linear map $\L^{(t)}$ being updated as follows:
\begin{equation} \label{eq:intermediatedictestimate}
\tilde{\L}^{(t+1)} = \L^\star \circ \xx^\star \circ \hat{\xx}^+.
\end{equation}
In order for the normalized version of $\tilde{\L}^{(t+1)}$ to be close to $\L^\star$ (in terms of the distance measure $\xi$), we require a deeper understanding of the structure of $\xx^\star \circ \hat{\xx}^+$, which is the focus of the next proposition.  This result relies on the set $\{{X^{(j)}}^\star\}_{j=1}^n$ being suitably isotropic, as characterized by the quantities $\Delta\!\left(\{{X^{(j)}}^\star\}_{j=1}^n \right)$ and $\Lambda\!\left(\{{X^{(j)}}^\star\}_{j=1}^n\right)$.

\begin{proposition} \label{thm:structuralresult}
Let $\{A^{(j)}\}_{j=1}^n \subset \R^{q \times q}$ and $\{B^{(j)}\}_{j=1}^n \subset \R^{q \times q}$ be two collections of matrices, and let $\mathfrak{A} : \bz \mapsto \sum_{j=1}^n A^{(j)} \bz_j$ and $\mathfrak{B} : \bz \mapsto \sum_{j=1}^n B^{(j)} \bz_j$ be linear maps from $\R^n$ to $\R^{q \times q}$ associated to these ensembles.  Let $\mathcal{Q} : \R^{q \times q} \rightarrow \R^{q \times q}$ be any invertible linear operator and denote $\omega = \max_j \left\|\mathcal{Q}(B^{(j)})  - A^{(j)} \right\|_F$.  If $\omega \leq \frac{\sqrt{\coveig\!\left(\{A^{(j)}\}_{j=1}^n\right)}}{20}$ and if $\frac{\covsup\!\left(\{A^{(j)}\}_{j=1}^n\right)} {\coveig\!\left(\{A^{(j)}\}_{j=1}^n\right)} \leq \frac{1}{6}$, then
\begin{equation} \label{eq:ensembleofdifferences}
\mathfrak{A} \circ \mathfrak{B}^{+} = \left(\sfi - \frac{1}{n\coveig\!\left(\{A^{(j)}\}_{j=1}^n\right)} \sum_{j=1}^{n} \left(\mathcal{Q}(B^{(j)}) - A^{(j)}\right) \boxtimes A^{(j)} + \sff \right) \circ \mathcal{Q},
\end{equation}
where $\|\sff\|_{\eu} \leq 16 q \frac{\omega^2}{\coveig\!\left(\{A^{(j)}\}_{j=1}^n\right)} + 2 q  \frac{\omega \covsup\!\left(\{A^{(j)}\}_{j=1}^n\right)} {\coveig\!\left(\{A^{(j)}\}_{j=1}^n\right)^{3/2}} $.
\end{proposition}

The proof of this proposition appears in Appendix \ref{apx:pseudoinverseexpand}, and it consists of two key elements.  First, as $\omega$ is bounded, the operator $\xx^\star \circ \hat{\xx}^{+}$ may be approximated as $\xx^\star \circ {\xx^\star}^+ \circ \mathcal{M}^{(t)}$.  Second, as the set $\{{X^{(j)}}^\star\}_{j=1}^n$ is near-isotropic based on the assumptions involving $\Delta\!\left(\{{X^{(j)}}^\star\}_{j=1}^n \right)$ and $\Lambda\!\left(\{{X^{(j)}}^\star\}_{j=1}^n\right)$, one can show that $\xx^\star \circ {\xx^\star}^+$ can be expanded suitably around the identity map $\sfi$.

The final step of our analysis is to consider the effect of normalization on the map $\tilde{\L}^{(t)}$ in \eqref{eq:intermediatedictestimate}.  Denoting the positive-definite rank-preserver that normalizes $\tilde{\L}^{(t+1)}$ by $\mathcal{N}_{\tilde{\L}^{(t+1)}}$, we have from Propositions \ref{thm:varietyconstrainederr} and \ref{thm:structuralresult} that the normalized map $\L^{(t+1)}$ obtained after the application of the Operator Sinkhorn iterative procedure to $\tilde{\L}^{(t+1)}$ can be expressed as:
\begin{equation*}
\L^{(t+1)} = \L^\star \circ \left(\sfi - \frac{1}{n\coveig\!\left(\{{X^{(j)}}^\star\}_{j=1}^n\right)} \sum_{j=1}^{n} \left(\mathcal{M}^{(t)}(\hat{X}^{(j)}) - {X^{(j)}}^\star\right) \boxtimes {X^{(j)}}^\star + \sff \right) \circ \mathcal{M}^{(t)} \circ \mathcal{N}_{\tilde{\L}^{(t+1)}},
\end{equation*}
where $\sff \in \mathbb{L}(\R^{q\times q})$ is suitably bounded.  As $\mathcal{M}^{(t)}$ and $\mathcal{N}_{\tilde{L}^{(t+1)}}$ are both rank-preservers, we need to prove that the expression within parentheses $\sfi \allowbreak - \allowbreak \frac{1}{n\coveig\!\left(\{{X^{(j)}}^\star\}_{j=1}^n\right)} \allowbreak \sum_{j=1}^{n} \left(\mathcal{M}^{(t)}(\hat{X}^{(j)}) - {X^{(j)}}^\star\right) \boxtimes {X^{(j)}}^\star + \sff$ is well-approximated as a rank-preserver so that $\xi_{\L^\star}(\L^{(t+1)})$ is suitably controlled.  To make progress on this front, we note that $\sfi = I \botimes I$ is a rank-preserver. Therefore, if $- \allowbreak \frac{1}{n\coveig\!\left(\{{X^{(j)}}^\star\}_{j=1}^n\right)} \allowbreak \sum_{j=1}^{n} \left(\mathcal{M}^{(t)}(\hat{X}^{(j)}) - {X^{(j)}}^\star\right) \boxtimes {X^{(j)}}^\star + \sff$ is small, a natural approach to characterizing how close $\sfi \allowbreak - \allowbreak \frac{1}{n\coveig\!\left(\{{X^{(j)}}^\star\}_{j=1}^n\right)} \allowbreak \sum_{j=1}^{n} \left(\mathcal{M}^{(t)}(\hat{X}^{(j)}) - {X^{(j)}}^\star\right) \boxtimes {X^{(j)}}^\star + \sff$ is to a rank-preserver is to express this quantity in terms of the following \emph{tangent space} at $\sfi$ with respect to the set of rank-preservers acting on the space of $q \times q$ matrices:
\begin{equation} \label{eq:tangentspaceati}
\mathbb{W} = \mathrm{span}\{I \botimes W_1 + W_2 \botimes I ~|~ W_1, W_2 \in \R^{q \times q}\}
\end{equation}
The next result gives such an expression.

\begin{proposition} \label{thm:induction}
Suppose $\sfd : \R^{q \times q} \rightarrow \R^{q \times q}$ is a linear operator such that $\|\sfd \|_{\eu} \leq 1/10$ and $\sfi : \R^{q \times q} \rightarrow \R^{q \times q}$ is the identity operator.  Then we have that
\begin{equation*}
\sfi + \sfd  = (\sfi + \cp_{\mathbb{W}^{\perp}} (\sfd) + \sfm ) \circ \mathcal{W}
\end{equation*}
where $\sfm : \R^{q \times q} \rightarrow \R^{q \times q}$ is a linear operator such that $\|\sfm\|_{\eu} \leq 5 \| \sfd \|_{\eu}^2 / \sqrt{q}$ and $\mathcal{W} : \R^{q \times q} \rightarrow \R^{q \times q}$ is a linear rank-preserver such that $\|\mathcal{W} - \sfi\|_2 \leq  3\|\sfd\|_{\eu}/\sqrt{q}$.  Here, the space $\mathbb{W}$ is as defined in \eqref{eq:tangentspaceati}.
\end{proposition}

The proof of this proposition appears in Appendix \ref{apx:linearizemanifold}.  As detailed in the proof of Theorem \ref{thm:localconvergence} in Section \ref{sec:analysis_maintheoremproof}, one can combine the preceding three results along with the observation that $c ~ \cp_{\ct(X^\star)} \preceq \left[\left({\L^\star_{\ct(X^\star)}}' \L^\star_{\ct(X^\star)}\right)^{-1} \right]_{\ct(X^\star) \rightarrow \R^{q \times q}} \preceq \tilde{c} ~ \cp_{\ct(X^\star)}$ for suitable constants $c,\tilde{c} > 0$ (from Lemma \ref{thm:weakincoherencebound} in Section \ref{sec:analysis_maintheoremproof} based on $\L^\star$ satisfying a suitable restricted isometry condition) to conclude that there exists an error term $\sfe^{(t+1)}$ at iteration $t+1$ (corresponding to the error term $\sfe^{(t)}$ at iteration $t$ that we fixed at the beginning of this argument) such that
\begin{equation} \label{eq:candidate_nexte}
\begin{aligned}
\sfe^{(t+1)} = & \cp_{\mathbb{W}^\perp} \circ \left[\frac{1}{n \Lambda\!(\{{X^{(j)}}^\star\}_{j=1}^n)} \sum_{j=1}^n \left({X^{(j)}}^\star \boxtimes {X^{(j)}}^\star \right) \botimes \cp_{\ct({X^{(j)}}^\star)} \right]({\L^\star}' \L^\star \circ \sfe^{(t)}) + \cp_{\mathbb{W}^\perp}(\sff) \\ &+ \mathcal{O}(\|\sfe^{(t)}\|_{\eu}^2).
\end{aligned}
\end{equation}
Thus, there are two `significant' terms in this expression that govern the size of $\|\sfe^{(t+1)}\|_{\eu}$.  To control the first term, we require a bound on the following operator norm:
\begin{equation} \label{eq:defn_omegaoperator}
\Omega(\{{X^{(j)}}^\star\}_{j=1}^n) := \left\|\cp_{\mathbb{W}^\perp} \circ \left[\frac{1}{n} \sum_{j=1}^n \left({X^{(j)}}^\star \boxtimes {X^{(j)}}^\star \right) \botimes \cp_{\ct({X^{(j)}}^\star)} \right] \right\|_2.
\end{equation}
Note that this operator belongs to $\mathbb{L}(\mathbb{L}(\R^{q \times q}))$.  In Section \ref{sec:analysis_maintheoremproof} we show that the first significant term in \eqref{eq:candidate_nexte} is bounded as $\frac{2 \|\L^\star\|_{2}^2 \Omega(\{{X^{(j)}}^\star\}_{j=1}^n)}{\Lambda(\{{X^{(j)}}^\star\}_{j=1}^n)} \|\sfe^{(t)}\|_{\eu}$.  For the second term in \eqref{eq:candidate_nexte}, we show in Section \ref{sec:analysis_maintheoremproof} that $\|\sff\|_{\eu} \lesssim \frac{q^2 \|\L^\star\|_{2} \Delta(\{{X^{(j)}}^\star\}_{j=1}^n)}{\Lambda(\{{X^{(j)}}^\star\}_{j=1}^n)} \|\sfe^{(t)}\|_{\eu}$ based on a bound on $\xi_{\L^\star}(\L^{(0)})$ on the initial guess.  Consequently, two of the key assumptions in Theorem \ref{thm:localconvergence} concern bounds on the quantities $\frac{\Omega(\{{X^{(j)}}^\star\}_{j=1}^n)} {\Lambda(\{{X^{(j)}}^\star\}_{j=1}^n)}$ and $\frac{\Delta(\{{X^{(j)}}^\star\}_{j=1}^n)} {\Lambda(\{{X^{(j)}}^\star\}_{j=1}^n)}$.

We note that the Operator Sinkhorn scaling procedure for normalization is crucial in our algorithm.  Aside from addressing the identifiability issues as discussed in Section \ref{sec:algorithm_identifiablity}, the incorporation of this method also plays an important role in the convergence of Algorithm \ref{alg:amsdpreg}.  Specifically, if we do not apply this procedure in each iteration of Algorithm \ref{alg:amsdpreg}, then the estimate of $\L^\star$ at the end of iteration $t+1$ would be $\tilde{\L}^{(t+1)}$ from \eqref{eq:intermediatedictestimate}.  In analyzing how close the image of the nuclear norm ball under $\tilde{\L}^{(t+1)}$ is to the image of the nuclear norm ball under $\L^\star$, we would need to consider how close $\xx^\star \circ \hat{\xx}^+$ is to an \emph{orthogonal} rank-preserver as opposed to an arbitrary rank preserver; in particular, we cannot apply Proposition \ref{thm:normalizednearothogonal} as $\tilde{\L}^{(t+1)}$ is not normalized.  In analogy to the discussion preceding Proposition \ref{thm:induction} and by noting that $\sfi = I \botimes I$ is an orthogonal rank-preserver, we could attempt to express $\xx^\star \circ \hat{\xx}^+$ in terms of the following tangent space at $\sfi$ with respect to the set of orthogonal rank-preservers:
\begin{equation} \label{eq:defn_subspacekronskewsym}
\mathbb{S} = \mathrm{span}\{I \botimes S_1 + S_2 \botimes I ~|~ S_1, S_2 \in \R^{q \times q} ~\text{and skew-symmetric}\}.
\end{equation}
Following similar reasoning as in the preceding paragraph, the convergence rate of our algorithm without normalization would be governed by $\left\|\cp_{\mathbb{S}^\perp} \circ \left[\frac{1}{n} \sum_{j=1}^n \left({X^{(j)}}^\star \boxtimes {X^{(j)}}^\star \right) \botimes \cp_{\ct({X^{(j)}}^\star)} \right] \right\|_2$.  This operator norm is, in general, much larger than the quantity $\Omega(\{{X^{(j)}}^\star\}_{j=1}^n)$ defined in \eqref{eq:defn_omegaoperator} as $\mathbb{S} \subset \mathbb{W}$, which can in turn affect the convergence of our algorithm.  In particular, for a natural random ensemble $\{{X^{(j)}}^\star\}_{j=1}^n$ of low-rank matrices described in Proposition \ref{thm:randensemble} in Section \ref{sec:analysis_randensemble}, the condition on $\Omega(\{{X^{(j)}}^\star\}_{j=1}^n)$ in Theorem \ref{thm:localconvergence} is satisfied while the analogous condition on $\left\|\cp_{\mathbb{S}^\perp} \circ \left[\frac{1}{n} \sum_{j=1}^n \left({X^{(j)}}^\star \boxtimes {X^{(j)}}^\star \right) \botimes \cp_{\ct({X^{(j)}}^\star)} \right] \right\|_2$ is violated (both of these conclusions hold with high probability), thus highlighting the importance of the inclusion of the normalization step for the convergence of our method; see the remarks following Proposition \ref{thm:randensemble} for details.

%We begin this stage of our proof by demonstrating that:
%\begin{equation} \label{eq:pseudoinversesimplified}
%\xx^\star \circ \hat{\xx}^\dag = \left(\sfi - \frac{q^2}{n r \Lambda\!\left(\{{X^{(j)}}^\star\}_{j=1}^n \right)\!} \sum_{j=1}^{n} \left(W_L \hat{X}^{(j)} W_R^{\prime} - {X^{(j)}}^\star \right) \boxtimes {X^{(j)}}^\star + \mathsf{G}^{(t)} + \mathsf{H}^{(t)} \right) \circ W_R \botimes W_L.
%\end{equation}
%If the set of low-rank matrices $\{{X^{(j)}}^\star\}_{j=1}^n$ is near-isotropic, i.e., the quantity $\Delta\!\left(\{{X^{(j)}}^\star\}_{j=1}^n \right)$ is suitably bounded above, then we prove that $\|\mathsf{G}^{(t)} \|_{\eu} \lesssim  \frac{q^4}{r} \Delta\!\left(\{{X^{(j)}}^\star\}_{j=1}^n \right) \|\L^\star\|_2 \|\sfe^{(t)}\|_{\eu}$. [*** IS THIS CORRECT AND CAN WE PROVIDE A LITTLE BIT MORE DETAIL ***]
%
%The final step of our analysis
%
%
%The crux of our proof is the following. Suppose we express every estimate as the form $\ca^{(t)} = \ca^* \circ (\sfi + \sfe^{(t)}) \circ W_R^{(t)} \botimes W_L^{(t)}$. We show that
%\begin{equation*}
%\| \sfe^{(t+1)} \|_{\eu} \lesssim (\frac{q^2}{r} \roc(\xx^*) \sigmacanorm^2 + \frac{q^4}{r} \covsup(\xx^*) \sigmacanorm )\| \sfe^{(t)} \|_{\eu}.
%\end{equation*}
%
%(1) Our first step is to analyze the variety constrained optimization that uses the estimate $\ca^{(t)}$ as a proxy for the map $\ca^*$. Specifically, we show that the solution to the following optimization instance
%\begin{equation*}
%	\hat{X} = \underset{X}{\mathrm{argmin}} \| \boldsymbol y - \ca^{(t)} (X) \|_2^2 \quad \mathrm{s.t.} \quad \mathrm{rank}(X) \leq r.
%\end{equation*}
%where $\boldsymbol y = \ca^*(X^*)$ for some rank-$r$ matrix $X^*$ has the following first order expansion
%\begin{equation} \label{eq:optsolnsummarized} W_L \hat{X} W_R^{\prime} = X^* - [(\ca^{*\prime}_{\ct(X^*)}\ca^{*}_{\ct(X^*)})^{-1}] \circ \ca^{*\prime} \ca^* \circ \sfe^{(t)} (X^*) + H^{(t)},
%\end{equation}
%where $H^{(t)}$ is a matrix satisfying $\|H^{(t)}\|_F = o(\|\sfe^{(t)}\|_{\eu}) $.
%
%(2) Our second step is to utilize the isotropy present within the dataset (i.e. $\Delta(\xx)$ being sufficiently small) to obtain the approximation
%\begin{equation} \label{eq:pseudoinversesimplified}
%	\xx^* \hat{\xx}^{\dagger} = \biggl(\sfi - \frac{q^2}{n r \coveig (\xx^*)} \sum_{i=1}^{n} (W_L \hat{X}_i W_R^{\prime} - X_i^*) \boxtimes X_i^* + \mathsf{G}^{(t)} + \mathsf{H}^{(t)} \biggr) \circ W_R \botimes W_L,
%\end{equation}
%where $\mathsf{G}^{(t)}$ and $\mathsf{H}^{(t)}$ are linear operators satisfying $\| \mathsf{G}^{(t)} \|_{\eu} \lesssim  \frac{q^4}{r} \covsup(\xx^*) \sigmacanorm \| \sfe^{(t)} \|$, and $\|\mathsf{H}^{(t)}\|_{\eu} = o( \| \sfe^{(t)} \|_{\eu})$.
%
%
%
%
%
%
%
%
%
%
%
%(3) The third step is to apply the first order expansion \eqref{eq:optsolnsummarized} to the approximation \eqref{eq:pseudoinversesimplified}, and subsequently decompose the resulting expression for $\xx^* \hat{\xx}^{\dagger}$ into a component in the manifold of Kronecker products, and another component in the normal space. This allows us to establish the relationship
%\begin{equation} \label{eq:highlevelproofeq}
%\sfe^{(t+1)} = - [\cp_{\mathbb{W}^{\perp}} \circ\mathcal{O}] ( \ca^{*\prime} \ca^* \circ \sfe^{(t)}) + \cp_{\mathbb{W}^{\perp}} \circ [\mathsf{G}^{(t)} + \mathsf{H}^{(t)}] +  \mathsf{J}^{(t)}
%\end{equation}
%where $\mathcal{O}$ is the linear operator
%\begin{equation*}
%\mathcal{O} (\mathsf{M}) = \frac{q^2}{nr \coveig(\xx^*)}\sum_{i=1}^{n} [(\ca^{*\prime}_{\ct(X_i^*)}\ca^{*}_{\ct(X_i^*)})^{-1}] \circ \mathsf{M} (X^*_i) \boxtimes X^*_i ,
%\end{equation*}
%and $\mathsf{J}^{(t)}$ is a linear map that represents the error incurred from linearizing the manifold of Kronecker products. Moreover, one can also show that $\| \cp_{\mathbb{W}^{\perp}} \circ \mathsf{G}^{(t)} \|_{\eu} \lesssim  \frac{q^4}{r} \covsup(\xx^*) \sigmacanorm \| \sfe^{(t)} \|$, and $\|\cp_{\mathbb{W}^{\perp}} \circ\mathsf{H}^{(t)}\|_{\eu},\| \mathsf{J}^{(t)}\|_{\eu}  = o( \| \sfe^{(t)} \|_{\eu})$.
%
%We briefly remark on the behavior of each term and the relation to the assumptions in Theorem [REF]. First the terms $\cp_{\mathbb{W}^{\perp}} \circ\mathsf{H}^{(t)}$ and $\mathsf{J}^{(t)}$ have sizes vanishing relative to $\sfe^{(t)}$, and hence are benign. Second the size of $\cp_{\mathbb{W}^{\perp}} \circ \mathsf{G}^{(t)}$, which is governed by the quantity $\Delta(\xx)$, vanishes with increasingly more isotropic data. Third the linear map $\mathcal{O}$ is a positive definite self-adjoint operator that has eigenvalues purely determined by the ambient dimension $d$, lifted dimension $q\times q$, and rank $r$, but are \emph{independent} of the number of observations $n$, for natural ensembles of matrices $\xx$ such as those considered in Proposition [REF]. To control the size of the operator norm of $\cp_{\mathbb{W}^{\perp}} \circ\mathcal{O}$, we note that for linear maps $\ca^*$ satisfying RIP we have the bound in PSD order
%\begin{equation*}
%\mathcal{O} = \frac{q^2}{nr \coveig(\xx^*)}\sum_{i=1}^{n} ( X_i^* \boxtimes X_i^*) \boldsymbol \botimes [(\ca^{*\prime}_{\ct(X_i^*)}\ca^{*}_{\ct(X_i^*)})^{-1}]  \preceq \frac{2q^2}{nr \coveig(\xx^*)}\sum_{i=1}^{n} ( X_i^* \boxtimes X_i^*) \boldsymbol \botimes \cp_{\ct(X_i^*)}.
%\end{equation*}
%This motivates defining the quantity $\Omega(\xx)$.


\subsection{Main Result} \label{sec:analysis_mainresult}

The following theorem gives the main result concerning the local convergence of our algorithm:

\begin{theorem} \label{thm:localconvergence} Let $\by^{(j)} = \L^\star\left({X^{(j)}}^\star \right), ~ j=1,\dots,n$, where $\L^\star : \R^{q \times q} \rightarrow \R^d$ is a linear map and $\left\{{X^{(j)}}^\star \right\}_{j=1}^n \subset \R^{q \times q}$.  Suppose the collection $\left\{{X^{(j)}}^\star \right\}_{j=1}^n \subset \R^{q \times q}$ satisfies the following conditions:
\begin{enumerate}
\item There exists $r < q$ and $s > 0$ such that $\mathrm{rank}\left({X^{(j)}}^\star \right) = r$ and $s \geq \sigma_1\left({X^{(j)}}^\star \right) \geq \sigma_r\left({X^{(j)}}^\star \right) \geq s / 2$ for each $j = 1,\dots,n$;
\item $\frac{\Omega\left(\left\{{X^{(j)}}^\star \right\}_{j=1}^n \right)}{\Lambda\left( \left\{{X^{(j)}}^\star \right\}_{j=1}^n \right)} \leq \frac{d}{40 q^2}$; and
\item $\frac{\Delta\left(\left\{{X^{(j)}}^\star \right\}_{j=1}^n \right)}{\Lambda\left( \left\{{X^{(j)}}^\star \right\}_{j=1}^n \right)} \leq \frac{\sqrt{d}}{150 q^3}$.
\end{enumerate}
Suppose the linear map $\L^\star : \R^{q \times q} \rightarrow \R^d$ satisfies the following conditions:
\begin{enumerate}
\item $\L^\star$ satisfies the restricted isometry condition $\delta_{4r}(\L^\star) \leq \frac{1}{20}$, where $r$ is the rank of each ${X^{(j)}}^\star$;
\item $\L^\star$ is normalized and surjective; and
\item $\|\L^\star\|_2^2 \leq \frac{5 q^2}{d}$.
\end{enumerate}
If we supply Algorithm \ref{alg:amsdpreg} with a normalized initial guess $\L^{(0)} : \R^{q \times q} \rightarrow \R^d$ with $\xi_{\L^\star}(\L^{(0)}) < \frac{1}{36000 q^{7/2} r^2 \|\L^\star\|_2^2}$, then the sequence $\{\L^{(t)}\}$ produced by the algorithm satisfies $\limsup_{t \rightarrow \infty} \frac{\xi_{\L^\star}(\L^{(t+1)})}{\xi_{\L^\star}(\L^{(t)})} \leq 2 \|\L^\star\|_2^2 \frac{\Omega\left(\left\{{X^{(j)}}^\star \right\}_{j=1}^n \right)}{\Lambda\left( \left\{{X^{(j)}}^\star \right\}_{j=1}^n \right)} + 15 q^2 \|\L^\star\|_2 \frac{\Delta\left(\left\{{X^{(j)}}^\star \right\}_{j=1}^n \right)}{\Lambda\left( \left\{{X^{(j)}}^\star \right\}_{j=1}^n \right)} < 1$.  In other words, $\xi_{\L^\star}(\L^{(t)}) \rightarrow 0$ with the rate of convergence bounded above by $2 \|\L^\star\|_2^2 \frac{\Omega\left(\left\{{X^{(j)}}^\star \right\}_{j=1}^n \right)}{\Lambda\left( \left\{{X^{(j)}}^\star \right\}_{j=1}^n \right)} + 15 q^2 \|\L^\star\|_2 \frac{\Delta\left(\left\{{X^{(j)}}^\star \right\}_{j=1}^n \right)}{\Lambda\left( \left\{{X^{(j)}}^\star \right\}_{j=1}^n \right)}$.  We assume here that Step $1$ of Algorithm \ref{alg:amsdpreg} is computed via Algorithm \ref{alg:svp}.
\end{theorem}

\textbf{Remarks}. $(i)$ In this result the assumption that Step $1$ of Algorithm \ref{alg:amsdpreg} is computed via Algorithm \ref{alg:svp} is made for the sake of concreteness.  A similar result and proof are possible if Step $1$ of Algorithm \ref{alg:amsdpreg} is instead computed by solving \eqref{eq:nuclearnormlasso} for a suitable choice of the regularization parameter. $(ii)$ In conjunction with Proposition \ref{thm:normalizednearothogonal}, this result implies that we obtain a linear map $\hat{\L}$ upon convergence of our algorithm such that the image of the nuclear norm ball in $\R^{q \times q}$ under $\hat{\L}$ is the same as it is under $\L^\star$.

%$(iii)$ The conditions on the linear map $\L^\star$ and on the low-rank matrices $\left\{{X^{(j)}}^\star \right\}_{j=1}^n$ are tight in *** TALK ABOUT IMPROVEMENT IN INITIAL GUESS ***

The proof of this theorem is given in Section \ref{sec:analysis_maintheoremproof}.  In words, our result states that under a restricted isometry condition on the linear map $\L^\star$ and an isotropy condition on the low-rank matrices $\left\{{X^{(j)}}^\star \right\}_{j=1}^n$, Algorithm \ref{alg:amsdpreg} is locally linearly convergent to the appropriate semidefinite-representable regularizer that promotes the type of structure contained in the data $\left\{\L^\star\left({X^{(j)}}^\star\right) \right\}_{j=1}^n$.  The restricted isometry condition on $\L^\star$ ensures that the geometry of the set of points $\left\{{X^{(j)}}^\star \right\}_{j=1}^n$ in $\R^{q \times q}$ is (approximately) preserved in the lower-dimensional space $\R^d$.  The isotropy condition on the collection $\left\{{X^{(j)}}^\star \right\}_{j=1}^n$ ensures that we have observations that lie on most of the low-dimensional faces of the regularizer, which gives us sufficient information to reconstruct the regularizer.

Results of this flavor have previously been obtained in the classical dictionary learning literature \cite{AAJN:16,AGTM:15}, although our analysis is more challenging in comparison to this prior work for two reasons.  First, two nearby sparse vectors with the same number of nonzero entries have the same support, while two nearby low-rank matrices with the same rank have different row/column spaces; geometrically, this translates to the point that two nearby sparse vectors have the same tangent space with respect to a suitably defined variety of sparse vectors, while two nearby low-rank matrices generically have different tangent spaces with respect to an appropriate variety of low-rank matrices.  Second (and more significant), the normalization step in classical dictionary learning is simple -- corresponding to scaling the columns of a matrix to have unit Euclidean norm, as discussed in Section \ref{sec:algorithm_cdl} -- while the normalization step in our setting based on Operator Sinkhorn scaling is substantially more complicated.  Indeed, one of the key aspects of our analysis is the relation between the stability properties of Operator Sinkhorn scaling and the tangent spaces to varieties of low-rank matrices, as is evident from the appearance of the parameter $\Omega\left(\left\{{X^{(j)}}^\star \right\}_{j=1}^n\right)$ in Theorem \ref{thm:localconvergence}.

%*** DO WE WANT TO SAY SOMETHING ABOUT CAUCHY SEQUENCE? ***

Theorem \ref{thm:localconvergence} provides conditions under which our algorithm exhibits local linear convergence to a linear map $\hat{\L}$ that specifies the same regularizer as $\L^\star$.  However, in practice we do not have access to $\L^\star$ and we require a stopping criterion to decide when to terminate our algorithm.  To this end, the next result states that under the same conditions as in Theorem \ref{thm:localconvergence}, the sequence of iterates $\{\L^{(t)}\}$ obtained from our algorithm also converges (the limit point is generically different from $\L^\star$, although they specify the same regularizer):

\begin{proposition} \label{thm:cauchy}
Under the same setup and assumptions as in Theorem \ref{thm:localconvergence}, the sequence of iterates $\{\L^{(t)}\}$ obtained from our algorithm is a Cauchy sequence.
\end{proposition}

This result is proved in Appendix \ref{apx:cauchy}.

%In practice, computing the regularizer distance between two normalized maps how close two normalized maps upto conjugation by rank-preservers, i.e., computing the regularizer distance between the maps is

\subsection{Ensembles Satisfying the Conditions of Theorem \ref{thm:localconvergence}} \label{sec:analysis_randensemble}

Theorem \ref{thm:localconvergence} gives deterministic conditions on the underlying data under which our algorithm recovers the correct regularizer.  In this section we demonstrate that these conditions are in fact satisfied with high probability by certain natural random ensembles.  Our first result states that random Gaussian linear maps upon normalization satisfy the requirements on the linear map in Theorem \ref{thm:localconvergence}:

\begin{proposition}\label{thm:gaussianmapsatisfy}
Let $\tilde{\L} : \R^{q \times q} \rightarrow \R^d$ be a linear map in which each of the $d$ component linear functionals are specified by matrices $\tilde{\L}_{i} \in \R^{q \times q}$ with i.i.d random Gaussian entries with mean zero and variance $1/d$.  Let $\L$ represent a normalized map obtained by composing $\tilde{\L}$ with a positive-definite rank-preserver.  Fix any $\delta <1$.  Then there exist positive constants $c_1,c_2,c_3$ depending only on $\delta$ such that if $d \geq c_1 q(r + \log q)$, then $(i)$ $\delta_{4r}(\L) \leq \delta$ and $(ii)$ $\|\L\|_2 \leq \sqrt{\frac{5q^2}{d}}$ with probability greater than $1 - c_2\exp(-c_3 d)$.
\end{proposition}

The proof of this result is given in Appendix \ref{apx:randlinmaps}.  As shown in \cite{CanPla:11} random Gaussian linear maps from $\R^{q \times q}$ to $\R^d$ satisfy the restricted isometry property for rank-$4r$ matrices if $d \gtrsim rq$ (and this bound is tight).  Our result shows that under a slightly stronger assumption on $d$, `most' linear maps satisfy the more restrictive requirements of Theorem \ref{thm:localconvergence}.  Next we consider families of random low-rank matrices:

\begin{proposition} \label{thm:randensemble} Let $\{X^{(j)}\}_{j=1}^{n}$ be an ensemble of matrices generated as $X^{(j)} = \sum_{i=1}^{r} s_{i}^{(j)} \bu_{i}^{(j)} \bv_{i}^{(j)\prime}$ with each $U^{(j)} = [\bu_{1}^{(j)}| \ldots | \bu_{r}^{(j)}], V^{(j)} = [\bv_{1}^{(j)}| \ldots | \bv_{r}^{(j)}] \in \R^{q \times r}$ being drawn independently from the Haar measure on $q \times r$ matrices with orthonormal columns, and each $s_{i}^{(j)}$ being drawn independently from $\mathcal{D}$, where $\mathcal{D}$ is any distribution supported on $[s/2,s]$ for some $s>0$.  Then for any $0 < t_1 \leq 1/4$ and $0 < t_2$, the conditions $(i)$ $\frac{\Delta\left(\left\{X^{(j)} \right\}_{j=1}^n \right)}{\Lambda\left( \left\{X^{(j)} \right\}_{j=1}^n \right)} \leq t_1 $ and (ii) $\frac{\Omega\left(\left\{X^{(j)} \right\}_{j=1}^n \right)}{\Lambda\left( \left\{X^{(j)} \right\}_{j=1}^n \right)} \leq 80\frac{ r}{q} + t_2$, are satisfied with probability greater than $1- 2q\exp\left(-\frac{n t_1^2}{200q^4}\right) - q \exp\left(-\frac{n t_2^2}{200q^4}\right)$.  In particular, the requirements in Theorem \ref{thm:localconvergence} for $d \gtrsim rq$ are satisfied with high probability by the ensemble $\{X^{(j)}\}_{j=1}^{n}$ provided $n \gtrsim \frac{q^{10}}{d}$.
\end{proposition}

Considering the requirements of Theorem \ref{thm:localconvergence} in the regime $d \gtrsim rq$ is not restrictive as this condition is necessary for the restricted isometry assumptions of Theorem \ref{thm:localconvergence} on $\L^\star$ to hold.  The proof of this result is given in Appendix \ref{apx:randensemble}.  Thus, in some sense, `most' (sufficiently large) sets of low-rank matrices satisfy the requirements of Theorem \ref{thm:localconvergence}.  We also note that for a collection of low-rank matrices $\left\{X^{(j)} \right\}_{j=1}^n$ generated according to the ensemble in this proposition, the ratio $\frac{\Delta\left(\left\{X^{(j)} \right\}_{j=1}^n \right)}{\Lambda\left( \left\{X^{(j)} \right\}_{j=1}^n \right)} \rightarrow 0$ as $n \rightarrow \infty$, while one can show that the ratio $\frac{\Omega\left(\left\{X^{(j)} \right\}_{j=1}^n \right)}{\Lambda\left( \left\{X^{(j)} \right\}_{j=1}^n \right)} \asymp \frac{r}{q}$ as $n \rightarrow \infty$.  Based on Theorem \ref{thm:localconvergence}, this observation implies that for data generated according to the ensemble in Proposition \ref{thm:randensemble}, the rate of convergence of Algorithm \ref{alg:amsdpreg} improves with an increase in the amount of data, but only up to a certain point beyond which the convergence rate plateaus.  We illustrate this property with a numerical experiment in Section \ref{sec:numexp_synthetic}.

\textbf{Remark.}  It is critical in the preceding result that we project onto the orthogonal complement of the subspace $\mathbb{W}$ from \eqref{eq:defn_omegaoperator} in the definition of $\Omega\left( \left\{X^{(j)} \right\}_{j=1}^n \right)$.  For a set of low-rank matrices $\left( \left\{X^{(j)} \right\}_{j=1}^n \right)$ drawn from the same ensemble as in Proposition \ref{thm:randensemble}, one can show that
$\| \cp_{\mathbb{S}^{\perp}} \circ \frac{1}{n}\sum_{j=1}^{n} (X^{(j)} \boxtimes X^{(j)}) \boldsymbol \botimes \cp_{\ct(X^{(j)})} \|_2 > c \Lambda\left( \left\{X^{(j)} \right\}_{j=1}^n \right)$ for a constant $c>0$ with high probability, where the subspace $\mathbb{S}$ is defined in \eqref{eq:defn_subspacekronskewsym}.  In the context of the discussion at the end of the preceding section, we have that the conditions of Theorem \ref{thm:localconvergence} are violated if we do not incorporate the normalization step via Operator Sinkhorn scaling, which in turn impacts the convergence of our algorithm.



%of Theorem[REF] on the linear map $\L^\star$ and on the set $\{{X^{(j)}}^\star\}_{j=1}^n$ of low-rank matrices

%\begin{proposition} \label{thm:randensemble} Fix an integer $r < q$ and any distribution $\mathcal{D}$ on $\R_+$ with finite second moment.  Suppose $\xx = \{X^{(j)}\}_{i=1}^{n}$ is an ensemble of matrices generated as follows. Each $X^{(j)} = \sum_{j=1}^{r} s_{i}^{(j)} u_{i}^{(j)} v_{i}^{(j)\prime}$ with $U^{(j)} = [u_{1}^{(j)}| \ldots | u_{r}^{(j)}], V^{(j)} = [v_{1}^{(j)}| \ldots | v_{r}^{(j)}] \in \R^{q \times r}$ being drawn independently from the Haar measure on $q \times r$ matrices with orthonormal columns, and each $s_{i}^{(j)}$ being drawn independently from $\mathcal{D}$.  Then with probability greater than $1-3 q^{-nd/(10^7 q^{10})}$ we have that:
%\begin{enumerate}
%\item $\covsup(\xx^*) \leq r \sqrt{d}/(900 q^5)$
%
%\item $\roc (\xx^*)  \leq  17 r^2 / q^3$.
%\end{enumerate}
%\end{proposition}
%
%\begin{corollary}
%Suppose we have an ensemble $\xx^\star = \{{X^{(j)}}^\star\}_{j=1}^n \subset \R^{q \times q}$ of rank-$r$ matrices generated according to the assumptions of Proposition \ref{thm:randensemble}, with the further condition that $\mathcal{D}$ is supported on $[\sigma/2,\sigma]$ for some $\sigma >0$.  If $n \gtrsim q^{10} / d$ and $d \gtrsim rq$ then the conditions of Theorem[REF] concerning $\xx^*$ are satisfied with high probability.
%\end{corollary}
%
%\begin{proof}
%Follows from Proposition[REF].
%\end{proof}

\subsection{Proof of Theorem \ref{thm:localconvergence}} \label{sec:analysis_maintheoremproof}

Before giving a proof of Theorem \ref{thm:localconvergence}, we state two relevant lemmas that are proved in Appendix \ref{apx:prelims}.

\begin{lemma}\label{thm:weakincoherencebound}
	Suppose a linear map $\L : \R^{q \times q} \rightarrow \R^d$ satisfies the restricted isometry condition $\delta_{2r}(\L) <1$.  For any $\ct:= \ct(X)$ with $X \in \R^{q \times q}$ and $\mathrm{rank}(X) \leq r$, we have that $(i)$ $1-\delta_{2r}\leq \lambda_{\min}(\mathcal{L}^{\prime}_{\ct} \mathcal{L}_{\ct}) \leq \lambda_{\max}(\mathcal{L}^{\prime}_{\ct} \mathcal{L}_{\ct}) \leq 1+\delta_{2r}$, $(ii)$ $\| (\L_\ct' \L_{\ct})^{-1} \|_{2}=\| \cp_{\ct} [(\mathcal{L}_{\ct}^{\prime} \mathcal{L}_{\ct})^{-1}]_{\ct} \cp_{\ct} \|_{2} \leq \frac{1}{1-\delta_{2r}} $, $(iii)$ $\| \cp_\ct \circ \L^{\prime} \L\|_{2} \leq \sqrt{1+\delta_{2r}}\|\L\|_2$, and $(iv)$ $\|\cp_\ct [(\L_{\ct}' \L_{\ct})^{-1}]_\ct \cp_\ct \allowbreak \circ \L' \L\|_{2} \leq \frac{\sqrt{1+\delta_{2r}}}{1-\delta_{2r}}\|\L\|_2$.  Here $\L_\ct' \L_\ct : \ct \rightarrow \ct$ is a self-adjoint linear map.
\end{lemma}

\begin{lemma}\label{thm:boundondelta}
Let $\left\{X^{(j)}\right\}_{j=1}^{n} \subset \mathbb{R}^{q\times q}$ be a collection of matrices, and let $s_{\min}:= \min_{j} \|X^{(j)}\|_F^2$ and $s_{\max}:=\max_{j} \|X^{(j)}\|_F^2$.  Then $\frac{s_{\min}}{q^2} - \Delta\left(\left\{X^{(j)}\right\}_{j=1}^{n}\right) \leq \Lambda \left(\left\{X^{(j)}\right\}_{j=1}^{n}\right) \leq \frac{s_{\max}}{q^2} + \Delta\left(\left\{X^{(j)}\right\}_{j=1}^{n}\right)$.
\end{lemma}

\begin{proof}[Proof of Theorem \ref{thm:localconvergence}]
To simplify the presentation of our proof we define the following quantities $\alpha_0:=36000q^{7/2}r^2\|\mathcal{L}^{\star}\|_2^2$, $\alpha_1:= 1500 r^{5/2} \|\mathcal{L}^{\star}\|_{2}^2$, $\alpha_2 := 3 \sqrt{r} \|\mathcal{L}^{\star}\|_{2}$, $\alpha_3:= 10 q^2 \|\mathcal{L}^{\star}\|_{2}$, $\alpha_4 := 5 (q^2/\sqrt{r}) \alpha_1$, $\alpha_5:= 80q^3 \alpha_2^2$, $\alpha_6:=5 (q^2/\sqrt{r}) \alpha_2$, and $\alpha_7: = \alpha_3 + \alpha_6/6 + 1/4$.  The specific interpretation of these quantities is not essential to the proof -- the pertinent detail is that they only depend on $q,r,\|\mathcal{L}^{\star}\|_{2}$.

To simplify notation in the proof we denote $\covsup:= \covsup \left(\left\{X^{(j)}\right\}_{j=1}^{n}\right)$, $\coveig:= \coveig \left(\left\{X^{(j)}\right\}_{j=1}^{n}\right)$, and $\roc:= \roc \left(\left\{X^{(j)}\right\}_{j=1}^{n}\right)$.  In addition we also denote $\ct^{(j)}:= \ct ({X^{(j)}}^{\star} )$.  Our proof proceeds by establishing the following assertion.  Suppose that the $t$-th iterate $\mathcal{L}^{(t)}$ is such that $\mathcal{L}^{(t)} = \mathcal{L}^{\star} \circ (\sfi + \sfe^{(t)}) \circ \mathcal{M}^{(t)}$, where $\mathcal{M}^{(t)}$ is a rank-preserver, and $\sfe^{(t)}$ is a linear operator that satisfies $\|\sfe^{(t)}\|_{\eu} < 1/ \alpha_0$.  Then the $t+1$-th iterate is of the form $\mathcal{L}^{(t+1)} = \mathcal{L}^{\star} \circ (\sfi + \sfe^{(t+1)}) \circ \mathcal{M}^{(t+1)}$ for some rank-preserver $\mathcal{M}^{(t+1)}$, and some linear operator $\sfe^{(t+1)}$ that satisfies
	\begin{equation} \label{eq:maininductivestep}
	\|\sfe^{(t+1)}\|_{\eu} \leq \gamma_0 \|\sfe^{(t)}\|_{\eu} + \gamma_1 \|\sfe^{(t)}\|_{\eu}^2,
	\end{equation}
	where $\gamma_0 = 2 \|\mathcal{L}^{\star}\|_{2}^2 (\roc/\coveig) + 15q^2 \|\mathcal{L}^{\star}\|_{2} (\covsup/\coveig)$, and $\gamma_1 = \alpha_4+\alpha_5 + 5\alpha_7^2/\sqrt{q}$.
	
	Before we prove this assertion, we note how it allows us to conclude the result.  By taking the infimum over $\sfe^{(t)}$ on the right hand side of \eqref{eq:maininductivestep} and by noting that $\xi_{\L^\star}(\L^{(0)}) \leq \|\sfe^{(t+1)}\|_{\eu}$, we have
	\begin{equation} \label{eq:maininductivestep_withxi}
	\xi_{\mathcal{L}^{\star}}(\mathcal{L}^{(t+1)}) \leq \gamma_0 \xi_{\mathcal{L}^{\star}}(\mathcal{L}^{(t)}) + \gamma_1 \xi_{\mathcal{L}^{\star}}(\mathcal{L}^{(t)})^2.
	\end{equation}
	One can check based on the initial assumption on $\xi_{\mathcal{L}^{\star}}(\mathcal{L}^{(0)})$ that $\gamma:=\gamma_0 + \gamma_1 \xi_{\mathcal{L}^{\star}}(\mathcal{L}^{(0)}) < 1$.  By employing an inductive argument one can establish that $\xi_{\mathcal{L}^{\star}}(\mathcal{L}^{(t+1)}) \leq \gamma \xi_{\mathcal{L}^{\star}}(\mathcal{L}^{(t)})$.  Thus $\xi_{\mathcal{L}^{\star}}(\mathcal{L}^{(t)}) \leq \gamma^{t} \xi_{\mathcal{L}^{\star}}(\mathcal{L}^{(0)}) \rightarrow 0$ as $t \rightarrow \infty$.  By dividing the expression in \eqref{eq:maininductivestep_withxi} throughout by $\xi_{\mathcal{L}^{\star}}(\mathcal{L}^{(t)})$, and subsequently taking the limit $t\rightarrow \infty$, we obtain the asymptotic rate of convergence
	\begin{equation*}
	\limsup_{t \rightarrow \infty} \frac{\xi_{\mathcal{L}^{\star}}(\mathcal{L}^{(t+1)})}{\xi_{\mathcal{L}^{\star}}(\mathcal{L}^{(t)})} \leq \limsup_{t\rightarrow \infty} \bigl(\gamma_0 + \gamma_1 \xi_{\mathcal{L}^{\star}}(\mathcal{L}^{(t)}) \bigr) = \gamma_0.
	\end{equation*}
	
	We proceed to prove the assertion.
		
	[Applying Proposition \ref{thm:varietyconstrainederr}]: Since $\|\sfe^{(t)}\|_{\eu} \leq \min\{\frac{1}{50\sqrt{q}}, \frac{1}{50\|\mathcal{L}^{\star}\|_{2}} \}$, by applying Proposition \ref{thm:varietyconstrainederr} with the choice of $X^{\star} = {X^{(j)}}^{\star}$,  $\sfe = \sfe^{(t)}$, $\mathcal{M} = \mathcal{M}^{(t)}$, and $\mathcal{L}^{\star}$, we have for each $j = 1, \dots, n$ that
\begin{equation} \label{eq:proofvaropt}
\begin{aligned}
	\mathcal{M}^{(t)} (\hat{X}^{(j)}) - {X^{(j)}}^{\star} = & - \biggl[\cp_{\ct^{(j)}} [(\mathcal{L}^{\star\prime}_{\ct^{(j)}}\mathcal{L}^{\star}_{\ct^{(j)}})^{-1}]_{\ct^{(j)}} \cp_{\ct^{(j)}} \circ \mathcal{L}^{\star\prime} \mathcal{L}^{\star} \circ \sfe^{(t)} \biggr] \bigl({X^{(j)}}^{\star}\bigr)+ \err^{(j)},
\end{aligned}
\end{equation}
where $\err^{(j)}$ is a matrix that satisfies $\|\err^{(j)}\|_F \leq \alpha_1 \|{X^{(j)}}^{\star}\|_2\|\sfe^{(t)}\|_{\eu}^2$.
	
	[Applying Proposition \ref{thm:structuralresult}]:
	First note that $\alpha_1 \|\sfe^{(t)} \|_{\eu} \leq \alpha_1 / \alpha_0 \leq \sqrt{r}\|\mathcal{L}^{\star}\|_{2} $.  Second by applying Lemma \ref{thm:boundondelta} to the ensemble $\bigl\{{X^{(j)}}^{\star}\bigr\}_{j=1}^{n}$, and noting that $\covsup \leq s^2 r/20q^2$ we have $\coveig \geq s^2 r/5q^2$.  Third by applying these inequalities and Lemma \ref{thm:weakincoherencebound} to \eqref{eq:proofvaropt} we have $\|\mathcal{M}^{(t)} (\hat{X}^{(j)}) - {X^{(j)}}^{\star} \|_F \leq ((\sqrt{1+\delta_{4r}})/(1-\delta_{4r})) \|\mathcal{L}^{\star}\|_{2} \|{X^{(j)}}^{\star}\|_{F} \|\sfe^{(t)}\|_{\eu} +  \alpha_1 \|{X^{(j)}}^{\star}\|_2 \| \sfe^{(t)} \|_{\eu}^{2} \leq s\alpha_2 / \alpha_0 \leq \sqrt{\coveig} / 20$.  Consequently, by applying Proposition \ref{thm:structuralresult} with the collections of matrices $\bigl\{ {X^{(j)}}^{\star} \bigr\}_{j=1}^{n}$ and $\bigl\{\hat{X}^{(j)} \bigr\}_{j=1}^{n}$, and with $\mathcal{Q} = \mathcal{M}^{(t)}$, we have
\begin{equation*}
\begin{aligned}
\xx^{\star} \circ \hat{\xx}^{+} ~ = ~ & \biggl( \sfi + 
\frac{1}{n \coveig} \sum_{j=1}^{n} \left(\left[ \cp_{\ct^{(j)}} [(\mathcal{L}^{\star\prime}_{\ct^{(j)}}\mathcal{L}^{\star}_{\ct^{(j)}})^{-1}]_{\ct^{(j)}} \cp_{\ct^{(j)}} \circ \mathcal{L}^{\star\prime} \mathcal{L}^{\star} \circ \sfe^{(t)} \right] \left({X^{(j)}}^{\star}\right) \right) \boxtimes {X^{(j)}}^{\star} \\  & - \frac{1}{n\coveig} \sum_{j=1}^{n} G^{(j)} \boxtimes {X^{(j)}}^{\star} + \sff \biggr) \circ \mathcal{M}^{(t)},
\end{aligned}
\end{equation*}
where $\xx^{\star}, \hat{\xx}$ are linear maps defined as $\xx^{\star}:\bz \mapsto \sum_{j=1}^{n} {X^{(j)}}^{\star} \bz_j$, $\hat{\xx}:\bz \mapsto \sum_{j=1}^{n} \hat{X}^{(j)} \bz_j$, and
	\begin{equation} \label{eq:feucbound}
	\|\sff\|_{\eu} \leq 16 q (s^2/\coveig)\alpha_2^2 \|\sfe^{(t)}\|_{\eu}^{2} + 2 q  (\covsup/\coveig)(s/\sqrt{\coveig})  \alpha_2 \| \sfe^{(t)}\|_{\eu} \leq \alpha_5\|\sfe^{(t)}\|_{\eu}^{2} + \alpha_6(\covsup/\coveig)\| \sfe^{(t)}\|_{\eu}.
	\end{equation}
	
	[Applying Proposition \ref{thm:induction}]:  We define
	\begin{equation*}
	\sfd := \frac{1}{n \coveig} \sum_{j=1}^{n} ([\cp_{\ct^{(j)}}[(\mathcal{L}^{\star\prime}_{\ct^{(j)}}\mathcal{L}^{\star}_{\ct^{(j)}})^{-1}]\cp_{\ct^{(j)}}  \circ \mathcal{L}^{\star\prime} \mathcal{L}^{\star} \circ \sfe^{(t)}] ({X^{(j)}}^{\star})) \boxtimes {X^{(j)}}^{\star} - \frac{1}{n\coveig} \sum_{j=1}^{n} G^{(j)} \boxtimes {X^{(j)}}^{\star} + \sff.
	\end{equation*}
	Our next step is to bound $\|\sfd \|_{\eu}$.  For any collection of matrices $\{A^{(j)} \}_{j=1}^{n}, \{B^{(j)} \}_{j=1}^{n} \subset \mathbb{R}^{q\times q}$ one has the inequality $\frac{1}{n} \|\sum_{j=1} A^{(j)}\botimes B^{(j)}\|_{\eu} \leq \max_j \|A^{(j)}\botimes B^{(j)}\|_{\eu} = \max_j \|A^{(j)}\|_F \|B^{(j)}\|_F$.  By combining this inequality with Lemma \ref{thm:weakincoherencebound} we obtain the bounds
	\begin{eqnarray} \label{eq:heucbound}
	& & \frac{1}{n\coveig} \left\|\sum_{j=1}^{n}  \bigl(\bigl[\cp_{\ct^{(j)}}[(\mathcal{L}^{\star\prime}_{\ct^{(j)}}\mathcal{L}^{\star}_{\ct^{(j)}})^{-1}]\cp_{\ct^{(j)}}  \circ \mathcal{L}^{\star\prime} \mathcal{L}^{\star} \circ \sfe^{(t)} \bigr] ({X^{(j)}}^{\star})\bigr) \boxtimes {X^{(j)}}^{\star}  \right\|_{\eu} \nonumber\\ 
	& &  \leq \frac{2 s^2 r \|\mathcal{L}^{\star}\|_{2}}{\coveig }   \|\sfe^{(t)}\|_{\eu} \leq \alpha_3 \|\sfe^{(t)}\|_{\eu},
	\end{eqnarray}
	and
	\begin{equation}\label{eq:geucbound}
	(1/n\coveig)\| \sum_{j=1}^{n} G^{(j)} \boxtimes {X^{(j)}}^{\star} \|_{\eu} \leq (\alpha_1 s^2 \sqrt{r}/ \coveig) \| \sfe^{(t)} \|_{\eu}^2 \leq \alpha_4 \| \sfe^{(t)} \|_{\eu}^2.
	\end{equation}
	Hence by combining \eqref{eq:feucbound}, \eqref{eq:heucbound}, and \eqref{eq:geucbound} we have $\|\sfd\|_{\eu} \leq \alpha_3 \|\sfe^{(t)}\|_{\eu} + \alpha_4 \| \sfe^{(t)} \|_{\eu}^2 + \alpha_5\|\sfe^{(t)}\|_{\eu}^{2} + \alpha_6(\covsup/\coveig)\| \sfe^{(t)}\|_{\eu} \leq \alpha_7 \| \sfe^{(t)}\|_{\eu} \leq \alpha_7 /\alpha_0 \leq 1/10$.  Consequently, by applying Proposition \ref{thm:induction} with this choice of $\sfd$, we have
	\begin{equation} \label{eq:xxhat}
	\xx^{\star} \circ \hat{\xx}^{+} = (\sfi + \cp_{\mathbb{W}^{\perp}} (\sfd) + \sfm ) \circ \mathcal{W} \circ \mathcal{M}^{(t)}, \quad \|\sfm\|_{\eu}\leq (5\alpha_7^2/\sqrt{q})\|\sfe^{(t)}\|^2_{\eu},
	\end{equation}
	for some rank-preserver $\mathcal{W}$.
	
	[Conclusion]: Recall from the description of the algorithm that the next iterate is given by $\mathcal{L}^{(t+1)} = \mathcal{L}^{\star} \circ \xx^{\star} \circ \hat{\xx}^{+} \circ \mathcal{N}_{\mathcal{L}^{\star} \circ \xx^{\star} \circ \hat{\xx}^{+}}$, where $\mathcal{N}_{\mathcal{L}^{\star} \circ \xx^{\star} \circ \hat{\xx}^{+}}$ is the unique positive definite rank-preserver that normalizes $\mathcal{L}^{\star} \circ \xx^{\star} \circ \hat{\xx}^{+}$.  We define $\sfe^{(t+1)} := \cp_{\mathbb{W}^{\perp}} (\sfd) + \sfm$, and hence
	\begin{equation} \label{eq:nextdictestimate}
	\mathcal{L}^{(t+1)} = \mathcal{L}^{\star} \circ (\sfi + \sfe^{(t+1)}) \circ \mathcal{M}^{(t+1)},
	\end{equation}
	where $\mathcal{M}^{(t+1)} = \mathcal{W} \circ \mathcal{M}^{(t)} \circ \mathcal{N}_{\mathcal{L}^{\star} \circ \xx^{\star} \circ \hat{\xx}^{+}}$ is a composition of rank-preservers, and hence is also a rank-preserver.  It remains to bound $\|\sfe^{(t+1)}\|_{\eu}$.%\footnote{To be precise, one also needs to check that $\mathfrak{T}_{\mathcal{L}^{\star} \circ \xx^{\star} \circ \hat{\xx}^{+}}$ is rank-indecomposable [REF-DISCUSSION] -- this condition is satisfied for generic linear maps, and hence we omit checking it.}
	
	%First we define the operators
%	\begin{eqnarray*}
%	\mathtt{A} &:=& \frac{2}{n} \sum_{j=1}^{n} ({X^{(j)}}^{\star} \boxtimes {X^{(j)}}^{\star}) \boldsymbol \botimes \cp_{\ct^{(j)}}, \\
%	\mathtt{B} &:=& \frac{1}{n} \sum_{j=1}^{n} ({X^{(j)}}^{\star} \boxtimes {X^{(j)}}^{\star}) \boldsymbol \botimes \cp_{\ct^{(j)}} [(\mathcal{L}^{\star\prime}_{\ct^{(j)}}\mathcal{L}^{\star}_{\ct^{(j)}})^{-1}]_{\ct^{(j)}} \cp_{\ct^{(j)}}.
%	\end{eqnarray*}

	As $\left\|\cp_{\ct^{(j)}} [(\mathcal{L}^{\star\prime}_{\ct^{(j)}}\mathcal{L}^{\star}_{\ct^{(j)}})^{-1}]_{\ct^{(j)}} \cp_{\ct^{(j)}} \right\|_{2} \leq 2$ from Lemma \ref{thm:weakincoherencebound}, we have $\cp_{\ct^{(j)}} [(\mathcal{L}^{\star\prime}_{\ct^{(j)}}\mathcal{L}^{\star}_{\ct^{(j)}})^{-1}]_{\ct^{(j)}} \cp_{\ct^{(j)}} \allowbreak \preceq 2\cp_{\ct^{(j)}} $, and hence $2 ({X^{(j)}}^{\star} \boxtimes {X^{(j)}}^{\star}) \botimes \cp_{\ct^{(j)}} \succeq ({X^{(j)}}^{\star} \boxtimes {X^{(j)}}^{\star}) \botimes \cp_{\ct^{(j)}} [(\mathcal{L}^{\star\prime}_{\ct^{(j)}}\mathcal{L}^{\star}_{\ct^{(j)}})^{-1}]_{\ct^{(j)}} \cp_{\ct^{(j)}}$.  Moreover, since $2 ({X^{(j)}}^{\star} \boxtimes {X^{(j)}}^{\star}) \botimes \cp_{\ct^{(j)}}$ and $ ({X^{(j)}}^{\star} \boxtimes {X^{(j)}}^{\star}) \botimes \cp_{\ct^{(j)}} [(\mathcal{L}^{\star\prime}_{\ct^{(j)}}\mathcal{L}^{\star}_{\ct^{(j)}})^{-1}]_{\ct^{(j)}} \cp_{\ct^{(j)}}$ are Kronecker products of positive semidefinite operators, they too are positive semidefinite operators, and hence $\cp_{\bbw^{\perp}} \circ (\frac{2}{n} \sum_{j=1}^{n} ({X^{(j)}}^{\star} \boxtimes {X^{(j)}}^{\star}) \botimes \cp_{\ct^{(j)}})^2 \circ \cp_{\bbw^{\perp}} \succeq \cp_{\bbw^{\perp}} \circ  (\frac{1}{n} \sum_{j=1}^{n} ({X^{(j)}}^{\star} \boxtimes {X^{(j)}}^{\star}) \botimes \cp_{\ct^{(j)}} [(\mathcal{L}^{\star\prime}_{\ct^{(j)}}\mathcal{L}^{\star}_{\ct^{(j)}})^{-1}]_{\ct^{(j)}} \cp_{\ct^{(j)}})^2 \circ	\cp_{\bbw^{\perp}}$.  This implies the bound 
	\begin{equation*}
	2 \roc \geq \left\| \cp_{\mathbb{W}^{\perp}} \circ \left(\frac{1}{n} \sum_{j=1}^{n} ({X^{(j)}}^{\star} \boxtimes {X^{(j)}}^{\star}) \botimes \cp_{\ct^{(j)}} [(\mathcal{L}^{\star\prime}_{\ct^{(j)}}\mathcal{L}^{\star}_{\ct^{(j)}})^{-1}]_{\ct^{(j)}} \cp_{\ct^{(j)}} \right) \right\|_2.
	\end{equation*}
	We combine this bound with the identity $\mathsf{L}(X) \boxtimes X = \mathsf{L} \circ (X\boxtimes X)$ to obtain
	\begin{eqnarray} \label{eq:opbound}
		& & \frac{1}{n\coveig}\biggl\| \cp_{\bbw^{\perp}} \biggl(\sum_{j=1}^{n} \biggl( \biggl[ \cp_{\ct^{(j)}} [(\mathcal{L}^{\star\prime}_{\ct^{(j)}}\mathcal{L}^{\star}_{\ct^{(j)}})^{-1}]_{\ct^{(j)}} \cp_{\ct^{(j)}} \circ \mathcal{L}^{\star\prime} \mathcal{L}^{\star} \circ \sfe^{(t)} \biggr] \bigl({X^{(j)}}^{\star}\bigr)\biggr) \boxtimes {X^{(j)}}^{\star} \biggr) \biggr\|_{\eu} \nonumber\\
		& = & \frac{1}{n\coveig} \biggl\| \biggl[\cp_{\mathbb{W}^{\perp}} \circ \biggl( \sum_{j=1}^{n} \bigl({X^{(j)}}^{\star} \boxtimes {X^{(j)}}^{\star} \bigr) \botimes \cp_{\ct^{(j)}} [(\mathcal{L}^{\star\prime}_{\ct^{(j)}}\mathcal{L}^{\star}_{\ct^{(j)}})^{-1}]_{\ct^{(j)}} \cp_{\ct^{(j)}}  \biggr) \biggr] (\mathcal{L}^{\star\prime} \mathcal{L}^{\star} \circ \sfe^{(t)}) \biggr\|_{\eu} \nonumber \\	
		& \leq & (2\roc/\coveig) \| \mathcal{L}^{\star\prime} \mathcal{L}^{\star} \circ \sfe^{(t)} \|_{\eu}  \leq (2\roc/\coveig) \|\mathcal{L}^{\star}\|_{2}^2 \|\sfe^{(t)} \|_{\eu}.
	\end{eqnarray}	
	From the definition of $\sfe^{(t+1)}$ we have the relation
	\begin{align} \label{eq:nextiterate}
	\sfe^{(t+1)} ~ = ~ & \cp_{\mathbb{W}^{\perp}} \biggl( \frac{1}{n \coveig} \sum_{j=1}^{n} [ \cp_{\ct^{(j)}} [(\mathcal{L}^{\star\prime}_{\ct^{(j)}}\mathcal{L}^{\star}_{\ct^{(j)}})^{-1}]_{\ct^{(j)}} \cp_{\ct^{(j)}} \circ \mathcal{L}^{\star\prime} \mathcal{L}^{\star} \circ \sfe^{(t)}] \bigl({X^{(j)}}^{\star}\bigr) \boxtimes {X^{(j)}}^{\star} \nonumber \\
	& + \frac{1}{n \coveig} \sum_{j=1}^{n} G^{(j)} \boxtimes {X^{(j)}}^{\star} + \sff \biggr) + \sfm.
	\end{align}
	Since $\cp_{\mathbb{W}^{\perp}}$ is a projector we have $(1/n \coveig) \| \cp_{\mathbb{W}^{\perp}} (\sum_{j=1}^{n} G^{(j)} \boxtimes {X^{(j)}}^{\star} ) \|_{\eu} \leq (1/n \coveig) \| \sum_{j=1}^{n} G^{(j)} \boxtimes {X^{(j)}}^{\star} \|_{\eu}$, and $\|\cp_{\mathbb{W}^{\perp}}(\sff)\|_{\eu} \leq \|\sff\|_{\eu}$.  Hence, by applying the bounds \eqref{eq:feucbound}, \eqref{eq:geucbound}, \eqref{eq:xxhat}, and \eqref{eq:opbound} to \eqref{eq:nextiterate}, we obtain
	\begin{eqnarray*} 
	\|\sfe^{(t+1)}\|_{\eu} & \leq & \left( (2\roc/\coveig) \|\mathcal{L}^{\star}\|_{2}^2 + \alpha_6 (\covsup/\coveig) \right) \|\sfe^{(t)} \|_{\eu} + \left( \alpha_4+\alpha_5 + 5\alpha_7^2/\sqrt{q} \right) \|\sfe^{(t)}\|_{\eu}^2 \\
	& \leq & \gamma_0\|\sfe^{(t)}\|_{\eu} + \gamma_1\|\sfe^{(t)}\|_{\eu}^{2}.
	\end{eqnarray*}
	This completes the proof.
\end{proof}



\section{Numerical Experiments} \label{sec:numexp}

%-- Analyzing Random Initialization: random can be improved, all random initializations seem to be equally good
%
%-- Comparison between Polyhedral and Semidefinite Descriptions: better descriptive power and also compare with PCA
%
%-- Utility of Learned Norms in Denoising: show denoising power

\begin{figure}
	\centering
	%	\begin{subfigure}{.5\textwidth}
	%		\centering
	%		\includegraphics[width=0.8\textwidth]{images/numberiterations_heat}
	%		\label{fig:numberiterations_heat}
	%	\end{subfigure}%
	\begin{subfigure}{.5\textwidth}
		\centering
		\includegraphics[width=0.8\textwidth]{numberiterations_line}
		\label{fig:numberiterations_graph}
	\end{subfigure}
	\caption{Average number of iterations required to identify correct regularizer as a function of the number of observations; each line represents a fixed noise level $\sigma$ denoting the amount of corruption in the initial guess (see Section \ref{sec:numexp_synthetic} for details of the experimental setup).}
	\label{fig:numberiterations}
\end{figure}

\subsection{Illustration with Synthetic Data} \label{sec:numexp_synthetic}
We begin with a demonstration of the utility of our algorithm in recovering a regularizer from synthetic data.  Our experiment qualitatively confirms the predictions of Theorem \ref{thm:localconvergence} regarding the rate of convergence.

\textbf{Setup.} We generate a standard Gaussian linear map $\L : \R^{7 \times 7} \rightarrow \R^{30}$ and we normalize it; denote the normalized version as $\L^\star$.  We generate data $\{\by^{(j)}\}_{j=1}^{1000}$ as $\by^{(j)} = \L^\star(\bu^{(j)} {\bv^{(j)}}')$, where each $\bu^{(j)}, \bv^{(j)}$ is drawn independently from the Haar measure on the unit sphere in $\R^{7}$.  We generate standard Gaussian maps $\mathcal{E}^{(i)} : \R^{7 \times 7} \rightarrow \R^{30}, ~ i=1,\dots,20$ that are used to corrupt $\L^\star$ in providing the initial guess to our algorithm.  Specifically, for each $\sigma \in \{0.125, 0.25, \dots, 2.5\}$ and each $\mathcal{E}^{(i)}, ~ i=1,\dots,20$ we supply as initial guess to our algorithm the normalized version of $\L^\star + \sigma \mathcal{E}^{(i)}$.  In addition we supply the subset $\{\by^{(j)}\}_{j=1}^m$ for each $m \in \{50, 100, \dots, 1000\}$ to our algorithm.  The objective of this experiment is to investigate the role of the number of data points (denoted by $m$) and the size of the error in the initial guess (denoted by $\sigma$) on the performance of our algorithm.

\textbf{Characterizing recovery of correct regularizer.} Before discussing the results, we describe a technique assessing whether our algorithm recovers the correct regularizer.  In particular, as we do not know of a tractable technique for computing the distance measure $\xi$ between two linear maps \eqref{eq:defn_distancebetweendicts}, we consider an alternative approach for computing the `distance' between two linear maps. For linear maps from $\R^{q \times q}$ to $\R^d$, we fix a set of unit-Euclidean-norm rank-one matrices $\left\{\bs^{(k)} {\bt^{(k)}}' \right\}_{k=1}^{\ell}$, where each $\bs^{(k)}, \bt^{(k)} \in \R^{q}$ is drawn uniformly from the Haar measure on the sphere and $\ell$ is chosen to be larger than $q^2$.  Given an estimate $\L : \R^{q \times q} \rightarrow \R^{d}$ of a linear map $\L^\star : \R^{q \times q} \rightarrow \R^d$, we compute the following
\begin{equation} \label{eq:distmeasuretwomaps}
\mathrm{dist}_{\L^\star}(\L):=\frac{1}{\ell} \sum_{k=1}^{\ell} ~ \underset{\substack{X \in \R^{q \times q} \\ \mathrm{rank}(X) \leq 1}}{\inf} \left\| \L^\star \left(\bs^{(k)} {\bt^{(k)}}'\right) - \L(X) \right\|_2^2.
\end{equation}
To compute the minimum for each term in the sum, we employ the heuristic described in Algorithm \ref{alg:svp}.  If $\L^\star$ satisfies a suitable restricted isometry condition for rank-one matrices and if $\L$ is specified as $\L^\star$ composed with a near-orthogonal rank-preserver, then we have that $\mathrm{dist}_{\L^\star}(\L) \approx 0$; in the opposite direction, as $\ell > q^2$, we have that $\mathrm{dist}_{\L^\star}(\L) \approx 0$ implies $\xi_{\L^{\star}}(\L) \approx 0$.  In our setting with $q = 7$ we set $\ell = 100$.  If our algorithm provides an estimate $\L$ such that $\mathrm{dist}_{\L^\star}(\L) < 10^{-3}$, then we declare that our method has succeeded in recovering the correct regularizer.

\begin{figure}
	\centering
	\begin{subfigure}{.5\textwidth}
		\centering
		\includegraphics[width=0.8\textwidth]{patches}
		%\caption{Image patches.}
		\label{fig:imagepatches}
	\end{subfigure}%
	\begin{subfigure}{.5\textwidth}
		\centering
		\includegraphics[width=0.62\textwidth]{TEST2}
		%\caption{Sample raw image.}
		\label{fig:samplerawimg}
	\end{subfigure}
	\caption{Image patches (left) obtained from larger raw images (sample on the right).}
	\label{fig:rawimages}
\end{figure}


\textbf{Results.} In Figure \ref{fig:numberiterations} we plot for each $\sigma \in \{0.125,0.25,\dots,2.5\}$ the average number of iterations -- taken over the $20$ different initial guesses specified by the normalized versions of $\L^\star + \sigma \mathcal{E}^{(i)}, ~ i=1,\dots,20$ -- required for Algorithm \ref{alg:amsdpreg} (with Step $1$ computed by solving \eqref{eq:varietyconstrainedopt} via Algorithm \ref{alg:svp}) to succeed in recovering the correct regularizer as a function of the number of data points $m$ supplied as input.  The different curves in the figure correspond to different noise levels (specified by $\sigma$) in the initial guess; that is, the curves higher up in the figure are associated to larger noise levels.  There are two main conclusions to be drawn from this result.  First, the average number of iterations grows as the initial guess is of increasingly poorer quality.  Second, and more interesting, is that the number of iterations required for convergence improves with an increase in the number of input data points, but only up to a certain stage beyond which the convergence rate seems to plateau (this is a feature at every noise level in this plot).  This observation confirms the predictions of Theorem \ref{thm:localconvergence} and of Proposition \ref{thm:randensemble} (specifically, see the discussion immediately following this proposition).


\subsection{Illustration with Natural Images} \label{sec:numexp_raw}

\subsubsection{Representing Natural Image Patches} \label{sec:numexp_raw_rep}

%The second experiment applies our algorithm to a dataset of natural images. Our results show that representations as projections as low-rank matrices offer better approximation power over representations as projections of sparse vectors (i.e. dictionary learning), and Principal Component Analysis (PCA), at representing the dataset.

The first stage of this experiment contrasts projections of low-rank matrices and projections of sparse vectors purely from the perspective of representing a collection of image patches.

\textbf{Setup.} We consider a dataset $\{ \by^{(j)} \}_{j=1}^{6480} \in \R^{64}$ of image patches.  This data is obtained by taking $8 \times 8$ patches from larger images of seagulls and considering these patches as well as their rotations, as is common in the dictionary learning literature; Figure \ref{fig:rawimages} gives an example of a seagull image as well as several smaller patches.  To ensure that we learned a centered and suitably isotropic norm, we center the entire dataset to ensure that the average of the $\by^{(j)}$'s is the origin and then scale each datapoint so that it has unit Euclidean norm.  We apply Algorithm \ref{alg:amsdpreg} (with Step $1$ computed by solving \eqref{eq:varietyconstrainedopt} via Algorithm \ref{alg:svp}) and the analog of this procedure for dictionary learning described in Section \ref{sec:algorithm_cdl}.  We assess the quality of the description of the dataset $\{ \by^{(j)} \}_{j=1}^{6480}$ as a projection of low-matrices (obtained using our approach) as opposed to a projection of sparse vectors (obtained using dictionary learning).


\textbf{Representation complexity.} To assess the performance of each representation framework, we require a characterization of the number of parameters needed to specify an image patch in each representation as well as the resulting quality of approximation.  Given a collection $\{\by^{(j)}\}_{j=1}^n \subset \R^d$, suppose we represent each point as $\by^{(j)} \approx \L (X^{(j)})$ for a linear map $\L : \R^{q \times q} \rightarrow \R^d$ and a rank-$r$ matrix $X^{(j)} \in \R^{q \times q}$.  The number of parameters required to specify each $X^{(j)}$ is $2qr - r^2$ and the number of parameters required to specify $\L$ is $d q^2$.  Consequently, the average number of parameters required to specify each $\by^{(j)}$ is $2qr - r^2 + \frac{dq^2}{n}$.  In a similar manner, if each $\by^{(j)} \approx L \bx^{(j)}$ for a linear map $L : \R^p \times \R^d$ and a vector $\bx^{(j)} \in \R^p$ with $s$ nonzero coordinates, the average number of parameters required to each $\by^{(j)}$ is $2s + \frac{dp}{n}$.  In each case, we assess the quality of the approximation by considering the average squared error over the entire set $\{\by^{(j)}\}_{j=1}^n$.

\textbf{Results.} We initialize both our algorithm and the dictionary learning method with random linear maps (suitably normalized in each case).  Before contrasting the two approaches we highlight the improvement in performance our method provides over a pure random linear map.  Specifically, Figure \ref{fig:progress} shows for several random initializations that our algorithm (as well as the alternating update method in dictionary learning) provides a significant refinement in approximation quality as the number of iterations increases.  Therefore, there is certainly value in employing our algorithm (even with a random initialization) to obtain better representations than pure random projections of low-rank matrices.  Next we proceed to a detailed comparison of the two representation frameworks.  We employ our approach to learn a representation of the image patch dataset with $q \in \{9, 10,\dots, 15 \}$ and the values of the rank $r$ chosen so that the overall representation complexity lies in the range $[17,33]$.  Similarly, we employ dictionary learning with $p \in \{100, 200, \dots, 1400\}$ and the values of the sparsity level $s$ chosen so that the overall representation complexity lies in the range $[17,33]$.  The left sub-plot in Figure \ref{fig:representanddenoise} gives a comparison of these two frameworks.  (To interpret the $y$-axis of the plot, note that the each data point is scaled to have unit norm.) Our approach provides an improvement over dictionary learning for small levels of representation complexity and is comparable at larger levels.

\begin{figure}
	\centering
	\begin{subfigure}{.5\textwidth}
		\centering
		\includegraphics[width=0.6\textwidth]{progress_lrm}
		%		\caption{Projections of low-rank matrices.}
		\label{fig:progress_lrm}
	\end{subfigure}%
	\begin{subfigure}{.5\textwidth}
		\centering
		\includegraphics[width=0.6\textwidth]{progress_sv}
		%		\caption{Projections of sparse vectors.}
		\label{fig:progress_sv}
	\end{subfigure}
	\caption{Progression in mean-squared error with increasing number of iterations with random initializations for learning a semidefinite regularizer (left) and a polyhedral regularizer (right).}
	\label{fig:progress}
\end{figure}

\subsubsection{Denoising Natural Image Patches} \label{sec:numexp_raw_den}

We compare the performance of polyhedral and semidefinite regularizers in denoising natural image patches corrupted by noise.

\textbf{Setup.} The $6480$ data points from the previous experiment are designated as a training set.  Here we consider an additional collection $\{\by^{(j)}_{\mathrm{test}}\}_{j=1}^{720} \subset \R^{64}$ of $8 \times 8$ test image patches obtained from larger seagull images (as with the training set), and subsequently shifted by an average of the pre-centered training set.  We corrupt each of these test points by i.i.d. Gaussian noise to obtain $\by^{(j)}_{\mathrm{obs}} = \by^{(j)}_{\mathrm{test}} + \bw^{(j)}, ~ j=1,\dots,720$, where each $\bw^{(j)} \sim \mathcal{N}(0, \sigma^2 I)$ with $\sigma^2$ chosen so that the average signal-to-noise ratio $\frac{1}{720} \sum_{j=1}^n \frac{\|\by^{(j)}_{\mathrm{test}}\|_{\ell_2}^2}{64 \sigma^2} \approx 18$.  Our objective is to investigate the denoising performance of the polyhedral and semidefinite regularizers (learned on the training set) on the dataset $\{\by^{(j)}_{\mathrm{obs}}\}_{j=1}^{720}$.  Specifically, we analyze the following proximal denoising procedure:
\begin{equation} \label{eq:proxoperator}
\hat{\by}_{\mathrm{denoise}} = \underset{\by \in \R^{64}}{\argmin} ~~~ \tfrac{1}{2} \|\by_{\mathrm{obs}} - \by\|_{\ell_2}^2 + \lambda \|\by\|,
\end{equation}
where $\|\cdot\|$ is a regularizer learned on the training set and $\lambda > 0$ is a regularization parameter.

\textbf{Computational complexity of regularizer.} To compare the performances of different regularizers, it is instructive to consider the cost associated with employing a regularizer for denoising.  In particular, the regularizers learned on the training set have unit-balls that are specified as linear images of the nuclear norm ball and the $\ell_1$ ball.  Consequently, the main cost associated with employing a regularizer is the computational complexity of solving the corresponding proximal denoising problem \eqref{eq:proxoperator}.  Thus, we analyze the normalized mean-squared denoising error $\frac{1}{720} \sum_{j=1}^n \frac{\|\by^{(j)}_{\mathrm{obs}} - \by^{(j)}_{\mathrm{denoise}}\|_{\ell_2}^2}{64 \sigma^2}$ of a regularizer as a function of the computational complexity of solving \eqref{eq:proxoperator}.  For a polyhedral norm $\|\cdot\| : \R^d \rightarrow \R$ with unit ball specified as the image under a linear map $L : \R^p \rightarrow \R^d$ of the $\ell_1$ ball in $\R^p$, we solve \eqref{eq:proxoperator} as follows by representing the norm $\|\cdot\|$ in a lifted manner:
\begin{equation} \label{eq:prox_lp}
\begin{aligned}
\hat{\by}_{\mathrm{denoise}} = \underset{\substack{\bx, \bz \in \R^p \\ s, t \in \R}}{\argmin} ~~~ & \tfrac{1}{2} s + \lambda t ~~~ \mathrm{s.t.} ~~~ \|\by_{\mathrm{obs}} - L \bx\|_{\ell_2}^2 \leq s , ~~~ \sum_{i=1}^p \bz_i \leq t , ~~~ \begin{pmatrix}\bz - \bx \\ \bz + \bx \end{pmatrix} \geq 0.
\end{aligned}
\end{equation}
To solve \eqref{eq:prox_lp} to an accuracy $\epsilon$ using an interior-point method with the usual logarithmic barriers for the nonnegative orthant and the second-order cone, we have that the number of operations required is $\sqrt{2p+2} \log\left(\frac{2p+2}{\epsilon \eta}\left((d+2p+2)^3 + (2p+2)^3\right) \right)$ for a barrier parameter $\eta$ \cite{NesNem:94,Ren:01}.  In a similar manner, for a semidefinite regularizer $\|\cdot\| : \R^d \rightarrow \R$ with unit ball specified as the image under a linear map $\L : \R^{q \times q} \rightarrow \R^d$ of the nuclear norm ball in $\R^{q \times q}$, we again solve \eqref{eq:proxoperator} as follows by representing the norm $\|\cdot\|$ in an analogous lifted manner:
\begin{equation} \label{eq:prox_sdp}
\begin{aligned}
\hat{\by}_{\mathrm{denoise}} = \underset{\substack{X \in \R^{q \times q} \\ Z_1, Z_2 \in \mathbb{S}^q \\ s, t \in \R}}{\argmin} ~~~ \tfrac{1}{2} s + \lambda t ~~~ \mathrm{s.t.} ~~~ \|\by_{\mathrm{obs}} - \L(X)\|_{\ell_2}^2 \leq s, ~ \tfrac{1}{2}\mathrm{trace}(Z_1+Z_2) \leq t, ~ \begin{pmatrix}Z_1 & X \\ X' & Z_2 \end{pmatrix} \succeq 0.
\end{aligned}
\end{equation}
As before, to solve \eqref{eq:prox_sdp} to an accuracy $\epsilon$ using an interior-point method with the usual logarithmic barriers for the positive-semidefinite cone and the second-order cone, we have that the number of operations required is $\sqrt{2q+2} \log\left(\frac{2q+2}{\epsilon \eta}\left((d+2{q \choose 2}+2)^3 + (2{q \choose 2}+2)^3\right) \right)$ for a barrier parameter $\eta$ \cite{Ren:01}.


%the the associated regularizer when   In the previous section, as the representation complexity increases, the regularizer we learn is increasingly tuned to the training set.  Moreover, the computational complexity of employing the learning regularizer .  Therefore, an instructive approach to analyzing the performance of the learned regularizers on the test set is to




\begin{figure}
	\centering
	\begin{subfigure}{.5\textwidth}
		\centering
		\includegraphics[width=0.7\textwidth]{compress}
		%	\caption{Comparison of the MSE of representing a dataset of image patches using semidefinite descriptions (red), and polyhedral descriptions (blue).}
		\label{fig:compresscompare}
	\end{subfigure}%
	\begin{subfigure}{.5\textwidth}
		\centering
		\includegraphics[width=0.7\textwidth]{denoising}
		%	\caption{Comparison of denoising performance of semidefinite representable norms (red) with polyhedral norms (blue)}
		\label{fig:denoisingfixrank}
	\end{subfigure}
	\caption{Comparison between dictionary learning (blue) and our approach (red) in representing natural image patches (left); comparison between polyhedral (blue) and semidefinite (right) regularizers in denoising natural image patches (right).}
	\label{fig:representanddenoise}
\end{figure}




\textbf{Results.} We learn semidefinite regularizers on the training set using Algorithm \ref{alg:amsdpreg} for $q \in \{9, \dots,20\}$ and for a rank of $1$.  We also learn polyhedral regularizers on the training set using dictionary learning for $p \in \{9^2, 10^2, \dots, 20^2\}$ and with corresponding sparsity levels in the range $\{\sqrt{p}-1, \sqrt{p}\}$ to ensure that the representation complexity matches the corresponding representation complexity of the images of rank-one matrices in the semidefinite case.  As the lifted dimensions $q^2$ and $p$ increase, the computational complexities of the associated proximal denoisers (with the learned regularizers) also increase.  The right sub-plot in Figure \ref{fig:representanddenoise} gives the average normalized mean-squared error over the noisy test data (generated as described above).  The optimal choice of the regularization parameter $\lambda$ for each regularizer is obtained by sweeping over a range to obtain the best denoising performance, as we have access to the underlying uncorrupted image patches $\{\by^{(j)}_{\mathrm{test}}\}_{j=1}^{720}$.  For both types of regularizers the denoising performance improves initially before degrading due to overfitting.  More significantly, given a fixed computational budget, these experiments suggest that semidefinite regularizers provide better performance than polyhedral regularizers in denoising image patches in our dataset.  The denoising operation \eqref{eq:proxoperator} is in fact a basic computational building block (often referred to as a proximal operator) in first-order algorithms for solving convex programs that arise in a range of inverse problems \cite{PB:14}.  As such, we expect the results of this section to be qualitatively indicative of the utility of our approach in other inferential tasks beyond denoising.

%The best-performing semidefinite regularizer provides marginally improved denoising performance over the best-performing polyhedral regularizer, but at a lower computational cost.  Stated differently

\section{Discussion} \label{sec:discussion}
Our paper describes an algorithmic framework for learning regularizers from data in settings in which prior domain-specific expertise is not directly available.  We learn these regularizers by computing a structured factorization of the data matrix, which is accomplished by combining techniques for the affine rank minimization problem with the Operator Sinkhorn scaling procedure.  The regularizers obtained using our method are convex and they can be computed via semidefinite programming.  Our approach may be viewed as a semidefinite analog of dictionary learning, which can be interpreted as a technique for learning polyhedral regularizers from data.  We discuss next some directions for future work:

\paragraph{Algorithmic questions:} It would be of interest to better understand the question of initialization for our algorithm.  Random initialization often works well in practice and it would be useful to provide theoretical support for this approach by building on recent work on other factorization problems \cite{GLM:16,SWQ:16c}.  There have also been efforts on data-driven strategies for initialization in dictionary learning by reducing the question to a type of clustering / community detection problem \cite{AAN:17,AGM:14}.  While the relation between clustering and estimating the elements of a finite atomic set is conceptually natural, identifying an analog of the clustering problem for estimating the image of a variety of rank-one matrices (which is a structured but infinite atomic set) is less clear; we seek such a conceptual link in order to develop an initialization strategy for our algorithm.  In a completely different direction, there is also recent work on a convex relaxation for the dictionary learning problem that avoids the difficulties associated with local minima \cite{BKS:15}; while this technique is considerably more expensive computationally in comparison with alternating updates, developing analogous convex relaxation approaches for the problem of learning semidefinite regularizers may subsequently point the way to efficient global techniques that are different from alternating updates.

\begin{figure}
	\centering
	\begin{subfigure}{.5\textwidth}
		\centering
		\includegraphics[width=0.7\textwidth]{sv_yy}
		%	\caption{Gram matrix of $400$ points in $\R^{500}$ generated as a random projection of sparse vectors in $\R^{900}$.}
		\label{fig:sparsegram}
	\end{subfigure}%
	\begin{subfigure}{.5\textwidth}
		\centering
		\includegraphics[width=0.7\textwidth]{lrm_yy}
		%	\caption{Gram matrix of $400$ points in $\R^{500}$ generated as a random projection of rank-one matrices in $\R^{30 \times 30}$.}
		\label{fig:lowrankgram}
	\end{subfigure}
	\caption{Gram matrices of images of sparse vectors (left) and low-rank matrices (right).}
	\label{fig:gram}
\end{figure}

\paragraph{Approximation-theoretic questions:} The focus of our paper has been on the algorithmic aspects of learning semidefinite regularizers from data.  It is of interest to investigate the power of finite atomic sets in comparison with atomic sets specified as images of determinantal varieties from a harmonic analysis perspective (for a fixed representation complexity; see Section \ref{sec:numexp_raw_rep} for a discussion on how these are defined).  For example, what types of data are better described using one representation framework versus the other?  As a simple preliminary illustration, we generate two sets of $400$ points in $\R^{500}$, with the first set being a random projection of sparse vectors in $\R^{900}$ and the second set being a random projection of rank-one matrices in $\R^{900}$ of the form $( \cdots ~ \cos(2 \pi \alpha_j t_i), ~ \sin(2 \pi \alpha_j t_i), ~ \cdots)' ~ ( \cdots ~ \cos(2 \pi \beta_j t_i), ~ \sin(2 \pi \beta_j t_i), ~ \cdots)$ for randomly chosen frequencies $\alpha_j, \beta_j$; the representation complexities of both these sets is the same.  Figure \ref{fig:gram} gives the Gram matrices associated with these datasets.  The dataset of images of sparse vectors appears to consist of `clustered' of `blocked' structure, while the dataset of images of low-rank matrices appears to consist of smoother `toroidal' structure.  We seek a better understanding of this phenomenon by analyzing the relative strengths of representations based on finite atomic sets versus images of low-rank matrices.  In a different direction, it is also of interest to explore other families of infinite atomic sets that yield tractable regularizers in other conic programming frameworks.  Specifically, dictionary learning and our approach provide linear and semidefinite programming regularizers, but there are other families of computationally efficient convex cones such as the power cone and the exponential cone; learning atomic sets that are amenable to optimization in these frameworks would lead to a broader suite of data-driven approaches for identifying regularizers.

%finite atomic sets versus images determinantal of representations?scheme th which of these two comrelative merits of approximating a dataset using a finite

%-- Algorithmic questions: initialization strategies, convex procedures for factorization
%
%-- Representation questions: other types of lifted representations via projections of other simple sets, consider true extension complexity
%
%-- Approximation-theoretic questions: comparison of approximation-theoretic power of sparse versus determinantal varieties

%\newpage

\section*{Appendix}
\appendix

\section{Proofs of Lemma \ref{thm:weakincoherencebound} and Lemma \ref{thm:boundondelta}} \label{apx:prelims}

%\noindent \textbf{Definition.} We use the following shorthand: given a linear map $\mathcal{L}:\mathbb{R}^{q \times q}\mapsto \mathbb{R}^{d}$ and a subspace $\ct$ we define $\mathcal{L}_{\ct} := \mathcal{L} \circ \cp_{\ct}$.

%\begin{proposition}[Near isometry on tangent spaces] \label{thm:nearisometryonrestrictedsubspaces} Let $X$ be a rank-$r$ matrix, and let $\ct:= \ct(X)$ be the tangent space with respect to the variety of matrices with rank at most $r$ at $X$. Suppose $\mathcal{L}$ obeys the RIP with constant $\delta_{2r} <1$. Then (i) $1-\delta_{2r} \leq \lambda(\mathcal{L}_{\ct}^{\prime} \mathcal{L}_{\ct}) \leq 1+ \delta_{2r}$, and (ii) $\mathcal{L}_{\ct}^{\prime} \mathcal{L}_{\ct} (X) = X + Z$ where $\|Z \|_F \leq \delta_{2r} \|X \|_F$.
%\end{proposition}

%\begin{proof}[Proof of Proposition \ref{thm:nearisometryonrestrictedsubspaces}]

%\end{proof}

%\noindent \textbf{Definition.} Suppose $\mathcal{L}$ is a linear map that satisfies the RIP with $\delta_{2r}  <1$, and let $\ct := \ct(X)$, where $\mathrm{rank}(X) \leq r$. Let $\cdot|_{\ct}$ be the restriction map to the subspace $\ct$. By Proposition \ref{thm:weakincoherencebound} the map $\mathcal{L}_{\ct}^{\prime} \mathcal{L}_{\ct}|_{\ct}$ is invertible. We define $[(\mathcal{L}_{\ct}^{\prime} \mathcal{L}_{\ct})^{-1}] : \mathbb{R}^{q\times q } \mapsto \mathbb{R}^{q\times q}$ as the \emph{embedding} of $(\mathcal{L}_{\ct}^{\prime} \mathcal{L}_{\ct}|_{\ct})^{-1}$, that is, $[(\mathcal{L}_{\ct}^{\prime} \mathcal{L}_{\ct})^{-1}]$ is the unique linear map satisfying
%\begin{enumerate}
%	\item $[(\mathcal{L}_{\ct}^{\prime} \mathcal{L}_{\ct})^{-1}] = \cp_{\ct} \circ [(\mathcal{L}_{\ct}^{\prime} \mathcal{L}_{\ct})^{-1}] \circ \cp_{\ct}$,
%	\item $( \mathcal{L}_{\ct}^{\prime} \mathcal{L}_{\ct}|_{\ct}) ([(\mathcal{L}_{\ct}^{\prime} \mathcal{L}_{\ct})^{-1}] (X)|_{\ct}) = X|_{\ct}$ for all $X \in \ct$.
%\end{enumerate}

\begin{proof}[Proof of Lemma \ref{thm:weakincoherencebound}]
	Note that if $X \in \ct$ then $X$ has rank at most $2r$. As a consequence of the restricted isometry property we have $(1-\delta_{2r}) \|X \|_F^2 \leq \| [\L \circ \cp_{\ct}] (X)\|_2^2 \leq (1+\delta_{2r}) \|X \|_F^2$. Since $X \in \ct$ is arbitrary we have $1 - \delta_{2r} \leq \lambda(\mathcal{L}_{\ct}^{\prime} \mathcal{L}_{\ct}) \leq 1+\delta_{2r}$, which proves (i).  This immediately implies the bound in (ii).  Moreover since $\|\mathcal{L} \circ \cp_{\ct}\|_2 = \|\cp_{\ct} \circ \L^{\prime} \L \circ \cp_{\ct} \|_2^{1/2} \leq \sqrt{1+\delta_{2r}}$, we have $\|\cp_{\ct} \circ \mathcal{L}^{\prime} \mathcal{L}\|_{2} \leq \sqrt{1+\delta_{2r}}\|\mathcal{L}\|_2 $, which is (iii). Last we have $\| \cp_{\ct}[(\mathcal{L}_{\ct}^{\prime} \mathcal{L}_{\ct})^{-1}]_{\ct} \cp_{\ct} \circ \mathcal{L}^{\prime} \mathcal{L}\|_{2} \leq \|\cp_{\ct} [(\mathcal{L}_{\ct}^{\prime} \mathcal{L}_{\ct})^{-1}]_{\ct} \cp_{\ct} \|_2 \| \cp_{\ct} \circ \mathcal{L}^{\prime} \mathcal{L}\|_{2} \leq \frac{\sqrt{1+\delta_{2r}}}{1-\delta_{2r}}\|\mathcal{L}\|_2$, which proves (iv).
\end{proof}

\begin{proof}[Proof of Lemma \ref{thm:boundondelta}]
	To simplify notation we omit $\left(\left\{X^{(j)}\right\}_{j=1}^{n}\right)$.  Since $\mathrm{trace}\left(\mathsf{\Sigma}\right) = \frac{1}{n}\sum_{j=1}^{n}\|X^{(j)}\|_F^2$, we have $\alpha_1 \leq \mathrm{trace}\left(\mathsf{\Sigma}\right) \leq \alpha_2$.  Next we have the inequalities $\left(\coveig  - \covsup \right) \sfi \preceq \mathsf{\Sigma} \preceq \left(\coveig + \covsup  \right) \sfi$.  The result follows by applying trace.
\end{proof}

%\emph{Proof of Lemma \ref{thm:weakincoherencebound}} Note that if $X \in \ct$ then $X$ has rank at most $2r$. By applying the restricted isometry condition we have $(1-\delta_{2r}) \|X \|_F^2 \leq X^{\prime} \mathcal{L}^{\prime}_{\ct} \mathcal{L}_{\ct} X \leq (1+\delta_{2r}) \|X \|_F^2$. Since $X$ is arbitrary we have $1 - \delta_{2r} \leq \lambda(\mathcal{L}_{\ct}^{\prime} \mathcal{L}_{\ct}) \leq 1+\delta_{2r}$, which proves (i).  This immediately implies the bound in (ii).  Moreover since $\|\mathcal{L}_{\ct}^{\prime}\|_2 \leq \sqrt{1+\delta_{2r}}$, we have $\| \mathcal{L}^{\prime}_{\ct} \mathcal{L}\|_{2} \leq \|\mathcal{L}\|_2 \sqrt{1+\delta_{2r}}$, which is (iii). Last we have $\| \cp_{\ct}[(\mathcal{L}_{\ct}^{\prime} \mathcal{L}_{\ct})^{-1}]_{\ct} \cp_{\ct} \circ \mathcal{L}^{\prime} \mathcal{L}\|_{2} \leq \|\cp_{\ct} [(\mathcal{L}_{\ct}^{\prime} \mathcal{L}_{\ct})^{-1}]_{\ct} \cp_{\ct} \|_2 \| \mathcal{L}^{\prime}_{\ct} \mathcal{L}\|_{2} \leq \frac{\sqrt{1+\delta_{2r}}}{1-\delta_{2r}}\|\mathcal{L}\|_2$, which proves (iv). $\endproof$

%\input{appendix_main}

\section{Proof of Proposition \ref{thm:randensemble}} \label{apx:randensemble}

In this section we prove that the ensemble of random matrices $\bigl\{X^{(j)}\bigr\}_{j=1}^{n}$ described in Proposition \ref{thm:randensemble} satisfy the deterministic conditions in Theorem \ref{thm:localconvergence} with high probability.  We begin with computing $\mathbb{E}_{\mathcal{D}} [X^{(j)} \boxtimes X^{(j)}]$, and $\mathbb{E}_{\mathcal{D}} [(X^{(j)} \boxtimes X^{(j)}) \botimes \cp_{\ct(X^{(j)})}]$.  Note that the random matrices $\{X^{(j)} \boxtimes X^{(j)}\}_{j=1}^{n}$ and the random operators $\{(X^{(j)} \boxtimes X^{(j)}) \botimes \cp_{\ct(X^{(j)})}\}_{j=1}^{n}$ are almost surely bounded above in spectral norm by construction.  This allows us to conclude Proposition \ref{thm:randensemble} with an application of the Matrix Hoeffding Inequality \cite{Tro:12}.


To simplify notation we adopt the following.  In the first two results we omit the superscript $j$ from $X^{(j)}$.  In the remainder of the section we let $\mathbb{E}=\mathbb{E}_{\mathcal{D}}$, $\bar{s}^2 := \mathbb{E} [s^2]$, $\{\be_i\}_{i=1}^{q} \subset \R^q$ be the set of standard basis vectors, and $\{E_{ij}\}_{i,j=1}^{q} \subset \mathbb{R}^{q\times q}$ be the set of matrices whose $(i,j)$-th entry is $1$ and is $0$ everywhere else.


\begin{proposition}\label{thm:xxmean}
	Suppose $X\sim\mathcal{D}$ as described in Proposition \ref{thm:randensemble}.  Then $\mathbb{E}[X \boxtimes X] = \bar{s}^2 (r/q^2) \sfi$.
\end{proposition}


\begin{proof}
	It suffices to show that $\mathbb{E} \langle X \boxtimes X,\be_w \be_x^{\prime} \boxtimes \be_y \be_z^{\prime} \rangle = \mathbb{E} \langle X, \be_w \be_x^{\prime}\rangle \langle X, \be_y \be_z^{\prime}\rangle = \delta_{wy} \delta_{xz} \bar{s}^2 (r/q^2)$. Let $X = \sum_{i=1}^{r} s_{i} \bu_{i} \bv_{i}^{\prime}$ as described in the statement of Proposition \ref{thm:randensemble}. Suppose we denote $\bu_{i} = (u_{i1},\ldots,u_{iq})^{\prime}$, and $\bv_{i} = (v_{i1},\ldots,v_{iq})^{\prime}$. By applying independence we have $\mathbb{E} \langle X, \be_w \be_x^{\prime}\rangle \langle X, \be_y \be_z^{\prime}\rangle = \mathbb{E}[(\sum_{i=1}^{r} s_i u_{iw} v_{ix})(\sum_{k=1}^{r} s_k u_{ky} v_{kz})] = \sum_{i,k=1}^{r} \mathbb{E}[s_{i}s_{k}] \mathbb{E}[u_{iw}u_{ky}] \mathbb{E}[v_{ix}v_{kz}]$.  There are two cases we need to consider.
	
	\noindent [Case $w \neq y$ or $x \neq z$]: Without loss of generality suppose that $w \neq y$. Then $\mathbb{E}[u_{iw}u_{ky}]=0$ for all $1\leq i,k \leq q$, and hence $\mathbb{E} \langle X \boxtimes X,\be_w \be_x^{\prime} \boxtimes \be_y \be_z^{\prime} \rangle =0$.
	
	\noindent [Case $w=y$ and $x=z$]: Note that if $i \neq k$ then $\mathbb{E}[u_{iw}u_{ky}]=\mathbb{E}[u_{iw}] \mathbb{E}[u_{ky}]=0$.  Since $\bu_i$ is a unit-norm vector distributed u.a.r., we have $\mathbb{E}[u_{ix}^2] = 1/q$. Hence $\mathbb{E} \langle X \boxtimes X,\be_w \be_x^{\prime} \boxtimes \be_y \be_z^{\prime} \rangle = \sum_{i=1}^{r} \mathbb{E}[s_{i}^2] \mathbb{E}[u_{iw}^2] \mathbb{E}[v_{ix}^2] = \bar{s}^2 r /q^2$. 
\end{proof}


Our next result requires the definition of certain subspaces of $\R^{q\times q}$ and $\mathbb{L}(\R^{q\times q})$.

We define the following subspaces in $\mathbb{R}^{q\times q}$: Let $\mathbb{S}:=\{W : W = W^{\prime}, W \in I^{\perp} \}$ be the subspace of symmetric matrices that are orthogonal to the identity, and $\mathbb{A}:=\{W : W = -W^{\prime}\}$ be the subspace of skew-symmetric matrices. It is clear that $\mathbb{R}^{q\times q} = \mathrm{Span}(I) \oplus \mathbb{S} \oplus \mathbb{A}$.

In addition to the subspace $\mathbb{W}$ defined in \eqref{eq:tangentspaceati}, we define the following subspaces in $\mathbb{L}(\R^{q\times q})$:
\begin{enumerate}
	\item $\mathbb{W}_{SS} := \mathrm{Span}(\{A \botimes B : A,B \in \mathbb{S} \})$,
	\item $\mathbb{W}_{AA} := \mathrm{Span}(\{A \botimes B : A,B \in \mathbb{A} \})$,
	\item $\mathbb{W}_{SA} := \mathrm{Span}(\{A \botimes B : A \in \mathbb{S}, B \in \mathbb{A} \})$,
	\item $\mathbb{W}_{AS} := \mathrm{Span}(\{A \botimes B : A \in \mathbb{A}, B \in \mathbb{S} \})$.
\end{enumerate}
Note that $\mathbb{L}(\R^{q\times q}) = \mathbb{W} \oplus \mathbb{W}_{SS} \oplus \mathbb{W}_{AA} \oplus \mathbb{W}_{SA} \oplus \mathbb{W}_{AS}$.  To verify this, first express an arbitrary linear map $\sfe \in \mathbb{L}(\R^{q\times q})$ as a sum of Kronecker products $\sfe = \sum_{i=1} A_i \botimes B_i$, second decompose each matrix $A_i,B_i$ into components in the subspaces $\{ \mathrm{Span}(I), \mathbb{S}, \mathbb{A} \}$, and third expand the expression.  The orthogonality between subspaces is immediate from the identity $\langle A_i \botimes B_i , A_j \botimes B_j \rangle  = \langle A_i , A_j \rangle \langle B_i, B_j \rangle$.


\begin{proposition}\label{thm:expectationofgiantoperator}
	Suppose $X\sim\mathcal{D}$ as described in Proposition \ref{thm:randensemble}.  Then
	\begin{equation*}
		\mathbb{E} [(X \boxtimes X) \botimes \cp_{\ct(X)}] = c_{\mathbb{W}} \sfi_{\mathbb{W}} + c_{\mathbb{W}_{SS}} \sfi_{\mathbb{W}_{SS}} + c_{\mathbb{W}_{AA}} \sfi_{\mathbb{W}_{AA}} + c_{\mathbb{W}_{SA}} \sfi_{\mathbb{W}_{SA}} + c_{\mathbb{W}_{AS}} \sfi_{\mathbb{W}_{AS}},
	\end{equation*} 
	where (i) $c_{\mathbb{W}} = \bar{s}^2 r(\frac{1}{q^2})$, (ii) $c_{\mathbb{W}_{SS}} = \bar{s}^2 r(\frac{1}{q^2} - \frac{(q-r)^2}{(q-1)^2(q+2)^2}) $, (iii) $c_{\mathbb{W}_{AA}} = \bar{s}^2 r(\frac{1}{q^2}- \frac{(q-r)^2}{q^2 (q-1)^2} )$,	and (iv) $c_{\mathbb{W}_{SA}} = c_{\mathbb{W}_{AS}} = \bar{s}^2 r(\frac{1}{q^2} - \frac{(q-r)^2}{q(q-1)^2(q+2)})$.
\end{proposition}


\begin{proof}
	The proof consists two parts, namely (i) to prove that the mean, when restricted to the respective subspaces described above, has diagonal entries as specified, and (ii) to prove that the off-diagonal elements are zero with respect to any basis that obeys the specified decomposition of $\mathbb{L}(\R^{q\times q})$.  In addition, it suffices to only consider linear maps that are Kronecker products since these maps generate the respective subspaces.  The following identity for all matrices $A_i,B_i,A_j,B_j$ is particularly useful
	\begin{equation}\label{eq:giantopinnp}
		\langle (A^{\prime}_i \botimes B_i) \boxtimes (A_j^{\prime} \botimes B_j) ,  \mathbb{E} [(X \boxtimes X) \botimes \cp_{\ct(X)}]\rangle = \mathbb{E}  \langle \cp_{\ct(X)} (B_j X A_j), \cp_{\ct(X)}(B_i X A_i) \rangle .
	\end{equation}
	One may equivalently describe the distribution of $X$ as follows -- let $X = U \Sigma_R V^{\prime}$, where $U,V$ are $q\times q$ matrices drawn from the Haar measure, and $\Sigma_R$ is a diagonal matrix whose first $r$ entries are drawn from $\mathcal{D}$, and the remaining entries are $0$ (to simplify notation we omit the dependence on $X$ in the matrices $U,V$).  Let $I_N = \mathrm{diag}(0,\ldots, 0, 1,\ldots ,1)$ be a diagonal matrix consisting of $q-r$ ones. Under this notation, the projector is simply the map $\cp_{\ct(X)} (Z) = Z - U I_N U^{\prime}ZV I_N V^{\prime} $.  The remainder of the proof is divided into the two parts outlined above.
	
	\noindent [Part (i)]: The restriction to diagonal entries correspond to the case $i=j$, and hence equation \eqref{eq:giantopinnp} simplifies to $\mathbb{E}  [\| \cp_{\ct(X)} (B X A) \|^2_F ]$.  Consequently we have
	\begin{equation*}
		\mathbb{E}  [\| \cp_{\ct(X)} (B X A) \|^2_F ] = \mathbb{E} [\|B U \Sigma_R V^{\prime} A \|^2_F ] - \mathbb{E} [ \| I_N U^{\prime} A U \Sigma_R V^{\prime} B V I_N \|_F^2  ].
	\end{equation*}
	
	First we compute $\mathbb{E} [ \| I_N U^{\prime} A U \Sigma_R V^{\prime} B V I_N \|_F^2  ]$. By the cyclicity of trace and iterated expectations we have
	\begin{eqnarray*}
		\mathbb{E} [ \| I_N U^{\prime} A U \Sigma_R V^{\prime} B V I_N \|_F^2  ] & = & \mathbb{E}[ \mathrm{trace} ( \Sigma_R^{1/2} U^{\prime} A^{\prime} U I_N U^{\prime} A U \Sigma_R V^{\prime} B V I_N  V^{\prime} B^{\prime} V \Sigma_R^{1/2})  ] \\
		& = & \mathbb{E}_{V}[ \mathbb{E}_{U} [\mathrm{trace} ( \Sigma_R^{1/2} U^{\prime} A^{\prime} U I_N U^{\prime} A U \Sigma_R V^{\prime} B V I_N  V^{\prime} B^{\prime} V \Sigma_R^{1/2})]].
	\end{eqnarray*}
	It suffices to compute $\mathbb{E} [\Sigma_R^{1/2} V^{\prime} B V I_N  V^{\prime} B^{\prime} V \Sigma_R^{1/2}]=\Sigma_R^{1/2} \mathbb{E} [ V^{\prime} B V I_N  V^{\prime} B^{\prime} V] \Sigma_R^{1/2}$ in the three cases corresponding to $B \in \{ \mathrm{Span}(I), \mathbb{S}, \mathbb{A} \}$ respectively.  By linearity and symmetry, it suffices to compute $\mathbb{E} [V^{\prime} B V E_{11}  V^{\prime} B^{\prime} V]$.  We split this computation into the following three separate cases.
	
	[Case $B \in \mathrm{Span}(I)$]:
	We have $I_N \Sigma_R^{1/2} = 0$, and hence the mean is the zero-matrix.  
	
	[Case $B \in \mathbb{A}$]:	
	Claim: If $B \in \mathbb{A}$, and $\|B\|_F = 1$, then $\mathbb{E} [V^{\prime} B V E_{11}  V^{\prime} B^{\prime} V] = (I- E_{11})/(q(q-1))$.
	
	Proof: We begin by noting that the off-diagonal entries are zero since $\mathbb{E} \langle E_{ij}, V^{\prime} B V E_{11}  V^{\prime} B^{\prime} V\rangle = \mathbb{E} (\bv_1^{\prime} B \bv_i)(\bv_1^{\prime} B \bv_j) = 0$ whenever $i \neq j$, as one of the indices $i,j$ appears exactly once (here we write $V=[\bv_1 | \ldots | \bv_q]$). By a symmetry argument we conclude that $\mathbb{E} [V^{\prime} B V E_{11}  V^{\prime} B^{\prime} V]  = \alpha I + \beta E_{11}$ for some $\alpha,\beta$. First we have $\mathbb{E} [ \mathrm{trace}(V^{\prime} B V E_{11}  V^{\prime} B^{\prime} V )] = \mathbb{E} [ \mathrm{trace}(B V E_{11} V^{\prime} B^{\prime})] = \mathrm{trace}(B \mathbb{E} [ V E_{11} V^{\prime}] B^{\prime}) = \mathrm{trace}(B (I/q) B^{\prime}) = 1/q$, which gives $\alpha q + \beta = 1/q$. Second since $B$ is asymmetric, $V^{\prime} B V$ is also asymmetric and hence is $0$ on the diagonals. Thus $\langle V^{\prime} B V E_{11}  V^{\prime} B^{\prime} V, E_{11} \rangle = 0$, which gives $\alpha + \beta = 0$. The two equations yields the values of $\alpha$ and $\beta$.
	
	[Case: $B \in \mathbb{S}$]:
	Claim: If $B \in \mathbb{S}$, and $\|B\|_F = 1$, then $\mathbb{E} [V^{\prime} B V E_{11}  V^{\prime} B^{\prime} V] = (I + (1-2/q) E_{11})/((q-1)(q+2))$.		
	
	Proof: With an identical argument as the previous claim one can show that $\mathbb{E} [V^{\prime} B V E_{11}  V^{\prime} B^{\prime} V] = \alpha I + \beta E_{11}$, where $\alpha q + \beta = 1/q$. Next $\mathbb{E} [ \langle V^{\prime} B V E_{11}  V^{\prime} B^{\prime} V, E_{11} \rangle ] = \mathbb{E} [(\bv_1^{\prime} B \bv_1)^2]$, where $\bv_1$ is a unit-norm vector distributed u.a.r. Since conjugation by orthogonal matrices preserves trace, and $\bv_1$ has the same distribution as $Q\bv_1$ for any orthogonal $Q$, we may assume that $B = \mathrm{diag}(b_{11},\ldots, b_{qq})$ is diagonal without loss of generality. Suppose we let $\bv_1 = (v_1,\ldots,v_q)^{\prime}$. Then $\mathbb{E} [(\bv_1^{\prime} B \bv_1)^2] = \mathbb{E} [ \sum b^2_{ii} v_{i}^4 + \sum_{i\neq j} b_{ii} b_{jj} v_{i}^2 v_{j}^2 ] = \mu_1 (\sum b_{ii}^2) + \mu_2 (\sum_{i \neq j} b_{ii} b_{jj} ) $, where $\mu_1 = \mathbb{E} [v_1^4]$, and $\mu_2 = \mathbb{E} [v_1^2 v_2^2]$. Since $\mathrm{trace}(B) = 0$, we have $\sum b_{ii}^2 = - \sum_{i \neq j} b_{ii} b_{jj}$. Last from Theorem 2 of \cite{Cho:13} we have $\mu_1 = 3 / (q (q+2))$, and $\mu_2 = 1/(q(q+2))$, which gives $\mathbb{E} [(\bv_1^{\prime} B \bv_1)^2] = 2/ (q(q+2))$, and hence $\alpha + \beta = 2/(q(q+2))$. The two equations yields the values of $\alpha$ and $\beta$.
	
	With a similar set of computations one can show that $\mathbb{E} [\|B U \Sigma_R V^{\prime} A \|^2_F ] = \bar{s}^2 r/q^2$ for arbitrary unit-norm $A,B$.  An additional set of simple computations will yield the diagonal entries, which completes the proof.  We omit these computations.
	
	\noindent [Part (ii)]: We claim that it suffices to show that $\mathbb{E} [ V^{\prime} A_i V E_{11} V^{\prime} A_j^{\prime} V] $ is the zero-matrix whenever $A_i,A_j \in \{ \mathrm{Span}(I), \mathbb{S}, \mathbb{A} \}$, and satisfy $\langle A_i, A_j \rangle = 0$.  We show how this proves the result.  Suppose $A_i \botimes B_i, A_j \botimes B_j$ satisfy $\langle A_i \botimes B_i , A_j \botimes B_j \rangle = \langle A_i, A_j \rangle \langle B_i, B_j \rangle = 0$.  Without loss of generality we may assume that $\langle A_i, A_j \rangle = 0$.  From equation \eqref{eq:giantopinnp} we have
	\begin{eqnarray*}
		\mathbb{E}  \langle \cp_{\ct(X)} (B_j X A_j), \cp_{\ct(X)}(B_i X A_i) \rangle & = & \mathbb{E} [ \mathrm{trace}( A_j^{\prime} V \Sigma_R U^{\prime} B_j^{\prime} B_i U \Sigma_R V^{\prime} A_i ) ] \\
		& - & \mathbb{E}[ \mathrm{trace} ( A^{\prime}_j V \Sigma_R U^{\prime} B_j U I_N U^{\prime} B_i U \Sigma_R V^{\prime} A_i V I_N V^{\prime} )].
	\end{eqnarray*}
	By cyclicity of trace and iterated expectations we have
	\begin{eqnarray*}
		&& \mathbb{E}[ \mathrm{trace} ( A^{\prime}_j V \Sigma_R U^{\prime} B_j U I_N U^{\prime} B_i U \Sigma_R V^{\prime} A_i V I_N V^{\prime} )] \\
		& =& \mathbb{E}_{U}[ \mathrm{trace} (\Sigma_R^{1/2} U^{\prime} B_j U I_N U^{\prime} B_i U \Sigma_R^{1/2} (\mathbb{E}_{V} [ \Sigma_R^{1/2} V^{\prime} A_i V I_N V^{\prime} A^{\prime}_j V \Sigma_R^{1/2} ]))] = 0,
	\end{eqnarray*}
	which proves part (ii) of the proof.  It leaves to prove the claim.  We do so by verifying that the matrix $\mathbb{E} [ V^{\prime} A_i V E_{11} V^{\prime} A_j^{\prime} V] $ is $0$ in every coordinate, which is equivalent to showing that $\mathbb{E} (\bv^{\prime}_{m} A_i \bv_1) (\bv^{\prime}_{n} A_j \bv_1) =0$ for all $m,n$.  There are three cases.
	
	[Case $m \neq n$]: Without loss of generality suppose that $m \neq 1$.  Then $\mathbb{E} (\bv^{\prime}_{m} A_i \bv_1) (\bv^{\prime}_{n} A_j \bv_1) = \mathbb{E}[\mathbb{E} [ (\bv^{\prime}_{m} A_i \bv_1) (\bv^{\prime}_{n} A_j \bv_1)|\bv_1,\bv_n]] = 0$.
	
	[Case $m = n = 1$]: We divide into further sub-cases depending on the subspaces $A_i,A_j$ belong to. If $A_i \in \mathbb{A}$ then $\bv_1^{\prime}A_i \bv_1 = 0$ since it is a scalar. Hence we eliminate the case where either matrix is in $\mathbb{A}$. Since $\langle A_i,A_j \rangle = 0$ it cannot be that both $A_i,A_j \in \mathrm{Span}(I)$. Suppose that $A_i = I / \sqrt{q}$ and $A_j \in \mathbb{S}$. Then $\mathbb{E} [ (\bv_1^{\prime}A_i \bv_1) (\bv_1^{\prime}A_j \bv_1)] = \mathbb{E} [ (\bv_1^{\prime}A_j \bv_1)]/\sqrt{q} = \mathbb{E} [ \mathrm{trace}(A_j \bv_1 \bv^{\prime}_1)]/\sqrt{q}=0$. Our remaining case is when $A_i,A_j \in \mathbb{S}$, and $\langle A_i,A_j \rangle = 0$. As before we let $\bv_1 = (v_1,\ldots,v_q)^{\prime}$. Then
	\begin{eqnarray*}
		& & \mathbb{E} [ (\bv^{\prime}_{1} A_i \bv_1) (\bv^{\prime}_1 A_j \bv_1)]  = \mathbb{E} [\sum_{pqrs} A_{i,pq} A_{j,rs} v_p v_q v_r v_s] \\
		& = & \sum_{p} A_{i,pp} A_{j,pp} \mathbb{E}[v_p^4] + \sum_{p\neq r} A_{i,pp} A_{j,rr} \mathbb{E}[v_p^2 v_r^2] + 2 \sum_{p\neq q} A_{i,pq} A_{j,pq}\mathbb{E}[v_p^2 v_q^2],
	\end{eqnarray*}
	where in the second equality we used the fact that $A_i,A_j$ are symmetric to obtain a factor of $2$ in the last term. Next we apply the relations $\mathbb{E}[v_p^4] = 3/(q(q+2))$, $ \mathbb{E}[v_p^2 v_r^2] = 1/(q(q+2))$, as well as the relations $0=\langle A_i, I \rangle \langle A_j, I\rangle=\sum_{p} A_{i,pp}A_{j,pp} + \sum_{p\neq r} A_{i,pp}A_{j,rr}$, and $0=\langle A_i, A_j \rangle = \sum_{p} A_{i,pp}A_{j,pp} + \sum_{p\neq q} A_{i,pq}A_{j,pq}$ to conclude that the mean is zero.
	
	[Case $m = n \neq 1$]: We have
	\begin{eqnarray*}
		\mathbb{E} [ (\bv^{\prime}_{m} A_i \bv_1) (\bv^{\prime}_m A_j \bv_1)] & = & \mathbb{E}[\mathbb{E} [ \mathrm{trace}(A_i \bv_1 \bv_1^{\prime} A_j^{\prime} \bv_m \bv^{\prime}_m)]|\bv_1] \\
		& = & \mathbb{E}[ \mathrm{trace}( A_i \bv_1 \bv_1^{\prime} A_j^{\prime} (I-\bv_1 \bv_1^{\prime})/(q-1))|\bv_1] \\
		& = & \mathbb{E} [ \mathrm{trace}( A_i \bv_1 \bv_1^{\prime} A_j^{\prime} /(q-1))] = \mathbb{E} [ \mathrm{trace}( A_i I A_j^{\prime} / (q(q-1)))] = 0,
	\end{eqnarray*}
	where the first equality applies the fact that, conditioned on $\bv_1$, $\mathbb{E}[\bv_m \bv^{\prime}_m]$ is the identity matrix in the subspace $\ct(\bv_1 \bv_1^{\prime})^{\perp}$ suitably scaled, and the second inequality applies the previous case.
\end{proof}


\begin{proof}[Proof of Proposition \ref{thm:randensemble}]	
	First we have $\mathsf{0} \preceq X^{(j)} \boxtimes X^{(j)} \preceq s^2 r \sfi$.  By Proposition \ref{thm:xxmean} we have $\mathbb{E}[X^{(j)} \boxtimes X^{(j)}] = (\bar{s}^2 r/q^2) \sfi$.  Since $(X^{(j)} \boxtimes X^{(j)} - (\bar{s}^2 r/q^2) \sfi)^2 \preceq s^4 r^2 \sfi$, we have $ \mathbb{P} ( \|(1/n)\sum_{i=1}^{n} X^{(j)} \boxtimes X^{(j)} - (\bar{s}^2 r/q^2)\sfi \| > t r s^2 ) \leq 2q \exp(- t^2 n / 8)$ via an application of the Matrix Hoeffding inequality (Theorem 1.3 in \cite{Tro:12}).
	
	
	Second we have $\|X^{(j)} \boxtimes X^{(j)}\|_2 \leq s^2 r$, and $\| \cp_{\ct(X^{(j)})}\|_2 = 1$, and hence $(X^{(j)} \boxtimes X^{(j)}) \botimes \cp_{\ct(X^{(j)})} \preceq s^2 r \sfi \botimes \sfi =: s^2 r \mathtt{I}$. From Proposition \ref{thm:expectationofgiantoperator} we have
	\begin{equation*}
		\mathbb{E}[ (X^{(j)} \boxtimes X^{(j)}) \botimes \cp_{\ct(X^{(j)})} ] \preceq \frac{\bar{s}^2 r}{q^2} \mathtt{I}_{\bbw} + \frac{16 \bar{s}^2 r^2}{q^3} \mathtt{I}_{\bbw^{\perp}}.
	\end{equation*}
	Since $((X^{(j)} \boxtimes X^{(j)}) \botimes \cp_{\ct(X^{(j)})} - r \mathtt{I})^2 \preceq s^4 r^2 \mathtt{I}$ we have 
	\begin{equation*}
		\mathbb{P} \left( \lambda_{\max} \left( \frac{1}{n} \sum_{i=1}^{n}  (X^{(j)} \boxtimes X^{(j)}) \botimes \cp_{\ct(X^{(j)})}- \mathbb{E} [(X^{(j)} \boxtimes X^{(j)}) \botimes \cp_{\ct(X^{(j)})}] \right) \geq t r s^2 \right) \leq q \exp( - t^2 n / 8) 
	\end{equation*}
	by an application of the Matrix Hoeffding inequality.
	
	Let $t = t_1 /(5q^2)$ in the first concentration bound, and $t= t_2 / (5q^2)$ in the second concentration bound.  Then $\covsup\biggl( \bigl\{ X^{(j)} \bigr\}_{j=1}^{n}\biggr) \leq t_1 s^2 r / (5q^2)$, and $\roc\biggl( \bigl\{ X^{(j)} \bigr\}_{j=1}^{n}\biggr) \leq 16s^2 r^2/q^3 + t_2 s^2 r / (5q^2)$, with probability greater than $1- 2q\exp(-n t_1^2 / (200q^4)) - q \exp(- n t_2^2 / (200q^4))$.  We condition on the event that both inequalities hold.  Since $\covsup\biggl( \bigl\{ X^{(j)} \bigr\}_{j=1}^{n}\biggr) \leq t_1 s^2 r / (5q^2) \leq s^2 r/(20q^2)$, by Lemma \ref{thm:boundondelta} we have $\coveig\biggl( \bigl\{ X^{(j)} \bigr\}_{j=1}^{n}\biggr) \geq s^2 r/(5q^2)$, and hence $\covsup\biggl( \bigl\{ X^{(j)} \bigr\}_{j=1}^{n}\biggr) / \coveig\biggl( \bigl\{ X^{(j)} \bigr\}_{j=1}^{n}\biggr) \leq t_1$, and $\roc\biggl( \bigl\{ X^{(j)} \bigr\}_{j=1}^{n}\biggr) / \coveig\biggl( \bigl\{ X^{(j)} \bigr\}_{j=1}^{n}\biggr) \leq 80r/q + t_2$.
\end{proof}


\section{Stability of Matrix and Operator Scaling} \label{apx:sinkhornstability}

In this section we prove a stability property of Sinkhorn scaling and Operator Sinkhorn scaling.  In Sinkhorn scaling, we show that if a matrix is close to being doubly stochastic and whose entries are suitably bounded away from $0$, then the resulting row and column scalings are close to $\be$, the all-ones vector.  We also prove the operator analog of this result.  These results are subsequently used to prove Propositions \ref{thm:normalizednearothogonal} and \ref{thm:gaussianmapsatisfy}, and may be of independent interest.

\subsection{Main results}

\begin{proposition}[Local stability of Matrix Scaling] \label{thm:sinkhornstability}
	Let $M \in \mathbb{R}^{q\times q}$ be a matrix such that 
	\begin{enumerate}
		\item $|\langle \be_i, M(\be_j)\rangle - 1/q| \leq 1/(2q)$ for all standard basis vectors $\be_i,\be_j$; and
		\item $\epsilon:= \max \{\|M \boldsymbol e - \boldsymbol e\|_{\infty},\| M^{\prime} \boldsymbol e - \boldsymbol e\|_{\infty} \} \leq 1/(48 \sqrt{q})$, where $\be = (1,\ldots,1)^{\prime}$.
	\end{enumerate}
	Let $D_1, D_2$ be diagonal matrices such that $D_2 M D_1$ is doubly stochastic.  Then 
	\begin{equation*}
		\| D_2 \botimes D_1 - \sfi \|_2 \leq 96 \sqrt{q} \epsilon.
	\end{equation*}
\end{proposition}

\begin{proposition}[Local stability of Operator Scaling] \label{thm:oepratorsinkhornstability}
	Let $\mathfrak{M}: \mathbb{S}^{q} \mapsto \mathbb{S}^{q}$ be a rank-indecomposable linear operator such that
	\begin{enumerate}
		\item $|\langle \bv\bv^{\prime}, \mathfrak{M} (\bu \bu^{\prime})\rangle -1/q| \leq 1/(2q)$ for all unit-norm vectors $\bu,\bv \in \mathbb{R}^{q}$; and
		\item $\epsilon:= \max \{ \| \mathfrak{M}(I)-I \|_{2} , \| \mathfrak{M}^{\prime}(I)-I \|_{2}\} \leq 1/(48\sqrt{q})$.
	\end{enumerate}
	Let $N_1, N_2 \in \mathbb{S}^{q}$ be positive definite matrices such that $ (N_2 \botimes N_2) \circ \mathfrak{M} \circ (N_1 \botimes N_1) $ is doubly stochastic.  Then $\| N_2^2 \botimes N_1^2 - \sfi \|_2 \leq 96 \sqrt{q} \epsilon$.  We also have $\| N_2 \botimes N_1 - \sfi \|_2 \leq 96 \sqrt{q} \epsilon$.
\end{proposition}

\subsection{Proofs}

The proof of Proposition \ref{thm:sinkhornstability} relies on the correspondence between diagonal matrices $D_1, D_2$ such that $D_2 M D_1$ is doubly stochastic, and the vectors  
$\boldsymbol \varepsilon, \boldsymbol \eta$ that minimize the following convex function
\begin{equation*}
	F(\boldsymbol \varepsilon, \boldsymbol \eta) = \sum_{ij} M_{ij} \exp( \varepsilon_i + \eta_j) - \sum_i \varepsilon_i - \sum \eta_j
\end{equation*}
via the map $(D_2)_{ii} = \exp(\varepsilon_i)$, and $(D_1)_{jj} = \exp(\eta_j)$ \cite{Gor:63}, see also \cite{KhaKal:91} -- this holds for all matrices $M$ with positive entries, and one can obtain this relationship by considering the first order conditions.  In the following we prove certain properties concerning the minima of $F$ (see Lemma \ref{thm:optimalf}).  The proof of Proposition \ref{thm:oepratorsinkhornstability} is a reduction to an analysis concerning the stability of matrix scaling. 

In the following we develop some lower estimates of sums of exponential functions, which we use to prove Proposition \ref{thm:sinkhornstability}.
\\

\noindent \textbf{Definition.} Let $\alpha\geq 0$. Define the function $c_{\alpha}: \mathbb{R} \mapsto \mathbb{R}$
\begin{equation*}
	c_{\alpha} (x)= \begin{cases}
		\frac{1}{2} \exp(-\alpha) x^2 \quad \text{ if } |x| \leq \alpha \\
		\frac{1}{2} \exp(-\alpha) \alpha |x| \quad \text{ if } |x| \geq \alpha
	\end{cases}
\end{equation*}

\noindent \textbf{Remark.} The function $c_{\alpha}$ is continuous by design.

\begin{lemma}\label{thm:lowerboundonexp}
	For all $x$
	\begin{equation*}
		\exp(x) \geq 1+x+c_{\alpha} (x).
	\end{equation*}
\end{lemma}

\begin{proof}[Proof of Lemma \ref{thm:lowerboundonexp}]
	The function $\exp(x)$ has second derivative $\exp(x)$, which is greater than $\exp(-\alpha)$ over all $x$ such that $|x| \leq \alpha$.  Hence, by strong convexity of $\exp(x)$, we have $\exp(x) \geq 1 + x + \frac{1}{2} \exp(-\alpha) x^2$ over the interval $[-\alpha,\alpha]$.
	
	As a consequence of the previous part we have $\exp(\alpha) \geq 1 + \alpha + c_{\alpha} (\alpha)$, and $\exp(-\alpha) \geq 1 - \alpha + c_{\alpha} (-\alpha)$.  Since the function $\exp(x)$ is convex, and $c_{\alpha}$ is linear in the intervals $(-\infty,-\alpha]$ and $[\alpha,\infty)$, it suffices to check that (i) the gradient of $\exp(x)$ at $x=\alpha$, which is $\exp(\alpha)$, exceeds that of $c_{\alpha}$, and (ii) the gradient of $c_{\alpha}$ exceeds that of $\exp(x)$ at $x = -\alpha$, which is $\exp(-\alpha)$.
	
	First we show that $\exp(\alpha) \geq 1 + \frac{1}{2} \exp(\alpha) \alpha$.  Since $\alpha \geq 0$ we have $1+2\alpha \geq \sqrt{1+2\alpha}$.  Hence $2 \exp(\alpha) \geq 2 + 2 \alpha \geq 1 + \sqrt{1 + 2\alpha}$.  By noting that the quadratic $2Z^2- 2Z - \alpha=0$ has roots $\frac{1}{2} \pm \frac{1}{2} \sqrt{1 + 2\alpha}$, we have the inequality $\exp(\alpha) \geq 1 + \frac{1}{2} \exp(-\alpha) \alpha$.
	
	Next we show that $\exp(-\alpha) \leq 1 - \frac{1}{2} \exp(-\alpha) \alpha$.  This is implied by the inequality $\exp(\alpha) \geq 1+\alpha \geq 1 + \alpha/2$, which holds since $\alpha \geq 0$.
\end{proof}

\begin{lemma}\label{thm:lbsumexps}
	Consider the sequence of reals $(\varepsilon_1,\ldots,\varepsilon_q)$, and $(\eta_1,\ldots, \eta_q)$. Then there is a constant $d \in \mathbb{R}$ for which
	\begin{equation*}
		\frac{1}{q}\sum_{ij} \exp(\varepsilon_i + \eta_j ) \geq q + \biggl( \sum_{ij} \varepsilon_i + \eta_j \biggr) + \biggl( \sum_{i} c_{\alpha} (\epsilon_i + d) + \sum_j c_{\alpha} (\eta_j - d) \biggr).
	\end{equation*}
\end{lemma}

\begin{proof} For all $d$ we one has the following
	\begin{eqnarray*}
		\frac{1}{q}\sum_{ij} \exp(\varepsilon_i + \eta_j ) & = & \frac{1}{q}\sum_{i} \exp(\varepsilon_i+d) \biggl( \sum_j \exp(\eta_j -d) \biggr) \\
		& \geq & \frac{1}{q}\sum_{i} \exp(\varepsilon_i+d) \biggl( \sum_j 1 + \eta_j-d + c_{\alpha} (\eta_j-d) \biggr) \\
		& = & \frac{1}{q}\biggl( \sum_j 1 + \eta_j-d + c_{\alpha} (\eta_j-d) \biggr) \biggl(\sum_{i} \exp(\varepsilon_i+d) \biggr) \\
		& \geq & \frac{1}{q}\biggl( \sum_j 1 + \eta_j-d + c_{\alpha} (\eta_j-d) \biggr) \biggl(\sum_{i} 1+\varepsilon_i+d + c_{\alpha} (\varepsilon_i+d) \biggr).
	\end{eqnarray*}
	Since $c$ is continuous, by applying the intermediate value theorem, there is a $d^{\star}$ for which
	\begin{equation*}
		\sum_j  \eta_j - d^{\star} + c_{\alpha} (\eta_j-d^{\star} ) = \sum_{i} \varepsilon_i + d^{\star} + c_{\alpha} (\varepsilon_i + d^{\star}),
	\end{equation*}
	and hence
	\begin{equation*}
		\frac{1}{q}\biggl( \sum_j 1 + \eta_j - d^{\star} + c_{\alpha} (\eta_j - d^{\star}) \biggr) \biggl(\sum_{i} 1+\varepsilon_i + d^{\star} + c_{\alpha} (\varepsilon_i + d^{\star}) \biggr) \geq q+ \biggl( \sum_{i} c_{\alpha} (\epsilon_i + d^{\star}) + \sum_j c_{\alpha} (\eta_j - d^{\star}) \biggr).
	\end{equation*}
\end{proof}

\begin{lemma} \label{thm:optimalf}
	Consider the sequence of reals $\boldsymbol \varepsilon := (\varepsilon_1,\ldots,\varepsilon_q)$, and $\boldsymbol \eta := (\eta_1,\ldots, \eta_q)$, and define 
	\begin{equation}
		F ( \boldsymbol{ \varepsilon}, \boldsymbol \eta) = \sum_{ij} M_{ij} \exp(\varepsilon_i + \eta_j) - \sum \varepsilon_i - \sum \eta_j,
	\end{equation}
	and $\epsilon_{ij} := M_{ij}- 1/q $. Suppose (i) $|\epsilon_{ij} | \leq 1/2q$, and (ii) $ \epsilon:= \max \{ |\sum_i \epsilon_{ij} |, |\sum_j \epsilon_{ij}| \}\leq 1/(24 \sqrt{q})$.  Let $\boldsymbol \varepsilon^{\star}, \boldsymbol \eta^{\star}$ be a minimizer of $F$.  Then $| \varepsilon_i^{\star} + \eta_j^{\star} | \leq 48 \sqrt{q} \epsilon$, for all $i,j$.
\end{lemma}

\begin{proof}
	Let $\boldsymbol \varepsilon,\boldsymbol \eta$ be such that $| \varepsilon_i + \eta_j | > 48 \sqrt{q} \epsilon$ for some $(i,j)$.  We show that $\boldsymbol \varepsilon,\boldsymbol \eta$ cannot be a minimum.
	
	Let $\alpha = 24 \sqrt{q}\epsilon $, and define the sets
	\begin{enumerate}
		\item $\mathcal{S}(\boldsymbol \varepsilon) = \{i: | \varepsilon_i | \geq \alpha \}$;
		\item $\mathcal{T}(\boldsymbol \varepsilon)  = \{i: \alpha > | \varepsilon_i | \geq 4 \epsilon \exp(\alpha) \}$; and
		\item $\mathcal{U}(\boldsymbol \varepsilon)  = \{i: 4 \epsilon \exp(\alpha) > | \varepsilon_i | \}$.
	\end{enumerate}
	In a similar fashion we define the sets $\mathcal{S}(\boldsymbol \eta), \mathcal{T}(\boldsymbol \eta), \mathcal{U}(\boldsymbol \eta)$.
	
	First since $\alpha \leq 1$, we have $\alpha \geq \alpha\exp(\alpha)/3 \geq 8 \sqrt{q} \epsilon \exp(\alpha)$, and hence
	\begin{equation*}
		\frac{1}{4} \biggl( \sum_{\mathcal{S}(\boldsymbol \varepsilon)} c_{\alpha}(\varepsilon_i) + \sum_{\mathcal{S}(\boldsymbol \eta)} c_{\alpha}(\eta_j) \biggr) \geq \epsilon \biggl( \sum_{\mathcal{S}(\boldsymbol \varepsilon)} |\varepsilon_i| + \sum_{\mathcal{S}(\boldsymbol \eta)} |\eta_j| \biggr).
	\end{equation*}
	
	Second 
	\begin{equation*}
		\frac{1}{2} \biggl(\sum_{\mathcal{T}(\boldsymbol \varepsilon)} c_{\alpha}(\varepsilon_i) + \sum_{\mathcal{T}(\boldsymbol \eta)} c_{\alpha}(\eta_j) \biggr) = \sum_{\mathcal{T}(\boldsymbol \varepsilon)} \frac{1}{4} \exp(-\alpha) \varepsilon_i^2 + \sum_{\mathcal{T}(\boldsymbol \eta)} \frac{1}{4} \exp(-\alpha) \eta_j^2  \geq \epsilon \biggl( \sum_{\mathcal{T}(\boldsymbol \varepsilon)} |\varepsilon_i| + \sum_{\mathcal{T}(\boldsymbol \eta)} |\eta_j| \biggr).
	\end{equation*}
	
	Third since there is an index $(i,j)$ such that $| \varepsilon_i + \eta_j | > 48 \sqrt{q} \epsilon$, one of the sets $\mathcal{S}(\boldsymbol \varepsilon), \mathcal{S}(\boldsymbol \eta)$ is nonempty. By noting that $\alpha \exp(-\alpha) \geq 8 \sqrt{q} \epsilon$, we have
	\begin{equation*}
		\frac{1}{4} \biggl( \sum_{\mathcal{S}(\boldsymbol \varepsilon)} c_{\alpha}(\varepsilon_i) + \sum_{\mathcal{S}(\boldsymbol \eta)} c_{\alpha}(\eta_j) \biggr) > \epsilon \times 2q \times 4 \epsilon\exp(\alpha) \geq \epsilon \biggl( \sum_{\mathcal{U}(\boldsymbol \varepsilon)} |\varepsilon_i| + \sum_{\mathcal{U}(\boldsymbol \eta)} |\eta_j| \biggr).
	\end{equation*}
	
	
	By summing these quantities and applying Proposition \ref{thm:lbsumexps} we have
	\begin{equation} \label{eq:flb_bound2}
		\frac{1}{2q} \sum \biggl(\exp(\varepsilon_i + \eta_j) - (\varepsilon_i + \eta_j ) - 1 \biggr) \geq \frac{1}{2} \biggl( \sum_{i} c_{\alpha}(\varepsilon_i) + \sum_{j} c_{\alpha}(\eta_j) \biggr) > | \sum \epsilon_{ij} (\varepsilon_i + \eta_j)|.
	\end{equation}
	
	Also, since $ \exp(\varepsilon_i + \eta_j ) -(\varepsilon_i + \eta_j) - 1 \geq 0$ for all $i,j$, and $|\epsilon_{ij}| \leq 1/(2q)$, we have 
	\begin{eqnarray}\label{eq:flb_bound1}
		\frac{1}{2q} \sum_{ij}   (\exp(\varepsilon_i + \eta_j ) -(\varepsilon_i + \eta_j) - 1) & \geq & |\epsilon| \sum_{ij}  (\exp(\varepsilon_i + \eta_j ) -(\varepsilon_i + \eta_j) - 1) \nonumber \\
		& \geq & |\sum_{ij} \epsilon_{ij} (\exp(\varepsilon_i + \eta_j ) -(\varepsilon_i + \eta_j) - 1) | .
	\end{eqnarray}
	
	By combining equations \eqref{eq:flb_bound2} and \eqref{eq:flb_bound1} we have
	\begin{equation*}
		\frac{1}{q} \sum_{ij}   (\exp(\varepsilon_i + \eta_j ) -(\varepsilon_i + \eta_j) - 1) > -\sum_{ij} \epsilon_{ij} (\exp(\varepsilon_i + \eta_j )  - 1),
	\end{equation*}
	which implies $F(\boldsymbol \varepsilon, \boldsymbol \eta) > F(\boldsymbol 0, \boldsymbol 0)$.
\end{proof}

\begin{proof}[Proof of Proposition \ref{thm:sinkhornstability}]
	By Lemma \ref{thm:optimalf} any minimum $\boldsymbol \varepsilon^{\star}, \boldsymbol \eta^{\star}$ satisfies $| \varepsilon_i^{\star} + \eta_j^{\star} | \leq 48 \sqrt{q} \epsilon$.  Hence by the one-to-one correspondence between the minima of $F$ and the diagonal scalings $D_1,D_2$ \cite{Gor:63}, we have $\| D_2 \botimes D_1 - \sfi \|_2 \leq \exp(48 \sqrt{q} \epsilon) -1 \leq 96 \sqrt{q} \epsilon$.
\end{proof}

\begin{proof}[Proof of Proposition \ref{thm:oepratorsinkhornstability}]
	Without loss of generality we may assume that $N_1,N_2$ are diagonal matrices, say $D_1,D_2$ respectively.  Define the matrix $M_{ij} = \langle \be_i \be_i^{\prime}, \mathfrak{M} (\be_j \be_j^{\prime}) \rangle$.  It is straightforward to check that $M$ satisfies the conditions of Proposition \ref{thm:sinkhornstability}; moreover, the condition that $ (N_2 \botimes N_2) \circ \mathfrak{M} \circ (N_1 \botimes N_1) $ is a doubly stochastic operator implies that $D_2^2 M D_1^2$ is a doubly stochastic matrix.  By Proposition \ref{thm:sinkhornstability} we have $\| D_1^2 \botimes D_2^2 - \sfi \|_2 \leq 96 \sqrt{q} \epsilon$, and hence $\| N_1^2 \botimes N_2^2 - \sfi \|_2 \leq 96 \sqrt{q} \epsilon$.  Since $N_1,N_2$ are self-adjoint, we also have $\| N_1 \botimes N_2 - \sfi \|_2 \leq 96 \sqrt{q} \epsilon$.
\end{proof}


\section{Proof of Proposition \ref{thm:gaussianmapsatisfy}} \label{apx:randlinmaps}

In this section we prove that Gaussian linear maps that are subsequently normalized satisfy the deterministic conditions in Theorem \ref{thm:localconvergence} concerning the linear map $\mathcal{L}^{\star}$ with high probability.  There are two steps to our proof.  First we state sufficient conditions for linear maps such that, when normalized, satisfy the deterministic conditions.  Second we show that Gaussian maps satisfy these sufficient conditions with high probability. 

We introduce the following parameter that measures how close a linear map $\L$ is to being normalized.

\begin{definition}
	Let $\mathcal{L} \in \mathbb{R}^{q\times q} \mapsto \mathbb{R}^d$ be a linear map. The \emph{nearly normalized parameter} of $\mathcal{L}$ is defined as
	\begin{equation*}
		\epsilon(\mathcal{L}) := \max \{ \|\mathfrak{T}_{\mathcal{L}} (I) - I\|_2, \|\mathfrak{T}_{\mathcal{L}}^{\prime} (I) - I\|_2\} .
	\end{equation*}
\end{definition}

\begin{proposition} \label{thm:ripnppequalsrip}
	Let $\mathcal{L} : \mathbb{R}^{q\times q} \mapsto \mathbb{R}^d$ be a linear map that satisfies (i) the restricted isometry condition $\delta_r(\mathcal{L})\leq 1/2$, and (ii) whose nearly normalized parameter satisfies $\epsilon (\mathcal{L}) \leq 1/ 650 \sqrt{q}$.  Let $\mathcal{L} \circ \mathcal{N}_{\mathcal{L}}$ be the normalized linear map where $\mathcal{N}_{\mathcal{L}}$ is a positive definite rank-preserver.  Then $\mathcal{L} \circ \mathcal{N}_{\mathcal{L}}$ satisfies the restricted isometry condition $\delta_r(\mathcal{L} \circ \mathcal{N}) \leq \bar{\delta_r}:= (1+\delta_r(\mathcal{L})) (1+ 96\sqrt{q} \epsilon (\mathcal{L}))^2 - 1 < 1$. Moreover, $\|\mathcal{L} \circ \mathcal{N}_{\mathcal{L}} \|_2 \leq (1+ 96\sqrt{q} \epsilon (\mathcal{L})) \|\mathcal{L}\|_2$.
\end{proposition}

\begin{proof}[Proof of Proposition \ref{thm:ripnppequalsrip}]
	Since $\mathcal{L}$ satisfies the restricted isometry condition $\delta_1(\mathcal{L}) \leq 1/2$, we have $|\langle \bv\bv^{\prime}, \mathfrak{T}_{\mathcal{L}} (\bu \bu^{\prime})\rangle -1/q| \leq 1/(2q)$ for all unit-norm vectors $\bu,\bv \in \mathbb{R}^{q}$.  In addition, the linear map $\mathcal{L}$ has nearly normalized parameter $\epsilon (\mathcal{L}) \leq 1/ 650 \sqrt{q}$.  Hence by applying Proposition \ref{thm:oepratorsinkhornstability} to the linear map $\mathfrak{T}_{\mathcal{L}}$, any pair of positive definite matrices $Q_2,Q_1$ such that $Q_2 \botimes Q_2 \circ \mathfrak{T}_{\mathcal{L}} \circ Q_1 \botimes Q_1$ is doubly stochastic satisfies $\|Q_2 \botimes Q_1 - \sfi \|_2 \leq 96 \sqrt{q} \epsilon(\mathcal{L})$.  By noting the correspondence between such matrices with the positive definite rank-preserver $\mathcal{N}_{\mathcal{L}}$ such that $\mathcal{L} \circ \mathcal{N}_{\mathcal{L}}$ is normalized via the relation $\mathcal{N}_{\mathcal{L}} = Q_2 \botimes Q_1$ (see Corollary \ref{thm:external_normalizablelinearmaps}), we have $\|\mathcal{N}_{\mathcal{L}}\|_2 \leq 1+96 \sqrt{q} \epsilon(\mathcal{L})$.
	
	Let $X$ be a matrix with rank at most $r$.  Then
	\begin{equation*}
		\| \mathcal{L} (\mathcal{N}_{\mathcal{L}} (X)) \|_F \leq \sqrt{1+\delta_r(\mathcal{L})} \| \mathcal{N}_{\mathcal{L}} \|_2 \|X\|_F \leq \sqrt{1+\delta_r(\mathcal{L})}(1 + 96 \sqrt{q} \epsilon(\mathcal{L}) ) \| X \|_F,
	\end{equation*}	
	and hence $\| \mathcal{L} (\mathcal{N}_{\mathcal{L}} (X)) \|_F^2 \leq (1+\bar{\delta_r}) \|X\|_F^2$.  A similar set of inequalities prove that $\| \mathcal{L} (\mathcal{N}_{\mathcal{L}} (X)) \|_F^2 \geq (1-\bar{\delta_r}) \|X\|_F^2$.  Last $\|\mathcal{L} \circ \mathcal{N}_{\mathcal{L}} \|_2 \leq \|\mathcal{L}\|_2\|\mathcal{N}_{\mathcal{L}} \|_2 \leq (1+96\sqrt{q}\epsilon)\|\mathcal{L}\|_2$.
\end{proof}

\begin{proposition} (\cite[Theorem II.13]{DavSza:01}) \label{thm:gaussianspec}
	Let $t>0$ be fixed.  Suppose $\mathcal{L} \sim \mathcal{N} (0,1/d)$.  Then with probability greater than $1 - \exp(- t^2d/2) $ we have $\|\mathcal{L}\|_2 \leq \sqrt{q^2/d} + 1 + t$.
\end{proposition}

\begin{proposition}(\cite[Theorem 2.3]{CanPla:11}) \label{thm:gaussianrip}
	Let $\delta<1$ be fixed.  There exists constants $c_1,c_2$ such that for $d \geq c_1 qr$, if $\mathcal{L} \sim \mathcal{N} (0,1/d)$, then with probability greater than $1 - 2 \exp(- c_2 d)$ the linear map $\mathcal{L}$ satisfies the restricted isometry condition $\delta_r(\mathcal{L}) \leq \delta$.
\end{proposition}

\begin{proposition}[Gaussian linear maps are nearly normalized]\label{thm:rowcoldev}
	Let $\epsilon>0$ be fixed.  Suppose $\mathcal{L} \sim \mathcal{N}(0, 1/d)$.  Then with probability greater than $1-4 (17/\epsilon)^{q} \exp( - dq\epsilon^2 / 32)$ the nearly normalized parameter of $\mathcal{L}$ is smaller than $\epsilon$.
\end{proposition}

Bounding the nearly normalized parameter of a Gaussian linear map exactly corresponds to computing the deviation of the sum of independent Wishart matrices from its mean in spectral norm.  To do so we appeal to the following concentration bound.

\begin{proposition}[Concentration of sum of Wishart Matrices] \label{thm:concentrationwishart}
	Let $\{X^{(j)} \}_{j=1}^{d}, X^{(j)} = G^{(j)} G^{(j)\prime}$, where $G^{(j)} \in \R^{q\times q}, G^{(j)} \sim \mathcal{N}(0,1/q)$, be a collection of independent Wishart matrices, and let $t<2$ be fixed.  Then $\mathbb{P} ( \|\frac{1}{d}\sum_{j=1}^{d} X^{(j)} - I  \|_2 \geq t) \leq 2(17/t)^{q} \exp( - dq t^2 / 32)$.
\end{proposition}

\begin{proof}[Proof of Proposition \ref{thm:concentrationwishart}]
	First note that $ \|\frac{1}{d}\sum_{j=1}^{d} X^{(j)} - I  \|_2 =\sup_{E} \langle \frac{1}{d}\sum_{j=1}^{d} X^{(j)} , E \rangle - 1$, where the supremum is taken over all symmetric unit-norm rank-one matrices $E$.  Second, by a standard set-covering argument, there is a set $\mathcal{S}_0 =\{\be_i \}$ that forms a $t / 8$-cover of the unit-sphere in $\R^q$, and satisfies $|\mathcal{S}_0| \leq (1+ 16/t)^{q}$.  Define $\mathcal{S} =\{ \pm \be_i \be_i^{\prime} \}, \be_i \in \mathcal{S}_0$.  Then for any rank-one positive definite matrix $\be \be^{\prime}$, there is a vector $\be_0 \in \mathcal{S}_0$ such that $\|\be_0 - \be\|_2 \leq t/8$, and hence $\|\be_0 \be_0^{\prime} - \be \be^{\prime}\|_F \leq \|\be_0 \be_0^{\prime} - \be \be_0^{\prime}\|_F + \|\be \be_0^{\prime} - \be \be^{\prime}\|_F \leq t/4$.  This implies that $\mathcal{S}$ is a $t/4$-cover for the set of rank-one matrices in $\mathbb{S}^{q}$; moreover, $|\mathcal{S}|\leq 2(1+ 16/t)^{q} \leq 2(17/t)^{q}$.
	
	Fix a matrix $E = E_i \in \mathcal{S}$.  Note that the Gaussian random ensemble is unitarily invariant.  Hence the inner product $\langle \frac{1}{d}\sum_{j=1}^{d}  X^{(j)}, E \rangle$ is in fact a sum of $dq$ independent $\chi^2$ random variables (scaled by $1/q$).  Thus by applying a suitable Bernstein inequality we have
	\begin{equation*}
		\mathbb{P} \left( \left\langle \frac{1}{d}\sum_{j=1}^{d}  X^{(j)}, E \right\rangle  - 1 \geq t/2 \right) \leq \exp(-dqt^2/32).
	\end{equation*}
	Hence by applying a union bound over the set $\mathcal{S}$ we have $ \langle \frac{1}{d}\sum_{j=1}^{d}  X^{(j)} , E_i \rangle - 1  \leq t/2$ for all $E_i \in \mathcal{S}$, with probability greater than $1- 2 (17/t)^{q} \exp(-dqt^2/32)$.  Define
	\begin{equation*}
		\kappa := \sup_{E, \mathrm{rank}(E)=1, \|E\|_F=1} \left\langle \frac{1}{d}\sum_{j=1}^{d}  X^{(j)} , E \right\rangle.
	\end{equation*}
	Let $E$ be any rank-one unit-norm matrix, and let $E_0 \in \mathcal{S}$ be such that $\|E - E_0 \|_F \leq t/4$.  Then
	\begin{equation*}
		\left\langle \frac{1}{d}\sum_{j=1}^{d} X^{(j)}, E \right\rangle = \left\langle \frac{1}{d}\sum_{j=1}^{d} X^{(j)}, E - E_0 \right\rangle + \left\langle \frac{1}{d}\sum_{j=1}^{d} X^{(j)}, E_0 \right\rangle \leq \kappa t/4 + 1 + t/2,
	\end{equation*}
	which implies
	\begin{equation*}
		\kappa \leq \kappa t/4 + 1 + t/2,
	\end{equation*}
	and hence $\kappa \leq 1 + t$.  This proves the result.
	%First note that there is a linear map $\mathcal{R}$ that reshapes inputs that are linear maps $\mathcal{L} \in \mathbb{R}^{q\times q} \mapsto \mathbb{R}^d$ to matrices $\mathbb{R}^{dq \times q}$ such that
	%\begin{equation*}
	%|\sum_{i} [M (\mathcal{L})]_{ij} - 1 | \leq \epsilon \Leftrightarrow | (1/q)\|\mathcal{R} (\mathcal{L}) \be_j \|_2^2 - 1 | \leq \epsilon.
	%\end{equation*}
	%One can define an analogous map $\tilde{\mathcal{R}}$ for sums over the indices $j$.  Hence NNP with parameter $\epsilon$ is equivalent to the condition that
	%\begin{equation*}
	%|(1/q)\|\mathcal{R} (\mathcal{L}) \be_j \|_2^2 - 1 | , | (1/q)\|\tilde{\mathcal{R}} (\mathcal{L}) \be_j \|_2^2 - 1 | \leq \epsilon
	%\end{equation*}
	%for all unit-vectors $\be$.  By Lemma [CITE] we have
	%\begin{equation*}
	%\mathbb{P} (| (1/q)\|\mathcal{R} (\mathcal{L}) \be_j \|_2^2 - 1 | \geq \epsilon/2) \leq 2 \exp(-dq\epsilon^2/32).
	%\end{equation*}
	%By applying a union bound we have $| (1/q)\|\mathcal{R} (\mathcal{L}) \be_j \|_2^2 - 1 | \leq \epsilon/2$ for all $\be_j \in \mathcal{S}$, with probability greater than $1- 2 (1+ 8/\epsilon)^q \exp(-dq\epsilon^2/32)$.  Define 
	%\begin{equation*}
	%\kappa_{\mathcal{R}}(\mathcal{L}):= \max_{\be \in \mathbb{R}^q, \| \be\|_2 = 1}  (1/q)\|\mathcal{R} (\mathcal{L}) \be \|_2^2 .
	%\end{equation*}
	%Let $\be \in \mathbb{R}^q$ be generic and let $\be_0 \in \mathcal{S}$ be such that $\|\be - \be_0 \|_2 \leq \epsilon/4$.  Then
	%\begin{equation*}
	%(1/q)\| \mathcal{R} (\mathcal{L}) \be \|_2 \leq (1/q)\| \mathcal{R} (\mathcal{L}) (\be - \be_0) \|_2 + (1/q)\| \mathcal{R} (\mathcal{L}) \be \|_2 \leq \kappa_{\mathcal{R}}(\mathcal{L})(\epsilon/4) + 1 + \epsilon/2.
	%\end{equation*}
	%This implies
	%\begin{equation*}
	%\kappa_{\mathcal{R}}(\mathcal{L}) \leq \kappa_{\mathcal{R}}(\mathcal{L})(\epsilon/4) + 1 + \epsilon/2
	%\end{equation*}
	%and hence $\kappa_{\mathcal{R}}(\mathcal{L}) \leq 1 + \epsilon$.  We repeat the same argument for $\kappa_{\tilde{\mathcal{R}}}(\mathcal{L})$.  The result follows from a union bound.
\end{proof}

\begin{proof}[Proof of Proposition \ref{thm:rowcoldev}]
	This is a direct application of Proposition \ref{thm:concentrationwishart} with $G^{(j)} =\sqrt{q/d} \mathcal{L}^{(j)}$ and $G^{(j)\prime} =\sqrt{q/d} \mathcal{L}^{(j)}$, followed by a union bound.
\end{proof}


\begin{proof}[Proof of Proposition \ref{thm:gaussianmapsatisfy}]
	We choose $t = 1/50$ in Proposition \ref{thm:gaussianspec}, $\delta = \delta_{4r}/2$ in Proposition \ref{thm:gaussianrip}, and $\epsilon = \delta / (960\sqrt{q})$ in Proposition \ref{thm:rowcoldev}.  Then there are constants $c_1, c_2, c_3$ such that if $d \geq c_1 (r + \log q) q$, then (i) $\|\tilde{\mathcal{L}}\|_2\leq \sqrt{q^2/d}+ 51/50 \leq (101/50) \sqrt{q^2/d}$, (ii) $\tilde{\mathcal{L}}$ satisfies the restricted isometry condition $\delta_{4r}(\tilde{\mathcal{L}}) \leq \delta_{4r}/2$, and (iii) $\tilde{\mathcal{L}}$ is nearly normalized with parameter $\epsilon(\tilde{\mathcal{L}}) \leq \delta_{4r}/ 960 \sqrt{q}$, with probability greater than $1 - c_2 \exp( - c_3 d)$.
	
	By applying Proposition \ref{thm:ripnppequalsrip} we conclude that the linear map $\mathcal{L}$ satisfies the restricted isometry condition $\delta_{4r}(\mathcal{L})\leq (1+\delta_{4r}/2) (1 + \delta_{4r}/10)^2 -1 \leq \delta_{4r}$, and $\|\mathcal{L} \|_2 \leq \sqrt{5q^2/d}$.
\end{proof}


\section{Proof of Proposition \ref{thm:normalizednearothogonal}}

\begin{proof}[Proof of Proposition \ref{thm:normalizednearothogonal}]
	First we check that the linear map $\mathcal{L}^{\star} \circ (\sfi + \sfe)$ satisfies the restricted isometry condition $\delta_1 (\mathcal{L}^{\star} \circ (\sfi + \sfe)) \leq 1/2$.  For any rank-one unit-norm matrix $X$ we have $\| [\mathcal{L}^{\star} \circ (\sfi + \sfe)] (X) \|_{2} \leq \| \mathcal{L}^{\star} (X) \|_{2} + \| \mathcal{L}^{\star} \circ \sfe (X) \|_{2} \leq \sqrt{1+ 1/10} + 1/150 \leq \sqrt{1+1/2}$.  A similar set of inequalities show that $\| [\mathcal{L}^{\star} \circ (\sfi + \sfe)] (X) \|_{2} \geq \sqrt{1 - 1/2}$.
	
	Second we check that the nearly normalized parameter of $\mathcal{L}^{\star} \circ (\sfi+\sfe)$ satisfies $\epsilon (\mathcal{L}^{\star} \circ (\sfi+\sfe)) \leq 1/ 48\sqrt{q}$.  Denote $\mathcal{E} := \mathcal{L}^{\star} \circ \sfe$.  For all unit-norm rank-one matrices $E$ we have $\|\mathcal{E}(E)\|_2^2\leq \|\mathcal{L}^{\star}\|_2^2\|\sfe\|_{\eu}^2$.  Hence for any unit-norm $\bu\in\mathbb{R}^{q}$ we have
	\begin{equation*}
		\frac{1}{q} \sum_{j=1}^{d} \langle \mathcal{E}^{(j)} \mathcal{E}^{(j)\prime}, \bu \bu^{\prime}\rangle  =  \frac{1}{q} \sum_{j=1}^{d} (\mathcal{E}^{(j)\prime} \bu)^{\prime} (\mathcal{E}^{(j)\prime} \bu) = \frac{1}{q}  \sum_{j=1}^{d} \sum_{k=1}^{q} (\mathcal{E}^{(j)\prime} \bu)_k^2 = \frac{1}{q} \sum_{k=1}^{q}  \| \mathcal{E} ( \bu \be_k^{\prime}) \|_2^2 \leq \|\mathcal{L}^{\star}\|_2^2\|\sfe\|_{\eu}^2.
	\end{equation*}
	Similarly, since $\mathcal{L}^{\star}$ is normalized, we have 
	\begin{equation*}
		\frac{1}{q} \sum_{j=1}^{d} \langle \mathcal{L}^{\star(j)} \mathcal{L}^{\star(j)\prime}, \bu \bu^{\prime}\rangle  =  \frac{1}{q} \sum_{j=1}^{d} (\mathcal{L}^{\star(j)\prime} \bu)^{\prime} (\mathcal{L}^{\star(j)\prime} \bu) = \frac{1}{q}  \sum_{j=1}^{d} \sum_{k=1}^{q} (\mathcal{L}^{\star(j)\prime} \bu)_k^2 = 1.
	\end{equation*}
	Hence
	\begin{eqnarray*}
		\langle \mathfrak{T}_{\mathcal{L}^{\star} \circ (\sfi+\sfe) } (I)  - I , \bu \bu^{\prime} \rangle   &= &\langle \mathfrak{T}_{\mathcal{L}^{\star} + \mathcal{E}} (I)  - \mathfrak{T}_{\mathcal{L}^{\star}}  (I) , \bu \bu^{\prime} \rangle \\
		& = & \frac{1}{q} \sum_{j=1}^{d} \langle \mathcal{E}^{(j)} \mathcal{E}^{(j)\prime}, \bu \bu^{\prime}\rangle + \frac{1}{q} \sum_{j=1}^{d} \langle \mathcal{L}^{\star(j)} \mathcal{E}^{(j)\prime}, \bu \bu^{\prime}\rangle + \frac{1}{q} \sum_{j=1}^{d} \langle \mathcal{E}^{(j)} \mathcal{L}^{\star(j)\prime}, \bu \bu^{\prime}\rangle \\
		& \leq & 3 \|\mathcal{L}^{\star}\|_2 \|\sfe\|_{\eu},
	\end{eqnarray*}
	where the last inequality applies Cauchy-Schwarz.  By taking the supremum over all unit-norm $\bu$ we have
	\begin{equation*}
		\| \mathfrak{T}_{\mathcal{L}^{\star} + \mathcal{E}} (I)  - I \|_2 \leq 3 \|\mathcal{L}^{\star}\|_2 \|\sfe\|_{\eu}.
	\end{equation*}
	Similarly $\| \mathfrak{T}_{\mathcal{L}^{\star} + \mathcal{E}}^{\prime} (I)  - I \|_2 \leq 3 \|\mathcal{L}^{\star}\|_2 \|\sfe\|_{\eu}$.  Thus $\epsilon (\mathcal{L}^{\star} \circ (\sfi+\sfe)) \leq 3 \|\mathcal{L}^{\star}\|_2 \|\sfe\|_{\eu} \leq 1/ 48\sqrt{q}$.
	
	
	
	%Second we verify that $\mathcal{L}$ satisfy NNP with $\epsilon \leq 1/ 48\sqrt{q}$.  Denote the rows of the linear map $\mathcal{L}^{\star} \circ \sfq$ and $\mathcal{L}^{\star} \circ \sfe \circ \sfq$ as $\{A^{(k)} \}_{k=1}^{d}$ and $\{E^{(k)} \}_{k=1}^{d}$ respectively, where $A^{(k)},E^{(k)} \in \mathbb{R}^{q\times q}$, $1\leq k \leq d$.  We have $\sum_{k=1}^d E_{ij}^{(k)2} = \|\mathcal{L}^{\star} \circ \sfe (\sfq (\boldsymbol e_j \boldsymbol e_i^{\prime}))\|_2^2 \leq \|\mathcal{L}^{\star}\|_{2}^2 \|\sfe\|_{\eu}^2 $.  By Cauchy-Schwarz we have $|\sum_{i=1}^q \sum_{k=1}^d A_{ij}^{(k)} E_{ij}^{(k)} | \leq (\sum_{i=1}^q \sum_{k=1}^d A^{(k)2}_{ij})^{1/2}(\sum_{i=1}^q \sum_{k=1}^d E^{(k)2}_{ij})^{1/2} \leq q \|\mathcal{L}^{\star}\|_{2} \|\sfe\|_{\eu}$.  Since $\mathcal{L}^{\star}$ is normalized we have $\sum_{i=1}^q \sum_{k=1}^d A^{(k)2}_{ij}=q$.  Hence $|\sum_{i=1}^{q} M(\mathcal{L} \circ \sfq)_{ij}-1 |= \frac{1}{q} |\sum_{i=1}^q \sum_{k=1}^d (A^{(k)2}_{ij} +E^{(k)2}_{ij} + 2 A^{(k)}_{ij} E^{(k)}_{ij}) - q| \leq \frac{1}{q} \sum_{i=1}^q \sum_{k=1}^d (E^{(k)2}_{ij} + 2 |A^{(k)}_{ij} E^{(k)}_{ij}|) \leq \|\mathcal{L}^{\star}\|_{2}^2 \|\sfe\|_{\eu}^2 + 2 \|\mathcal{L}^{\star}\|_{2} \|\sfe\|_{\eu} \leq 3 \|\mathcal{L}^{\star}\|_{2} \|\sfe\|_{\eu}$.  Similarly one also has $|\sum_{j=1}^{q} M(\mathcal{L}\circ \sfq)_{ij}-1 |\leq 3 \|\mathcal{L}^{\star}\|_{2} \|\sfe\|_{\eu}$.
	
	The result follows by applying Proposition \ref{thm:oepratorsinkhornstability} to the linear map $\mathfrak{T}_{\mathcal{L}^{\star} \circ (\sfi+\sfe)}$.
\end{proof}


\section{Proof of Proposition \ref{thm:varietyconstrainederr}} \label{apx:varietyanalysis}

The proof of Proposition \ref{thm:varietyconstrainederr} is based on the following result concerning affine rank minimization, which may be of independent interest.

\begin{proposition} \label{thm:varietyconstrainedoptimization}
	Suppose $X^{\star}$ is a $q\times q$ rank-$r$ matrix satisfying $\sigma_r (X^{\star}) \geq 1/2$. Let $\by = \mathcal{L}(X^{\star}) + \bz$, where the linear map $\mathcal{L}$ satisfies the restricted isometry condition $\delta_{4r}(\mathcal{L}) \leq 1/10$, and $\| \mathcal{L}^{\prime} \bz\|_2 =: \epsilon \leq 1 / (20 r^{3/2})$. Let $\hat{X}$ be the optimal solution to the following
	\begin{equation*}
		\hat{X}~=~\underset{X}{\mathrm{argmin}}~\| \by - \mathcal{L}(X) \|^2_2 \quad \quad \mathrm{s.t.} \quad \quad \mathrm{rank}(X) \leq \mathrm{rank}(X^{\star}).
	\end{equation*}
	Then (i) $\| \hat{X} - X^{\star} \|_2 \leq 4 \sqrt{r} \epsilon$, and (ii) $ \hat{X} - X^{\star} = \cp_{\ct(X^{\star})} [(\mathcal{L}^{\prime}_{\ct(X^{\star})}\mathcal{L}_{\ct(X^{\star})})^{-1}]_{\ct(X^{\star})} \cp_{\ct(X^{\star})} (\mathcal{L}_{\ct(X^{\star})}^{\prime} \bz) + G$, where $\|G\|_F \leq 650 r^{3/2} \epsilon^2 $. %$ \| (\hat{X} - X^{\star}) - \cp_{\ct(X^{\star})} [(\mathcal{L}^{\prime}_{\ct(X^{\star})}\mathcal{L}_{\ct(X^{\star})})^{-1}]_{\ct(X^{\star})} \cp_{\ct(X^{\star})} (\mathcal{L}_{\ct(X^{\star})}^{\prime} \bz) \|_F \leq 650 r^{3/2} \epsilon^2 $.
\end{proposition}

The proof of Proposition \ref{thm:varietyconstrainedoptimization} requires two preliminary results which we state and prove first.  Our development relies on results from matrix perturbation theory; we refer the reader to \cite{Kat:66,SteSun:1990} for detailed expositions.  Several of our results are minor modifications of analogous results in \cite{CPW:12}.

The following result and the accompanying proof is a minor modification of Proposition 2.2 in the supplementary material (s.m.) of \cite{CPW:12},  and its proof.  The modification allows us to provide a bound that does not scale with the ambient dimension.

\begin{proposition} \label{thm:lgmmod}
	Let $X_1,X_2 \in \mathbb{R}^{q\times q}$ be rank-$r$ matrices. Let $\sigma$ be the smallest nonzero singular value of $X_1$, and suppose that $\| X_1 - X_2 \|_2 \leq \sigma / 8$. Then $\| \cp_{\ct(X_1)^{\perp}} (X_2) \|_F \leq \sqrt{r} \| X_1 - X_2 \|_2^2 / (3\sigma)$, and $\| \cp_{\ct(X_1)^{\perp}} (X_2) \|_2 \leq \| X_1 - X_2 \|_2^2 / (5\sigma)$.
\end{proposition}

\begin{proof} [Proof of Proposition \ref{thm:lgmmod}]
	Let $\Delta = X_2 - X_1$, and $\kappa = \sigma /4$.  By combining equation (1.5) in the s.m. of \cite{CPW:12} with the proofs of Propositions 1.2 and 2.2 in the s.m. of \cite{CPW:12} it can be shown that $ \cp_{\ct(X_1)^{\perp}} (X_2)  = (1/(2\pi i))\oint_{\mathcal{C}_{\kappa}} \zeta [X_1-\zeta I]^{-1} \Delta [X_1-\zeta I]^{-1} \Delta [X_2-\zeta I]^{-1} d \zeta $, where the contour integral is taken along $\mathcal{C}_{\kappa}$ defined as the circle centered at the origin with radius $\kappa$.
	
	By a careful use of the inequality $\|AB\|_F \leq \|A\|_2 \|B\|_F$, we have $\|[X_1-\zeta I]^{-1} \Delta [X_1-\zeta I]^{-1} \Delta [X_2-\zeta I]^{-1}\|_F \leq \|[X_1-\zeta I]^{-1}\|_2 \|\Delta \|_F \|[X_1-\zeta I]^{-1}\|_2 \|\Delta\|_2 \|[X_2-\zeta I]^{-1}\|_2$. Since $\Delta$ is the difference of two rank-$r$ matrices we have $\|\Delta\|_F \leq \sqrt{2r}\|\Delta \|_2$. We proceed to apply the same bounds as those used in the proof of Proposition 1.2 in the s.m. of \cite{CPW:12} to obtain $\|\cp_{\ct(X_1)^{\perp}}(X_2)\|_F \leq \sqrt{2r}\kappa^2 \|\Delta\|_2^2 / ((\sigma-\kappa)^2(\sigma - 3\kappa /2)) \leq \sqrt{r} \| X_1 - X_2 \|_2^2 / (3\sigma)$.
	
	The proof of the second inequality follows from a similar argument.
\end{proof}

We define the following distance measure between two subspaces $\ct_1$ and $\ct_2$ \cite{CPW:12}
\begin{equation*}
	\rho(\ct_1,\ct_2) := \underset{\|N\|_2 \leq 1}{\sup} \| \cp_{\ct_1}-\cp_{\ct_2}(N) \|_2.
\end{equation*}
This definition is useful for quantifying the distance between tangent spaces with respect to the variety of low-rank matrices for pairs of nearby matrices.

\begin{lemma}\label{thm:pertubationrestrictedlinearmap}
	Suppose $X_1$ and $X_2$ are matrices with rank at most $r$, and satisfy $\| X_1 - X_2 \|_2 \leq \sigma / 8$, where $\sigma$ is the smallest non-zero eigenvalue value of $X_2$. Let $\ct_1: \ct(X_1)$ and $\ct_2: = \ct(X_2)$ be tangent spaces on the variety of matrices with rank at most $r$ at the points $X_1$ and $X_2$ respectively. Let $\mathcal{L}$ be a linear map satisfying the restricted isometry condition $\delta_{4r}(\mathcal{L}) \leq 1/10$. Suppose $Z_i \in \ct_i$, $i \in \{1,2\}$, then $\|\cp_{\ct_1}[(\mathcal{L}^{\prime}_{\ct_1}\mathcal{L}_{\ct_1})^{-1}]_{\ct_1} \cp_{\ct_1} (Z_1) - \cp_{\ct_2}[(\mathcal{L}^{\prime}_{\ct_2}\mathcal{L}_{\ct_2})^{-1}]_{\ct_2} \cp_{\ct_2} (Z_2) \|_F \leq 4.3 \sqrt{r} \| Z_1 - Z_2\|_2 + 16 r\|X_1 - X_2 \|_2 \|Z_2 \|_2 / \sigma$.
\end{lemma}

\begin{proof}[Proof of Lemma \ref{thm:pertubationrestrictedlinearmap}]
	
	To simplify notation we denote $Y_i = \cp_{\ct_i} [(\mathcal{L}^{\prime}_{\ct_i}\mathcal{L}_{\ct_i})^{-1}]_{\ct_i} \cp_{\ct_i} (Z_i)$, $i \in \{1,2\}$.  From the triangle inequality we have $\| Y_1 - Y_2 \|_F \leq \| \cp_{\ct_1^{\perp}} (Y_1 - Y_2) \|_F + \| \cp_{\ct_1} (Y_1 - Y_2) \|_F $. We bound both components separately.
	
	[$ \| \cp_{\ct_1^{\perp}} (Y_1 - Y_2) \|_F $]: From Proposition 2.1 of the s.m. of \cite{CPW:12} we have $\rho(\ct_1,\ct_2) \leq \frac{2}{\sigma}\|X_1-X_2\|_2$.  From Proposition \ref{thm:weakincoherencebound} we have $\| Y_2 - Z_2 \|_F \leq \delta_{4r} \| Y_2 \|_F \leq \frac{\delta_{4r}}{1-\delta_{4r}} \| Z_2 \|_F \leq \frac{\sqrt{2r}\delta_{4r}}{1-\delta_{4r}} \| Z_2 \|_2$. Hence
	\begin{eqnarray*}
		\| \cp_{\ct_1^{\perp}}(Y_2 - Z_2)\|_F & = & \| [\sfi-\cp_{\ct_1}]([\cp_{\ct_1} - \cp_{\ct_2}](Y_2 - Z_2))\|_F \\
		& \leq & 2 \sqrt{r} \| [\cp_{\ct_1} - \cp_{\ct_2}](Y_2 - Z_2) \|_2 \\
		& \leq & 2 \sqrt{r} \rho(\ct_1,\ct_2)\|Y_2 - Z_2\|_2 \leq \frac{4\sqrt{2}r}{\sigma} \frac{\delta_{4r}}{1-\delta_{4r}} \|X_1-X_2\|_2 \| Z_2 \|_2 .
	\end{eqnarray*}
	Next
	\begin{equation*}
		\| \cp_{\ct_1^{\perp}}( Z_2)\|_F  = \| \cp_{\ct_1^{\perp}}( Z_1 - Z_2)\|_F \leq \| Z_1 - Z_2 \|_F \leq 2 \sqrt{r} \| Z_1 - Z_2 \|_2.
	\end{equation*}	
	By combining both bounds we have
	\begin{eqnarray*}
		\| \cp_{\ct_1^{\perp}}(Y_1 - Y_2)\|_F & = & \| \cp_{\ct_1^{\perp}}( Y_2)\|_F \leq \| \cp_{\ct_1^{\perp}}( Z_2)\|_F + \| \cp_{\ct_1^{\perp}}( Y_2 - Z_2 )\|_F \\
		& \leq & 2\sqrt{r} \| Z_1 - Z_2 \|_2 + \frac{4\sqrt{2}r}{\sigma} \frac{\delta_{4r}}{1-\delta_{4r}} \|X_1-X_2\|_2 \| Z_2 \|_2.
	\end{eqnarray*}
	
	[$ \| \cp_{\ct_1} (Y_1 - Y_2) \|_F $]: Define the linear map $\mathsf{G} =  \mathcal{L}^{\prime}_{\ct_1 \cup \ct_2} \mathcal{L}_{\ct_1 \cup \ct_2} $. By Proposition \ref{thm:weakincoherencebound} and the triangle inequality we have
	\begin{align*}
		& ~~ \| \cp_{\ct_1} (Y_1 - Y_2) \|_F \\
		\leq & ~~  \frac{1}{1- \delta_{4r}} \| [\cp_{\ct_1} \circ \mathsf{G} \circ \cp_{\ct_1}] (Y_1 - Y_2) \|_F \\
		\leq & ~~  \frac{1}{1- \delta_{4r}}  ( \| [\cp_{\ct_1} \circ \mathsf{G} \circ \cp_{\ct_1}] (Y_1) - [\cp_{\ct_2} \circ \mathsf{G} \circ \cp_{\ct_2}](Y_2) \|_F  \\
		& \quad + \|[\cp_{\ct_2} \circ \mathsf{G} \circ \cp_{\ct_2}] (Y_2) - [\cp_{\ct_1} \circ \mathsf{G} \circ \cp_{\ct_2}] (Y_2) \|_F + \|[\cp_{\ct_1} \circ \mathsf{G} \circ \cp_{\ct_2}] (Y_2) - [\cp_{\ct_1} \circ \mathsf{G} \circ \cp_{\ct_1}] (Y_2) \|_F) \\
		\leq & ~~  \frac{1}{1- \delta_{4r}} (2 \sqrt{r} \|Z_1 - Z_2 \|_2 + 4\sqrt{2} r \rho(\ct_1,\ct_2) \frac{1+\delta_{4r}}{1-\delta_{4r}} \|Z\|_2 ).
	\end{align*}	
	
	The last inequality is justified as follows. First $ \|[\cp_{\ct_2} \circ \mathsf{G} \circ \cp_{\ct_2}] (Y_2) - [\cp_{\ct_1} \circ \mathsf{G} \circ \cp_{\ct_2}] (Y_2) \|_F \leq 2 \sqrt{r} \| [\cp_{\ct_2} \circ \mathsf{G} \circ \cp_{\ct_2}] (Y_2) - [\cp_{\ct_1} \circ \mathsf{G} \circ \cp_{\ct_2}] (Y_2) \|_2 \leq 2 \sqrt{r} \rho(\ct_1,\ct_2) \| \mathsf{G} (Y_2)\|_2 $, where $ \|\mathsf{G}(Y_2) \|_2 \leq \|\mathsf{G}(Y_2) \|_F \leq (1+\delta_{4r}) \| Y_2 \|_F \leq \frac{1+ \delta_{4r}}{1 - \delta_{4r}} \|Z_2\|_F \leq \sqrt{2r}\frac{1+ \delta_{4r}}{1 - \delta_{4r}} \| Z_2 \|_2$.  Second $\|[\cp_{\ct_1} \circ \mathsf{G} \circ \cp_{\ct_2}] (Y_2) - [\cp_{\ct_1} \circ \mathsf{G} \circ \cp_{\ct_1}] (Y_2) \|_F = \|[\cp_{\ct_1} \circ \mathsf{G} \circ (\cp_{\ct_1} - \cp_{\ct_2})](Y_2)\|_F \leq \| [\mathsf{G} \circ (\cp_{\ct_1} - \cp_{\ct_2})] (Y_2)\|_F \leq (1+\delta_{4r})\| [\cp_{\ct_1} - \cp_{\ct_2}](Y_2)\|_F \leq 2 \sqrt{r} (1+\delta_{4r})\| [\cp_{\ct_1} - \cp_{\ct_2}] (Y_2)\|_2 \leq 2 \sqrt{r} (1+\delta_{4r}) \rho(\ct_1,\ct_2) \|Y_2\|_2$, where $\| Y_2 \|_2 \leq \|Y_2 \|_F \leq \frac{\sqrt{2r}}{1 - \delta_{4r}} \| Z \|_2$.
\end{proof}

\begin{proof}[Proof of Proposition \ref{thm:varietyconstrainedoptimization}] We prove (i) and (ii) in sequence.
	
	[(i)]: Let $\hat{X}_o$ be the optimal solution to the following
	\begin{equation*}
		\hat{X}_o ~=~ \underset{X}{\mathrm{argmin}}~\| \by - \mathcal{L}(X) \|^2_2 \quad\quad
		\mathrm{s.t.} \quad\quad \mathrm{rank}(X) \leq r, \quad\quad \| X - X^{\star} \|_2 \leq 4 \sqrt{r} \epsilon.
	\end{equation*}
	Since $4 \sqrt{r} \epsilon < 1/2 \leq \sigma_r (X^{\star})$, $\hat{X}_o$ has rank exactly $r$, and hence is a smooth point with respect to the variety of matrices with rank at most $r$. Define the tangent space $\hat{\ct}:= \ct (\hat{X}_o)$, and the matrix $\hat{X}_c$ as the solution to the following optimization instance
	\begin{equation*}
		\hat{X}_{c}~=~\underset{X}{\mathrm{argmin}}~\| \by - \mathcal{L}(X) \|^2_2  \quad\quad \mathrm{s.t.} \quad\quad X \in \hat{\ct}, \quad\quad \| X - X^{\star} \|_2 \leq 4 \sqrt{r} \epsilon.
	\end{equation*}
	Here $\hat{X}_c$ is the solution to the optimization instance where the constraint $X \in \hat{\ct}$, which is convex, replaces the only non-convex constraint in the previous optimization instance. Hence $\hat{X}_{c} = \hat{X}_o$.  Define $\hat{X}_{\hat{\ct}}$ as the solution to the following optimization instance
	\begin{equation*}
		\hat{X}_{\hat{\ct}}~=~\underset{X}{\mathrm{argmin}} ~\| \by - \mathcal{L}(X) \|^2_2 \quad \quad
		\mathrm{s.t.} \quad \quad X \in \hat{\ct}.
	\end{equation*}
	The first order condition is given by $\mathcal{L}^{\prime} \mathcal{L} ( \hat{X}_{\hat{\ct}} - X^{\star}) - \mathcal{L}^{\prime} \bz + Q_{\hat{\ct}^{\perp}} = 0$,	where $Q_{\hat{\ct}^{\perp}} \in \hat{\ct}^{\perp}$ is the Lagrange multiplier associated to the constraint $X \in \hat{\ct}$. Project the above equation onto the subspace $\hat{\ct}$ to obtain $[\cp_{\hat{\ct}} \circ \mathcal{L}^{\prime} \mathcal{L} \circ \cp_{\hat{\ct}}] ( \hat{X}_{\hat{\ct}} - X^{\star}) = [\cp_{\hat{\ct}} \circ \mathcal{L}^{\prime} \mathcal{L} \circ \cp_{\hat{\ct}^{\perp}}] (X^{\star}) + \cp_{\hat{\ct}} (\mathcal{L}^{\prime}\bz)$, and hence
	\begin{equation*}
		\hat{X}_{\hat{\ct}} - X^{\star} = \cp_{\hat{\ct}} [(\mathcal{L}^{\prime}_{\hat{\ct}}\mathcal{L}_{\hat{\ct}})^{-1}]_{\hat{\ct}} \cp_{\hat{\ct}} \circ \left( [\mathcal{L}^{\prime} \mathcal{L} \circ \cp_{\hat{\ct}^{\perp}}] (X^{\star}) + \mathcal{L}^{\prime}\bz \right) - \cp_{\hat{\ct}^{\perp}} (X^{\star}).
	\end{equation*}
	
	We proceed to bound $\| \hat{X}_{\hat{\ct}} - X^{\star}\|_2$.
	First we have $\| \hat{X}_{c} - X^{\star}\|_2 \leq 4 \sqrt{r} \epsilon \leq 1/5$, and hence $\sigma_r (\hat{X}_c) \geq 19/80$. Second by applying Proposition \ref{thm:lgmmod}, we have $\| \cp_{\hat{\ct}^{\perp}} (X^{\star}) \|_2 = \| \cp_{\hat{\ct}^{\perp}} (\hat{X}_c - X^{\star}) \|_2 \leq (4 \sqrt{r} \epsilon)^2 / (5 \sigma_r (\hat{X}_c)) \leq 256 r \epsilon^2/19$, and $\|\cp_{\hat{\ct}^{\perp}} (X^{\star})\|_F \leq 1280 r^{3/2} \epsilon^2 / 57$. Third by Proposition \ref{thm:weakincoherencebound} and noting the inequality $\|\cdot\|_2 \leq \|\cdot\|_F$ we have 
	\begin{eqnarray*}
		\| \cp_{\hat{\ct}} [(\mathcal{L}^{\prime}_{\hat{\ct}}\mathcal{L}_{\hat{\ct}})^{-1}]_{\hat{\ct}} \cp_{\hat{\ct}} (\mathcal{L}^{\prime} \bz) \|_2 & \leq & \| \cp_{\hat{\ct}} [(\mathcal{L}^{\prime}_{\hat{\ct}}\mathcal{L}_{\hat{\ct}})^{-1}]_{\hat{\ct}} \cp_{\hat{\ct}} \|_{2} \| \cp_{\hat{\ct}}(\mathcal{L}^{\prime} \bz) \|_F \\
		& \leq & 2\sqrt{2r}\| \mathcal{L}^{\prime} \bz \|_2 /(1 - \delta_{4r}) \leq 16\sqrt{r} \epsilon/5.
	\end{eqnarray*}
	Fourth by Proposition 2.7 in \cite{GM:11} we have 
	\begin{eqnarray*}
		\| [\cp_{\hat{\ct}} [(\mathcal{L}^{\prime}_{\hat{\ct}}\mathcal{L}_{\hat{\ct}})^{-1}]_{\hat{\ct}} \cp_{\hat{\ct}} \circ \mathcal{L}^{\prime} \mathcal{L} \circ \cp_{\hat{\ct}^{\perp}}]  (X^{\star}) \|_{2} & \leq &  \| \cp_{\hat{\ct}} [(\mathcal{L}^{\prime}_{\hat{\ct}}\mathcal{L}_{\hat{\ct}})^{-1}]_{\hat{\ct}} \cp_{\hat{\ct}} \|_{2}  \| [\cp_{\hat{\ct}} \circ \mathcal{L}^{\prime} \mathcal{L} \circ \cp_{\hat{\ct}^{\perp}}] (X^{\star}_{\hat{\ct}^{\perp}}) \|_{F} \\
		& \leq & \delta_{4r} \|\cp_{\hat{\ct}^{\perp}}(X^{\star})\|_F /(1 - \delta_{4r}) \leq 1280r^{3/2} \epsilon^2/513 .
	\end{eqnarray*}
	
	
	
	Last, we combine the bounds to obtain $\| \hat{X} - X^{\star} \|_2 \leq 256 r \epsilon^2 / 19 + 16 \sqrt{r} \epsilon/5 + 1280 r^{3/2} \epsilon^2 / 513 < 4 \sqrt{r} \epsilon $. This implies that the constraint $\| X - X^{\star} \|_2 \leq 4 \sqrt{r} \epsilon$ for $\hat{X}_c$ is inactive, so all the optimal solutions coincide.
	
	[(ii)]: We have 
	%	\begin{eqnarray*}
	%	(\hat{X} - X^{\star}) - \cp_{\ct^{\star}} [(\mathcal{L}^{\prime}_{\ct^{\star}}\mathcal{L}_{\ct^{\star}})^{-1}]_{\ct^{\star}} \cp_{\ct^{\star}} (\mathcal{L}^{\prime} \bz) & = & \cp_{\hat{\ct}} [(\mathcal{L}^{\prime}_{\hat{\ct}}\mathcal{L}_{\hat{\ct}})^{-1}]_{\hat{\ct}} \cp_{\hat{\ct}} (\mathcal{L}^{\prime} \bz) - \cp_{\ct^{\star}} [(\mathcal{L}^{\prime}_{\ct^{\star}}\mathcal{L}_{\ct^{\star}})^{-1}]_{\ct^{\star}} \cp_{\ct^{\star}} ( \mathcal{L}^{\prime} \bz) \\
	%	& + &  \cp_{\hat{\ct}} [(\mathcal{L}^{\prime}_{\hat{\ct}}\mathcal{L}_{\hat{\ct}})^{-1}]_{\hat{\ct}} \cp_{\hat{\ct}} \circ \mathcal{L}^{\prime} \mathcal{L} \circ \cp_{\hat{\ct}^{\perp}} (X^{\star})  - \cp_{\hat{\ct}^{\perp}} (X^{\star}).
	%	\end{eqnarray*}
	\begin{eqnarray*}
		G & = & \cp_{\hat{\ct}} [(\mathcal{L}^{\prime}_{\hat{\ct}}\mathcal{L}_{\hat{\ct}})^{-1}]_{\hat{\ct}} \cp_{\hat{\ct}} (\mathcal{L}^{\prime} \bz) - \cp_{\ct^{\star}} [(\mathcal{L}^{\prime}_{\ct^{\star}}\mathcal{L}_{\ct^{\star}})^{-1}]_{\ct^{\star}} \cp_{\ct^{\star}} ( \mathcal{L}^{\prime} \bz) \\
		& + &  [\cp_{\hat{\ct}} [(\mathcal{L}^{\prime}_{\hat{\ct}}\mathcal{L}_{\hat{\ct}})^{-1}]_{\hat{\ct}} \cp_{\hat{\ct}} \circ \mathcal{L}^{\prime} \mathcal{L} \circ \cp_{\hat{\ct}^{\perp}}] (X^{\star})  - \cp_{\hat{\ct}^{\perp}} (X^{\star}).
	\end{eqnarray*}
	We deal with the contributions of each term separately. 
	
	First $\| [\cp_{\ct^{\star}} - \cp_{\hat{\ct}}] (\mathcal{L}^{\prime} \bz) \|_2 \leq \rho(\hat{\ct},\ct^{\star}) \| \mathcal{L}^{\prime} \bz\|_2 \leq \frac{2}{\sigma_{r}(X^{\star})} \| \hat{X} - X^{\star} \|_2 \epsilon \leq 19 \sqrt{r} \epsilon^2$, where the second inequality applies Proposition 2.1 of the s.m. of \cite{CPW:12}.  Second $\|\cp_{\ct^{\star}} (\mathcal{L}^{\prime} \bz)\|_2 \leq 2\|\mathcal{L}^{\prime} \bz\|_2 = 2 \epsilon$. Hence by applying Lemma \ref{thm:pertubationrestrictedlinearmap} with the choice of $Z_1 = \cp_{\hat{\ct}}(\mathcal{L}^{\prime} \bz)$ and $Z_2 = \cp_{\ct^{\star}} (\mathcal{L}^{\prime} \bz)$ we obtain $\| \cp_{\ct^{\star}} [(\mathcal{L}^{\prime}_{\ct^{\star}}\mathcal{L}_{\ct^{\star}})^{-1}]_{\ct^{\star}} \cp_{\ct^{\star}} (\mathcal{L}^{\prime} \bz) - \cp_{\hat{\ct}} [(\mathcal{L}^{\prime}_{\hat{\ct}}\mathcal{L}_{\hat{\ct}})^{-1}]_{\hat{\ct}} \cp_{\hat{\ct}} (\mathcal{L}^{\prime} \bz )\|_F \leq 82 r \epsilon^2 + 539 r^{3/2} \epsilon^2$.  Third we have  $\| [\cp_{\hat{\ct}} [(\mathcal{L}^{\prime}_{\hat{\ct}}\mathcal{L}_{\hat{\ct}})^{-1}]_{\hat{\ct}} \cp_{\hat{\ct}} \circ \mathcal{L}^{\prime} \mathcal{L} \circ \cp_{\hat{\ct}^{\perp}}] (X^{\star}) \|_{F} \leq 1280 r^{3/2} \epsilon^2 / 513$, and $\|\cp_{\hat{\ct}^{\perp}}(X^{\star})\|_F \leq 1280 r^{3/2} \epsilon^2 / 57$.
	
	The bound follows by summing up these bounds.
\end{proof}

The proof of Proposition \ref{thm:varietyconstrainederr} requires two additional preliminary results; in particular, the first establishes the restricted isometry condition for linear maps that are near linear maps that already satisfy the restricted isometry condition.

% % % % % % % % % % % % % % % % % % % % % % % % % % % % % % 
% Restricted isometry condition for nearby linear maps
% % % % % % % % % % % % % % % % % % % % % % % % % % % % % % 

\begin{proposition} \label{thm:ripfornearbylinearmaps}
	Suppose $\mathcal{L}^{\star}$ is a linear map that satisfies the restricted isometry condition $\delta_{r}(\mathcal{L}^{\star}) \leq 1/ 20$.  Let $\sfe$ be a linear operator such that $\|\sfe\|_{\eu} \leq 1/50\|\mathcal{L}^{\star}\|_{2}$.  Then $\mathcal{L} = \mathcal{L}^{\star} \circ (\sfi + \sfe)$ satisfies the restricted isometry condition $\delta_{r}(\mathcal{L}) \leq 1/10$.
\end{proposition}

\begin{proof}[Proof of Proposition \ref{thm:ripfornearbylinearmaps}]  Let $X$ be a matrix with rank at most $r$.  Then
	\begin{equation*}
		\| \mathcal{L} (X) \|_2 \leq \| \mathcal{L}^{\star} (X) \|_2 + \| \mathcal{L}^{\star} \circ \sfe  (X) \|_2 \leq (\sqrt{1+ \delta_{r}(\mathcal{L}^{\star})} + 1/50) \|X\|_F \leq \sqrt{1 + 1/10} \|X \|_F
	\end{equation*}
	A similar argument also proves the lower bound $\| \mathcal{L} (X) \|_2 \geq \sqrt{1 - 1/10} \|X\|_F$.
\end{proof}

\begin{lemma}\label{thm:operatorf2to2} Suppose $\mathcal{L}$ satisfies the restricted isometry condition $\delta_1(\mathcal{L}) < 1$.  Then $\|\mathcal{L}^{\prime} \mathcal{L}\|_{F,2}\leq \sqrt{2(1+\delta_1(\mathcal{L}))} \| \mathcal{L} \|_2 $.
\end{lemma}

\begin{proof}
	Let $Z \in \mathrm{argmax}_{X:\|X\|_F\leq 1} \| \mathcal{L}^{\prime} \mathcal{L} (X)\|_2$, and let $\ct$ be the tangent space of the rank-one matrix corresponding to the largest singular value of $Z$. Then $\sup_{X:\|X\|_F \leq 1} \| \mathcal{L}^{\prime} \mathcal{L} (X)\|_2 \leq \sup_{X:\|X\|_F \leq 1} \| [\cp_{\ct} \circ \mathcal{L}^{\prime} \mathcal{L}] (X)\|_2 \leq \sqrt{2} \sup_{X:\|X\|_F \leq 1} \| [\cp_{\ct} \circ \mathcal{L}^{\prime} \mathcal{L}] (X)\|_F \leq \sqrt{2} \| \cp_{\ct} \circ \mathcal{L}^{\prime} \mathcal{L} \|_2 $.  By Proposition \ref{thm:weakincoherencebound} we have $\sqrt{2} \| \cp_{\ct} \circ \mathcal{L}^{\prime} \mathcal{L} \|_2 \leq \sqrt{2(1+\delta_1(\mathcal{L}))} \|\mathcal{L}\|_2$.
\end{proof}


\begin{proof}[Proof of Proposition \ref{thm:varietyconstrainederr}]
	To simplify notation we denote $\ct:=\ct(X^{\star})$.  Without loss of generality we may assume that $\|X^{\star}\|_2=1$.  By the triangle inequality we have
	\begin{align*}
		& ~~ \| (X^{\star} - \mathcal{M}(\hat{X})) - \cp_{\ct}[(\mathcal{L}^{\star\prime}_{\ct} \mathcal{L}^{\star}_{\ct})^{-1}]_{\ct} \cp_{\ct} \circ \mathcal{L}^{\star\prime} \mathcal{L}^{\star} \circ \sfe (X^{\star}) \|_F \\
		\leq & ~~ \| (X^{\star} - \mathcal{M}(\hat{X})) - \cp_{\ct} [( (\sfi+\sfe^{\prime}) \circ \mathcal{L}^{\star\prime} \mathcal{L}^{\star} \circ (\sfi+\sfe) |_{\ct} )^{-1}]_{\ct} \cp_{\ct} \circ (\sfi+\sfe^{\prime}) \circ \mathcal{L}^{\star\prime} \mathcal{L}^{\star} \circ \sfe (X^{\star}) \|_F \\
		+ & ~~ \| (\cp_{\ct} [((\sfi+\sfe^{\prime}) \circ \mathcal{L}^{\star\prime} \mathcal{L}^{\star} \circ (\sfi+\sfe)|_{\ct} )^{-1}]_{\ct} \cp_{\ct} - \cp_{\ct} [(\mathcal{L}^{\star\prime}_{\ct} \mathcal{L}^{\star}_{\ct} )^{-1}]_{\ct} \cp_{\ct} ) \circ (\sfi+\sfe^{\prime}) \circ \mathcal{L}^{\star\prime} \mathcal{L}^{\star} \circ \sfe (X^{\star}) \|_F \\
		+ & ~~ \| \cp_{\ct} [(\mathcal{L}^{\star\prime}_{\ct} \mathcal{L}^{\star}_{\ct} )^{-1}]_{\ct} \cp_{\ct} \circ (\sfi+\sfe^{\prime}) \circ \mathcal{L}^{\star\prime} \mathcal{L}^{\star} \circ \sfe (X^{\star}) - \cp_{\ct} [(\mathcal{L}^{\star\prime}_{\ct} \mathcal{L}^{\star}_{\ct} )^{-1}]_{\ct} \cp_{\ct} \circ \mathcal{L}^{\star\prime} \mathcal{L}^{\star} \circ \sfe (X^{\star}) \|_F
	\end{align*}
	We bound each term separately.
	
	[First term]:
	First by Proposition \ref{thm:ripfornearbylinearmaps} the linear map $\mathcal{L}^{\star} \circ (\sfi+\sfe)$ satisfies the restricted isometry condition $\delta_{4r}(\mathcal{L}^{\star} \circ (\sfi+\sfe)) \leq 1/10$.  Second we have $\|\sfi+\sfe\|_{2,2}\leq 1+\sqrt{q} \|\sfe\|_{\eu} \leq 51/50$.  Third from Proposition \ref{thm:operatorf2to2} we have $\|\mathcal{L}^{\star\prime}\mathcal{L}^{\star}\|_{F,2} \leq \sqrt{2(1+\delta_{4r}(\mathcal{L}^{\star}))} \|\mathcal{L}^{\star}\|_{2}$.  Fourth $\|\sfe(X^{\star})\|_{F} \leq \sqrt{r} \|\sfe\|_{\eu}$.  Hence
	\begin{equation*}
		\| (\sfi+\sfe^{\prime}) \circ \mathcal{L}^{\star\prime} \mathcal{L}^{\star} \circ \sfe (X^{\star}) \|_2 \leq \|\sfi+\sfe\|_{2,2} \|\mathcal{L}^{\star\prime}\mathcal{L}^{\star}\|_{F,2} \|\sfe\|_{\eu}\|X^{\star}\|_F \leq (3/2) \sqrt{r} \|\mathcal{L}^{\star}\|_{2} \|\sfe\|_{\eu} \leq 1/(20r^{3/2}).
	\end{equation*}
	Consequently, by applying Proposition \ref{thm:varietyconstrainedoptimization} to the optimization instance \eqref{eq:varietyoptimization}, we have
	\begin{eqnarray*}
		& & \| (X^{\star} - \mathcal{M}(\hat{X})) - \cp_{\ct} [( (\sfi+\sfe^{\prime}) \circ \mathcal{L}^{\star\prime} \mathcal{L}^{\star} \circ (\sfi+\sfe) |_{\ct} )^{-1}]_{\ct} \cp_{\ct} \circ (\sfi+\sfe^{\prime}) \circ \mathcal{L}^{\star\prime} \mathcal{L}^{\star} \circ \sfe (X^{\star}) \|_F \\
		& \leq & 1470 r^{5/2} \|\mathcal{L}^{\star}\|_{2}^2 \|\sfe\|_{\eu}^2.
	\end{eqnarray*}
	
	[Second term]:  First by Proposition \ref{thm:weakincoherencebound} we have $\|\cp_{\ct}[(\mathcal{L}^{\star\prime}_{\ct} \mathcal{L}^{\star}_{\ct} )^{-1}]_{\ct} \cp_{\ct} \|_2 \leq 20/19$.  Second by the triangle inequality we have $\| \cp_{\ct} \circ (\sfi+\sfe^{\prime}) \circ \mathcal{L}^{\star\prime} \mathcal{L}^{\star} \circ (\sfi+\sfe) \circ \cp_{\ct} - \cp_{\ct} \circ \mathcal{L}^{\star\prime} \mathcal{L}^{\star} \circ \cp_{\ct} \|_2 \leq 3 \|\mathcal{L}^{\star}\|_{2} \|\sfe\|_{\eu}$.  Third by utilizing the identity $(\mathsf{A} - \mathsf{B})^{-1} = \mathsf{A}^{-1} + \mathsf{A}^{-1} \circ \mathsf{B} \circ \mathsf{A}^{-1} + \mathsf{A}^{-1} \circ \mathsf{B} \circ \mathsf{A}^{-1} \circ \mathsf{B} \circ \mathsf{A}^{-1} + \ldots $ with $\mathsf{A} = \cp_{\ct} [(\mathcal{L}^{\star\prime}_{\ct} \mathcal{L}^{\star}_{\ct})^{-1}]_{\ct} \cp_{\ct}$ and $\mathsf{B} = \cp_{\ct} [((\sfi+\sfe)^{\prime} \circ \mathcal{L}^{\star\prime} \mathcal{L}^{\star} \circ (\sfi+\sfe)|_{\ct})^{-1}]_{\ct} \cp_{\ct}$ we obtain
	\begin{equation*}
		\| \cp_{\ct} [((\sfi+\sfe^{\prime}) \circ \mathcal{L}^{\star\prime} \mathcal{L}^{\star} \circ (\sfi+\sfe)|_{\ct} )^{-1}]_{\ct} \cp_{\ct} - \cp_{\ct} [(\mathcal{L}^{\star\prime}_{\ct} \mathcal{L}^{\star}_{\ct} )^{-1}]_{\ct} \cp_{\ct} \|_2 \leq 4 \|\mathcal{L}^{\star}\|_{2} \|\sfe\|_{\eu}.
	\end{equation*}
	Fourth $\|\cp_{\ct} \circ (\sfi+\sfe^{\prime}) \circ \mathcal{L}^{\star\prime} \mathcal{L}^{\star} \circ \sfe (X^{\star}) \|_F \leq (11/10) \sqrt{r} \|\mathcal{L}^{\star}\|_{2} \|\sfe\|_{\eu}$.  Hence
	\begin{eqnarray*}
		& & \| (\cp_{\ct} [((\sfi+\sfe^{\prime}) \circ \mathcal{L}^{\star\prime} \mathcal{L}^{\star} \circ (\sfi+\sfe)|_{\ct} )^{-1}]_{\ct} \cp_{\ct} - \cp_{\ct} [(\mathcal{L}^{\star\prime}_{\ct} \mathcal{L}^{\star}_{\ct} )^{-1}]_{\ct} \cp_{\ct} ) \circ (\sfi+\sfe^{\prime}) \circ \mathcal{L}^{\star\prime} \mathcal{L}^{\star} \circ \sfe (X^{\star}) \|_F \\
		& \leq & 5\sqrt{r} \|\mathcal{L}^{\star}\|_{2}^2 \|\sfe\|_{\eu}^2.
	\end{eqnarray*}
	
	[Third Term]:  We have
	\begin{eqnarray*}
		&& \| \cp_{\ct} [(\mathcal{L}^{\star\prime}_{\ct} \mathcal{L}^{\star}_{\ct} )^{-1}]_{\ct} \cp_{\ct} \circ (\sfi+\sfe^{\prime}) \circ \mathcal{L}^{\star\prime} \mathcal{L}^{\star} \circ \sfe (X^{\star}) - \cp_{\ct} [(\mathcal{L}^{\star\prime}_{\ct} \mathcal{L}^{\star}_{\ct} )^{-1}]_{\ct} \cp_{\ct} \circ \mathcal{L}^{\star\prime} \mathcal{L}^{\star} \circ \sfe (X^{\star}) \|_F \\
		& \leq & \| \cp_{\ct} [(\mathcal{L}^{\star\prime}_{\ct} \mathcal{L}^{\star}_{\ct} )^{-1}]_{\ct} \cp_{\ct} \|_2 \|\sfe^{\prime}\|_{2} \|\mathcal{L}^{\star}\|_{2}^2 \|\sfe(X^{\star})\|_F \leq 2 \sqrt{r} \|\mathcal{L}^{\star}\|_{2}^2 \|\sfe\|_{\eu}^2.
	\end{eqnarray*}
	
	[Conclude]:  The result follows by summing each bound and applying Proposition \ref{thm:operatorf2to2}.
\end{proof}


\section{Proof of Proposition \ref{thm:structuralresult}} \label{apx:pseudoinverseexpand}

\begin{proof} [Proof of Proposition \ref{thm:structuralresult}]
	To simplify notation we let $\coveig := \coveig \left( \left\{ {X^{(j)}}^{\star}\right\}_{j=1}^{n} \right)$, $\covsup := \covsup \left( \left\{ {X^{(j)}}^{\star}\right\}_{j=1}^{n} \right) $, and $\dd$ be the linear map defined as $\dd:\bz \mapsto \sum_{j=1}^{n} (\mathcal{Q}(\hat{X}^{(j)}) - X^{(j)\star})\bz_j $. In addition we define $\epsilon:= (1/\sqrt{n\coveig}) \|\dd\|_2$.
	
	Since $\|(1/n\coveig)\xx^{\star} \circ \xx^{\star\prime} - \sfi \| \leq \covsup / \coveig \leq 1/6$, we have the bounds $\|((1/n\coveig)\xx^{\star} \circ \xx^{\star\prime})^{-1}\|_2 , \|(1/n\coveig)\xx^{\star}\circ\xx^{\star\prime}\|_2 \leq 6/5$. We also have the bound $\epsilon \leq 1/20$.
	
	We proceed to bound the following. First $\| \dd\circ \dd^{\prime} \circ (\xx^{\star}\circ\xx^{\star \prime})^{-1} \|_{2} \leq \| \dd\|_{2}^2 \| (\xx^{\star}\circ\xx^{\star \prime})^{-1} \|_{2} \leq (6/5) \epsilon^2$.  Second $\| \dd \circ \xx^{\star\prime} \circ (\xx^{\star} \circ \xx^{\star\prime})^{-1} \|_{2} \leq \| \dd \|_{2} \|\xx^{\star\prime} \|_{2} \| (\xx^{\star}\circ \xx^{\star\prime})^{-1} \|_{2} \leq \epsilon (6/5)^{3/2}$. Third $\| \xx^{\star}\circ\dd^{\prime}\circ (\xx^{\star} \circ \xx^{\star\prime})^{-1} \|_{2} \leq \epsilon (6/5)^{3/2}$.
	
	We apply the above error bounds and formally expand the following inverse (as a power series) to obtain
	\begin{eqnarray*}
		& & \bigl((\xx^{\star} + \dd)\circ(\xx^{\star} + \dd)^{\prime} \bigr)^{-1} \\
		&=& \bigl(\bigl( \sfi + \dd\circ\xx^{\star\prime}\circ (\xx^{\star}\circ \xx^{\star\prime})^{-1} + \xx^{\star}\circ \dd^{\prime}\circ(\xx^{\star}\circ \xx^{\star\prime})^{-1} + \sfe_1 \bigr)\circ \xx^{\star} \circ \xx^{\star\prime} \bigr)^{-1} \\
		&=& (\xx^{\star} \circ \xx^{\star\prime})^{-1} \bigl( \sfi - \dd \circ \xx^{\star\prime} \circ (\xx^{\star} \circ \xx^{\star\prime})^{-1} - \xx^{\star}\circ\dd^{\prime}\circ(\xx^{\star} \circ \xx^{\star\prime})^{-1} + \sfe_2 \bigr),
	\end{eqnarray*}
	where $\sfe_1, \sfe_2$ are error terms satisfying $\| \sfe_1 \|_{2} \leq (6/5) \epsilon^2$, and $\| \sfe_2 \|_{2} = \| -\sfe_1 + (\dd \circ \xx^{\star\prime} \circ (\xx^{\star} \circ \xx^{\star\prime})^{-1} + \xx^{\star} \circ \dd^{\prime} \circ (\xx^{\star} \circ \xx^{\star\prime})^{-1} + \sfe_1)^2  - (\ldots)^3 \|_{2} \leq ( \|\sfe_1 \|_{2} + \|\dd\circ \xx^{\star\prime} \circ (\xx^{\star} \circ \xx^{\star\prime})^{-1} + \xx^{\star} \circ \dd^{\prime} \circ (\xx^{\star} \circ \xx^{\star\prime})^{-1} + \sfe_1\|_{2}^2 + \ldots ) \leq (6/5) \epsilon^2 + (\epsilon (6/5) (\epsilon+2\sqrt{6/5}))^2 + \ldots \leq 1.2 \epsilon^2 + (3\epsilon)^2 + (3\epsilon)^3 + \ldots \leq 12\epsilon^2$.
	
	We apply the above expansion to derive the following approximation of the term $\xx^{\star} \circ (\xx^{\star} + \dd)^{+}$
	\begin{eqnarray*}
		& & \xx^{\star}\circ(\xx^{\star} + \dd)^{+} \\
		&=& \xx^{\star} \circ (\xx^{\star} + \dd)^{\prime} \circ \bigl((\xx^{\star} + \dd)\circ(\xx^{\star} + \dd)^{\prime} \bigr)^{-1} \\
		&=& (\xx^{\star}\circ\xx^{\star\prime} + \dd\circ \xx^{\star\prime} )\circ(\xx^{\star}\circ \xx^{\star\prime})^{-1} \bigl( \sfi - \dd \circ \xx^{\star\prime} \circ (\xx^{\star} \circ \xx^{\star\prime})^{-1} - \xx^{\star} \circ \dd^{\prime}\circ(\xx^{\star} \circ \xx^{\star\prime})^{-1} + \sfe_2 \bigr) \\
		&=& (\sfi - \dd \circ \xx^{\star+} + \sfe_3 ),
	\end{eqnarray*}
	where $\sfe_3$ satisfies $\| \sfe_3\|_{2} \leq 2(\epsilon (6/5)^{3/2})^2 + \| \sfe_2 \|_{2} \leq 16\epsilon^2$. 
	
	Next we write $((1/n\coveig)\xx^{\star} \circ \xx^{\star\prime})^{-1} = \sfi + \sfe_4$, where $\|\sfe_4\|_{2} \leq 6\covsup / (5\coveig)$. This allows us to write
	\begin{equation*}
		\xx^{\star} \circ (\xx^{\star} + \dd)^{+} = \sfi - \dd \circ \xx^{\star\prime} \circ (\xx^{\star} \circ \xx^{\star\prime})^{-1} + \sfe_3 =\sfi - (1/n\coveig) \dd \circ \xx^{\star \prime} + \sff,
	\end{equation*}
	where $\|\sff\|_{2} \leq  \| \sfe_3 \|_{2} + \| \dd \circ \xx^{\star\prime} \circ \sfe_4 \|_{2} /n\coveig \leq \| \sfe_3 \|_{2} + \epsilon (6/5)^{1/2} \| \sfe_4 \|_{2} \leq 16 \epsilon^2 + 2 \epsilon \covsup/\coveig$.  The result follows by noting that $\|\sff\|_{\eu} \leq q \|\sff\|_{2}$, and that $\xx^{\star} \circ \hat{\xx}^{+} = \xx^{\star} \circ (\xx^{\star} + \dd)^{+} \circ \mathcal{Q}$.
\end{proof}


\section{Proof of Proposition \ref{thm:induction}} \label{apx:linearizemanifold}

\begin{proposition}\label{thm:projectorwbound}
	Given a linear map $\sfe$, there exists matrices $E_L$, $E_R$ such that
	$\cp_{\mathbb{W}} (\sfe) = I \botimes E_L + E_R \botimes I$, and $\|E_L\|_F,\|E_R\|_F \leq \|\sfe\|_{\eu}/\sqrt{q}$.
\end{proposition}

\begin{proof}[Proof of Proposition \ref{thm:projectorwbound}]
	Define the subspaces $\mathbb{W}_R := \{ S \botimes I : S \in \mathbb{R}^{q \times q} \}$ and $\mathbb{W}_L := \{ I \botimes S : S \in \mathbb{R}^{q \times q} \}$.  Note that $\mathbb{W}_R \cap \mathbb{W}_L = \mathrm{Span}(\sfi)$, and hence $\cp_{\mathbb{W}} = \cp_{\mathbb{W}_R \cap \mathrm{Span}(\sfi)^{\perp}} + \cp_{\mathbb{W}_L \cap \mathrm{Span}(\sfi)^{\perp}} + \cp_{\mathrm{Span}(\sfi)}$.
	
	Let $\sfe$ be an arbitrary linear operator, and let $E_L$ and $E_R$ be matrices such that $E_R \botimes I = \cp_{\mathbb{W}_R \cap \mathrm{Span}(\sfi)^{\perp}} (\sfe) + \frac{1}{2} \cp_{\mathrm{Span}(\sfi)} (\sfe)$, and $I \botimes E_L = \cp_{\mathbb{W}_L \cap \mathrm{Span}(\sfi)^{\perp}} (\sfe) + \frac{1}{2} \cp_{\mathrm{Span}(\sfi)} (\sfe)$.  For $i \in \{L,R\}$ we have the following.  Since $\cp_{\mathbb{W}_i \cap \mathrm{Span}(\sfi)^{\perp}}$ and $\frac{1}{2} \cp_{\mathrm{Span}(\sfi)}$ are projectors onto orthogonal subspaces with spectral norm $1$ and $1/2$ respectively, we have $\| E_i \botimes I \|_{\eu} \leq \| \sfe \|_{\eu}$. Moreover, since $\| E_i \botimes I \|_{\eu} = \|E_i\|_F \|I\|_F$, we have $\|E_i\|_F \leq \|\sfe\|_{\eu}/\sqrt{q}$.
\end{proof}

\begin{proof}[Proof of Proposition \ref{thm:induction}]
	By applying Proposition \ref{thm:projectorwbound}, we have $\cp_{\mathbb{W}}(\sfd) := I\botimes E_L + E_R\botimes I $, for some pair of matrices $E_L,E_R \in \R^{q\times q}$ such that $\|E_L\|_F, \|E_R\|_F \leq \|\sfd\|_{\eu} / \sqrt{q}$.  Moreover $I+E_R$ and $I+E_L$ are invertible, and hence
	\begin{equation*}
		\sfi + \sfd = (\sfi + (\cp_{\mathbb{W}^{\perp}}(\sfd) - E_R \botimes E_L) \circ (I+E_R)^{-1} \botimes (I+E_L)^{-1}) \circ (I + E_R) \botimes (I + E_L).
	\end{equation*}
	We define $\sfm = (\cp_{\mathbb{W}^{\perp}}(\sfd) - E_R \botimes E_L) \circ (I+E_R)^{-1} \botimes (I+E_L)^{-1}$, and we define $\mathcal{W} = (I+E_R)\botimes (I+E_L)$.  It immediately follows that $\|\mathcal{W} - \sfi\|_2 \leq 3\|\sfd\|_{\eu}/\sqrt{q}$.  Note that $\|(I+E_i)^{-1}\|\leq 10/9$, $i\in\{L,R\}$, and $\| (I+E_R)^{-1} \botimes (I+E_L)^{-1} \|_2 \leq 100/81$.  We also have $\|E_R\botimes E_L\|_{\eu} = \|E_R \|_F \|E_L\|_F \leq \|\sfd \|^2_{\eu} /q$.  By noting that $\| (I+E_i)^{-1} - I \|_2 \leq (10/9)\|E_i\|_2$, $i\in\{L,R\}$, we have $\| (I+E_R)^{-1} \botimes (I+E_L)^{-1} - I \botimes I\|_2 \leq 3\|\sfd \|_{\eu} / \sqrt{q}$.
	
	Hence
	$\|\sfm\|_{\eu} \leq \| \cp_{\mathbb{W}^{\perp}}(\sfd) \|_{\eu} \| (I+E_R)^{-1} \botimes (I+E_L)^{-1} - I \botimes I\|_2 + \| E_R \botimes E_L \|_{\eu} \| (I+E_R)^{-1} \botimes (I+E_L)^{-1} \|_2 \leq 5 \| \sfd \|_{\eu}^2 / \sqrt{q}$.
\end{proof}

\section{Proof of Proposition \ref{thm:cauchy}} \label{apx:cauchy}

\begin{proof}[Proof of Proposition \ref{thm:cauchy}]
	To simplify notation in the proof we denote $\alpha_8:=\alpha_8(q,\mathcal{L}^{\star}) = 96 \sqrt{q} \|\mathcal{L}^{\star}\|_{2} $.  We show that
	\begin{equation} \label{eq:pointwisesufficientcondition}
		\| \mathcal{L}^{(t)} - \mathcal{L}^{(t+1)} \|_2 \leq \alpha_{9} \xi_{\mathcal{L}^{\star}}(\mathcal{L}^{(t)}),
	\end{equation}
	for some function $\alpha_9:=\alpha_9(q,r,\mathcal{L}^{\star})$ that we specify later.  In the proof of Theorem \ref{thm:localconvergence} we showed that $\xi_{\mathcal{L}^{\star}}(\mathcal{L}^{(t)}) \leq \gamma^{t} \xi_{\mathcal{L}^{\star}}(\mathcal{L}^{(0)})$ for some $\gamma<1$.  Hence establishing \eqref{eq:pointwisesufficientcondition} immediately implies that the sequence $\{\mathcal{L}^{(t)} \}_{t=1}^{\infty}$ is Cauchy.
	
	Our proof builds on the proof of Theorem \ref{thm:localconvergence}.  Let
	\begin{equation*}
		\mathcal{L}^{(t)} = \mathcal{L}^{\star} \circ (\sfi + \sfe^{(t)}) \circ \mathcal{M}
	\end{equation*}
	where $\sfe^{(t)}$ is a linear map that satisfies $\|\sfe^{(t)}\|_{\eu} < 1/\alpha_0$.  In the proof of Theorem \ref{thm:localconvergence} we show that
	\begin{equation*}
		\mathcal{L}^{(t+1)} = \mathcal{L}^{\star} \circ (\sfi+\sfe^{(t+1)}) \circ \mathcal{W} \circ \mathcal{M} \circ \mathcal{N},
	\end{equation*}
	where $\|\sfe^{(t+1)}\|_{\eu} \leq \|\sfe^{(t)}\|_{\eu}$, $\mathcal{W}$ is a rank-preserver, and $\mathcal{N}$ is a positive definite rank-preserver.  Moreover, as a consequence of applying Proposition \ref{thm:induction} to establish \eqref{eq:nextdictestimate} in the proof, we obtain the bound $\|\mathcal{W} - \sfi\|_2 \leq 3 \alpha_7\|\sfe^{(t)}\|_{\eu}$.  We use these bounds and relations to prove \eqref{eq:pointwisesufficientcondition}.
	
	By the triangle inequality we have
	\begin{eqnarray} \label{eq:cauchytriangleineq}
		\| \mathcal{L}^{(t)} - \mathcal{L}^{(t+1)} \|_2 & \leq &  \| \mathcal{L}^{\star} \circ \sfe^{(t)} \circ \mathcal{M} \|_2 + \| \mathcal{L}^{\star} \circ \sfe^{(t+1)} \circ \mathcal{W} \circ \mathcal{M} \circ \mathcal{N} \|_2 \nonumber \\
		& + & \| \mathcal{L}^{\star}  \circ \mathcal{M} \circ (\mathcal{N} - \sfi) \|_2 + \| \mathcal{L}^{\star} \circ (\mathcal{W} - \sfi) \circ \mathcal{M} \circ \mathcal{N} \|_2.
	\end{eqnarray}
	
	By Proposition \ref{thm:normalizednearothogonal} applied to the pairs of linear maps $\mathcal{L}^{(t)},\mathcal{L}^{\star}$ and $\mathcal{L}^{(t+1)},\mathcal{L}^{\star}$ we have $\|\mathcal{M} - \mathcal{Q}_1 \|_{2}, \|\mathcal{W}  \circ \mathcal{M} \circ \mathcal{N} - \mathcal{Q}_2\|_{2}\leq \alpha_8 \|\sfe^{(t)}\|_{\eu}$, for some pair of orthogonal rank-preservers $\mathcal{Q}_1, \mathcal{Q}_2$.  Since $\alpha_8/\alpha_0\leq 1$ we have $\|\mathcal{M}\|_{2} \leq 2$ and $\|\mathcal{W}  \circ \mathcal{M} \circ \mathcal{N}\|_{2} \leq 2$.  Consequently $\| \mathcal{L}^{\star} \circ \sfe^{(t)} \circ \mathcal{M} \|_2, \| \mathcal{L}^{\star} \circ \sfe^{(t+1)} \circ \mathcal{W} \circ \mathcal{M} \circ \mathcal{N} \|_2 \leq 2\|\mathcal{L}^{\star}\|_2\|\sfe^{(t)}\|_{\eu}$.
	
	Next we bound $\|\mathcal{N} - \sfi\|_2$.  By utilizing $\|\mathcal{W}  \circ \mathcal{M} \circ \mathcal{N} - \mathcal{Q}_2\|_{2} \leq \alpha_8 / \alpha_0$, $\|\mathcal{M} - \mathcal{Q}_1 \|_{2} \leq \alpha_8 \|\sfe^{(t)}\|_{\eu}$, and $\|\mathcal{W} - \sfi\|_2 \leq 3 \alpha_7\|\sfe^{(t)}\|_{\eu}$, one can show that $\|\mathcal{N}-\mathcal{Q}_3\|_2 \leq (6 \alpha_7 + 2\alpha_8 + 2) \|\sfe^{(t)}\|_{\eu}$, where $\mathcal{Q}_3 = \mathcal{Q}_1^{\prime} \circ \mathcal{Q}_2$ is an orthogonal rank-preserver.  Since $\mathcal{N}$ is self-adjoint, we have $\|\mathcal{N}^2-\sfi\|_2 \leq 3 (6 \alpha_7 + 2\alpha_8 + 2) \|\sfe^{(t)}\|_{\eu}$, and hence $\|\mathcal{N}-\sfi\|_2 \leq 3(6 \alpha_7 + 2\alpha_8 + 2)\|\sfe^{(t)}\|_{\eu}$.  This also implies the bound $\|\mathcal{N}\|_2 \leq 3$.
	
	We apply these bounds to obtain $\| \mathcal{L}^{\star}  \circ \mathcal{M} \circ (\mathcal{N} - \sfi) \|_2 \leq 6 (6 \alpha_7 + 2\alpha_8 + 2) \|\mathcal{L}^{\star}\|_2 \|\sfe^{(t)}\|_{\eu}$, and $\| \mathcal{L}^{\star} \circ (\mathcal{W} - \sfi) \circ \mathcal{M} \circ \mathcal{N} \|_2 \leq 9 \alpha_7 \|\mathcal{L}^{\star}\|_2 \|\sfe^{(t)}\|_2$.
	
	We define $\alpha_9 := (4 + 6 (6 \alpha_7 + 2\alpha_8 + 2) + 9 \alpha_7 )\|\mathcal{L}^{\star}\|_2$ (these are exactly the sum of the coefficients of $\|\sfe^{(t)}\|_{\eu}$ in the above bounds).  The result follows by adding these bounds, and subsequently taking the infimum over $\sfe^{(t)}$ in \eqref{eq:cauchytriangleineq}.
\end{proof}


\section*{Acknowledgements}

The authors were supported in part by NSF Career award CCF-1350590, by Air Force Office of Scientific Research grants FA9550-14-1-0098 and FA9550-16-1-0210, by a Sloan research fellowship, and an A*STAR (Agency for Science, Technology, and Research, Singapore) fellowship.

\bibliography{bib_psdreg}

\end{document}
