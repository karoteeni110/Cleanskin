%!TEX root = main.tex

In this section we move to infer correct entailment relation
from the tree alignments.
There are different granularities in designing the 
set of entailment relations. 
In the refined Natural Logic framework \cite{maccartney2009extended}
there are 7 entailment relations of equivalence, forward entailment,
reverse entailment, negation, alternation, cover, and independence.
In our experiments we use the Stanford Natural Language Inference
\cite{bowman2015large} dataset with 3 entailment relations of
entailment, neutral, and contradiction. \footnote{
We do not explicitly consider complicated language phenomena like
implicatives and factives.}


\subsection{Entailment Relation Composition}
Composing entailment relations induced from fractals of the sentence
to form the sentence-level entailment relation is highly non-trivial.
For the Natural Logic framework \newcite{maccartney2009extended}
provided a $7\times 7$ matrix to handle all the possible combinations for
the entailment relation composition. However the composition result
still can not be deterministically decided.
Recent research of \newcite{bowman2015recursive}
propose to learn the entailment relation composition matrix 
with neural networks, but it still seems difficult.

Here we propose a simple model for entailment representation.
In our model, each entailment relation is represented as a vector 
$e\in \mathbb{R}^{1\times r}$, where $r$ is the length
of the entailment relation representation.
The composition of two entailment relations $e_1$
and $e_2$ is modeled by the outer product between the two vectors:
$e_1\otimes e_2$, which gives a result matrix of $\mathbb{R}^{r\times r}$.
We then treat this matrix as a vector of $\mathbb{R}^{1\times r^2}$
and use this vector as an input for a non-linear layer
and map it to a vector of $\mathbb{R}^{1\times r}$ as the
composition result.

A formal summarization of the above procedure is
\[\tilde{e_0} = \sigma(e_1\otimes e_2\cdot W_e),\]
where $\tilde{e_0}$ is the composition result,
and $W_e$ is the matrix of $\mathbb{R}^{r^2 \times r}$
that maps the tensor dot product result to a vector of 
length $r$.\footnote{We abuse the mathematical notation
and assume an automatic conversion from a matrix of 
$\mathbb{R}^{r\times r}$ to a vector of $\mathbb{R}^{r^2}$.
\label{footnote:abuse2}}

The intuition behind our design is that,
we consider each dimension of the entailment vector as a feature
value, and the outer product and conversion from matrix 
to vector is just to collect feature bigrams of the composition.
At last we use a non-linear layer to learn a simple classifier
to get the correct entailment from these bigram features.

As a special case of our design when $r=3$ and
there is no non-linear projection instead of a linear projection,
we can consider each element of the entailment vector 
as an indicator of how likely the entailment belongs
to one of the three entailment relation candidates (entailment,
neutral, and contradiction).
Our composition operation is just a linear classifier 
based on the bigrams of the indicators of the entailment
relations being composed.

\begin{table*}
\begin{center}
\begin{tabular}{c|ccccc}
Method & $k$ & $\card{\theta}_M$ & Train & Dev. & Test\\
\hline\hline
LSTM sent. embedding \cite{bowman2015large} 
& 100 & 221k & 84.8 & - & 77.6\\ \hline
Sparse Features + Classifier \cite{bowman2015large} 
& - & - & 99.7 & - & 78.2\\ \hline
LSTM w/ sentences concatenation \cite{rocktaschel2015reasoning} & 
116 & 252k &83.5 & 82.1 & 80.9\\ \hline
LSTM + word-by-word attention \cite{rocktaschel2015reasoning}
& 100 & 252k & 85.3 & 83.7 & 83.5\\ 
\hline
mLSTM \cite{wang2015learning} & 150 & 544k &91.0 & 86.2 & 85.7\\ 
\hline
mLSTM \cite{wang2015learning} & 300 & 1.9m &92.0 & 86.9 & 86.1\\ 
\hline \hline
LSTM sent. embedding (our implement. of \newcite{bowman2015large})&100& 241k &79.0 & 78.9 & 78.4 \\ \hline
Binary Tree-LSTM (our implementation of \newcite{tai2015improved})& 100& 211k &82.4 & 80.2 & 79.9\\ \hline
\hline
Binary Tree-LSTM + Structured Attention & 100 & 241k& 82.7 & 82.1 & 82.0\\
\hline 
Binary Tree-LSTM + Structured Entailment ($r=20$) & 100 & 221k & 83.8 & 83.6 & 83.2
\end{tabular}
\end{center}
\caption{Comparison between our structured attention and 
structured entailment models with other existing methods.
Column $k$ specifies the length of the meaning representations.
Column $\card{\theta}_M$ specifies the number of parameters
without the word embedding parameters.
\label{tab:results}}
\end{table*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Structured Entailment Model}

Now we combine the entailment composition with the attention model
in Section~\ref{sec:attention}.

The combined model is similar to the attention model 
(Equations~\ref{eq:attention-sim}-\ref{eq:attention-repr})
that it calculates the entailments following the
hypothesis tree in a bottom-up order.

The transitions at each tree node is summarized by Equations~\ref{eq:ent-sim} and \ref{eq:ent-ent}.

\begin{align}
& M(q_0) = \tanh(W_y \norm{Y - h(q_0)\otimes \mathbbm{1}})\label{eq:ent-sim}\\
& E(q_0) = \sigma(M(q_0)W_{E,M} \notag\\
& \pushright{+E(q_1)^TTE(q_2)W_{E,E})\qquad}\label{eq:ent-ent}
\end{align}

Equation~\ref{eq:ent-sim} is exactly the same as 
Equation~\ref{eq:attention-sim} in the attention model,
which calculates the relatedness between 
the given hypothesis node $q_0$ and every premise node.

Equation~\ref{eq:ent-ent} calculates the entailment
relation with attention $E\in\mathbb{R}^{\card{p}\times r}$ 
for the given hypothesis node $q_0$.
In matrix $E$ each row vector corresponds to a node in the premise tree,
and the value of the elements in the row vector indicates 
the strongness of the alignment and the entailment relation.

$E$ is calculated using two terms. The first term $W_{E,M}M(q_0)$
considers the pairwise relatedness between $q_0$ and every premise node.
The second term $W_{E,E}E(q_1)TE(q_2)$ considers the 
attention and entailment relation propagated from $q_0$'s children
in one step.

This matrix-tensor-matrix dot product might seem difficult to interpret.
Here we can consider a simplified case where $T$ is a 3-d 
identity tensor, i.e., $T_{i,j,k}=1$ if.f. $i=j=k$.
The result $\tilde{E}(q_0) = E(q_1)TE(q_2)$ is a tensor of 
$\mathbb{R}^{\card{p}\times r\times r}$.

If we take a look at $i$th slice of $\tilde{E}(q_0)$, 
\[
\tilde{E}(q_0)_i =
\begin{bmatrix}
E(q_1)_{i,1}E(q_2)_{i,1} & \cdots & E(q_1)_{i,1}E(q_2)_{i,r}\\
E(q_1)_{i,2}E(q_2)_{i,1} & \cdots & E(q_1)_{i,2}E(q_2)_{i,r}\\
\vdots & \ddots & \vdots \\
E(q_1)_{i,r}E(q_2)_{i,1} & \cdots & E(q_1)_{i,r}E(q_2)_{i,r}
\end{bmatrix},
\]
which essentially means
$\tilde{E}(q_0)_i = E(q_1)\otimes E(q_2)$.

This explains why this tensor product performs 
the operation of entailment relation composition.
After this the result is mapped to a matrix
of $\mathbb{R}^{\card{p}\times r}$ by a dot product
with $W_{E,E}$.\footnote{
Again we abuse the mathematical notations here and assume an
automatic conversion similar to Footnote~\ref{footnote:abuse2}
on the last two dimensions of the dot product result.
Then the converted result is mapped 
by a linear projection.
}

Finally at the root of the hypothesis tree, 
we induce the entailment relation by 
summing up the entailment matrix $E_{root}$
alone the dimension of the nodes,
linearly project the result to a vector of
length of the real number of relations in the dataset,
and pass through a softmax layer to make it a probability
distribution.

Again we use cross-entropy as the optimization objective in the training.

