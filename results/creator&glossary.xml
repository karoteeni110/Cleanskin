<document class="ltx_authors_1line" prefix="dcterms: http://purl.org/dc/terms/">
  <title>Rollback and Forking Detection for Trusted Execution Environments using Lightweight Collective Memory 
  </title><creator>
     
Marcus Brandenburger IBM Research - Zurich 
 
  </creator><creator>
     Christian Cachin IBM Research - Zurich 
  </creator><creator>
     Matthias Lorenz TU Braunschweig 
  </creator><creator>
     R&#252;diger Kapitza TU Braunschweig 
 
  </creator><date /><abstract>
     Novel hardware-aided trusted execution environments, as provided by    ( ),
enable to execute applications in a secure context that enforces
confidentiality and integrity of the application state even when the host
system is misbehaving.
While this paves the way towards secure and trustworthy cloud
computing, essential system support to protect persistent application state
against rollback and forking attacks is missing. 
     In this paper we present  LCM  &#8211; a lightweight protocol to establish a
collective memory amongst all clients of a remote application to detect
integrity and consistency violations.
LCM enables the   &#8211; a lightweight protocol to establish a
collective memory amongst all clients of a remote application to detect
integrity and consistency violations.
LCM enables the  detection of rollback attacks  against the remote
application, enforces the consistency notion of
  against the remote
application, enforces the consistency notion of
 fork-linearizability  and notifies clients about operation
stability. The protocol exploits the trusted execution environment,
complements it with simple client-side operations, and maintains only
small, constant storage at the clients.
This simplifies the solution compared to previous approaches, where
the clients had to
verify all operations initiated by other clients.
We have implemented LCM and demonstrated its advantages with a key-value store
application. The evaluation shows that it introduces low network and
computation overhead; in particular, a LCM-protected key-value store
achieves 0.72x &#8211; 0.98x of a SGX-secured key-value store throughput.  and notifies clients about operation
stability. The protocol exploits the trusted execution environment,
complements it with simple client-side operations, and maintains only
small, constant storage at the clients.
This simplifies the solution compared to previous approaches, where
the clients had to
verify all operations initiated by other clients.
We have implemented LCM and demonstrated its advantages with a key-value store
application. The evaluation shows that it introduces low network and
computation overhead; in particular, a LCM-protected key-value store
achieves 0.72x &#8211; 0.98x of a SGX-secured key-value store throughput. 
  </abstract><glossarydefinition>
     SGX 
     SGX 
     Intel&#8217;s Software Guard Extensions 
     Intel&#8217;s Software Guard Extensions 
  </glossarydefinition><glossarydefinition>
     EPC 
     EPC 
     Enclave Page Cache 
     Enclave Page Cache 
  </glossarydefinition><glossarydefinition>
     EPCM 
     EPCM 
     Enclave Page Cache Map 
     Enclave Page Cache Map 
  </glossarydefinition><glossarydefinition>
     MMU 
     MMU 
     Memory Management Unit 
     Memory Management Unit 
  </glossarydefinition><glossarydefinition>
     AEX 
     AEX 
     Asynchronous Enclave Exit 
     Asynchronous Enclave Exit 
  </glossarydefinition><glossarydefinition>
     AEP 
     AEP 
     Asynchronous Exit Pointer 
     Asynchronous Exit Pointer 
  </glossarydefinition><glossarydefinition>
     ISR 
     ISR 
     Interrupt Service Routine 
     Interrupt Service Routine 
  </glossarydefinition><glossarydefinition>
     OS 
     OS 
     operating system 
     operating system 
  </glossarydefinition><glossarydefinition>
     DoS 
     DoS 
     denial-of-service 
     denial-of-service 
  </glossarydefinition><glossarydefinition>
     SDK 
     SDK 
     Software Development Kit 
     Software Development Kit 
  </glossarydefinition><glossarydefinition>
     TCB 
     TCB 
     trusted computing base 
     trusted computing base 
  </glossarydefinition><glossarydefinition>
     EDL 
     EDL 
     Enclave Description Language 
     Enclave Description Language 
  </glossarydefinition><glossarydefinition>
     API 
     API 
     application programming interface 
     application programming interface 
  </glossarydefinition><glossarydefinition>
     TEE 
     TEE 
     Trusted Execution Environment 
     Trusted Execution Environment 
  </glossarydefinition><glossarydefinition>
     TEE 
  </glossarydefinition><glossarydefinition>
     trusted execution environments 
  </glossarydefinition><section title="Introduction">
    <para>
       Despite numerous efforts by industry and academia cloud computing suffers still from trust issues&#160; .
This is not surprising as companies possess limited control once their applications and data enter the cloud.
Users have to trust the operating personal and a complex software stack composed of management software, virtualization layers, as well as commodity operating systems.
On top, cloud providers are typically reluctant to share their exact system details because this information is critical for their business. 
    </para><para>
       The recently released Software Guard Extensions (SGX)&#160;  technology of Intel is expected to make a change, as it addresses trust issues that customer face when outsourcing services to off-site locations and still gives cloud providers the freedom to not disclose their system details.
SGX offers an instruction set extension that allows to establish trusted execution contexts, called  enclaves .
These enclaves might be tailored and comprise only a small dedicated fraction of an application&#160; .
These enclaves might be tailored and comprise only a small dedicated fraction of an application&#160;  or can contain an entire legacy application and the
necessary operating system support&#160; .
Thereby, the plaintext of enclave-protected data and code is only available for computation inside the CPU, and encrypted as soon as it leaves the CPU package again.
In this way, enclave-residing data is even guarded against unauthorized accesses by higher privileged code and from attackers with administrative rights and physical access. 
    </para><para>
       While SGX can be considered as a big step forward towards trustworthy cloud computing, some attack vectors nevertheless remain.
One important open issue are  rollback  and   and  forking attacks  on stateful applications that make use of persistent storage.
Whereas SGX provides mechanisms against main-memory replay attacks, persistent storage is not under the direct control of SGX and therefore harder to secure.
The need to handle system restarts, operating system crashes, and power outages makes a completely secure solution for state continuity difficult to achieve.
Baumann et al.&#160;  on stateful applications that make use of persistent storage.
Whereas SGX provides mechanisms against main-memory replay attacks, persistent storage is not under the direct control of SGX and therefore harder to secure.
The need to handle system restarts, operating system crashes, and power outages makes a completely secure solution for state continuity difficult to achieve.
Baumann et al.&#160;  who pioneered the field by proposing  application enclaves  acknowledge this issue and suggest to use a central external service that is contacted on every request.
However, this only delegates the problem to an external entity, demands additional remote communication and adds another single point of failure.
Strackx and Piessens&#160;  acknowledge this issue and suggest to use a central external service that is contacted on every request.
However, this only delegates the problem to an external entity, demands additional remote communication and adds another single point of failure.
Strackx and Piessens&#160; , on the other hand, proposed abstractions on top of hardware-based trusted counters.
This and similar approaches   enable immediate detection of forking attacks but suffer from bad performance, as writing and reading trusted non-volatile counters for every request is time-consuming.
Finally, there are a number of approaches that do not rely on secure execution contexts, such as enclaves, but utilize only plain resources of an untrusted provider&#160; .
These systems typically require cooperating clients to verify each server
response. In particular, this comes with additional communication
overhead between clients and server, and requires costly
cryptographic verification. 
    </para><para>
       In this paper we present  Lightweight Collective Memory (LCM)  &#8211; a distributed protocol to establish a collective memory amongst all clients of a remote application to detect integrity and consistency violations.
By leveraging   &#8211; a distributed protocol to establish a collective memory amongst all clients of a remote application to detect integrity and consistency violations.
By leveraging  , such as SGX, LCM keeps client interaction and service state confidential.
It ensures fork-linearizability&#160; , which denotes the strongest consistency notion among the clients that can be achieved in the presence of rollback attacks without direct client-to-client communication and in absence of trusted non-volatile memory.
Furthermore, LCM notifies clients about operation stability.
This criteria refers to  stable  operations where a client can be sure that its request has been acknowledged by a designated number of other clients.
A typical size would be the majority of clients.
Finally, compared to previous approaches that rely on trusted counters,
applications secured by LCM can be migrated across physical TEEs
and maintain their capability
to detect rollback attacks and to enforce fork-linearizability.  operations where a client can be sure that its request has been acknowledged by a designated number of other clients.
A typical size would be the majority of clients.
Finally, compared to previous approaches that rely on trusted counters,
applications secured by LCM can be migrated across physical TEEs
and maintain their capability
to detect rollback attacks and to enforce fork-linearizability. 
    </para><para>
       We implemented LCM as a Java and C++ framework and demonstrate its advantage by
securing a key-value store. We evaluated the performance of the prototype by
using the YCSB benchmark and compare with native execution and SGX-secured approaches.
It turns out that a SGX-secured key-value store achieves 0.42x &#8211; 0.78x performance of
unprotected native execution. However, the performance of LCM is 0.72x &#8211; 0.98x of the SGX-secured key-value store throughput, while on top enabling rollback and forking
detection. 
    </para><para>
       The remainder of the paper is structured as follows.
Sec.&#160;  provides a detailed problem description
and outlines the necessary background. In Sec.&#160; ,
past solutions for state continuity are discussed and the goals of
LCM are stated.
Next, the overall architecture and the LCM protocol are introduced
in Sec.&#160; .
Sec.&#160;  provides the details of our implementation using SGX.
Subsequently, Sec.&#160;  explains the evaluation results.
Finally, Sec.&#160;  outlines related approaches while Sec.&#160;  concludes the paper. 
    </para></section>
  <section title="Problem description">
    <subsection title="System model">
      <para>
         We consider an asynchronous distributed system with&#160; 
 clients &#160; &#160;  and a  server &#160; &#160; . The server contains
a  trusted execution environment&#160;(TEE) , which hosts a  , which hosts a  trusted
execution context &#160; &#160; ; this is an isolated, protected container that
runs an application protocol and is trusted by the clients.
A protocol&#160;  specifies the behavior of the clients, the server&#160; , and
the trusted execution context&#160; . All clients are  correct ,
follow&#160; ,
follow&#160; , and mutually trust each other; clients and the server may crash
but are able to recover with the help of stable storage, which they can
access through  load  and   and  store  operations.
In contrast,&#160;  operations.
In contrast,&#160;  is correct but runs under the control of&#160;  as explained
in detail later;   does not have direct access to stable storage and may
lose its state.
The server is either correct and follows&#160;  or is  Byzantine ,
deviating arbitrarily from&#160; ,
deviating arbitrarily from&#160; . 
      </para><para>
         The clients and   interact by exchanging
messages as specified by&#160; . They communicate indirectly through the server
which should forward messages among them. If   is correct, then their
communication is reliable and respects  first-in first-out (FIFO) 
semantics; otherwise,  
semantics; otherwise,   may arbitrarily interfere with their messages.
Clients have limited communication capabilities beyond this and
do not interact with each other normally.
The clients invoke a stateful application functionality&#160; , which provides
a set of operations;   defines a response and a state
change for every operation. The operations are executed by&#160;  inside the
TEE and, therefore, the state of   is protected from a potentially
malicious&#160; .
We use the standard notions of executions, histories, sequential histories,
real-time order, concurrency, and well-formed executions from the
distributed-computing literature&#160; . In particular, every operation execution is
represented by an  invocation event  and a   and a  response event . An
operation is called  . An
operation is called  complete  when a client receives a response
event. Two operations are   when a client receives a response
event. Two operations are  concurrent  if the invocation event of one
of them occurs before the other operation is complete.  if the invocation event of one
of them occurs before the other operation is complete. 
      </para></subsection>
    <subsection title="Trusted execution context">
      <para>
         A TEE provides a secure context for executing applications, isolated from
the server that hosts the TEE. It protects the confidentiality and
integrity of code and data for the application running inside the execution
context. More specifically,
a trusted execution context&#160;  is instantiated with a protocol&#160; , which
defines the program code executed by&#160; . After server&#160;  has created some
 ,   may start, terminate, and restart   at
its discretion.
Once&#160;  has been created,   running within   cannot be modified
anymore nor may any other protocol&#160;  be executed in&#160; .
The server may also create and run multiple
instances of&#160;  concurrently.
The time between instantiation and termination of&#160;  is called an
 epoch . The entire lifetime of a trusted execution context can span
multiple epochs. . The entire lifetime of a trusted execution context can span
multiple epochs. 
      </para><para>
         The TEE provides access to a secure random number generator that allows to
build cryptographic primitives, such as key generation, encryption and digital
signatures.
The TEE operates a cryptographic key-management infrastructure rooted
in a secret key protected by the TEE, which may
provide a program-specific key to a trusted execution context.
That is, a function   is available to   when
it executes protocol&#160;  and returns a secret key&#160;  that is specific
to&#160;  and the TEE. Another  , which is also instantiated with  , obtains the
same  , but any   running&#160;  or any other TEE
obtains a key different from&#160; . 
      </para><para>
         The clients can verify that a trusted execution context has been
instantiated with a certain protocol&#160;  and that&#160;  is indeed running
inside the TEE. This is essential for the assumption that&#160;  is trusted.
For this purpose clients leverage a procedure called remote
attestation&#160; .
In short, a client with prior information about   sends a challenge to
  and in return receives a cryptographic proof&#160;  that reflects&#160; 
and the underlying TEE. The client then verifies&#160;  and becomes
convinced that&#160;  runs&#160; , based on the cryptographic protocol and on its
trust in the&#160;TEE. 
      </para><para>
         Furthermore,   is equipped with a small protected memory area&#160;  that
can only be accessed by&#160; . It holds the execution-specific state as
defined by&#160; . Neither the server nor any other trusted execution context
can access or modify&#160; .
The protected memory is volatile, thus   is only accessible within an
epoch of&#160; . In other words, when&#160;  stops, crashes, or restarts, then  
is lost. This is not an issue for stateless protocols, but services
without state are generally not very useful; in realistic applications,
where the server maintains some state,   must be restored after&#160;  has
been restarted.
For this reason&#160;  is stored externally on stable storage using  load 
and  
and  store , so that  , so that   can access state from another epoch. 
      </para></subsection>
    <subsection title="Threats">
      <para>
         Normally server   is correct, but it may become  malicious  and
behave incorrectly, when corrupted by an attacker or affected by a software
bug. A malicious server has full control over the
operating system, applications, and the data residing in memory and
stable storage,
but it cannot tamper with code and data in the trusted execution context.
This means   and
behave incorrectly, when corrupted by an attacker or affected by a software
bug. A malicious server has full control over the
operating system, applications, and the data residing in memory and
stable storage,
but it cannot tamper with code and data in the trusted execution context.
This means   is correct and follows&#160;  even though   is malicious. 
      </para><para>
         However,   controls every interaction of   with the environment. A
malicious server may intercept, modify, reorder, discard, or replay
messages to and from&#160; . Although some of those attacks can be prevented
by establishing a secure channel between a client and&#160; , a malicious  
may simply discard their messages; such a  denial-of-service (DoS) 
attack is outside the scope of this work, however. 
attack is outside the scope of this work, however. 
      </para><para>
         The trusted execution context must consider anything that it receives as
untrusted. In particular, this holds when&#160;  accesses the stable storage
through  load  and   and  store , in order to persist its state&#160; , in order to persist its state&#160; . With
a correct  ,  load  always returns the state that has been
  always returns the state that has been
 stored  most recently. For protecting against a malicious&#160;  most recently. For protecting against a malicious&#160; , the
trusted execution context uses encryption and authentication to protect&#160; 
before it leaves&#160; .
Yet, a malicious server may still return a correctly protected but outdated
state to&#160; . We call such a consistency violation a  rollback
attack . In particular, a malicious server may restart&#160; . In particular, a malicious server may restart&#160;  at any time
and  load  its memory from some state that&#160;  its memory from some state that&#160;  has  stored  earlier.  earlier. 
      </para><para>
         Furthermore, a malicious server may start multiple instances of a trusted
execution context and let the clients interact with different instances over
time. In this way, clients may be separated so that they only see
operations of other clients talking to the same instance.
Even if the TEE can run only a single&#160;  at a time,   can multiplex
different copies of the trusted context.
The malicious server might supply a different, but valid state to each
trusted execution context instance, similar to a rollback attack. This
clearly violates the consistency of the data, so that the responses from
different trusted execution contexts to the clients diverge. We call this
a  forking attack ; it is more general than a rollback attack because
multiple instances of&#160; ; it is more general than a rollback attack because
multiple instances of&#160;  answer concurrently to the clients.
Note that with a single instance of   a forking attack always involves at
least one rollback attack.
It is well-known that clients cannot detect rollback and
forking attacks in asynchronous systems, unless they communicate directly
with each other&#160; . 
      </para></subsection>
  </section>
  <section title="Protecting against Forking Attacks">
    <subsection title="Trusted monotonic counters">
      <para>
         For defending an execution context&#160;  against a forking attack, we need to
assure  state continuity , i.e., that the state of&#160; , i.e., that the state of&#160;  evolves
continuously and is never rolled back. One might think that   could simply
maintain a cryptographic hash of&#160;  inside the TEE whenever it
 stores &#160; &#160;  and verify that upon a  load  operation. However, this
does not work because the memory of   operation. However, this
does not work because the memory of   and the TEE is volatile and
disappears when the epoch ends. 
      </para><para>
         To overcome this,&#160;  will need non-volatile storage that survives reboots.
Such defenses have been proposed in the form of an  attested
append-only memory  (A2M)&#160;  (A2M)&#160;  or a  trusted incrementor 
(TrInc)&#160; 
(TrInc)&#160; . These works demonstrate that the functionality
needed from the trusted non-volatile storage can be reduced to a
 trusted monotonic counter &#160;(TMC). &#160;(TMC). 
      </para><para>
         In more detail, suppose   has access to a TMC that is located in the TEE,
the TMC uses a non-volatile storage location that survives power loss, and
the TMC&#8217;s state and its communication with&#160;  are protected from&#160; .
Whenever&#160;   stores      at the untrusted server, it increments the
counter and includes the counter value with the state. When&#160;  is
restored, e.g. after a reboot, it  loads  its state from   its state from  , extracts
the counter value, reads the TMC, and compares it to the extracted counter.
Since&#160;  protects all  stored  data cryptographically with a key known
only inside the TEE, the server cannot tamper with the counter attached
to&#160;  data cryptographically with a key known
only inside the TEE, the server cannot tamper with the counter attached
to&#160; . This allows   to detect rollback attacks.
However, this approach suffers from several disadvantages as we argue now. 
      </para><para>
         First, it is not easy to tolerate concurrent crashes and maintain
liveness&#160;  at the same time; that is, when&#160;  has incremented the TMC but
the server crashes before the counter value has been saved to the
non-volatile trusted area, then&#160;  might falsely accuse the correct server
of performing a rollback attack. The reason is   cannot differentiate
between a rollback attack and a server crash.
In order to tolerate crashes, one can resort to complex schemes that ensure
state continuity, which increment the TMC, save it persistently, and write
state to disk atomically; they either need
hardware modifactions&#160;  or perform a variation of 2-phase
commit&#160; , but the latter only works for deterministic
operations, which can be replayed by   and always give the same output. 
      </para><para>
         Second, TMC-based solutions often suffer from limited performance.
Typically, TMCs are implemented using TPMs which are well known to be
slow&#160; . The reason is that in order to prevent a counter
overflow, the TMC artificially increases the time to increment the counter
to several milliseconds. Although a response time of a several
milliseconds is acceptable for, say, digital right management (DRM), this
has a negative impact on the throughput of a server application
that processes requests at a high rate. 
      </para><para>
         Finally, the main disadvantage of any TMC-based approach is the lack of
location transparency. That is, the TMC is normally bound to one trusted
execution environment within one server. However, in modern cloud-computing
platforms, applications must be able to scale and run on different servers
during their lifetime. This may already be caused by system
maintenance. For the end-user this should be completely transparent, but a
trusted execution context cannot be stopped on one server and restarted on
a different server with the same TMC; this is exactly what trusted
hardware should prevent. Therefore this would require a migration
protocol that needs the help of a trusted party. 
      </para><para>
         For these reasons, we do not consider any solution that requires extra
hardware or restricts the application to be deterministic in this paper.
Instead we exploit the guarantees available with standard TEEs. 
      </para></subsection>
    <subsection title="Ensuring consistency at the clients">
      <para>
         In the model considered here the TEE does not prevent a malicious server
from mounting rollback and forking attacks and from isolating the clients
from each other. The best possible option is to ensure that the clients
remain &#8220;synchronized&#8221; with each other as much as possible and to mitigate
attacks through this. 
      </para><subsubsection title="Fork-linearizability">
        <para>
            Fork-linearizability &#160; &#160;  denotes the strongest consistency
notion among the clients that can be achieved in the presence of rollback attacks
and without client-to-client communication. This well-established notion
ensures that whenever the malicious server has separated two clients, they
can never be joined again to see mutually inconsistent responses from the
server, without one of them detecting the attack. In essence, the server
has to pretend that the inconsistency remains forever. Clearly, the
clients can detect this though a lightweight, out-of-band mechanism. 
        </para><para>
           Protocols that ensure fork-linearizability work by embedding information
about the causal past of each operation into the requests from client to
servers&#160; . They use hash chains, Merkle
trees, and vector clocks for representing the past history of operations
and their context. Such protocols are very similar to the use of hash
chains in blockchain platforms&#160; , cryptocurrencies such as
Bitcoin, and Certificate Transparency&#160; . 
        </para><para>
           The standard notion of  linearizability &#160; &#160;  requires that
the operations of all clients appear to execute atomically in one sequence,
and that the atomic sequence respects the real-time partial order of the
operations that the clients observe.  Fork-linearizability  is
defined as an extension of this, which relaxes the condition of one
sequence to permit multiple &#8220;forks&#8221; of an
execution&#160;  is
defined as an extension of this, which relaxes the condition of one
sequence to permit multiple &#8220;forks&#8221; of an
execution&#160; . Under fork-linearizability, every
client observes a linearizable history and when an operation is observed by
multiple clients, the history of events occurring before the operation is
the same. In this context, the  view  of a client&#160;  of a client&#160;  denotes a
correct, serialized history of operations for the functionality&#160; , which
includes all operations of&#160; . For a more formal treatment we refer to
the literature&#160; . 
        </para><para>
           Unfortunately, fork-linearizability cannot be achieved without taking into
account that some client operations on a correct server are blocked until
other, concurrent operations terminate&#160; . This inherent
limitation has led to the relaxed notions, such as weak
fork-linearizability. In FAUST&#160; , for instance, an operation
returns a response to the client that is not guaranteed to be immediately
fork-linearizable or linearizable, but the protocol notifies the client
later when it knows that other clients have observed the operation as well.
This is captured by the notion of  stability , discussed next. , discussed next. 
        </para></subsubsection>
      <subsubsection title="Operation stability">
        <para>
           We now define a way to inform the client about those of its
operations that have reached some level of consistency
with respect to other clients.
More precisely, we call an operation&#160;  by a client&#160;   stable 
with respect to another client&#160; 
with respect to another client&#160;  if the views of   and   both
include&#160; . In other words,   knows that   has observed&#160; 
and that   was forced to take into account any effects of&#160;  in later
service responses to&#160; . 
        </para><para>
           Operation stability has also been used by&#160; .
Here we use it as follows. We augment the response event of every
operation with two numbers: a  sequence number , which is
assigned by the protocol to the operation that completes; and a
 , which is
assigned by the protocol to the operation that completes; and a
 stable sequence number , which denotes the latest stable
sequence number of this client. The sequence numbers returned at one client are strictly increasing; the stable
sequence numbers never decrease. , which denotes the latest stable
sequence number of this client. The sequence numbers returned at one client are strictly increasing; the stable
sequence numbers never decrease. 
        </para><theorem title="Definition 1 ( Operation stability). Operation stability).">
          <para>
             Let   be a complete operation of   that returns sequence
number&#160; . We say that   is  stable w.r.t.&#160;a client   after
  after
  completes any operation that returns a sequence number that
is bigger than&#160; . Operation   of   is always stable w.r.t.  . 
          </para><para>
             For a set of clients&#160;  that includes&#160; , an operation   of&#160; 
is  stable w.r.t. the set of clients  , when  , when   is stable
w.r.t.&#160;all&#160; . An operation that is stable w.r.t.&#160;all clients
is simply called  stable . . 
          </para></theorem>
        <para>
           One may use different strengths of stability; for example, an operation
might take a long time until it becomes stable (because all clients
must observe it), but it might already be stable at a subset of the clients
much earlier. A particularly useful subset is a majority quorum of the
clients. 
        </para><theorem title="Definition 2 ( Operation stability among a majority&#160; Operation stability among a majority&#160; ).">
          <para>
             An operation   of   is  stable among a majority  of clients, when
  of clients, when
  is stable w.r.t. a set of clients  , where  . 
          </para></theorem>
        <para>
           Note that any subsequence of a history that contains only operations that are stable among a majority is linearizable. 
        </para></subsubsection>
    </subsection>
  </section>
  <section title="Lightweight Collective Memory">
    <para>
       This section introduces  Lightweight Collective Memory     (LCM) , a protocol that allows
a group of mutually trusting clients to run a service on a (potentially
malicious) remote server. It benefits from a trusted execution context&#160; , a protocol that allows
a group of mutually trusting clients to run a service on a (potentially
malicious) remote server. It benefits from a trusted execution context&#160; 
that runs on the server and executes the operations on behalf of the
clients.
LCM facilitates the detection of forking and rollback attacks against  
by ensuring fork-linearizability for every client operation. Moreover, LCM indicates which operations are stable among a majority; this permits
clients to infer when their operations are linearizable.
The LCM protocol benefits from the security guarantees of the TEE; in
contrast to all previous protocols in the line of work originating with
Mazi&#232;res and Shasha&#160; , aiming at fork-linearizable
semantics, the clients do not verify operation results here. Clients only
handle metadata and rely on the TEE for producing correct responses. 
    </para><subsection title="Overview">
      <para>
         LCM executes a stateful functionality&#160;  inside a trusted execution
context&#160;  that is instantiated with the LCM protocol and also runs&#160; .
The trusted&#160;  constructs a hash chain from the history of all operations
that it executes and embeds this information in its responses to the
clients. 
      </para><para>
         A client invokes an operation by sending an encrypted  invoke 
message to the (untrusted) server&#160; 
message to the (untrusted) server&#160; , which forwards all incoming messages
to&#160; .
After   has decrypted this, it first verifies that the view of the client
is consistent with&#160; &#8217;s own history. Then   executes the operation and
assigns a sequence number to it. The operation produces an output for the
client and may modify the state of&#160; . The output is returned to the
client in a  reply  message, together with the sequence number and
the latest stable operation (represented as a sequence number).
When the client receives the   message, together with the sequence number and
the latest stable operation (represented as a sequence number).
When the client receives the  reply  message, it completes the
operation and returns the result, the assigned sequence number, and the
majority-stable sequence number. The latter informs the client about the
stability of its earlier operations.  message, it completes the
operation and returns the result, the assigned sequence number, and the
majority-stable sequence number. The latter informs the client about the
stability of its earlier operations. 
      </para><para>
         Fig.&#160;  shows the protocol interaction.
For simplicity we assume that each client invokes operations sequentially,
that is, it invokes a new operation only after completing the
previous operation.
For protecting&#160;  against a malicious  , three cryptographic keys are
used: 
        </para><para>
         All encryption operations use  authenticated encryption  with a
symmetric-key&#160;  with a
symmetric-key&#160;  and two functions   and
  for a message&#160;  and ciphertext&#160; .
Authenticated encryption produces a ciphertext integrated with a
message-authentication code&#160;(MAC); it protects the content from leaking
information to&#160;  and prevents that   tampers with messages or stored
data by altering ciphertext.
The  hash function  in LCM, denoted   in LCM, denoted  , can be any
cryptographically secure collision-free hash function; it maps a bit
string&#160;  of arbitrary length to a short, unique hash value&#160; . 
      </para></subsection>
    <subsection title="Protocol">
      <subsubsection title="Invocation at the client (Alg.&#160; )">
        <para>
           The client uses variables   and&#160;  to hold sequence
numbers for the last operation by   and the last operation stable among
a majority, respectively. In addition, the client stores  , the hash
chain value computed by&#160;  corresponding to its most recent operation
(with sequence number&#160; ).
When   invokes an operation  , it buffers&#160;  in a variable&#160;  and
sends an encrypted  invoke  message containing   message containing  ,  ,  ,
and&#160; . The latter two values represent the context in which&#160; 
invokes  ; they result from  &#8217;s last operation. 
        </para></subsubsection>
      <subsubsection title="Execution at&#160;  (Alg.&#160; )">
        <para>
           The trusted execution context&#160;  maintains the sequence number of the most
recently executed operation in a counter   and a corresponding hash-chain
value in&#160; .
  processes the operations of the clients sequentially. When&#160;  receives an
 invoke  message from   message from  , it decrypts the message with&#160;  and
signals a violation if the message does not have valid authentication. Then
  verifies that   sent by the client correspond to the last
operation response that   has returned to  . For this purpose,  
maintains a map&#160;  indexed by client identifier, where entry&#160;  holds the
sequence number of the last acknowledged operation by  , the
sequence number and corresponding hash chain value after the last operation
by&#160; . Again, when an inconsistency is detected, then   halts.
This verification is essential for the protocol and has three goals:
First, it acknowledges the previous operation by   in the sense that
  learns that   has actually received the reply for its last
invocation.
Second, this detects message-replay attacks. When a malicious&#160; 
forwards the same  invoke  message multiple times,   message multiple times,   can easily
filter these out with&#160; .
Finally, the verification detects rollback or forking attacks
because the client sends the condensed view of its own history contained in
  and  . 
        </para><para>
           If sequence number and hash chain value verification is successful, then  
increments the sequence number&#160; , and calls&#160; , which applies the
operation&#160;  to state&#160;  and yields the corresponding result&#160;  according
to&#160; . Next,   extends the hash chain   by setting this to  .
With the information from the  invoke  message,   message,   also determines if
more operations have become stable. It uses the data in   and a function
 majority-stable  that returns&#160;  that returns&#160; , the highest sequence number of an
operation stable among a majority. 
        </para><para>
           Then   sends a  reply  message to   message to   encrypted with  ,
containing the sequence number&#160; , the hash chain value&#160; , the
result&#160; , the stable operation&#160; , and the client&#8217;s previous hash chain
value  . Before sending  reply ,  ,   also needs to  store 
the current state for recovering from a crash. For this,  
the current state for recovering from a crash. For this,   encrypts the
service state&#160; , the protocol state&#160; , and the key&#160;  using
 auth-encrypt  with   with   and  stores  this as a   this as a  blob 
through&#160; 
through&#160; . 
        </para></subsubsection>
      <subsubsection title="Verification at the client">
        <para>
           When   receives a&#160; reply  message, it uses   message, it uses   to decrypt
the contents and extracts  ,  ,  ,  , and&#160; . The client
verifies that the previous hash chain value&#160;  is equal to its own  , in
order to match the&#160; reply  message to its most recent   message to its most recent  invoke .
Next,  .
Next,   stores the new sequence number and hash chain value  
and outputs the operation result&#160;  and the majority-stable operation&#160; .
These two sequence numbers allow the client to keep track of the operation
history. In particular, a majority of clients have observed all operations
with sequence numbers up to&#160; . Any operation of   with the sequence
number&#160;  is now stable among a majority.
For correct functioning of the protocol, the state of each client must be
recoverable from stable storage if a client crashes. For simplicity this
is not part of the pseudocode. 
        </para></subsubsection>
      <subsubsection title="Server">
        <para>
           The (correct) server&#160;  runs a TEE and hosts&#160; , which is initially
created by an admin. Whenever&#160;  reboots or crashes, it restarts&#160; .
Recall that a malicious&#160;  may restart the trusted execution context at
any time or even spawn multiple instances.
Furthermore, a correct   forwards all messages between the clients and
the trusted execution context in FIFO order. A malicious server, in
contrast, can discard, reorder or delay messages. 
        </para></subsubsection>
      <subsubsection title="Protocol details">
        <para>
           In the pseudocode in Alg.&#160; &#8211; , the
symbol   denotes the concatenation of bit strings, and the
 assert  statement, parameterized by a condition (where   statement, parameterized by a condition (where   matches
any value), immediately terminates the protocol when the condition is false. The clients and  
use this to signal that the server misbehaved. Note that  auth-decrypt 
may also signal an error; this is equivalent to an assert&#160; 
may also signal an error; this is equivalent to an assert&#160; false  statement.  statement. 
        </para><float>
            LCM Protocol for client&#160; 
             LCM Protocol for client&#160; 
         
        </float><float>
            LCM Protocol for trusted execution context  
             LCM Protocol for trusted execution context  
         
      </float></subsubsection>
    </subsection>
    <subsection title="Bootstrapping">
      <para>
         Bootstrapping sets up the necessary cryptographic keys and security
contexts for trusted execution. It consists of three phases:
(1) creating a trusted execution context&#160;  on a remote server; (2) remote
attestation and provisioning of&#160; ; and (3) key distribution among the
group of clients. 
      </para><para>
         In the first phase, a special  admin  client instructs the server to
create a new trusted execution context&#160;  client instructs the server to
create a new trusted execution context&#160;  for running protocol&#160;LCM (Alg.&#160; ). When&#160;  starts this protocol, it enters
 init  first.
Function   first.
Function  init  is also executed after a reboot, where it first
  is also executed after a reboot, where it first
 loads  the encrypted state from stable storage. During initialization
no such state exists yet.  the encrypted state from stable storage. During initialization
no such state exists yet. 
      </para><para>
         Second, the admin initiates the remote attestation process, to verify
that&#160;  has been started correctly and is running LCM. Remote attestation
is a core function of the TEE and produces a cryptographic proof, which
convinces the admin that   indeed runs LCM. If a malicious   would
instantiate&#160;  with some&#160;  this verification will reveal it.
Note that the remote attestation protocol also convinces the admin that&#160; 
is actually executed on the TEE and protected against a malicious server. 
      </para><para>
         Finally, the admin generates two secret keys,   for securing the
communication and   for storing the protocol state, and injects them
into&#160;  through a secure channel provided by the TEE.
After   has received the keys, it initializes the protocol and service
states, and retrieves a sealing key&#160;  from the
TEE. Recall that   is used to encrypt the state, and that   and
  together are  stored  encrypted&#160;  encrypted&#160; . Since   is
generated in a deterministic way in the trusted hardware of the TEE,  
can recover its state from an earlier epoch using the stable storage of&#160; 
after a crash. And because every   running on a different physical TEE
obtains a different sealing key, this binds the state of   to the
hardware.
The admin also distributes the communication key to the
clients using a secure channel to each of them. 
      </para></subsection>
    <subsection title="System reboot and recovery">
      <para>
         The server&#160;  controls starting and stopping&#160; . As argued before, the
TEE is stateless and, therefore,   cannot distinguish a reboot after a
crash from an attack by&#160; . In order to tolerate server crashes and
reboots without administrative intervention, but also to facilitate planned
restarts, the application state is  stored  on stable storage.  on stable storage. 
      </para><para>
         When the server reboots after a crash, it recreates the trusted execution
context&#160;  that runs LCM.   then enters  init , which first tries
to  , which first tries
to  load  a previous state and resumes from there when it
exists. As   a previous state and resumes from there when it
exists. As   obtains   from the TEE, it can
decrypt and authenticate   and the state with&#160; ; the state also
contains   for communicating with the clients.   recovers   form
the state and can easily derive   from   by looking up the client
which executed the last operation in&#160; . Formally,   is an array of
  triples, and   returns the index of the triples
with the highest sequence number&#160; . 
      </para><para>
           has now entered a new epoch and is ready to continue request processing
without remote attestation. The clients trust that   runs LCM from the
initial verification step during bootstrapping and from the binding of the
sealing key ( ) to the TEE through the secure hardware. Recall that
  recovers the communication key&#160;  via the sealing key. Once a
client can engage in encrypted and properly authenticated communication,
protected through  , to some TEE, the trust of the client from the
initial attestation extends to the current holder of&#160; . 
      </para></subsection>
    <subsection title="Stability">
      <para>
         For determining the stability of operations,   maintains the map&#160;  with
two sequence numbers for every client. One sequence number of the last
acknowledged operation, and another sequence number of the last operation.
The function   returns the sequence number of the
operation that is stable among a majority, that is, the largest acknowledged
sequence number in   that is less than or equal to  more 
than  
than   sequence numbers in&#160; .
Stability indicates to the clients when their operations have been observed
by others and helps detecting forking attacks. When the server is correct
and all clients periodically invoke operations, then all operations become
stable eventually. In the case of a forking attack, where one or more
clients are separated, the operations of the forked clients will cease
to become stable. 
      </para><para>
         The client protocol returns the sequence number   and the majority-stable
sequence number&#160;  together with the operation result. This enables the
client to track the progress of the operation history. Depending on the
application, a client might want to verify that some critical operation has
become stable or wait until it does before invoking new operations. Note
that the client protocol as described in Alg.&#160;  only
receives stability updates when it invokes new operations. If the client
needs to be informed earlier about the stability of past operations, it can
simply invoke  dummy operations  periodically, as introduced by
FAUST&#160;  periodically, as introduced by
FAUST&#160; .
Alternatively, Alg.&#160;  could be extended to support a
callback mechanism, where clients can register for notifications of
stability updates, as also used in Venus&#160; . 
      </para></subsection>
    <subsection title="Extensions">
      <subsubsection title="Tolerating server crashes">
        <para>
           As the server might crash, we now extend the protocol to allow&#160;  to recover and continue processing. In the simple case
where&#160;  crashes while it is idling, the correct server restarts it and
continues with the protocol as described before. On the other hand, when
  crashes during the processing of a client request, we differentiate
between two cases: either it crashes  before  the   the  store  operation
returns and has saved the application and protocol state or
  operation
returns and has saved the application and protocol state or
 afterwards . . 
        </para><para>
           Therefore, we equip the client with a retry mechanism: When the client has not
received a reply until a timer expires, it sends the message again,
but marks it as a retry attempt. In the first case (  crashes before
successfully stores), the server will restart   and it
eventually receives the retry message. The verification of the sequence
number&#160;  and the hash chain value&#160;  ensures that the lost message
has not already been processed.   simply continues processing and
returns the reply to the client. In the second case (when&#160;  crashes
after stores), the verification of   and   fails since
  stored in   is bigger than the value received from&#160; . The
retry marker instructs   to not consider this as a rollback attack.
Therefore, we extend the protocol state&#160;  to store the last operation
result&#160;  as well. Then   can retrieve the result from&#160;  and
(re)send the  reply  message.  message. 
        </para></subsubsection>
      <subsubsection title="Server migration">
        <para>
           Since location transparency is a major advantage in cloud computing, we
also include a migration mechanisms that allows to move a trusted execution
context&#160;  to a different host system.
There are two trusted execution context instances involved,   on the origin
system and   on the target system to which the protocol migrates.
Migration requires cooperation between the two machines and that the
server&#8217;s stable storage can be accessed from the origin and the target
system, for instance by using shared remote storage. 
        </para><para>
           The migration works as follows. The (correct) origin server signals the
target server to start a trusted execution context&#160;  and to prepare it
for migration. Normally,   would try to retrieve a state encryption key
  from stable storage but since it was encrypted with the sealing key
of   on the origin system,   cannot obtain it. For this reason,  
takes over the role of the admin and bootstraps   according to the
earlier description. After a successfully remote attestation,   injects the
state encryption key&#160;  via a secure channel. At this point,   stops
processing requests and provides its current state to&#160; ; then  
restores the application and protocol state, resumes executing requests,
and is still able to uphold the guarantees of LCM against rollback and forking
attacks. 
        </para><para>
           This migration mechanism does not require a trusted party and works
completely transparently for the clients. However, when the origin system
crashes without any possibility to recover, e.g., when the TEE hardware
malfunctions, then an intervention by a trusted admin is required.
In contrast to the solutions based on a TMC mentioned in
Sec.&#160; , this migration mechanism is more robust to server
failures. In particular, the migration of a TMC always requires an admin
to read the last TMC value from the origin system and to update the TMC on
the target system with the correct counter value. Clearly, this fails if
the origin system becomes inaccessible. LCM still allows migration because
the TEE is stateless and because the state is stored on remote storage.
Our proposed migration scheme is similar to&#160; . 
        </para></subsubsection>
      <subsubsection title="Group membership">
        <para>
           In a practical system, the group of clients will dynamically change, as
clients may be removed from the collaboration group and new clients may
join.
Although the protocol formulation uses a static client group, it is easy to
extend LCM for handling dynamic changes. When a new client joins the
group, the admin sends the shared secret&#160;  for secure communication
with the trusted execution context to the new client and instructs   to
include the client in the protocol state. For removing a client, the admin
generates a new fresh communication key&#160;  and distributes it to all
remaining clients. Then the admin sends a removal request with&#160;  to
 , which uses the fresh key afterwards. 
        </para></subsubsection>
    </subsection>
  </section>
  <section title="Implementation">
    <para>
       The LCM protocol relies on our assumptions as described in
Sec.&#160;  and can be implemented with any TEE technolgy such as Intel SGX. 
    </para><subsection title="Intel SGX">
      <para>
          &#160;  adds
hardware enforced security to the Intel CPU architecture.
  enables applications to execute certain code in a trusted execution
context, also called  enclave . Enclaves are isolated and a hardware
enforced mechanism guarantees the confidentiality and the integrity of an
enclave even if the entire system is compromised.
Moreover, the  . Enclaves are isolated and a hardware
enforced mechanism guarantees the confidentiality and the integrity of an
enclave even if the entire system is compromised.
Moreover, the   platform checks that an application has not been tampered
with when loading code and data at initialization into an enclave.
  offers an attestation mechanism&#160;  for enclaves that allows
to prove to a remote third party that an enclave runs a given application on
an actual   platform.
For utilizing the system&#8217;s persistent storage and at the same time preserving
data confidentiality and integrity,   supports data sealing. It permits to
unseal data only by the origin enclave or another enclave by the same
enclave developer.
In the   programming model, applications in an enclave are considered to be
trusted whereas all other applications (even the operating system) are
untrusted. Typically, those enclave applications are small, hence, it is less
likely to expose vulnerabilities.
Using the   Software Development Kit&#160;(SDK)&#160; 
enables developers to divide their applications into a trusted component
(enclave) and untrusted component. The trusted component is signed by the
developer.
For bridging the trust border between enclaves and untrusted components, SGX
provides the Enclave Definition Language&#160;(EDL) that is used by enclave
developers to specify an interface and generate &#8220;gateway&#8221; code comprising
Enclave calls (ecall) and Outside calls (ocall). 
      </para><subsubsection title="Enclave protection">
        <para>
             features two properties that are essential to execute code securely in an
enclave.
First,   verifies that an enclave is instantiated with the correct
application.
The enclave code contains an Enclave Signature (SIGSTRUCT)
produced by the enclave developer that allows the   platform to detect
whether the code of the enclave has been tampered with. In particular,
SIGSTRUCT comprises an enclave measurement (a cryptographic hash that
identifies the code and data), a signature over the measurement, and the enclave developer&#8217;s public key, that
serves as the identity of the enclave developer. When the enclave is loaded,
the CPU verifies the signature and calculates the enclave measurement and compares it to the measurement
in SIGSTRUCT; if they match the enclave completes its instantiation
successfully.
Second,   protects against any access and modification from untrusted
components.
To this end, the enclave resides in an isolated memory area called enclave
page cache (EPC) that can not be accessed from outside an enclave. This is
enforced through a memory access control mechanism. The EPC size is limited
to 128 MB, thus, when enclave reaches that limit or a context switch occurs,
pages are moved to DRAM. A memory encryption engine&#160; 
protects pages when swapping between EPC and DRAM in terms of confidentially,
integrity, as wells provides replay attack prevention.
Those two mechanisms prevent any untrusted component from accessing or
modifying the enclave memory.
Note that this mechanism only protects the in-memory state but not persistent
state of an enclave. When an enclave is terminated, all in-memory state is
lost. 
        </para></subsubsection>
      <subsubsection title="Enclave attestation">
        <para>
             supports remote attestation&#160;  that demonstrates to a remote
client that an enclave runs a given application inside a   platform
and therefore can be considered to be trustworthy.
This is vital for establishing trust in an enclave application and is required
prior provisioning any secrets or protected data.
The remote attestation briefly works as follows:
A remote client sends a challenge to the enclave including a nonce. The
enclave produces a report that comprises some metadata including a short hash
value of the application code, the enclave developer identity, and additional
user data. The user data contains the nonce. Note that enclave developers may
also include custom values in the user data, for instance, some information
about the current enclave state. Additionally, the report comprises a MAC
that is produced using a report key provided by the   platform.
A special enclave, so called Quoting enclave, receives the report and
validates it by using the same report key. The   platform enforces that
only enclaves are able to retrieve this report key, thus, are able to create
and verify report structures.
If the verification succeeds, the Quoting enclave signs the report with a
platform specific key and replaces the MAC with the signature.   leverages
a group signature scheme (EPID&#160; ) that does not
reveal the identity of the platform. In other words, the signature states
that some   platform has produces that signature. The signed report
(Quote) is sent to the remote client which then validates
signature (using an EPID infrastructure), verifies integrity of the attest, and
finally checks that that the Quote matches the challenge using the nonce. 
        </para></subsubsection>
      <subsubsection title="Data sealing">
        <para>
           Application code and data are secured while residing within an enclave.
However, when an enclave is terminated the data is lost and can not be
recovered when the enclave restarts again. Therefore,   features a sealing
mechanism&#160;  based on
AES-GCM-128 that allows to encrypt and authenticate data before it leaves an
enclave by using a special sealing key provided by the   platform.
In particular,   provides two types of sealing: An enclave identity based
sealing that only allows enclaves running the same application to unseal the data;
and enclave developer based sealing where all enclaves, which are developed (signed) by the same developer, can unseal the data. 
        </para></subsubsection>
    </subsection>
    <subsection title="LCM framework">
      <para>
         We implemented LCM as a framework in Java and C++ consisting of a
client-side and a server-side library that can be integrated with SGX-enabled
applications which require rollback and forking detection for persistent state.
The LCM client-library is implemented in Java and follows the description as
presented in Alg.&#160; . It uses AES-GCM with 128-bit keys
provided by the Java Cryptography Extension to protect the confidentiality and
integrity of all protocol messages. The LCM client-library uses a simple
network interface including methods for sending and receiving protocol
message. This allows to reuse an existing application network stack instead of
handling the communication with the server by our library.
The LCM server-side library is implemented in C++ using the Intel   SDK
(Version&#160;1.6)&#160; . It only utilizes trusted libraries
provided by the   SDK, such as  libsgx_tcrypto  for cryptographic
hashing and encryption. In particular, we use SHA-256 for constructing the
hash chain and AES-GCM with 128-bit keys for encrypting the protocol messages,
as well as the protocol and application state. The state encryption key is
encrypted using the   for cryptographic
hashing and encryption. In particular, we use SHA-256 for constructing the
hash chain and AES-GCM with 128-bit keys for encrypting the protocol messages,
as well as the protocol and application state. The state encryption key is
encrypted using the   sealing function before storing persistently.
We defined two interfaces that must be implemented by the enclave application.
First, an operation processor, that receives a client operation and returns the
operation result; and second, a serialization interface that returns the
application state as a byte sequence.
The implementation does not strictly follow the Alg.&#160;  as
presented in Sec.&#160; . That is, we optimized the code in order
to eliminate the ocall when storing the application and protocol state at
server&#8217;s persistent storage. Instead, we piggyback the encrypted data
together with the reply message.
Furthermore, we implemented operation batching mechanism where the LCM
protocol receives multiple invoke messages with a single ecall. In contrast
to Alg.&#160; , the application and protocol state is stored
once per batch.
Our current proof of concept does not make use of remote attestation.
However, this can be easily extended using the mechanisms as
provided by the   SDK. 
      </para></subsection>
    <subsection title="Building applications with LCM">
      <para>
         In order to demonstrate our LCM framework we integrated LCM with a simple
persistent key-value store (KVS) running in an enclave on a remote server. The
prototype architecture is shown in Fig.&#160; . Clients and the
server communicating via TCP socket connections.
A KVS stores data objects in a flat namespace, where each object is identified
by a unique name or key. The KVS is implemented using trusted libraries
provided by the   SDK. In particular, we use  std::map  for storing
key-values pairs as strings of arbitrary length. The current version of the
  for storing
key-values pairs as strings of arbitrary length. The current version of the
  SDK does not support  std::unordered_map  which would be our first
choice due to its constant access time.  which would be our first
choice due to its constant access time. 
      </para><para>
         Clients invoke  get ,  ,  put  and   and  del  operations through the KVS
client which instantiates the LCM client-library.
A server application handles the socket communications, implements an
interface for stable storage (disk), and hosts an enclave running the server-side
LCM protocol and the KVS.
When the server application receives a client request (  operations through the KVS
client which instantiates the LCM client-library.
A server application handles the socket communications, implements an
interface for stable storage (disk), and hosts an enclave running the server-side
LCM protocol and the KVS.
When the server application receives a client request ( invoke 
message), it collects the message in a bounded queue (batch). Once the queue
has reached the limit or no more client request are available, the server
application performs an ecall and passes the collected (batched) messages to
the enclave. The LCM protocol processes them sequentially and returns the
corresponding  
message), it collects the message in a bounded queue (batch). Once the queue
has reached the limit or no more client request are available, the server
application performs an ecall and passes the collected (batched) messages to
the enclave. The LCM protocol processes them sequentially and returns the
corresponding  reply  messages for each client request and the
aggregated application and protocol state. The server application then,
writes the encrypted states to disk and forwards the   messages for each client request and the
aggregated application and protocol state. The server application then,
writes the encrypted states to disk and forwards the  reply  messages to
the clients. Note in order to achieve crash tolerance, the server application
has to write the state synchronously to disk (fsync), this clearly decreases
the performance.
Our prototype implementation of a LCM-protected key-value store comprises
about 4000 sloc, where enclave components comprise around 2200 sloc. The rest
is for the untrusted server implementation including the storage and network
code. The KVS client and the LCM client-library add additional 1600 sloc to
the prototype.  messages to
the clients. Note in order to achieve crash tolerance, the server application
has to write the state synchronously to disk (fsync), this clearly decreases
the performance.
Our prototype implementation of a LCM-protected key-value store comprises
about 4000 sloc, where enclave components comprise around 2200 sloc. The rest
is for the untrusted server implementation including the storage and network
code. The KVS client and the LCM client-library add additional 1600 sloc to
the prototype. 
      </para></subsection>
  </section>
  <section title="Evaluation">
    <para>
       We evaluated the overhead of LCM with a set of benchmarks using YCSB and
compare it against a  -secured key-value store without
rollback and forking protection. Furthermore, we compare the performance of LCM against a trusted monotonic counter approach and unprotected Redis. 
    </para><subsection title="Experiment setup">
      <para>
         The experiments use a Dell Optiplex 7040 desktop machine with an i7-6700
Intel CPU that is SGX-capable to run the server. It is equipped with 8&#160;GB of
memory, 1&#160;Gbps network connection and a SSD drive.
We simulate clients on a virtual machine with 24 virtual CPUs and 8&#160;GB of
memory, running YCSB as workload generator using Oracle Java (JRE 8, build
1.8.0_111-b14).
All machines run Ubuntu Linux 14.04.4 Server with the generic 4.4.0-47 Linux
kernel.
The evaluation is driven by  YCSB &#160; &#160; , an extensible tool for
benchmarking key-value stores. It supports many different key-value stores,
such as Redis&#160;( ),
Cassandra&#160;( ) and many more. YCSB
comes with a set of core workloads spanning different application scenarios.
For the evaluation we use workload A with a mix of 50/50  put  and
  and
 get  operations and show the overall throughput of all clients. Every
reported data point is taken over a period of 30 seconds.
We integrated the KVS client including the LCM client-library with YCSB.
As a baseline for our experiments we use our KVS (see Sec.&#160;  operations and show the overall throughput of all clients. Every
reported data point is taken over a period of 30 seconds.
We integrated the KVS client including the LCM client-library with YCSB.
As a baseline for our experiments we use our KVS (see Sec.&#160; )
protected with  .
For the comparison with Redis and our native KVS implementation we use
Stunnel&#160;( ) to secure the communication
with the clients. Redis has been originally designed for deployment in
private networks, thus, it does not support TLS connections.
LCM and the  -based KVS prototype establish a secure communication with the
clients by using AES-GCM encryption with 128 bit keys. In order to simplify
the evaluation process we use predefined encryption keys. 
      </para></subsection>
    <subsection title="Enclave memory">
      <para>
         In a preliminary experiment we evaluated the memory consumption of the  
key-value store. We inserted one million objects and measured
the enclave heap allocation using  sgx-gdb . Each object with a key size
of 40&#160;byte and&#160;100&#160;byte values.
For 300000 objects we measured an allocation of&#160;93&#160;MB enclave memory whereas
we expected only about&#160;40&#160;MB. It turned out that the KVS implementation based
on  . Each object with a key size
of 40&#160;byte and&#160;100&#160;byte values.
For 300000 objects we measured an allocation of&#160;93&#160;MB enclave memory whereas
we expected only about&#160;40&#160;MB. It turned out that the KVS implementation based
on  std::map&#161;std::string, std::string&#191;  comes with a memory overhead of
about 134%. In particular, the string key-value pairs consume about 280 byte
whereas the map data structure allocates additional&#160;48&#160;byte for each object
for maintaining an internal search structure.
Moreover, we measured the latency of   comes with a memory overhead of
about 134%. In particular, the string key-value pairs consume about 280 byte
whereas the map data structure allocates additional&#160;48&#160;byte for each object
for maintaining an internal search structure.
Moreover, we measured the latency of  put  and   and  get  operations for
different number of objects. As the EPC is limited, we expected a performance
drop when the number of objects increases and the   operations for
different number of objects. As the EPC is limited, we expected a performance
drop when the number of objects increases and the   driver starts swapping
EPC pages as also reported in&#160; . We observed
that the latency increases up to 240% when the KVS holds more than 300000
objects.
We refrain from showing the graph due to page reasons.
We assume that this hardware restriction will be addressed in future CPU releases
and thus choose our further evaluation workloads to fit into the EPC. 
      </para></subsection>
    <subsection title="LCM protocol message">
      <para>
         We first study how the LCM protocol message overhead affects the throughput.
As described in Sec.&#160; , LCM sends additional
metadata, such as the sequence number and hash chain value, along
with a client request. In particular, our LCM implementation adds 45 byte to
an operation invocation and 46 byte to a result. This overhead remains
constant for varying operation and result sizes.
In order to evaluate this, we run the experiment with 8 clients for 1000
objects of size 100 to 2500&#160;byte. Fig.&#160;  shows that
the throughput of LCM behaves similar to the plain   KVS. As expected, we
observe that LCM introduces an overhead but it decreases with bigger object
sizes. In particular, for objects with the size of 100 byte the throughput is
20.12% and for objects with size of 2500 byte it is 10.96% lower compared
to the plain   KVS. 
      </para></subsection>
    <subsection title="The throughput of LCM">
      <para>
         We also study the overall throughput of LCM by increasing the number of
clients. This workload uses up to 32 clients, 1000 objects with a fixed size
of 100 byte. Each object key is 40 byte. In this experiment we compare two
variants of LCM against a KVS without   (&#8220;Native&#8221;), SGX-secured KVS
(&#8220;SGX&#8221;), Redis, and SGX-secured KVS with emulated trusted monotonic counter
(&#8220;SGX+TMC&#8221;). We run LCM and   without batching enabled and with batching
of up to&#160;16 operations.
We configured Redis to use an append log strategy for persistence. We also
disabled fsync (synchronous disk writes) for Redis as well as for our KVS
prototypes.
As Fig.&#160;  shows, the throughput of Redis
and the Native KVS scale almost linear. In contrast, LCM and   reach
saturation already with 8 clients. We observed that the   KVS reaches 0.42x
&#8211; 0.78x the throughput of the Native KVS. LCM, on the other hand, reaches
0.67x &#8211; 0.95x the throughput of the   KVS, with batching even 0.72x &#8211; 0.98x.
The reason is, LCM and   are single threaded applications and perform the
encryption of every client request inside the enclave. Although, Redis and
Native KVS are also single threaded, they leverage Stunnel that uses multiple
processes to encrypt/decrypt client communication. That way, secure
communication becomes a bottleneck. 
      </para></subsection>
    <subsection title="The performance impact of Trusted Monotonic Counter">
      <para>
         In this experiment we investigate the performance impact of Trusted Monotonic
Counters (TMC) when used to protect against rollback and forking attacks. The
current version (Version 1.6) of the   driver and SDK do not yet support
Intel&#8217;s Trusted
Monotonic Counter&#160;  on Linux. However, on Windows&#160;  they are available provided by the Intel management engine (ME)
that stores the counter in non-volatile memory.
We measured an average latency of 60ms to increment a   TMC on Windows,
whereas&#160;  reported even higher latency of about 95ms. We
emulated the TMC on Linux by using a simple counter followed by setting the
thread to sleep for 60ms when incrementing the counter.
As Fig.&#160;  shows, the throughput remains
constant for the emulated TMC with an average of 12 operations per seconds,
wheres LCM with batching, on the other hand is 96x &#8211; 2063x faster.
However, by using trusted monotonic counters rollback
and forking attacks can be detected immediately but this comes with low
throughput. 
      </para></subsection>
    <subsection title="The costs of crash tolerance">
      <para>
         Finally, we study the performance overhead induced by synchronous disk writes
when storing the application and protocol state that is necessary to support
crash tolerance.
We performed the same experiment as in Sec.&#160;  but enabled
fsync for our KVS prototypes as well as for Redis. We expect much lower
performance compared to asynchronous writes.
Fig.&#160;  shows the throughput with different
number of clients with synchronous storing. As expected, fsync introduces high
latency when writing to disk. In particular, we observed that throughput of
Native,  , LCM, and   with TMC remain constant whereas Redis,   and
LCM with batching scale.   KVS achieves 0.98x of the Native KVS throughput
and LCM without batching achieves 0.69x of   KVS throughput. In contrast,
LCM with batching reaches 0.72x &#8211; 9.87x the throughput of the   KVS and
0.71x &#8211; 0.75x the throughput of   KVS with batching.
The experiment shows, that expensive storing operations reduce the relative
overhead introduced by   but can be reduced by leveraging batching
mechanisms. 
      </para></subsection>
  </section>
  <section title="Related Work">
    <para>
       With the advent of  , trusted computing has achieved a new level of
practicality with the aim of wide-spread deployment.
Recent publications detail how legacy applications&#160; ,
micro services&#160; , data intensive programming&#160; 
but also specific services&#160;  can be secured on top of an infrastructure where  only  the CPU needs to be trusted.
While   the CPU needs to be trusted.
While   provides special means to detect memory replay attacks against the enclave&#160; , external memory remains unprotected.
Accordingly, additional measures are necessary to prevent rollback and forking attacks mounted through external memory and secondary storage.
The latter is especially complicated if an enclave is restarted (e.g. due to crashes or system maintenance reboots).
As a pragmatic solution, the Windows   SDK&#160;  offers trusted counters that are linked non-volatile memory inside the Intel management engine (ME).
However, trusted non-volatile counters as provided by a TPM are slow, e.g. adding 35-95&#160;ms latency for each operation depending on the hardware platform, as different reports show&#160; .
Thus, in essence, all hardware-based solutions that rely on trusted counters and are consulted on every request of a secured service suffer from performance problems&#160; .
An additional issue of current trusted counter TPM-based realizations is wear out if used very frequently.
Strackx and Piessens&#160;  specifically address this problem by clever usage strategies, however the performance problems remain.
Recent work&#160;  proposes a complementary approach to LCM where
enclaves across multiple systems assist each other in order to prevent
rollback attacks. This requires multiple enclaves to interact with each other
to store and retrieve version information from the group of enclaves wheres in
LCM it is stored at the clients. 
    </para><para>
       Another line of work addresses the problem of rollback and forking attacks
without relying on trusted components.
With only a single client, the classic approach&#160;  for memory
checking uses a hash tree where the client stores the root. Many systems
build on this approach to protect remote storage services (e.g.,
Athos&#160; ).
In the multi-client model, Mazi&#232;res et al.&#160;  introduced the
notion of fork-linearizability and implemented SUNDR&#160; , which
confines rollback attacks to always present a view to each client that is
consistent with its past operations; thereby fork-linearizability makes it
much simpler to detect integrity and consistency violations on remote file
storage.
Cachin et al.&#160;  improved the efficiency of SUNDR and proved that
there is no wait-free emulation of fork-linearizable storage. That is,
sometimes clients are blocked until an operation by another client has
finished. Systems such as SPORC&#160; , FAUST&#160; , and
Venus&#160;  avoid blocking by weakening the consistency guarantees.
Others have explored aborting operations&#160;  and improved
the efficiency by reducing the computation and communication
overhead&#160; .
Mobius&#160;  uses forking properties in the context of
disconnected operations.
Previous systems have explored the guarantees of fork-linearizable for different
applications&#160;  and generic services&#160; . 
    </para><para>
       LCM combines the best features of these two technologies, trusted execution
environments and protocol-enforced consistency. It also addresses rollback
and forking attacks on TEEs as much as possible without
introducing impractical limitations into a service. 
    </para></section>
  <section title="Conclusion">
    <para>
       This work has focused on a shortcoming of trusted computing technology,
which affects current trusted execution environments (TEEs), such as Intel
SGX. In particular, the trusted execution contexts or &#8220;enclaves&#8221; are
stateless, lose their memory when a crash occurs, and need support from the
host for state continuity. But since the host is also the adversary of the
TEE according to the security model, it is actually impossible to implement
protocols that survive crashes seamlessly and prevent rollback attacks at
the same time without introducing extra
hardware. 
    </para><para>
       As a solution we have introduced Lightweight Collective Memory, a system for  detecting 
rollback and forking attacks that ensures the consistency notion of
fork-linearizable and determines when operations become stable. The LCM protocol complements TEE technology with a lightweight mechanism for
maintaining consistency information by the clients. 
rollback and forking attacks that ensures the consistency notion of
fork-linearizable and determines when operations become stable. The LCM protocol complements TEE technology with a lightweight mechanism for
maintaining consistency information by the clients. 
    </para></section>
  <section title="Acknowledgments">
    <para>
       We thank
Anil Kurmus, Cecilia Boschini, Manu Drijvers, Kai Samelin, David Barrera and Raoul Strackx
for interesting discussions and the anonymous reviewers of DSN 2017
for valuable comments.
This work has been supported in part by the European Commission through the
Horizon 2020 Framework Programme (H2020-ICT-2014-1) under grant agreements
number 644371&#160;WITDOM and 644579&#160;ESCUDO-CLOUD and in part by the Swiss State
Secretariat for Education, Research and Innovation (SERI) under contracts
number 15.0098 and 15.0087. 
    </para></section>
  <bibliography /></document>